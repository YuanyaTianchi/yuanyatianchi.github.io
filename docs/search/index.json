[{"content":"hugo  git、go环境 下载解压并配置环境变量：https://github.com/gohugoio/hugo/releases/download/v0.78.0/hugo_0.78.0_Windows-64bit.zip  #创建项目\rhugo new site yuanyatianchi.github.io\r#主题下载\rcd testblog/themes\rgit clone https://github.com/amzrk2/hugo-theme-fuji.git fuji\r#下好后将fuji\\exampleSite目录下的context、config.toml复制到项目根目录testblog替换原来的文件\r#启动\rhugo server\r 访问本地测试：http://localhost:1313\n config.toml修改  #修改baseURL为你自己的gitpage\rbaseURL = \u0026quot;https://yuanyatianchi.github.io\u0026quot;\r  部署  #打包主题，所有的静态页面默认生成到public目录，-d指定目录生成\rhugo -t fuji -d docs\r#关联远程库\rgit init\rgit remote add origin git@github.com:YuanyaTianchi/yuanyatianchi.github.io\rgit commit -a -m \u0026quot;yuanyatianchi's blog\u0026quot;\rgit push\r 远程库注意设置gitpage，设置到docs作为源\n","date":"2020-11-11","permalink":"https://yuanyatianchi.github.io/post/it.blog.hugo/","tags":["it","blog"],"title":"Hugo"},{"content":"算法 DP  dp：dp 问题的一般形式就是求最值，求解dp的核心问题是穷举。因为要求最值，肯定要把所有可行的答案穷举出来，然后在其中找最值。  重叠子问题：动态规划的穷举有点特别，因为这类问题存在「重叠子问题」，如果暴力穷举的话效率会极其低下，所以需要「备忘录」即「DP table」来优化穷举过程，避免不必要的计算 最优子结构：动态规划问题一定会具备「最优子结构」，才能通过子问题的最值得到原问题的最值。要符合「最优子结构」，子问题间必须互相独立。子问题之间不能相互影响其结果  比如考试，每门科目的成绩都是互相独立的。你的原问题是考出最高的总成绩，那么你的子问题就是要把语文考到最高，数学考到最高…… 为了每门课考到最高，你要把每门课相应的选择题分数拿到最高，填空题分数拿到最高…… 当然，最终就是你每门课都是满分，这就是最高的总成绩。 得到了正确的结果：最高的总成绩就是总分。因为这个过程符合最优子结构，“每门科目考到最高”这些子问题是互相独立，互不干扰的。 但是，如果加一个条件：你的语文成绩和数学成绩会互相制约，数学分数高，语文分数就会降低，反之亦然。这样的话，显然你能考到的最高总成绩就达不到总分了，按刚才那个思路就会得到错误的结果。因为子问题并不独立，语文数学成绩无法同时最优，所以最优子结构被破坏。   状态转移方程：虽然动态规划的核心思想就是穷举求最值，但是问题可以千变万化，穷举所有可行解其实并不是一件容易的事，只有列出正确的「状态转移方程」，才能正确地穷举。写出状态转移方程是最困难的    标准动态规划  状态转移方程定义步骤  base case：先明确基本情况，一般是问题最小时的确定结果。base case将作为状态转移方程的最小状态的结果 状态：找到「状态」，状态会不断向base case靠近，是原问题和子问题中会变化的变量，一般来说就是问题给出变量中的某个或某些。状态将作为状态转移方程的参数 选择：导致「状态」产生变化的行为  选择不一定是线性的，比如【凑零钱】中   定义dp数组/函数：综合上面3项，化为可以将父问题化为子问题的规律，按照规律写出状态转移方程式 构造路径：如果需要构造路径，只需要在过程中随dp[W][N]用完全一致的流程构造一个pathDp[W][N]即可   代码实现步骤：虽说有自顶向下递归、自底向上循环两种方式，但其步骤是基本一致的，不过标准的dp一般采用自底向上循环的方式  dp table：初始化一个 dp table。根据状态转移方程参数的范围初始化 dp table 的大小  有时在这个大小的基础上+1，可以有各种用途，比如下面【斐波那契】【凑零钱】【最大公共子串长度】中   extreme：初始化最值。一般采用一个单独的变量专门记录最值  当然也有其它方法，比如下面【凑零钱】中   base case：写出，在自顶向下递归中表现为递归结束条件，在自底向上循环中表现为 DP table 最前内容 状态转移：如下    # 初始化 base case\rdp[0][0][...] = base\r# 进行状态转移\rfor 状态1 in 状态1的所有取值：\rfor 状态2 in 状态2的所有取值：\rfor ...\rdp[状态1][状态2][...] = 求最值(选择1，选择2...)\r 斐波那契  斐波那契数列没有求最值，所以严格来说不是动态规划问题，但是可以很好的表示重叠子问题，状态转移方程也很简单，即斐波那契本身 f(n)=1,n=1,2；f(n)=f(n-1)+f(n-2),n\u0026gt;2 ，状态即斐波那契的函数参数 n。让dp初始化为n+1长度，单纯就是让下标跟n对应，方便写代码  自顶向下递归\nfunc fib(n int) {\rif (n \u0026lt; 1) {\rreturn 0\r}\r// 备忘录初始化\rmemo := make([]int, n+1)\rreturn helper(meno, n)\r}\rfunc helper(memo []int, n int) int {\r// base case\rif n == 1 || n == 2 {\rreturn 1\r}\r// 先从备忘录获取，存在子问题结果就直接返回\rif memo[n] != 0 {\rreturn memo[n]\r}\r// 状态转移方程。记录结果到备忘录，再返回\rmemo[n] = helper(memo, n-1) + helper(memo, n-2)\rreturn memo[n]\r}\r 自底向上循环\nfunc fib(n int) int {\r// 状态范围锁定\rif n \u0026lt; 0 {\rreturn 0\r}\r// dp table\rdp := make([]int, n+1)\r// base case\rdp[1], dp[2] = 1, 1\r// 自底向上循环\rfor i := 3; i \u0026lt;= n; i++ {\r// 状态转移方程\rdp[i] = dp[i-1] + dp[i-2]\r}\rreturn dp[n]\r}\r 状态压缩：这里空间复杂度从 o(n) 压缩到了 o(1)。一般来说是把一个二维的 DP table 压缩成一维，即 o(n^2) 压缩到 o(n)\nfunc fib(n int) (res int) {\r// 状态范围锁定\rif n \u0026lt; 0 {\rreturn 0\r}\r// 只需要用2个变量记录上一次结果和上上次结果即可\rpre, prePre = 1, 1\r// 自底向上循环\rfor i := 3; i \u0026lt;= n; i++ {\r// 状态转移方程\rres = pre + prePre\r// 更新记录\rprePre, pre = pre, res\r}\rreturn res\r}\r 凑零钱   标准的动态规划问题，并且选择不是线性的\n  有 k 种面值的硬币，面值分别为 c1, c2 ... ck，每种硬币的数量无限，再给一个总金额 amount，问你最少需要几枚硬币凑出这个金额，如果不可能凑出，算法返回 -1 。比如说 k = 3，面值分别为 1，2，5，总金额 amount = 11。那么最少需要 3 枚硬币凑出，即 11 = 5 + 5 + 1。算法的函数签名如下\n  // coins 中是可选硬币面值，amount 是目标金额\rint coinChange(int[] coins, int amount);\r   这个问题符合最优子结构：比如你想求 amount = 11 时的最少硬币数（原问题），如果你知道凑出 amount = 10 的最少硬币数（子问题），你只需要把子问题的答案加一（再选一枚面值为 1 的硬币）就是原问题的答案。因为硬币的数量是没有限制的，所以子问题之间没有相互制，是互相独立的。（如果存在硬币数量限制，好像就可以使用贪心的思想解题了？）\n  状态转移方程\n base case：目标金额 amount 为 0 时算法返回 0 状态：由于硬币数量无限，硬币的面额也是题目给定的，只有目标金额会不断地向 base case 靠近，所以唯一的「状态」就是目标金额 amount。 选择：目标金额为什么变化呢，因为你在选择硬币，你每选择一枚硬币，就相当于减少了目标金额。所以说所有硬币的面值，就是你的「选择」 dp定义：dp[n]=-1,n\u0026lt;0；dp[n]=0,n=0；dp[n]=min(res, dp[n-coin in coins]),n\u0026gt;0，res 是问题迭代过程中记录最小值的变量    # 伪码框架\rdef coinChange(coins: List[int], amount: int):\r# 定义：要凑出金额 n，至少要 dp(n) 个硬币\rdef dp(n):\r# 做选择，选择需要硬币最少的那个结果\rfor coin in coins:\rres = min(res, 1 + dp(n - coin))\rreturn res\r# 题目要求的最终结果是 dp(amount)\rreturn dp(amount)\r 无备忘录穷举\ndef coinChange(coins: List[int], amount: int):\rdef dp(n):\r# base case\rif n == 0: return 0\rif n \u0026lt; 0: return -1\r# 求最小值，所以初始化为正无穷\rres = float('INF')\rfor coin in coins:\rsubproblem = dp(n - coin)\r# 子问题无解，跳过\rif subproblem == -1: continue\rres = min(res, 1 + subproblem)\rreturn res if res != float('INF') else -1\rreturn dp(amount)\r 带备忘录递归\ndef coinChange(coins: List[int], amount: int):\r# 备忘录\rmemo = dict()\rdef dp(n):\r# 查备忘录，避免重复计算\rif n in memo: return memo[n]\r# base case\rif n == 0: return 0\rif n \u0026lt; 0: return -1\rres = float('INF')\rfor coin in coins:\rsubproblem = dp(n - coin)\rif subproblem == -1: continue\rres = min(res, 1 + subproblem)\r# 记入备忘录\rmemo[n] = res if res != float('INF') else -1\rreturn memo[n]\rreturn dp(amount)\r 带备忘录循环： dp 数组初始化大小和值为 amount + 1 ，是因为凑成 amount 金额的硬币数最多只可能等于 amount（全用 1 元面值的硬币），所以初始化为 amount + 1 就相当于初始化为正无穷，便于后续取最小值，这也是可以这样写dp[i] = min(dp[i], 1 + dp[i - coin])的原因。\nint coinChange(vector\u0026lt;int\u0026gt;\u0026amp; coins, int amount) {\r// 数组大小为 amount + 1，初始值也为 amount + 1\rvector\u0026lt;int\u0026gt; dp(amount + 1, amount + 1);\r// base case\rdp[0] = 0;\r// 外层 for 循环在遍历所有状态的所有取值\rfor (int i = 0; i \u0026lt; dp.size(); i++) {\r// 内层 for 循环在求所有选择的最小值\rfor (int coin : coins) {\r// 子问题无解，跳过\rif (i - coin \u0026lt; 0) continue;\rdp[i] = min(dp[i], 1 + dp[i - coin]); //\r}\r}\rreturn (dp[amount] == amount + 1) ? -1 : dp[amount];\r}\r 背包 01背包  问题：有n个重量和价值分别为wi，vi的物品，从这些物品中挑选出总重量不超过W的物品，求所有挑选方案中价值总和的最大值。因为对每个物品只有选和不选两种情况，所以这个问题称为01背包  限制：1≤n≤100。1\u0026lt;wi，vi≤100。1≤W≤10000 输入：n=4，(w,v)={(2,3),(1,2),(3,4),(2,2)}，W=5。4个物品，重量和价值分别如(w,v)描述，总重W描述 输出：7(选择第0、1、3号物品)   状态：总重量W、物品N 选择：选或不选第N件物品 状态转移方程：dp[W][N]表示，总重量不超过W，在前N件物品中进行选择，能选到的最大价值  dp[W][N] = 0, W==0 || N==0：base case dp[W][N] = max(dp[W][N-1], dp[W-ws[N-1]][N-1]+vs[N-1]), W\u0026gt;0 \u0026amp;\u0026amp; N\u0026gt;0：如果不选择第N件物品dp[W][N]可以转化为dp[W][N-1]问题，如果选择第N件物品dp[W][N]可以转化为dp[W-ws[N-1]][N-1]+vs[N-1]问题，择其大即可得到dp[W][N]的结果    // 循环\rfunc OneZeroPackage(W, N int, ws, vs []int) [][]int {\r// 默认所有值都是0，base case已就位\rdp := make([][]int, W+1)\rfor i := 0; i \u0026lt; len(dp); i++ {\rdp[i] = make([]int, N+1)\r}\rfor i := 1; i \u0026lt;= W; i++ {\rfor j := 1; j \u0026lt;= N; j++ {\r// 重量不足时只能不选\rif i \u0026lt; ws[j-1] {\rdp[i][j] = dp[i][j-1]\rcontinue\r}\ra := dp[i][j-1]\rb := dp[i-ws[j-1]][j-1] + vs[j-1]\r// 重量足够谁大选谁\rif a \u0026gt; b {\rdp[i][j] = a\r} else {\rdp[i][j] = b\r}\r}\r}\rreturn dp\r}\r // 递归\rfunc OneZeroPackage(W, N int, ws, vs []int) [][]int {\rvar trance []int\r// 默认所有值都是0，base case已就位\rdp := make([][]int, W+1)\rfor i := 0; i \u0026lt; len(dp); i++ {\rdp[i] = make([]int, N+1)\r}\rfor i := 1; i \u0026lt;= W; i++ {\rrecursion(i, N, ws, vs, \u0026amp;dp, trance)\r}\rreturn dp\r}\rfunc recursion(W, N int, ws, vs []int, dp *[][]int) (res int) {\rif (*dp)[W][N] != 0 {\rreturn (*dp)[W][N]\r}\rif W == 0 || N == 0 {\rreturn 0\r}\rif W \u0026lt; ws[N-1] {\rres = recursion(W, N-1, ws, vs, dp)\r} else {\ra := recursion(W, N-1, ws, vs, dp)\rb := recursion(W-ws[N-1], N-1, ws, vs, dp) + vs[N-1]\rif a \u0026gt; b {\rres = a\r} else {\rres = b\r}\r}\r(*dp)[W][N] = res\rreturn (*dp)[W][N]\r}\r 路径追踪  只需要在过程中随dp[W][N]用完全一致的流程构造一个pathDp[W][N]即可。这里用[]int类型元素作为路径  func OneZeroPackage(W, N int, ws, vs []int) ([][]int, [][][]int) {\r// 默认所有值都是0，base case已就位\rdp := make([][]int, W+1)\rfor i := 0; i \u0026lt; len(dp); i++ {\rdp[i] = make([]int, N+1)\r}\rpathDp := make([][][]int, W+1)\rfor i := 0; i \u0026lt; len(pathDp); i++ {\rpathDp[i] = make([][]int, N+1)\r}\rfor i := 1; i \u0026lt;= W; i++ {\rfor j := 1; j \u0026lt;= N; j++ {\r// 重量不足时只能不选\rif i \u0026lt; ws[j-1] {\rpathDp[i][j] = pathDp[i][j-1]\rdp[i][j] = dp[i][j-1]\rcontinue\r}\ra := dp[i][j-1]\rb := dp[i-ws[j-1]][j-1] + vs[j-1]\r// 重量足够谁大选谁\rif a \u0026gt; b {\rpathDp[i][j] = pathDp[i][j-1]\rdp[i][j] = a\r} else {\rpathDp[i][j] = append(pathDp[i-ws[j-1]][j-1], j)\rdp[i][j] = b\r}\r}\r}\rreturn dp, pathDp\r}\r 贪心 区间调度问题  给出很多形如 [start, end] 的闭区间，设计一个算法，算出这些区间中最多有几个互不相交的区间  intvs = [[1,3], [2,4], [3,6]]，这些区间最多有 2 个区间互不相交，即 [[1,3], [3,6]]，算法应该返回 2 注意边界相同并不算相交 这个问题在生活中的应用广泛，比如你今天有好几个活动，每个活动都可以用区间 [start, end] 表示开始和结束的时间，请问你今天**最多能参加几个活动呢？**显然你一个人不能同时参加两个活动，所以说这个问题就是求这些时间区间的最大不相交子集。   思路其实很简单，可以分为以下三步：  从区间集合 intvs 中选择一个区间 x，这个 x 是在当前所有区间中结束最早的（end 最小）。 把所有与 x 区间相交的区间从区间集合 intvs 中删除。 重复步骤 1 和 2，直到 intvs 为空为止。之前选出的那些 x 就是最大不相交子集。    原来的 分治：原问题的解即子问题的解的合并，树式\n贪心：遵循当前最优，适合链式\n动态规划：局部最优解来推导全局最优解，树式\n经典动态规划  动态规划和贪心算法都是一种递推算法，均用局部最优解来推导全局最优解，用于子问题之间不完全独立的情况，是对遍历解空间（暴力遍历）的一种优化，当问题具有最优子结构时，可用动规，把问题结构解析成树 贪心，考虑的不是局部最优，而是当前最优，当前最优不一定是全局最优，所以可能要考虑扩大范围，取更大一个范围的局部当前最优。把问题（数据）结构解析成链表 动态规划（dp，dynamic programming）：代表了这一类问题（最优子结构、重叠子问题、子问题最优性（贪心））的—般解法，是设计方法或者策略，不是具体算法 本质是递推，核心是找到状态转移的方式，写出dp方程 形式:  记忆型递归（从大往小拆） 递推（从小往大推）：**通过逐步扩大（即动态）问题条件的范围（即规划）**来逐步挑选最优解，大范围条件的解是小范围条件的解集合中的最优解   举例:  01背包问题 钢条切割问题 数字三角形问题（滚动数组) 最长公共子序列问题 完全背包问题 最长上升子序列问题   重叠（交叉）子问题  发现：当用一个传统的dfs或者递归求导一个问题最优解的时候，发现递归树展开时，不同节点问题上有重叠的解。比如斐波那契 f(n)=f(n-1)+f(n-2)，又f(n-1)=f(n-2)+f(n-3)，所以f(n-2)就是一个重叠，即解决f(n-1)的时候已经解决了f(n-2)的这一子问题，所以如果斐波那契能把哪些计算过的值都利用起来，将变成一个O(n)的问题，而不是O(!n) 记忆型递归：即计算过的值都用一个map映射存储，空间换时间 递推 当然Fibonacci不能算是一个恰当的动态规划例子，因为它太特殊了，是一个子问题完全重叠的特例    01背包  有n个重量和价值分别为wi，vi的物品，从这些物品中挑选出总重量不超过W的物品，求所有挑选方案中价值总和的最大值。因为对每个物品只有选和不选两种情况，所以这个问题称为01背包  限制：1≤n≤100。1\u0026lt;wi，vi≤100。1≤W≤10000 输入：n=4，(w,v)={(2,3),(1,2),(3,4),(2,2)}，W=5。4个物品，重量和价值分别如(w,v)描述，总重W描述 输出：7(选择第0、1、3号物品)    贪心策略  贪心是动态规划的一个特例 遵循某种规则，不断(贪心地)选取当前最优策略，最终找到最优解 难点：当前最优未必是整体最优  硬币问题 有1、5、10、50、100、500元的硬币各c1、c5、c10、c50、c100、c500（输入变量）枚。现在要用这些硬币来支付A元，最少需要多\n少枚硬币？假定本题至少存在一种支付方案00\n0\u0026lt;ci\u0026lt;10^9，0\u0026lt;A\u0026lt;10^9\n输入：第一行有六个数字，分别代表从小到大6种面值的硬币的个数；第二行为A，代表需支付的A元\n样例\n输入\n3 2 1 3 0 2\n620\n输出\n6\n分析：如果通过遍历解空间去计算用多少张500，是 i=1、2、3、4\u0026hellip;去试 i * 500\u0026lt;=A时各种情况，而贪心只考虑眼前最优，直接取最大值A/500，然后继续用剩下的去考虑同样策略最优\nvar (\rcoinValues = []int{1, 5, 10, 50, 100, 500} //硬币面值\rcoinCounts = [6]int{3, 2, 1, 3, 0, 2} //各面值硬币个数，索引与硬币面值对应。来自输入\r)\rfunc main() {\rfmt.Println(f(620, 5))\r}\rfunc f(a int, i int) int {\rif a \u0026lt;= 0 {\rreturn 0\r}\rif i == 0 {\rreturn a\r}\rcoinValue := coinValues[i] //当前硬币面值\rn := a / coinValue //需要多少个当前coin\rif n \u0026gt; coinCounts[i] { //是否有足够的硬币数\rn = coinCounts[i]\r}\rn += f(a-coinValue*n, i-1) //用剩下的面值继续处理\rreturn n\r}\r 快速渡河 POJ-1700，北大POJ一个题\nN个人渡船，1艘船，一次只能载2人，但是得有1人再把船划回来，每个人划船速度不同，每2人划船速度由慢的那个人决定，需要一种策略使最短时间内所有人都渡河\n输入：\n1\n人数：4\n每个人渡河时间：1 2 5 10\n输出：\n最短渡河总时间17\n 分析：可以发现，不同的思路，在不同的情况下也不一定某一种思路更优  一般思路：为了回程更快，始终让最快的带其它所有人。  组合为：去1、2，回1，去1、5，回1，去1、10，总时间为：2+1+5+1+10=19 如果是1、5、6、7的一组人，则总时间为5+1+6+1+7=20 如果是a、b、c、d（小大顺序）的一组人，总时间为2a+b+c+d   另辟蹊径：因为速度由慢的人决定，让最慢的两人一起渡河，让最快的两个其一回程  组合为：去1、2，回1，去5、10，回2，去1、2。总时间为2+1+10+2+2=17 如果是1、5、6、7的一组人，总时间为：5+1+7+5+5=23 如果是a、b、c、d（小大顺序）的一组人，总时间为a+3b+d   比较：2a+b+c+d、a+3b+d，抵消相同部分即比较a+c、b，根据两者大小选择不同方案 扩展n人：假设n个人渡河时间按顺序放在一个数组p中，  一般思路：很简单，每次返回都是最快的p[0]  虽然是每来回1次就已经成为一个循环，一个来回时间为p[0]+p[n] 但是为了与下面的思路比较时间，以2次来回计算1次时间进行比较即可，时间为2*p[0]+p[n-1]+p[n]   始终保持让最慢的两人一起渡河，让最快的两个其一回程，其实n个人与4个人是一样的流程：  初始状态是：有n个人都在左边未渡河 去p[0]、p[1]，回p[0]，去p[n]、p[n-1]，回p[1]，使用时间p[0]+2*p[1]+p[n] 目前状态是，p[0]-p[n-2]都在左边未渡河，可以发现，其实经过这么一轮循环，结果是渡河了2个最慢的人然后我们又回到了n-2个人渡河的状态，以同样的方式迭代即可，每次减少2个最慢的人   2*p[0]+p[n-1]+p[n]、p[0]+2*p[1]+p[n]比较，即p[0]+p[n-1]、2*p[1] 每2个人渡河进行比较选取其中当前的最优思路即可，完成贪心      var (\rtimes = []int{} //每个人渡河时间\r)\rfunc main() {\rfmt.Println(f(times))\r}\rfunc f(times []int) int {\rleft := len(times) //未渡河人数\rtimeSum := 0 //渡河总时间\rfor left \u0026gt; 0 {\rif left == 1 {\rtimeSum += times[0]\r} else if left == 2 {\rtimeSum += times[1]\r} else if left == 3 {\rtimeSum += times[0] + times[1] + times[2]\r} else {\r/*始终由时间最短的人回程*/\rtime1 := 2*times[0] + times[left-1] + times[left-2] //2*p[0]+p[n-1]+p[n]\rtime2 := times[0] + 2*times[1] + times[left-1] //p[0]+2*p[1]+p[n]\rif time1 \u0026lt; time2 {\rtimeSum += time1\r} else {\rtimeSum += time2\r}\r}\r}\rreturn timeSum\r}\r 区间问题 区间调度 有n项工作，每项工作分别在 si 时间开始,在 ti 时间结束. 对于每项工作，你都可以选择参与与否.如果选择了参与，那么自始至终都必须全程参与。 此外，参与工作的时间段不能重复(即使是开始的瞬间和结束的瞬间的重叠也是不允许的)， 你的目标是参与尽可能多的工作， 那么最多能参与多少项工作呢？\n思路：总是选结束时间最早的工作\n代码思路：因为工作不一定是有序的，用一个job结构体封装job的开始时间和结束时间，实现比较方法，使其可以通过结束时间排序，排好序后按总是选结束时间最早的工作这个思路来即可\n区间选点 POJ1201\n有一些区间，问最少的点命中所有区间\n思路：也是按区间结束值进行排序，总是选区间结束值，下一次再选择开始值大于该结束值且结束值最近的点即可\n加条件：除了开始时间、结束时间，每个区间追加区间命中数这个属性，使每个区间必须被指定数量的点命中\n最大公共子串长度 // `dp[i][j]= (a[i]==b[j]?dp[i-1][j-1]+1:0)`\r// 这样是正确的，但是当i=0或者j=0时，dp会下标越界，这种时候一般让i、j使用的时候都比原来+1即可，即dp[1][1]来存a[0]和b[0]的结果即可\r// 就变成dp[i][j]= (a[i-1]==b[j-1]?dp[i-1][j-1]+1:)，到代码中体现就是让dp的长宽+1，i、j从1开始遍历，a、b串取元素的时候用i-1、j-1即可\r// dp[0][0]本来初始就是0不影响长度加算，所以就是让dp[0][0]作为基本情况，如果在其它问题中有初始值也可以用最尾或者最末来初始化一个值这样用\rfunc f(a, b string) (res int) {\rvar (\raLen, bLen = len(a)+1, len(b)+1\rdp = make([][]int, aLen)\r)\rfor i:=0; i\u0026lt;aLen; i++{\rdp[i] = make([]int, bLen)\r}\rfor i:=1; i\u0026lt;aLen; i++ {\rfor j:=1; j\u0026lt;bLen; j++ {\rif a[i-1]==b[j-1] {\rdp[i][j] = dp[i-1][j-1]+1\rif dp[i][j]\u0026gt;res{\rres = dp[i][j]\r}\r}\r}\r}\rreturn\r}\r DFS   DFS：深度优先遍历，也叫回溯算法。不像动态规划存在重叠子问题可以优化，回溯算法就是纯暴力穷举，复杂度一般都很高。某种程度上来看，动态规划的暴力求解阶段就是回溯算法\n  过程：解决一个回溯问题，实际上就是一个决策树的遍历过程\n 路径：记录已经做过的选择 选择列表：当前节点可以做的选择。 结束条件：到达决策树底层，无法再做选择的条件。    代码实现步骤：其核心是 for 循环里面的递归，在递归调用之前「做选择」，在递归调用之后「撤销选择」\n 参数：将「选择列表」和「路径」作为递归函数的参数 结束条件：也表现为递归的结束条件，到达决策树底层，将路径添加到结果集 循环递归选择：核心就是 for 循环中，在递归调用之前「做选择」，在递归调用之后「撤销选择」  进入循环时，如果「当前选择」已经在「路径」中了（可以用单独写一个方法遍历判断），表示已经选择过了，直接continue跳过即可。比如【全排列】【N皇后】中 或者也可以，「做选择」时，先将该选择从选择列表移除，然后将选择添加到路径；「撤销选择」时，先从路径中移除选择，再将该选择再加入选择列表 当然还有有更好的方法，通过交换元素达到目的      result = []\rdef backtrack(路径, 选择列表):\rif 满足结束条件，到达决策树底层，无法再做选择的条件:\rresult.add(路径)\rreturn\rfor 选择 in 选择列表:\rif 路径 包含 选择:\rcontinue\r路径.add(选择)\rbacktrack(路径, 选择列表)\r路径.remove(选择)\r 全排列  列出n 个不重复的数的全排列 比方说给三个数 [1,2,3]，你肯定不会无规律地乱穷举，一般是这样：先固定第一位为 1，然后第二位可以是 2，那么第三位只能是 3；然后可以把第二位变成 3，第三位就只能是 2 了；然后就只能变化第一位，变成 2，然后再穷举后两位……，就形成了一个决策树，为啥说这是决策树呢，因为你在每个节点上其实都在做决策，定义的 backtrack 函数其实就像一个指针，在这棵树上游走，同时要正确维护每个节点的属性，每当走到树的底层，其「路径」就是一个全排列。  路径：记录在 track 中 选择列表：nums 中不存在于 track 的那些元素 结束条件：nums 中的元素全都在 track 中出现    List\u0026lt;List\u0026lt;Integer\u0026gt;\u0026gt; res = new LinkedList\u0026lt;\u0026gt;();\r/* 主函数，输入一组不重复的数字，返回它们的全排列 */\rList\u0026lt;List\u0026lt;Integer\u0026gt;\u0026gt; permute(int[] nums) {\r// 记录「路径」\rLinkedList\u0026lt;Integer\u0026gt; track = new LinkedList\u0026lt;\u0026gt;();\rbacktrack(nums, track);\rreturn res;\r}\rvoid backtrack(int[] nums, LinkedList\u0026lt;Integer\u0026gt; track) {\r// 触发结束条件\rif (track.size() == nums.length) {\rres.add(new LinkedList(track));\rreturn;\r}\rfor (int i = 0; i \u0026lt; nums.length; i++) {\r// 排除不合法的选择，这里稍微做了些变通，没有显式记录「选择列表」，而是通过 nums 和 track 推导出当前的选择列表，所以后面不需要从选择列表删除元素和添加回来\rif (track.contains(nums[i]))\rcontinue;\r// 做选择\rtrack.add(nums[i]);\r// 进入下一层决策树\rbacktrack(nums, track);\r// 取消选择\rtrack.removeLast();\r}\r}\r N皇后  N×N 的棋盘防止N个皇后，使得它们不能互相攻击，问有多少种放法。皇后可以攻击 同一行、同一列、左上、左下、右上、右下 8个方向任意距离的单位 本质上跟全排列问题差不多，决策树的每一层表示棋盘上的每一行；每个节点可以做出的选择是，在该行的任意一列放置一个皇后  路径：board 中小于 row 的那些行都已经成功放置了皇后 选择列表：第 row 行的所有列都是放置皇后的选择 结束条件：row 超过 board 的最后一行   最坏时间复杂度仍然是 O(N^(N+1))，而且无法优化  vector\u0026lt;vector\u0026lt;string\u0026gt;\u0026gt; res;\r/* 输入棋盘边长 n，返回所有合法的放置 */\rvector\u0026lt;vector\u0026lt;string\u0026gt;\u0026gt; solveNQueens(int n) {\r// '.' 表示空，'Q' 表示皇后，初始化空棋盘。\rvector\u0026lt;string\u0026gt; board(n, string(n, '.'));\rbacktrack(board, 0);\rreturn res;\r}\rvoid backtrack(vector\u0026lt;string\u0026gt;\u0026amp; board, int row) {\r// 触发结束条件\rif (row == board.size()) {\rres.push_back(board);\rreturn;\r}\rint n = board[row].size();\rfor (int col = 0; col \u0026lt; n; col++) {\r// 排除不合法选择\rif (!isValid(board, row, col)) continue;\r// 做选择\rboard[row][col] = 'Q';\r// 进入下一行决策\rbacktrack(board, row + 1);\r// 撤销选择\rboard[row][col] = '.';\r}\r}\r/* 是否可以在 board[row][col] 放置皇后？ */\rbool isValid(vector\u0026lt;string\u0026gt;\u0026amp; board, int row, int col) {\rint n = board.size();\r// 检查列是否有皇后互相冲突\rfor (int i = 0; i \u0026lt; n; i++) {\rif (board[i][col] == 'Q')\rreturn false;\r}\r// 检查右上方是否有皇后互相冲突\rfor (int i = row - 1, j = col + 1; i \u0026gt;= 0 \u0026amp;\u0026amp; j \u0026lt; n; i--, j++) {\rif (board[i][j] == 'Q')\rreturn false;\r}\r// 检查左上方是否有皇后互相冲突\rfor (int i = row - 1, j = col - 1;\ri \u0026gt;= 0 \u0026amp;\u0026amp; j \u0026gt;= 0; i--, j--) {\rif (board[i][j] == 'Q')\rreturn false;\r}\rreturn true;\r}\r  有的时候，我们并不想得到所有合法的答案，只想要一个答案。比如解数独的算法，找所有解法复杂度太高，只要找到一种解法就可以。只要稍微修改一下回溯算法的代码  // 函数找到一个答案后就返回 true\rbool backtrack(vector\u0026lt;string\u0026gt;\u0026amp; board, int row) {\r// 触发结束条件\rif (row == board.size()) {\rres.push_back(board);\rreturn true;\r}\r...\rfor (int col = 0; col \u0026lt; n; col++) {\r...\rboard[row][col] = 'Q';\rif (backtrack(board, row + 1))\rreturn true;\rboard[row][col] = '.';\r}\rreturn false;\r}\r BFS   常见场景：问题的本质就是让你在一幅「图」中找到从起点 start 到终点 target 的最近距离，这个例子听起来很枯燥，但是 BFS 算法问题其实都是在干这个事儿\n 广义的描述可以有各种变体，比如走迷宫，有的格子是围墙不能走，从起点到终点的最短距离是多少？如果这个迷宫带「传送门」可以瞬间传送呢？ 比如说两个单词，要求你通过某些替换，把其中一个变成另一个，每次只能替换一个字符，最少要替换几次？ 比如说连连看游戏，两个方块消除的条件不仅仅是图案相同，还得保证两个方块之间的最短连线不能多于两个拐点。你玩连连看，点击两个坐标，游戏是如何判断它俩的最短连线有几个拐点的？    BFS 找到的路径一定是最短的，但代价就是空间复杂度比 DFS 大很多\n BFS 可以找到最短距离，但是空间复杂度高。处理二叉树问题的例子，假设给你的这个二叉树是满二叉树，节点数为 N，对于 DFS 算法来说，空间复杂度无非就是递归堆栈，最坏情况下顶多就是树的高度，也就是 O(logN)， DFS 不能找最短路径吗？其实也是可以的，但是时间复杂度相对高很多。你想啊，DFS 实际上是靠递归的堆栈记录走过的路径，你要找到最短路径，肯定得把二叉树中所有树杈都探索完才能对比出最短的路径有多长    过程：就是在一个图中从一个start节点开始扩散，直到找到target节点，通过一个队列存储下一步要扩散的节点，通过一个集合存储所有已经走过的点，以避免走回头路\n 要扩散的节点：一般用一个队列存储，也可以是链表，或者其它的方便增删元素的结构 已扩散的节点：    代码实现步骤：\n 声明队列q、集合visited、扩散步数step，将start节点加入 q 和 visited，因为直到start就要扩散了，就直接加入到已扩散的节点无妨，当然改变一下位置在循环中再加入visited也无妨 在q.len=0之前一直循环：  遍历q：  移除元素，表示已扩散 判断是否到是target节点，到达就返回step 遍历元素的相邻节点  如果不在已扩散的节点中，则将该相邻节点加入          // 计算从起点 start 到终点 target 的最近距离\rint BFS(Node start, Node target) {\rQueue\u0026lt;Node\u0026gt; q; // 核心数据结构\rSet\u0026lt;Node\u0026gt; visited; // 避免走回头路，即避免再次走到走过的节点\rq.offer(start); // 将起点加入队列\rvisited.add(start);\rint step = 0; // 记录扩散的步数\rwhile (q not empty) {\rint sz = q.size();\r/* 将当前队列中的所有节点向四周扩散 */\rfor (int i = 0; i \u0026lt; sz; i++) {\rNode cur = q.poll();\r/* 划重点：这里判断是否到达终点 */\rif (cur is target)\rreturn step;\r/* 将 cur 的相邻节点加入队列。cur.adj() 泛指 cur 相邻的节点 */\rfor (Node x : cur.adj())\rif (x not in visited) {\rq.offer(x);\rvisited.add(x);\r}\r}\r/* 划重点：更新步数在这里 */\rstep++;\r}\r}\r 二叉树的最小深度 https://leetcode-cn.com/problems/minimum-depth-of-binary-tree/\n 显然起点就是 root 根节点，终点就是最靠近根节点的那个「叶子节点」嘛，叶子节点就是两个子节点都是 null 的节点  int minDepth(TreeNode root) {\rif (root == null) return 0;\rQueue\u0026lt;TreeNode\u0026gt; q = new LinkedList\u0026lt;\u0026gt;();\rq.offer(root);\r// root 本身就是一层，depth 初始化为 1\rint depth = 1;\rwhile (!q.isEmpty()) {\rint sz = q.size();\r/* 将当前队列中的所有节点向四周扩散 */\rfor (int i = 0; i \u0026lt; sz; i++) {\rTreeNode cur = q.poll();\r/* 判断是否到达终点 */\rif (cur.left == null \u0026amp;\u0026amp; cur.right == null) return depth;\r/* 将 cur 的相邻节点加入队列 */\rif (cur.left != null)\rq.offer(cur.left);\rif (cur.right != null) q.offer(cur.right);\r}\r/* 这里增加步数 */\rdepth++;\r}\rreturn depth;\r}\r 打开转盘锁 https://leetcode-cn.com/problems/open-the-lock/\nint openLock(String[] deadends, String target) {\r// 记录需要跳过的死亡密码\rSet\u0026lt;String\u0026gt; deads = new HashSet\u0026lt;\u0026gt;();\rfor (String s : deadends) deads.add(s);\r// 记录已经穷举过的密码，防止走回头路\rSet\u0026lt;String\u0026gt; visited = new HashSet\u0026lt;\u0026gt;();\rQueue\u0026lt;String\u0026gt; q = new LinkedList\u0026lt;\u0026gt;();\r// 从起点开始启动广度优先搜索\rint step = 0;\rq.offer(\u0026quot;0000\u0026quot;);\rvisited.add(\u0026quot;0000\u0026quot;);\rwhile (!q.isEmpty()) {\rint sz = q.size();\r/* 将当前队列中的所有节点向周围扩散 */\rfor (int i = 0; i \u0026lt; sz; i++) {\rString cur = q.poll();\r/* 判断是否到达终点 */\rif (deads.contains(cur))\rcontinue;\rif (cur.equals(target))\rreturn step;\r/* 将一个节点的未遍历相邻节点加入队列 */\rfor (int j = 0; j \u0026lt; 4; j++) {\rString up = plusOne(cur, j);\rif (!visited.contains(up)) {\rq.offer(up);\rvisited.add(up);\r}\rString down = minusOne(cur, j);\rif (!visited.contains(down)) {\rq.offer(down);\rvisited.add(down);\r}\r}\r}\r/* 在这里增加步数 */\rstep++;\r}\r// 如果穷举完都没找到目标密码，那就是找不到了\rreturn -1;\r}\r 双向 BFS 优化  无论传统 BFS 还是双向 BFS，无论做不做优化，从 Big O 衡量标准来看，时间复杂度都是一样的，只能说双向 BFS 是一种 trick，算法运行的速度会相对快一点 传统的 BFS 框架就是从起点开始向四周扩散，遇到终点时停止；而双向 BFS 则是从起点和终点同时开始扩散，当两边有交集的时候停止。 双向 BFS 还是遵循 BFS 算法框架的，只是不再使用队列，而是使用 HashSet 方便快速判断两个集合是否有交集。另外的一个技巧点就是 while 循环的最后交换 q1 和 q2 的内容，所以只要默认扩散 q1 就相当于轮流扩散 q1 和 q2。 不过，双向 BFS 也有局限，因为你必须知道终点在哪里。二叉树最小高度的问题，你一开始根本就不知道终点在哪里，也就无法使用双向 BFS；但是第二个密码锁的问题，是可以使用双向 BFS 算法来提高效率的  int openLock(String[] deadends, String target) {\rSet\u0026lt;String\u0026gt; deads = new HashSet\u0026lt;\u0026gt;();\rfor (String s : deadends) deads.add(s);\r// 用集合不用队列，可以快速判断元素是否存在\rSet\u0026lt;String\u0026gt; q1 = new HashSet\u0026lt;\u0026gt;();\rSet\u0026lt;String\u0026gt; q2 = new HashSet\u0026lt;\u0026gt;();\rSet\u0026lt;String\u0026gt; visited = new HashSet\u0026lt;\u0026gt;();\rint step = 0;\rq1.add(\u0026quot;0000\u0026quot;);\rq2.add(target);\rwhile (!q1.isEmpty() \u0026amp;\u0026amp; !q2.isEmpty()) {\r// 哈希集合在遍历的过程中不能修改，用 temp 存储扩散结果\rSet\u0026lt;String\u0026gt; temp = new HashSet\u0026lt;\u0026gt;();\r/* 将 q1 中的所有节点向周围扩散 */\rfor (String cur : q1) {\r/* 判断是否到达终点 */\rif (deads.contains(cur))\rcontinue;\rif (q2.contains(cur))\rreturn step;\rvisited.add(cur);\r/* 将一个节点的未遍历相邻节点加入集合 */\rfor (int j = 0; j \u0026lt; 4; j++) {\rString up = plusOne(cur, j);\rif (!visited.contains(up))\rtemp.add(up);\rString down = minusOne(cur, j);\rif (!visited.contains(down))\rtemp.add(down);\r}\r}\r/* 在这里增加步数 */\rstep++;\r// temp 相当于 q1\r// 这里交换 q1 q2，下一轮 while 就是扩散 q2\rq1 = q2;\rq2 = temp;\r}\rreturn -1;\r}\r  双向 BFS 还有一个优化，就是在 while 循环开始时做一个判断：因为按照 BFS 的逻辑，队列（集合）中的元素越多，扩散之后新的队列（集合）中的元素就越多；在双向 BFS 算法中，如果我们每次都选择一个较小的集合进行扩散，那么占用的空间增长速度就会慢一些，效率就会高一些  // ...\rwhile (!q1.isEmpty() \u0026amp;\u0026amp; !q2.isEmpty()) {\rif (q1.size() \u0026gt; q2.size()) {\r// 交换 q1 和 q2\rtemp = q1;\rq1 = q2;\rq2 = temp;\r}\r// ...\r ","date":"0001-01-01","permalink":"https://yuanyatianchi.github.io/post/it.algorithm/","tags":["it","Algorithm"],"title":"Algorithm"},{"content":"重构笔记 一个例子\n// 影片类型\rtype MovieType int\rvar (\rCHILD_RENTS MovieType = 2 // 儿童租用的\rREGULAR MovieType = 0 // 普通的\rNEW_RELEASE MovieType = 1 // 新发布的\r)\r// 影片\rtype Movie struct {\rTitle string\rPriceCode MovieType\r}\r // 租借记录\rtype Rental struct {\rMovie *Movie\rDaysRented int // 租借天数\r}\r // 顾客\rtype Customer struct {\rName string\rRentals []*Rental\r}\r// 添加租借记录\rfunc (cus *Customer) AddRental(rental *Rental) {\rcus.Rentals = append(cus.Rentals, rental)\r}\r// 生成详单\rfunc (cus *Customer) Statement() string {\rvar (\rtotalAmount float64 = 0 // 总价\rfrequentRenterPoints = 0 // 常客积分点\rresult = \u0026quot;Rental Record for \u0026quot; + cus.Name + \u0026quot;\\n\u0026quot; // 详单结果，页头内容\r)\r// 根据\rfor _, item := range cus.Rentals {\rvar thisAmount float64 = 0\rswitch item.Movie.PriceCode {\rcase REGULAR:\rthisAmount += 2\rif item.DaysRented \u0026gt; 2 {\rthisAmount += float64(item.DaysRented-2) * 1.5\r}\rbreak\rcase NEW_RELEASE:\rthisAmount += float64(item.DaysRented) * 3\rbreak\rcase CHILD_RENTS:\rthisAmount += 1.5\rif item.DaysRented \u0026gt; 3 {\rthisAmount += float64(item.DaysRented-3) * 1.5\r}\rbreak\r}\rfrequentRenterPoints++\r// 新书租借超过两天额外增加积分\rif item.Movie.PriceCode == NEW_RELEASE \u0026amp;\u0026amp; item.DaysRented \u0026gt; 1 {\rfrequentRenterPoints++\r}\rtotalAmount += thisAmount\rresult += \u0026quot;\\t\u0026quot; + item.Movie.Title + \u0026quot;\\t\u0026quot; + strconv.Itoa(int(thisAmount)) // TODO: float64 转 string\r}\r// 详单结果，页脚内容\rresult += \u0026quot;Amount owed is\u0026quot; + strconv.Itoa(int(totalAmount)) + \u0026quot;\\n\u0026quot;\rresult += \u0026quot;Your earned \u0026quot; + strconv.Itoa(frequentRenterPoints) + \u0026quot; frequent renter points\u0026quot; + \u0026quot;\\n\u0026quot;\rreturn result\r}\r 快速而随性的设计一个简单的程序并没有错，但如果这是复杂系统中具有代表性的一段，那对这个程序的信心就要动摇了。Customer 中长长的 Statement 做的事情太多，做了很多原本应该由其它结构体完成的事情\n在这个例子里，希望以html格式输出详单（而不是字符串）\n函数应该是可复用的，功能尽量单一的，且需要根据其使用的数据判定它应该属于哪个文件或者对象\n第一步必须是为即将修改的代码构建一组可靠的测试环境，好的测试是重构的根本\n提炼函数内的一块代码时，先找出这块代码所使用的局部变量和参数。任何不会被修改的变量都可以直接作为提炼函数的参数传入，被修改的变量可以作为提炼函数的返回值。\n提炼函数中的变量因为上下文改变，需要的话可以考虑调整变量命名，使其更符合新的上下文语意\ngoland可以选中一块代码，通过 右键-Refactor-Extract Method 直接提取成单独方法\n重构技术是以微小的步伐修改程序，这样即使犯下错误，也可以很容易测试发现，而不必花大量时间调试\n可以选择减少或去除临时变量，它们可能会被到处传递并引用，会助长冗长而复杂的代码，去除它们的代价是也许会产生更多次的计算而牺牲一些性能，这需要根据临时变量的可维护性和计算临时变量的性能之间权衡。有些计算结果的过程中用到的临时变量，可以直接将该结果获取变为方法，就可以有效去除临时变量\n","date":"0001-01-01","permalink":"https://yuanyatianchi.github.io/post/it.coderebuild/","tags":["it","CodeRebuild"],"title":"CodeRebuild"},{"content":"Docker 官网：https://www.docker.com/\n仓库：https://hub.docker.com/\nhello $ uname -r #查看centOS系统内核版本，Docker要求内核版本高于3.10\r$ yum update #升级软件包及内核（选做）\r 使用存储库安装：https://docs.docker.com/engine/install/centos/\n# 设置稳定存储库（配置docker源）\rwget https://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repo -O /etc/yum.repos.d/docker-ce.repo\r# 查看 yum 源上有哪些 docker-ce 版本\ryum list docker-ce --showduplicates|sort -r\r# 安装最新版本的docker-ce\ryum install docker-ce -y\r# 如果是centos8，默认使用podman代替docker，所以需要containerd.io，根据提示安装对应版本即可\ryum install -y https://download.docker.com/linux/centos/7/x86_64/stable/Packages/containerd.io-1.2.2-3.3.el7.x86_64.rpm\r#启动\rsystemctl enable docker #设置docker为开机启动\rsystemctl start docker #启动docker\r doocker镜像加速：https://cr.console.aliyun.com/cn-hangzhou/instances/mirrors 获取阿里docker镜像加速地址。以网易的镜像地址为例，修改/etc/docker/daemon.json\n{\r\u0026quot;registry-mirrors\u0026quot;: [\u0026quot;https://ohm5orzk.mirror.aliyuncs.com\u0026quot;]\r}\r 然后重启docker即可\nservice docker restart\r client 命令：https://docs.docker.com/reference/\n$ docker --help\r$ docker version #版本信息\r$ docker info #相信信息\r 镜像 docker中一个centos镜像大小200m不到，docker相当于隔离了进程，\n$ docker images #本地镜像列表\r$ docker search \u0026lt;关键字\u0026gt; #检索镜像\r$ docker pull \u0026lt;镜像名\u0026gt;[:\u0026lt;tag\u0026gt;] #拉取镜像。tag指定版本，缺省则为latest\r$ docker inspect \u0026lt;镜像名\u0026gt;#查看镜像详细信息\r$ docker rmi \u0026lt;镜像id\u0026gt; #删除指定镜像\rdocker network ps\rdocker network rm \u0026lt;网络名\u0026gt; #注意几个默认的不要删除\r 容器  镜像是一种轻量级、可执行的独立软件包，用来打包软件运行环境和基于运行环境开发的软件，它包含运行某个软件所需的所有内容，包括代码、运行时、库、环境变量和配置文件。 UnionFS (联合文件系统) : Union文件系统(UnionFS) 是一种分层、 轻量级并且高性能的文件系统，它支持对文件系统的修改作为一次提交来一层层的叠加，同时可以将不同目录挂载到同一个虚拟文件系统下(unite several directories into a single virtual filesystem)。Union 文件系统是Docker镜像的基础。镜像可以通过分层来进行继承，基于基础镜像(没有父镜像)，可以制作各种具体的应用镜像。  特性：一次同时加载多个文件系统，但从外面看起来，只能看到一个文件系统，联合加载会把各层文件系统叠加起来，这样最终的文件系统会包含所有底层的文件和目录   Docker镜像加载原理：docker的镜像实际上由一层一层的文件系统组成，这种层级的文件系统UnionFS.  bootfs(boot file system)主要包含bootloader和kernel，bootloader主要是引导加载kernel，Linux刚启动时会加载bootfs文件系统，在Docker镜像的最底层是bootfs。这一层与我们典型的Linux/Unix系统是一样的，包含boot加载器和内核。当boot加载完成之后整个内核就都在内存中了，此时内存的使用权己由bootfs转交给内核，此时系统也会卸载bootfs。 rootfs (root file system)，在bootfs之 上。 包含的就是典型Linux系统中的/dev, /proc, /bin, /etc等标准目录和文件。rootfs 就是各种不同的操作系统发行版，比如Ubuntu，Centos等 等。 对于一个精简的OS，rootfs可以很小，只需要包括最基本的命令、工具和程序库就可以了,因为底层直接用Host的kernel,自己只需要提供rootfs就行了。由此可见对于不同的linux发行版, bootfs基本是一致的, rootfs会有差别,因此不同的发行版可以公用bootfs。   分层的镜像：以我们的pull为例，在下载的过程中我们可以看到docker的镜像好像是在一层一层的在下载很多镜像 最大的一个好处就是：共享资源  比如:有多个镜像都从相同的base镜像构建而来，那么宿主机只需在磁盘上保存一-份base镜像，同时内存中也只需加载一份base镜像，就可以为所有容器服务了。而且镜像的每一层都可以被共享。   Docker镜像都是只读的。当容器启动时，一个新的可写层被加载到镜像的顶部。这一层通常被称作“容器层”，“容 器层”之下的都叫\u0026quot;镜像层  $ docker ps #容器列表。-a表示所有包括未运行的容器\r$ docker logs \u0026lt;container-name | container-id\u0026gt; #查看指定容器日志\r$ docker run [--name \u0026lt;container-name\u0026gt;] [-d|-it] [-p \u0026lt;container-port\u0026gt;:\u0026lt;port\u0026gt;] [-e \u0026lt;key\u0026gt;:\u0026lt;value\u0026gt;] \u0026lt;image-id\u0026gt; #运行指定容器。--name为容器取名；-d守护式运行（即后台运行），-it交互式运行（即进入容器交互）；-p指定将容器内的端口映射到实体机的端口；-e配置环境参数\r$ docker start \u0026lt;container-name | container-id\u0026gt; #启动\r$ docker restart \u0026lt;container-name | container-id\u0026gt; #重启\r$ docker stop \u0026lt;container-name | container-id\u0026gt; #停止\r$ docker stop \u0026lt;container-name | container-id\u0026gt; #强制停止\r$ docker rm \u0026lt;container-id\u0026gt; #删除指定容器\r$ docker rm -f $(docker ps -a -q) #条件批量删除\r$ docker ps-a-q | xargs docker rm #条件批量删除\r$ docker top \u0026lt;container-id\u0026gt;\r$ docker commit \u0026lt;container-id\u0026gt; \u0026lt;要创建的目标镜像名:[标签名]\u0026gt; -a=\u0026quot;\u0026lt;作者\u0026gt;\u0026quot; -m=\u0026quot;\u0026lt;提交的描述信息\u0026gt;\u0026quot;\r#提交容器副本使之成为一个新的镜像，可以docker ps -a看到\r 执行容器\n$ docker exec -it \u0026lt;container-id\u0026gt; \u0026lt;bashshell\u0026gt; #执行伪终端。-it表示-i -t，将为容器分配伪终端，可以进行交互\r$ docker exec -it \u0026lt;container-id\u0026gt; /bin/bash\r$ docker exec -it \u0026lt;container-id\u0026gt; bash\r$ docker exec -it \u0026lt;container-id\u0026gt; ls /\r$ docker attach \u0026lt;container-id\u0026gt; #重新进入\r$ exit #退出容器交互并停止容器\rctrl+q+p #退出容器交互但容器不停止\r$ docker cp \u0026lt;container-id\u0026gt;:/xxx/xxx.log /app/temp #拷贝容器中的文件到本地宿主主机\r docker exec -it mysql8 bash #进入容器名为mysql8的容器\rmysql -uroot -p123456 #进入mysql8后，可以使用mysql命令行，登录mysql\rgrant all privileges on *.* to 'root'@'%'; #添加权限，%表示所有ip可以登入，%可以替换为具体ip以给某ip开放root登入权限\r#也可以这样到表中修改权限\ruse mysql; #进到mysql库\rselect host, user, plugin from user; #查看user表，可以看到root的host是localhost，只允许本地登入\rupdate user set host='%' where user='root' #修改权限，%表示所有ip可以登入，%可以替换为具体ip以给某ip开放root登入权限\rflush privileges; #重新加载权限\r 容器数据卷  持久化  将运用与运行的环境打包形成容器运行，运行可以伴随着容器，但是我们对数据的要求希望是持久化的 容器之间希望有可能共享数据 Docker容器产生的数据，如果不通过docker commit生成新的镜像，使得数据做为镜像的一 部分保存下来，那么当容器删除后，数据自然也就没有了。 为了能保存数据在docker中我们使用卷。   卷就是目录或文件，存在于-一个或多个容器中，由docker挂 载到容器，但不属于联合文件系统，因此能够绕过Union File System提供一些用于持续存储或共享数据的特性:卷的设计目的就是数据的持久化，完全独立于容器的生存周期，因此Docker不会在容器删除时删除其挂载的数据卷  1:数据卷可在容器之间共享或重用数据 2: 卷中的更改可以直接生效 3:数据卷中的更改不会包含在镜像的更新中 4:数据卷的生命周期一直持续到没有容器使用它为止    容器卷添加 直接命令添加 $ docker run \u0026lt;指定镜像\u0026gt; -it -v \u0026lt;/宿主机绝对路径目录\u0026gt;:\u0026lt;/容器内目录\u0026gt; #-v指定host与container的数据同步目录，如果目录不存在则将创建目录，数据将实现实时同步，两边都可读写。-v参数可以多使用，形成一一对应的多对同步目录\r$ docker run \u0026lt;指定镜像\u0026gt; -it -v \u0026lt;/宿主机绝对路径目录\u0026gt;:\u0026lt;/容器内目录:ro\u0026gt; #限制容器目录只读\r DockerFile添加  新建/mydocker文件夹并进入 在Dockerfile中使用VOLUME指令来给镜像添加一个或多个数据卷  VOLUME[\u0026quot;/dataVolumeContainer\u0026rdquo;,\u0026quot;/dataVolumeContainer2\u0026rdquo;, \u0026ldquo;/dataVolumeContainer3\u0026rdquo;] 出于可移植和分享的考虑，用-v主机目录:容器目录这种方法不能够直接在Dockerfile中实现。由于宿主机目录是依赖于特定宿主机的，并不能够保证在所有的宿主机上都存在这样的特定目录。但是会有默认的目录，通过docker inspect即可查看到    #dockerfile，按序执行的脚本，每个FROM的镜像都会作为一层环境被装载\rFROM centos\rVOLUME [\u0026quot; /dataVolumeContainerA\u0026quot; ,\u0026quot; /dataVolumeContainerB\u0026quot; ]\rCMD echo \u0026quot;finished, -------- success1\u0026quot;\rCMD /bin/bash\r   docker build生成镜像\n$ docker build -f /mydocker/dockerfile -t yuanya/centos . #-f指定要构建的dockerfile；-t为生成的镜像取名\r$ docker run yuanya/centos -d #如果后面出现容器数据卷只能读的情况，就加上--privileged=true赋予容器扩展的特权\r$ docker inspect yuanya/centos\r   数据卷容器  命名的容器挂载数据卷，其它容器通过挂载这个(父容器)实现数据共享，挂载数据卷的容器，称之为数据卷容器 即容器间传递共享(\u0026ndash;volumes-from)  $ docker run yuanya/centos -it --name dc01 #启动一个01\r$ docker run yuanya/centos -it --name dc02 --volumes-from dc01 #将使dc02与dc01共享VOLUME配置的数据卷\r$ docker run yuanya/centos -it --name dc03 --volumes-from dc01 #将使dc03与dc01共享VOLUME配置的数据卷\r#即完成dc01、dc02、dc03之间相互共享VOLUME配置的数据卷\r DockerFile  Dockerfile是用来构建Docker镜像的构建文件，是由一系列命令和参数构成的脚本  #相当于docker run 时加 bash\rCMD /bin/bash\rCMD [\u0026quot;/bin/bash\u0026quot;]\r dockerfile解析过程\n Dockerfile内容基础知识  每条保留字指令都必须为大写字母且后面要跟随至少一个参数 指令按照从上到下，顺序执行 #表示注释 每条指令都会创建一个新的镜像层，并对镜像进行提交   保留字指令（关键字）  FROM：基础镜像，当前新镜像是基于哪个镜像的 MAINTAINER：镜像维护者 的姓名和邮箱地址 RUN：容器构建时需要运行的命令 EXPOSE：当前容器对外暴露出的端口 WORKDIR：指定在创建容器后，进入容器伪终端的默认工作目录 ENV：用来在构建镜像过程中设置环境变量  如 ENV MY_ PATH /usr/mytest。这个环境变量可以在后续的任何RUN指令中使用，这就如同在命令前面指定了环境变量前缀一-样；也可以在其它指令中直接使用这些环境变量，比如: WORKDIR $MY_ PATH   ADD：copy + 解压缩。将宿主机目录下的文件拷贝进镜像且ADD命令会自动处理URL和解压tar压缩包 COPY：类似ADD，拷贝文件和目录到镜像中。将从构建上下文目录中\u0026lt;源路径\u0026gt;的文件/目录复制到新的一层的镜像内的\u0026lt;目标路径\u0026gt;位置  shell格式：COPY src des exec格式：COPY [\u0026ldquo;src\u0026rdquo;, \u0026ldquo;dest\u0026rdquo;]   VOLUME：容器数据卷， 用于数据保存和持久化工作 CMD  指定一个容器启动时要运行的命令。CMD指令的格式和RUN 相似,也是两种格式  shell格式：CMD \u0026lt;命令\u0026gt; exec格式：CMD [\u0026ldquo;可执行文件\u0026rdquo;，”参数1\u0026rdquo;， “参数2\u0026hellip;] 参数列表格式：CMD [\u0026ldquo;参数1\u0026rdquo;，“参数2\u0026rdquo;\u0026hellip;]。在指定了ENTRYPOINT 指令后，用CMD 指定具体的参 数。   Dockerfile中可以有多个CMD指令，但只有最后一个生效，CMD会被docker run之后的参数替换.。   ENTRYPOINT  指定一个容器启动时要运行的命令 ENTRYPOINT的目的和CMD一样，都是在指定容器启动程序及参数，但是命令是追加的，不会覆盖前面的命令，比如在是 ENTRYPOINT [\u0026ldquo;可执行文件\u0026rdquo;，”参数1\u0026rdquo;]，运行时docker run xxx -i，将变为ENTRYPOINT [\u0026ldquo;可执行文件\u0026rdquo;，”参数1\u0026rdquo;，\u0026quot;-i\u0026rdquo;]，等于可以追加参数的效果   ONBUILD：当构建一个被继承的Dockerfile时运行命令，父镜像在被子继承后父镜像的onbuild被触发    案例\n  Base镜像(scratch)：DockerHub中99%的镜像都是通过在base镜像中安装和配置需要的软件构建出来的\n scratch：an explicitly empty image, especially for building images \u0026ldquo;FROM scratch\u0026rdquo;    centos的镜像很精简，我们通过dockerfile自定义一个centos镜像使其拥有vim、ifconfig等命令及其它自定义内容\n/mydocker/dockerfile01\nfrom centos\rMAINTAINER yuanya\u0026lt;yuanyatianchi@google.com\u0026gt;\rENV MY_PATH /tmp\rWORKDIR $MY_PATH\rRUN yum -y install vim\rRUN yum -y install net-tools\rEXPOSE 80\rCMD echo $MY_PATH\rCMD echo \u0026quot;success-ok\u0026quot;\rCMD /bin/bash\r $ docker build -f /mydocker/dockerfile01 -t mycentos:1.0 #构建\r$ docker run - it mycentos:1.0 #运行\r$ docker history \u0026lt;容器id\u0026gt; #查看历史\r   定制tomcat9\nFROM centos\rMAINTAINER zzyy\u0026lt; zzyybs@126. com\u0026gt;\r#把宿主机当前上下文的C. txt拷贝到容器/usr/local/路径下\rCOPY c.txt /usr/local/cincontainer.txt\r#把java与tomcat添加到容器中\rADD jdk 8u171- linux -x64. tar .gz /usr/local/\rADD apache tomcat 9.0.8. tar.gz /usr/local/\r#安装vim编辑器\rRUN yum -y install vim\r#设置工作访问时候的WORKDIR路径，登录落脚点\rENV MYPATH /usr/local\rWORKDIR $MYPATH\r#配置java与tomcat环境变量\rENV JAVA HOME /usr/local/jdk1.8.0_ 171\rENV CLASSPATH $JAVA HOME/lib/dt . jar :$JAVA_ HOME/1ib/ tools. jan\rENV CATALINA HOME /usr/local/apache-tomcat-9.0.8\rENV CATALINA BASE /usr/ local/ apache-tomcat-9.0.8\rENV PATH $PATH:$JAVA_ HOME/bin: $CATAL INA HOME/lib: $CATAL INA HOME/bin\r#容器运行时监听的端口\rEXPOSE 8080\r#启动时运行tomcat\r# ENTRYPOINT [\u0026quot; /usr/local/ apache -tomcat 9.0.8/bin/startup.sh\u0026quot; ]\r# CMD [\u0026quot; /usr/local/apache -tomcat- 9.0. 8/bin/catalina. sh\u0026quot; ,\u0026quot;run\u0026quot;]\rCMD /usr/1ocal/apache-tomcat-9.0.8/bin/startup.sh \u0026amp;\u0026amp; tail -F /usr/local/apache-tomcat-9.0.8/bin/1ogs/catalina.out\r docker run -d -p 9080:8080 --name myt9\r-V /zzyyuse/mydockerfile/tomcat9/tesf:/usr/local/apache-tomcat-9.0.8/webapps/ test\r-V /zzyyuse/mydockerfile/tomcat9/tomcat9logs/:/usr/local/apache-tomcat-9.0.8/logs\r--pr ivi leged=true\rzzyytomcat9\r#设置容器卷，可以使我们直接把服务部署到主机tomcat9/test即可同步到容器tomcat-9.0.8/webapps/test上运行了\r 软件 MySql $ docker pull mysql:5.7\r$ docker run mysql:5.7 --name mysql01 -d -p 3306:3306 -e MYSQL_ROOT_PASSWORD=yuanya #设置MYSQL_ROOT_PASSWORD，否则密码随机\r ElesticSearch $ docker pull elasticsearch:7.5.2\r$ docker run --name elasticsearch -d -p 9200:9200 -p 9300:9300 -e \u0026quot;discovery.type=single-node\u0026quot; -e \u0026quot;ES_JAVA_OPTS=-Xms256m -Xmx256m\u0026quot; elasticsearch:tag\r Kibana $ ocker pull kibana:7.5.2\r$ docker run --name kibana --link elasticsearch_CONTAINER_ID:elasticsearch -d -p 5601:5601 kibana:tag\r RabbitMQ 带有management的是带有web控制台的，通过15672端口访问控制台，缺省账号密码guest\n$ docker pull rabbitmq:3.7.24-management $ docker run -d -p 5672:5672 -p 15672:15672 rabbitmq:3.7.24-management\r ","date":"0001-01-01","permalink":"https://yuanyatianchi.github.io/post/it.cloudnative.docker/","tags":["it","cloudnative"],"title":"Docker"},{"content":"etcd clients Golang https://github.com/etcd-io/etcd/tree/master/clientv3\ngo get go.etcd.io/etcd/clientv3 #解决包导入版本问题：https://segmentfault.com/q/1010000021762281/\r client中有KV、Lease、Watcher等对象，通过这些对象来操作kv对、租约、监听器等\nkv func main() {\rvar (\rerr error\rconfig clientv3.Config\rclient *clientv3.Client\r/*kv：用于操作kv*/\rkv clientv3.KV\r/*KeyValue：保存etcd响应kv信息的结构体\rKey：键\rValue：值\rCreateRevision：创建时的Revision\rModRevision：修改时的ModRevision Version：kv的版本，等于该kv的put次数*/\rkvPair *mvccpb.KeyValue\r/*PutResponse：保存put响应信息的结构体\rHeader：响应头信息，包括档次操作的 ClusterId集群id、MemberId节点id、Revision、RaftTerm等\rPrevKv：kv被put后的上一个kv信息，如果是put的是新的kv则prevKv=nil*/\rputResp *clientv3.PutResponse\r/*GetResponse：保存get响应信息的结构体\rHeader：响应头信息\rCount：kv数量\rKvs：kv数组*/\rgetResp *clientv3.GetResponse\r/*DeleteResponse：保存del响应信息的结构体\rHeader：响应头信息\rDeleted：删除的kv数量\rPrevKvs：删除的所有kv信息，kv数组*/\rdelResp *clientv3.DeleteResponse\r/*OP：对get、put、del等操作做了抽象，用同样的调用方式执行不同的操作，返回相同的响应结构体*/\rputOp clientv3.Op\rgetOp clientv3.Op\ropResp clientv3.OpResponse\r)\r/*客户端配置*/\rconfig = clientv3.Config{\rEndpoints: []string{\u0026quot;127.0.0.1:2379\u0026quot;}, //集群地址\rDialTimeout: 5 * time.Second, //连接超时时间\r}\r/*建立连接*/\rif client, err = clientv3.New(config); err != nil {\rfmt.Println(err)\rreturn\r}\rdefer client.Close()\rfmt.Println(\u0026quot;连接etcd成功\u0026quot;)\r/*KV：提供操作 etcd kv 的相关功能*/\rkv = clientv3.NewKV(client)\r/*Put\rWithPrevKV：响应将带上PrevKv*/\rif putResp, err = kv.Put(context.TODO(), \u0026quot;/cron/jobs/job1\u0026quot;, \u0026quot;JOB1\u0026quot;); err != nil {\rfmt.Println(err)\rreturn\r}\rfmt.Println(\u0026quot;put Revision:\u0026quot;, putResp.Header.Revision)\rif putResp, err = kv.Put(context.TODO(), \u0026quot;/cron/jobs/job2\u0026quot;, \u0026quot;JOB2\u0026quot;, clientv3.WithPrevKV()); err != nil {\rfmt.Println(err)\rreturn\r}\rif putResp.PrevKv != nil {\rfmt.Println(\u0026quot;put PrevKv:\u0026quot;, string(putResp.PrevKv.Key), string(putResp.PrevKv.Value))\r}\r/*Get\rWithCountOnly：仅返回符合条件的个数\rWithPrefix：根据前缀获取kv\rWithFromKey：从某个key开始。配合WithLimit(n)获取从某个key开始的n个kv*/\rif getResp, err = kv.Get(context.TODO(), \u0026quot;/cron/jobs/job1\u0026quot;); err != nil {\rfmt.Println(err)\rreturn\r}\rfmt.Println(\u0026quot;get:\u0026quot;, getResp.Count, getResp.Kvs)\rif getResp, err = kv.Get(context.TODO(), \u0026quot;/cron/jobs/\u0026quot;, clientv3.WithPrefix()); err != nil {\rfmt.Println(err)\r}\rfmt.Println(\u0026quot;get:\u0026quot;, getResp.Count, getResp.Kvs)\r/*del\rWithPrevKV：返回删除的所有kv\rWithPrefix：删除带有某前缀的所有kv\rWithFromKey：删除从某个key开始的所有kv。配合WithLimit(n)则删除key开始的n个kv*/\rif delResp, err = kv.Delete(context.TODO(), \u0026quot;/cron/jobs/job2\u0026quot;, clientv3.WithPrevKV()); err != nil {\rfmt.Println(err)\rreturn\r}\rfmt.Println(\u0026quot;del:\u0026quot;, delResp.Deleted, delResp.PrevKvs)\rif len(delResp.PrevKvs) != 0 {\rfor _, kvPair = range delResp.PrevKvs {\rfmt.Println(\u0026quot;del:\u0026quot;, string(kvPair.Key), string(kvPair.Value), kvPair.ModRevision)\r}\r}\r/*OpPut*/\rputOp = clientv3.OpPut(\u0026quot;/cron/jobs/job3\u0026quot;, \u0026quot;JOB3\u0026quot;) //创建操作\rif opResp, err = kv.Do(context.TODO(), putOp); err != nil { //执行操作\rfmt.Println(err)\rreturn\r}\rfmt.Println(opResp.Put().Header.Revision)\r/*OpGet*/\rgetOp = clientv3.OpGet(\u0026quot;/cron/jobs/job8\u0026quot;)\rif opResp, err = kv.Do(context.TODO(), getOp); err != nil {\rfmt.Println(err)\rreturn\r}\rfmt.Println(opResp.Get().Kvs[0].ModRevision, string(opResp.Get().Kvs[0].Value)) //创建时CreateRevision=ModRevision\r}\r lease func main() {\rvar (\rerr error\rconfig clientv3.Config\rclient *clientv3.Client\rkv clientv3.KV\rputResp *clientv3.PutResponse\r/*lease：租约*/\rlease clientv3.Lease\rleaseGrantResp *clientv3.LeaseGrantResponse\rleaseKeepAliveResp *clientv3.LeaseKeepAliveResponse\rleaseKeepAliveRespChan \u0026lt;-chan *clientv3.LeaseKeepAliveResponse\r)\rconfig = clientv3.Config{\rEndpoints: []string{\u0026quot;127.0.0.1:2379\u0026quot;},\rDialTimeout: 5 * time.Second,\r}\rif client, err = clientv3.New(config); err != nil {\rfmt.Println(err)\rreturn\r}\rdefer client.Close()\r/*创建租约*/\rlease = clientv3.NewLease(client)\r/*申请租约。单位：秒*/\rif leaseGrantResp, err = lease.Grant(context.TODO(), 10); err != nil {\rfmt.Println(err)\rreturn\r}\r/*KeepAlive：续租\r将启动一个协程自动延续心跳续租，默认每3秒续租一次，续租时间为申请租约时的时间，KeepAliveOnce则只续租一次\r续租不是由原来的祖约时间加新的租约时间，而是重新设置租约时间，从续租时开始计算\r返回一个只读chan，它将定时将续租的响应写入chan，我们可以读取出来查看\r通过context取消或者过期取消等，也可以使心跳中断，ctx, _ := context.WithTimeout(context, 5*time.Second)*/\rif leaseKeepAliveRespChan, err = lease.KeepAlive(context.TODO(), leaseGrantResp.ID); err != nil {\rfmt.Println(err)\rreturn\r}\r/*读取leaseKeepAliveRespChan*/\rgo func() {\rfor {\rselect {\rcase leaseKeepAliveResp = \u0026lt;-leaseKeepAliveRespChan:\rif leaseKeepAliveResp == nil { //说明租约已经终止，可能是故障导致客户端与服务器失联了，或者我们主动让其释放的\rfmt.Println(\u0026quot;租约失效\u0026quot;)\rgoto END\r} else {\rfmt.Println(\u0026quot;收到续租响应\u0026quot;, leaseKeepAliveResp.ID)\r}\r}\r}\rEND:\r}()\r//用lease实现分布式锁，如果程序正常运行，谁抢到了key就等于抢到了锁，在持有锁的时候锁不应该过期，则需要延续租期，直到主动释放\r//并且如果程序宕掉了，锁也不会永远留在etcd，没有程序续租自然就自动过期了\r/*关联租约：put的kv与lease关联，等于为kv设置了过期时间，通过WithLease(leaseId)实现*/\rkv = clientv3.NewKV(client)\rif putResp, err = kv.Put(context.TODO(), \u0026quot;/cron/lock/job1\u0026quot;, \u0026quot;\u0026quot;, clientv3.WithLease(leaseGrantResp.ID)); err != nil {\rfmt.Println(err)\rreturn\r}\rfmt.Println(\u0026quot;写入成功\u0026quot;, putResp.Header.Revision)\r}\r watcher func main() {\rvar (\rerr error\rconfig clientv3.Config\rclient *clientv3.Client\rkv clientv3.KV\rgetResp *clientv3.GetResponse\rwatchStartRevision int64\r/*watcher：监听器*/\rwatcher clientv3.Watcher\rwatchResp clientv3.WatchResponse\rwatchRespChan \u0026lt;-chan clientv3.WatchResponse\revent *clientv3.Event\r)\rconfig = clientv3.Config{\rEndpoints: []string{\u0026quot;127.0.0.1:2379\u0026quot;},\rDialTimeout: 5 * time.Second,\r}\rif client, err = clientv3.New(config); err != nil {\rfmt.Println(err)\rreturn\r}\rdefer client.Close()\rkv = clientv3.NewKV(client)\r/*模拟kv不断变化*/\rgo func() {\rfor {\rkv.Put(context.TODO(), \u0026quot;/cron/jobs/job7\u0026quot;, \u0026quot;JOB7\u0026quot;)\rkv.Delete(context.TODO(), \u0026quot;/cron/jobs/job7\u0026quot;)\rtime.Sleep(time.Second)\r}\r}()\r/*获取kv来得到开始监听的Revision：需要从当前Revision的下一个Revision开始监听*/\rif getResp, err = kv.Get(context.TODO(), \u0026quot;/cron/jobs/job7\u0026quot;); err != nil {\rfmt.Println(err)\rreturn\r}\rif getResp.Count != 0 {\rfmt.Println(getResp.Kvs[0].Value)\r}\rwatchStartRevision = getResp.Header.Revision + 1 //需要从当前Revision的下一个Revision开始监听\r/*创建监听器*/\rwatcher = clientv3.NewWatcher(client)\rfmt.Println(\u0026quot;从该revision开始监听：\u0026quot;, watchStartRevision)\r/*Watch：开启监听，通过WithRev(watchStartRevision)指定开始监听的版本\r返回一个只读chan，它将定时将监听的响应写入chan\r通过context可以取消监听*/\rctx, cancel := context.WithCancel(context.TODO())\rtime.AfterFunc(5*time.Second, cancel)\rwatchRespChan = watcher.Watch(ctx, \u0026quot;/cron/jobs/job7\u0026quot;, clientv3.WithRev(watchStartRevision))\r/*从watchRespChan读取监听响应*/\rfor watchResp = range watchRespChan {\rfor _, event = range watchResp.Events {\rswitch event.Type {\rcase mvccpb.PUT:\rfmt.Println(\u0026quot;put:\u0026quot;, event.Kv.CreateRevision, event.Kv.ModRevision)\rcase mvccpb.DELETE:\rfmt.Println(\u0026quot;del:\u0026quot;, event.Kv.ModRevision)\r}\r}\r}\r}\r 分布式乐观锁 /*分布式乐观锁：op操作、lease实现锁自动过期、txn事务：if else then\r思路：上锁、处理业务、释放锁*/\rfunc main() {\rvar (\rerr error\rctx context.Context\rcancel context.CancelFunc\rconfig clientv3.Config\rclient *clientv3.Client\rlease clientv3.Lease\rleaseGrantResp *clientv3.LeaseGrantResponse\rleaseKeepAliveResp *clientv3.LeaseKeepAliveResponse\rleaseKeepAliveRespChan \u0026lt;-chan *clientv3.LeaseKeepAliveResponse\rkv clientv3.KV\rtxn clientv3.Txn\rtxnResp *clientv3.TxnResponse\r)\rconfig = clientv3.Config{\rEndpoints: []string{\u0026quot;127.0.0.1:2379\u0026quot;},\rDialTimeout: 5 * time.Second,\r}\rif client, err = clientv3.New(config); err != nil {\rfmt.Println(err)\rreturn\r}\rdefer client.Close()\r/*1.上锁：创建租约，自动续租，拿租约去抢占一个key，谁第一个put上这个key，即抢到锁了*/\r/*租约：创建租约，自动续租*/\rlease = clientv3.NewLease(client)\rif leaseGrantResp, err = lease.Grant(context.TODO(), 10); err != nil {\rfmt.Println(err)\rreturn\r}\rdefer lease.Revoke(context.TODO(), leaseGrantResp.ID) //确保函数退出时，立即释放租约\rctx, cancel = context.WithCancel(context.TODO())\rdefer cancel() //确保函数退出时，停止自动续租\rif leaseKeepAliveRespChan, err = lease.KeepAlive(ctx, leaseGrantResp.ID); err != nil {\rfmt.Println(err)\rreturn\r}\rgo func() {\rfor {\rselect {\rcase leaseKeepAliveResp = \u0026lt;-leaseKeepAliveRespChan:\rif leaseKeepAliveResp == nil {\rfmt.Println(\u0026quot;租约失效\u0026quot;)\rgoto END\r} else {\rfmt.Println(\u0026quot;收到续租响应\u0026quot;, leaseKeepAliveResp.ID)\r}\r}\r}\rEND:\r}()\r/*抢key：通过txn事务完成，if key不存在，then put这个key，else抢锁失败*/\rkv = clientv3.NewKV(client)\rtxn = kv.Txn(context.TODO()) //创建事务\rtxn.If(clientv3.Compare(clientv3.CreateRevision(\u0026quot;/cron/lock/job9\u0026quot;), \u0026quot;=\u0026quot;, 0)). //如果key不存在（key的CreateRevision=0）\rThen(clientv3.OpPut(\u0026quot;/cron/lock/job9\u0026quot;, \u0026quot;job9lock\u0026quot;, clientv3.WithLease(leaseGrantResp.ID))). //执行putOp，可传多个op\rElse(clientv3.OpGet(\u0026quot;/cron/lock/job9\u0026quot;)) //抢锁失败，取出来看看。。。同样是可以传入多个op\rif txnResp, err = txn.Commit(); err != nil { //提交事务\rfmt.Println(err)\rreturn\r}\rif !txnResp.Succeeded { //判断是否抢到了锁，如果没抢到，前面else是get操作，这里获取一下getResp(即rangeResp)\rfmt.Println(\u0026quot;锁已被占用：\u0026quot;, string(txnResp.Responses[0].GetResponseRange().Kvs[0].Value))\rreturn\r}\r/*2.处理业务\r已经在锁内了，很安全了*/\rfmt.Println(\u0026quot;处理业务\u0026quot;)\rtime.Sleep(10 * time.Second)\r/*3.释放锁：取消自动续租，立即释放租约，释放租约同时该key也会被删掉，以实现立马释放锁\r前面defer做完了*/\r}\r etcd协调服务  功能介绍  核心特性  将数据存储在集群中的高可用K-V存储 允许应用实时监听存储中的K-V的变化 能够容忍单点故障，能够应对网络分区（网络分区：分布式系统一般都有多个节点，如果把某几个节点与另几个节点之间的网络切断，系统仍能够短暂容忍这种问题提供服务）   重量级用户：  kubernetes：Google开源容器调度平台  etcd支撑：服务发现、集群状态存储、配置同步   Cloud Founder：Vmware开源容器调度平台  etcd支撑：集群状态存储、配置同步、分布式锁     传统存储模型  单点存储：单点故障 主从复制：master宕机，slave选为master需要时间，可以读，但无master不可写；主从之间数据同步延迟，master宕机可能损失数据，有些场景下这是不可容忍的     原理介绍  抽屉理论：大多数理论  一个班级61人 有一个秘密，告知给班里的31个人 那么随便挑选31个人，一定有1个人知道秘密   etcd与Raft的关系：所以etcd只需要把kv存储在raft算法的日志（预写日志）里，即可实现强一致性同步  Raft是强一致的集群日志同步算法（一致性hash） etcd是一个分布式KV存储 etcd利用raft算法在集群中同步key-value   quorum模型：大多数模型，是一个二阶段模型  例如集群需要2n+1个节点，假设是1个leader，2n个follower  第一阶段：调用者将kv传给leader，但leader不会立刻响应调用者，而是日志实时复制( Replication)，leader会给n个follower做日志拷贝 第二阶段：本地提交（写入kv存储），返回客户端。算上leader，就是n+1个节点保存了这个日志，刚好大多数了，就可以本地提交，返回客户端。并且会异步通知folloer完成提交   日志实时复制：写入性能差，官方数据是约1000次/秒   日志格式  index按序递增，其中装的kv对     Raft  Raft日志概念  replication:日志在leader生成，向follower复制， 达到各个节点的日志序列最终一致 term：任期，重新选举产生的leader，其term单调递增 log index:日志行在日志序列的下标   Raft异常场景：详见图。可能没来得及复制就宕掉了，选举来去，比较乱， Raft异常安全  选举leader需要半数以上节点参与 节点commit日志最多的允许选举为leader commit日志同样多,则term、 index越大的允许选举为leader   Raft工作示例：  leader如果被网络分区了，那么它收到一个a=1的kv写入请求时，会不断重试复制日志，但是不断失败。这称为悬而未决 而分区另一端的follower们，失去leader时则，会选举新的leader，假如加收新的a=2，并复制给大多数 旧的leader重连上集群时，称为follower，丢弃a=1，同步a=2，因为现在a=2才是大多数   Raft保证  提交成功的请求，一定不会丢 各个节点的数据将最终一致     交互协议：etcp支持的协议  通用的HTTP+JSON协议，性能低效 SDK内置GRPC协议（google的），性能高效：基于IDL、HTTP/2、Protobuffer 3   重要特性  底层存储是按key有序排列的，可以顺序遍历 因为key有序，所以etcd天然支持按目录结构高效遍历 支持复杂事务，提供类似if\u0026hellip;then .. else \u0026hellip; 的事务能力 基于租约机制实现key的TTL过期   key有序存储  存储引擎是按Key有序排列的，如下3个字符串key，是前缀有序的。可以用来模拟目录结构，获取子目录内容，只需要seek到不大于/feature-flags/的第一个key，开始向后scan即可  /config/database /feature-flags/verbost-logging /feature-flags/redesign     MVCC多版本控制。提交版本(revision)在etcd中单调递增；同key维护多个历史版本，用于实现watch机制；历史版本过多，可以通过执行compact命令完成删减  Put /key1 value1-\u0026gt; revision=1 Put /key2 value2-\u0026gt; revision=2 Put /key1 value3-\u0026gt; revision=3   监听KV变化：通过watch机制，可以监听某个key，或者某个目录(key前缀)的连续变化。常用于分布式系统的配置分发、状态同步。即那些app就可以监听到配置的变化  watch工作原理：假如有多版本存储 a=1 rev=1、b=2 rev=2、a=3 rev=3。见图  SDK发出请求 watch key=a from rev=1，即想要监听key为a的kv对，从revision=1开始监听 etcd就会建立一个watcher监听者，扫描历史revision，发现要找的key，即有a=1 rev=1、a=3 rev=3都是key=a的，并且从rev=1开始监听 etcd则将推送a=1 rev=1、a=3 rev=3这两个记录到我们的SDK。如果是from rev=3，则只有一条记录     lease租约  如SDK通过Grant创建10秒租约，即租约生命期只有10秒 etcd返回一个租约id，如Lease ID=5 SDK可以put a=1 with lease=5，即写入kv对时带上租约id到存储引擎，存储引擎与租约建立关联 当租约过期则将delete a，删除过期数据 SDK可以定期向租约keepAlive续租，a就可以持续保存   实践任务  搭建单机etcd，熟悉命令行操作 golang调用etcd的put/get/delete/lease/watch方法 使用txn事务功能，实现分布式乐观锁    部署 https://github.com/etcd-io/etcd\n安装：https://github.com/etcd-io/etcd/releases/download/v3.3.21/etcd-v3.3.21-linux-amd64.tar.gz，在linux上解压即可\ntar -zxvf etcd-v3.3.21-linux-amd64.tar.gz\rcd etcd-v3.3.21-linux-amd64\rnohup ./etcd --listen-client-urls 'http://0.0.0.0:2379' --advertise-client-urls 'http://0.0.0.0:2379' \u0026amp; #启动时指定0.0.0.0，否则只有本地\rless nohup.out #查看nohup.out，可以在最底部看到服务启动了，并且可以在前面看到选举信息，只有一个节点，自己投给自己称为了leader\r./etcdctl #查看etcd的控制命令\rETCDCTL_API=3 ./etcdctl #根据./etcdctl查看到的警告需要在./etcdctl前加环境变量ETCDCTL_API=3才能使用v3版本的API\rETCDCTL_API=3 ./etcdctl put \u0026quot;name\u0026quot; \u0026quot;yuanya\u0026quot; #put存入一个kv对\rETCDCTL_API=3 ./etcdctl get \u0026quot;name\u0026quot; #获取\rETCDCTL_API=3 ./etcdctl del \u0026quot;name\u0026quot; #删除\r API 打开一个命令窗口，路径形式的key\nETCDCTL_API=3 ./etcdctl put \u0026quot;/cron/jobs/job1\u0026quot; \u0026quot;{一段json...job1}\u0026quot; #因为etcd是按key有序的，则我们保存kv时，key都以路径的形式来保存则非常方便\rETCDCTL_API=3 ./etcdctl put \u0026quot;/cron/jobs/job2\u0026quot; \u0026quot;{一段json...job2}\u0026quot;\rETCDCTL_API=3 ./etcdctl get \u0026quot;/cron/jobs/job1\u0026quot; #可以获取某一个job\rETCDCTL_API=3 ./etcdctl get -h #查看get帮助，可以发现有--prefix前缀查询、有--from-key从某个key开始查询等\rETCDCTL_API=3 ./etcdctl get \u0026quot;/cron/jobs/\u0026quot; --prefix #返回了job1和job2\r 打开另一个命令窗口，试试监听\nETCDCTL_API=3 ./etcdctl watch -h #watch，监听某个key或者某些key\rETCDCTL_API=3 ./etcdctl get \u0026quot;/cron/jobs/\u0026quot; --prefix #监听以某个字符串为前缀的key\r 在第一个命令窗口修改、删除 job2，可以发现监听的窗口中输出了我们的操作内容\nETCDCTL_API=3 ./etcdctl put \u0026quot;/cron/jobs/job2\u0026quot; \u0026quot;{...job2}\u0026quot;\rETCDCTL_API=3 ./etcdctl del \u0026quot;/cron/jobs/job2\u0026quot;\r ","date":"0001-01-01","permalink":"https://yuanyatianchi.github.io/post/it.db.etcd/","tags":["it","db"],"title":"etcd"},{"content":"git  192.30.253.113 github.com：修改host，提高GitHub的push和pull速度 命令行符号  ：可写可不写 \u0026lt;\u0026gt;：必须写且需要用你自己的内容替换 { }：必须在其中做出选择(选项之间以 | 隔开)   结构  工作区 暂存区 本地库    https://www.yiibai.com/git\napi clone  克隆项目。一般建议使用ssh，https有可能出一些不知所以的问题  #ssh\rgit clone git@github.com:YuanyaTianchi/yuanyatianchi.git\r#https\rgit clone https://github.com/YuanyaTianchi/yuanyatianchi.git\r remote  远程库操作  #查看远程库名字\rgit remote\r#查看远程仓库详细地址\rgit remote -v\r#\r  git remote add []  ：创建repository  repositoryName：一般命名为origin   git remote remove ：删除远程的仓库的所有跟踪分支和配置设置 git remote rename  ：重命名远程仓库在本地的简称 git remote show ：查看某个远程仓库的详细信息  git pull \u0026lt;\u0026ndash;rebase\u0026gt;   \u0026lt;\u0026ndash;allow-unrelated-histories\u0026gt;：获取远程仓库项目文件  \u0026ndash;allow-unrelated-histories：可选参数，可以合并两个独立启动仓库的历史   提交到本地仓库后再推送到远程仓库 git push \u0026lt;\u0026ndash;set-upstream\u0026gt;  ：推送到远程仓库 git remote rm：删除源(origin)  pull  git pull [option]... \u0026lt;远程主机名\u0026gt; [branch-name]:[local-branch-name]  -q：    git pull origin remoteBranch:localBranch #获取远程origin上的分支branch1，并合并到本地的分支branch2\r push revert https://juejin.im/post/6844903614767448072\n git revert [将被回退的版本]   reset  git reset [要回退到的版本]  log  git log  git log --graph --abbrev-commit --pretty=oneline #图形化，hash值简化，单行\r merge 我的需求在feature/tianchi/xxx分支上写完，要合并到develop里面，需要先选择到开发分支再去merge我的分支，会是Merge branch 'feature/tianchi/xxx' into 'develop'，千万不要在我的分支上去merge开发分支，会变成Merge branch 'develop' into 'feature/tianchi/xxx'\n#正确操作如下\rgit pull origin develop:develop\rgit checkout develop\rgit merge feature/tianchi/xxx\r rebase  rebase  -r：\u0026ndash;rebase-merges简写。    git rebase -r\r cfg 仓库：每个仓库的Git配置文件都放在.git/config文件中\n全局：家目录下.gitconfig文件\n别名 [alias]\rch = checkout\rco = commit\rbr = branch\rst = status\rlg = log --graph --abbrev-commit --pretty=oneline\r 本地库操作 本地库初始化  在cmd中进到项目目录下 git init：本地库初始化。生成一个.git目录，该目录中存放的是本地库相关的子目录和文件 设置签名：username和email用于区分不同开发人员的身份。这里设置的签名和登录远程库(代码托管中心)的账号密码没有任何关系。默认项目级别(仓库级别)仅在当前本地库范围内生效，项目级别信息保存在.git/config下。系统用户级别：指定\u0026ndash;global，系统用户级别信息保存在系统用户家目录下的.config文件中。项目级别优先于系统用户级别。至少设置一个  git config [\u0026ndash;global] user.name ：设置用户名 git config [\u0026ndash;global] user.email ：设置Email地址 cat .git/cogfig：查看本地库配置文件    基本操作  git help \u0026lt;命令\u0026gt;：查看该命令文档  文件操作  git status：查看状态。  on branch master表示在主分支上，no commits yet表示无提交内容 红色文件表示未添加到暂存区中，绿色表示已添加到暂存区中   git add ：将文件从工作区添加到暂存区。unstage表示从暂存区中移除 git rm ：从暂存区删除 git commit [-m ] [-a] ：将文件从暂存区中提交到本地库，添加后会进入vim，写本次提交描述内容  -m：无需编辑vim，直接在后面写入描述内容，； -a：无需git add操作，直接添加\u0026amp;提交，不过就不存在暂存区的撤销操作时间了 结果内容：  root-commit：表示根提交(第一次提交) 数字编号：暂时粗略认为是本次提交的版本号      版本移动  git log [\u0026ndash;pretty={oneline}] [\u0026ndash;oneline]：查看日志版本，只显示当前及以前版本。这里有2个版本的记录，这里hash值表示该次提交的索引，HEAD是指向当前版本的指针  \u0026ndash;pretty=oneline将每个版本只以一行显示 \u0026ndash;oneline不仅一行显示还只显示部分hash值   git reflog：查看日志，显示前后所有版本以及版本移动  HEAD@{n}表示移动到对应版本指针需要移动n步   git reset \u0026ndash;{hard/mixed/soft} HEAD ：索引移动(推荐)  \u0026ndash;mixed在本地枯移动HEAD指针，重置暂存区，默认策略 \u0026ndash;hard在本地库移动HEAD指针，重置暂存区，重置工作区（会删除文件）， \u0026ndash;soft仅在本地库移动HEAD指针   git reset \u0026ndash;hard HEAD^^^：^移动，只能后退，3个^即表示后退3个版本 git reset \u0026ndash;hard HEAD~：~移动，只能后退，n即表示后退n个版本 git checkout  ：选择某个版本的文件到工作区 git diff [HEAD] []：在修改工作区文件之后  无HEAD：表示工作区与暂存区的该文件比较，显示文件变化 有HEAD：表示工作区与本地库的该文件比较，显示文件变化 无：将比较当前工作区中的所有文件    分支管理 分支：在版本控制过程中，使用多条线同时推进多个任务。同时并行推进多个功能开发，提高开发效率。各个分支在开发过程中，如果一个分支开发失败，不会对其他分支有任何影响，删除重新开始即可\n git branch [-v] [-a]：  -v：显示版本号 -a：包括远程分支   git branch ：创建分支 git branch -D ：删除分支 git checkout ：切换分支 git merge ：合并某分支，将当前所在分支与某分支合并。  分支合并冲突：当两个分支都修改了同一个文件中同一行的内容，合并时取舍哪个分支的该处内容git是无法判断的，只需要vim手动编辑后再添加提交(提交不能带文件名)即可    clean git clean -f\rgit clean -d -fx #强制删除Untracked files\r 合并工程  假如有master、test、develop分支，现在有一个需求从master切出来一个demand，需求子任务从demand切出来一个demand/child：git pull origin demand:demand，git ch demand，git ch -b demand/child  任务开发完后，需要merge demand/child into develop。切到任务分支git ch demand/child，拉取develop分支git pull origin develop:develop，会有很多改变，会因版本而merge develop into demand/child的相关提示，中止mergegit merge --abort，不能让merge develop into demand/child发生，这反了，切到devgit ch develop，merge demand/child into developgit merge demand/child，编译检查go build xxx，提交之前再检查版本git lg -5是否是merge demand/child into develop，推送git push origin develop:develop develop环境调试完，需要merge demand/child into demand。切到任务分支git ch demand/child，直接拉取远程demand并合并到demand/childgit pull origin demand:demand/child，因为demand本来就是demand/child的源，所以直接合并之后要再合并回去git push origin demand/child:demand，不会有什么影响，也能将其他人的更改合并进来，如果develop也这么做的话，会将demand/child不需要的develop的内容合并进来，那就只有回退版本再重新操作了    ","date":"0001-01-01","permalink":"https://yuanyatianchi.github.io/post/it.versioncontrol.git/","tags":["it","versioncontrol"],"title":"git"},{"content":"Kafka http://kafka.apache.org/\nclient go 链接kafka使用第三方库 https://github.com/Shopify/sarama ，sarama v1.20之后的版本加入了zstd压缩算法，编译需要用到cgo，win上需要下载 http://mingw-w64.org/doku.php/download ，当然也可以使用v1.20之前的版本\ngot get -u github.com/Shopify/sarama\r /*kafka地址列表*/\rvar KAFKA_ADDR_LIST = []string{\u0026quot;127.0.0.1:9092\u0026quot;}\r /*生产者*/\rfunc Producer() {\rvar (\rconfig *sarama.Config //客户端配置\rsyncProducer sarama.SyncProducer //Producer客户端\rproducerMessage *sarama.ProducerMessage //producer消息\r)\r//构造配置\rconfig = sarama.NewConfig()\rconfig.Producer.RequiredAcks = sarama.WaitForAll //发送完数据需要leader和follow都确认\rconfig.Producer.Partitioner = sarama.NewRandomPartitioner //新选出一个partition\rconfig.Producer.Return.Successes = true //成功交付的消息将在success channel返回\r//通过配置构造producer客户端\rsyncProducer, err := sarama.NewSyncProducer(KAFKA_ADDR_LIST, config)\rif err != nil {\rfmt.Println(\u0026quot;producer closed, err:\u0026quot;, err)\rreturn\r}\rdefer syncProducer.Close()\r//构造消息\rproducerMessage = \u0026amp;sarama.ProducerMessage{\rTopic: \u0026quot;web_log\u0026quot;, //指定Topic\rValue: sarama.StringEncoder(\u0026quot;this is a test log\u0026quot;), //消息内容\r}\r//发送消息\rpid, offset, err := syncProducer.SendMessage(producerMessage)\rif err != nil {\rfmt.Println(\u0026quot;send msg failed, err:\u0026quot;, err)\rreturn\r}\rfmt.Printf(\u0026quot;pid:%v offset:%v\\n\u0026quot;, pid, offset)\r}\r /*消费者*/\rfunc Consumer() (err error) {\rvar (\rconsumer sarama.Consumer //consumer客户端\r)\r//构造consumer实例\rconsumer, err = sarama.NewConsumer(KAFKA_ADDR_LIST, nil)\rif err != nil {\rfmt.Printf(\u0026quot;fail to start consumer, err:%v\\n\u0026quot;, err)\rreturn err\r}\r//通过topic取到所有的分区\rpartitionList, err := consumer.Partitions(\u0026quot;web_log\u0026quot;)\rif err != nil {\rfmt.Printf(\u0026quot;fail to get list of partition:err%v\\n\u0026quot;, err)\rreturn\r}\rfmt.Println(partitionList)\r//遍历所有的分区\rfor partition := range partitionList {\r//针对每个分区创建一个对应的分区消费者\rpartitionConsumer, err := consumer.ConsumePartition(\u0026quot;web_log\u0026quot;, int32(partition), sarama.OffsetNewest)\rif err != nil {\rfmt.Printf(\u0026quot;failed to start consumer for partition %d,err:%v\\n\u0026quot;, partition, err)\rreturn\r}\rdefer partitionConsumer.AsyncClose()\r// 异步从每个分区消费信息\rgo func(sarama.PartitionConsumer) {\rfor msg := range partitionConsumer.Messages() {\rfmt.Printf(\u0026quot;Partition:%d Offset:%d Key:%v Value:%v\u0026quot;, msg.Partition, msg.Offset, msg.Key, msg.Value)\r}\r}(partitionConsumer)\r}\rreturn nil\r}\r hello #通过docker-compose启动\rdocker-compose up -d #默认执行./docker-compose.yml\rdocker-compose -f ./docker-compose-kafka.yml up -d #指定yml文件\r version: '2'\rservices:\rzookeeper:\rimage: wurstmeister/zookeeper\rcontainer_name: zookeeper01\rports:\r- \u0026quot;2181:2181\u0026quot;\rkafka:\rimage: wurstmeister/zookeeper\rcontainer_name: kafka01\rports:\r- \u0026quot;9092:9092\u0026quot;\renvironment:\rKAFKA_ADVERTISED_HOST_NAME: 172.16.69.166\rKAFKA_ZOOKEEPER_CONNECT: zookeeper01:2181 #这里的zookeeper由上面container_name指定\rdepends_on:\r- zookeeper01\r 3zookeeper+3kafka的docker-compose.yml\nversion: '3.4' services: zoo1: image: zookeeper:3.4 restart: always hostname: zoo1 container_name: zoo1 ports: - 2184:2181 volumes: - \u0026quot;/home/zk/workspace/volumes/zkcluster/zoo1/data:/data\u0026quot; - \u0026quot;/home/zk/workspace/volumes/zkcluster/zoo1/datalog:/datalog\u0026quot; environment: ZOO_MY_ID: 1 ZOO_SERVERS: server.1=0.0.0.0:2888:3888 server.2=zoo2:2888:3888 server.3=zoo3:2888:3888 networks: kafka: ipv4_address: 172.30.0.11 zoo2: image: zookeeper:3.4 restart: always hostname: zoo2 container_name: zoo2 ports: - 2185:2181 volumes: - \u0026quot;/home/zk/workspace/volumes/zkcluster/zoo2/data:/data\u0026quot; - \u0026quot;/home/zk/workspace/volumes/zkcluster/zoo2/datalog:/datalog\u0026quot; environment: ZOO_MY_ID: 2 ZOO_SERVERS: server.1=zoo1:2888:3888 server.2=0.0.0.0:2888:3888 server.3=zoo3:2888:3888 networks: kafka: ipv4_address: 172.30.0.12 zoo3: image: zookeeper:3.4 restart: always hostname: zoo3 container_name: zoo3 ports: - 2186:2181 volumes: - \u0026quot;/home/zk/workspace/volumes/zkcluster/zoo3/data:/data\u0026quot; - \u0026quot;/home/zk/workspace/volumes/zkcluster/zoo3/datalog:/datalog\u0026quot; environment: ZOO_MY_ID: 3 ZOO_SERVERS: server.1=zoo1:2888:3888 server.2=zoo2:2888:3888 server.3=0.0.0.0:2888:3888 networks: kafka: ipv4_address: 172.30.0.13 kafka1: image: wurstmeister/kafka restart: always hostname: kafka1 container_name: kafka1 privileged: true ports: - 9092:9092 environment: KAFKA_ADVERTISED_HOST_NAME: kafka1 KAFKA_LISTENERS: PLAINTEXT://kafka1:9092 KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://kafka1:9092 KAFKA_ADVERTISED_PORT: 9092 KAFKA_ZOOKEEPER_CONNECT: zoo1:2181,zoo2:2181,zoo3:2181 volumes: - /home/zk/workspace/volumes/kafkaCluster/kafka1/logs:/kafka networks: kafka: ipv4_address: 172.30.1.11 extra_hosts: - \u0026quot;zoo1:172.30.0.11\u0026quot; - \u0026quot;zoo2:172.30.0.12\u0026quot; - \u0026quot;zoo3:172.30.0.13\u0026quot; depends_on: - zoo1 - zoo2 - zoo3 external_links: - zoo1 - zoo2 - zoo3 kafka2: image: wurstmeister/kafka restart: always hostname: kafka2 container_name: kafka2 privileged: true ports: - 9093:9093 environment: KAFKA_ADVERTISED_HOST_NAME: kafka2 KAFKA_LISTENERS: PLAINTEXT://kafka2:9093 KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://kafka2:9093 KAFKA_ADVERTISED_PORT: 9093 KAFKA_ZOOKEEPER_CONNECT: zoo1:2181,zoo2:2181,zoo3:2181 volumes: - /home/zk/workspace/volumes/kafkaCluster/kafka2/logs:/kafka networks: kafka: ipv4_address: 172.30.1.12 extra_hosts: - \u0026quot;zoo1:172.30.0.11\u0026quot; - \u0026quot;zoo2:172.30.0.12\u0026quot; - \u0026quot;zoo3:172.30.0.13\u0026quot; depends_on: - zoo1 - zoo2 - zoo3 external_links: - zoo1 - zoo2 - zoo3 kafka3: image: wurstmeister/kafka restart: always hostname: kafka3 container_name: kafka3 privileged: true ports: - 9094:9094 environment: KAFKA_ADVERTISED_HOST_NAME: kafka3 KAFKA_LISTENERS: PLAINTEXT://kafka3:9094 KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://kafka3:9094 KAFKA_ADVERTISED_PORT: 9094 KAFKA_ZOOKEEPER_CONNECT: zoo1:2181,zoo2:2181,zoo3:2181 volumes: - /home/zk/workspace/volumes/kafkaCluster/kafka3/logs:/kafka networks: kafka: ipv4_address: 172.30.1.13 extra_hosts: - \u0026quot;zoo1:172.30.0.11\u0026quot; - \u0026quot;zoo2:172.30.0.12\u0026quot; - \u0026quot;zoo3:172.30.0.13\u0026quot; depends_on: - zoo1 - zoo2 - zoo3 external_links: - zoo1 - zoo2 - zoo3 networks: kafka: external: name: kafka\r Kafka是一个分布式的基于发布/订阅模式的消息队列(MessageQueue)，主要应用于大数据实时处理领域。\n  消息队列  解耦：允许你独立的扩展或修改两边的处理过程，只要确保它们遵守同样的接口约束。 可恢复性：系统的一部分组件失效时，不会影响到整个系统。消息队列降低了进程间的耦合度，所 以即使一个处理消息的进程挂掉，加入队列中的消息仍然可以在系统恢复后被处理。 缓冲。有助于控制和优化数据流经过系统的速度，解决生产消息和消费消息的处理速度不一致的情况。 灵活性\u0026amp;峰值处理能力。 在访问量剧增的情况下，应用仍然需要继续发挥作用，但是这样的突发流量并不常见。 如果为以能处理这类峰值访问为标准来投入资源随时待命无疑是巨大的浪费。使用消息队列 能够使关键组件顶住突发的访问压力，而不会因为突发的超负荷的请求而完全崩溃。 异步通信。 很多时候，用户不想也不需要立即处理消息。消息队列提供了异步处理机制，允许用户 把一个消息放入队列，但并不立即处理它。想向队列中放入多少消息就放多少，然后在需要 的时候再去处理它们。。 模式  点对点模式(一对一，消费者主动拉取数据，消息收到后消息清除) 消息生产者生产消息发送到Queue中,然后消息消费者从Queue中取出并且消费消息。心 消息被消费以后，queue 中不再有存储，所以消息消费者不可能消费到已经被消费的消息。. Queue支持存在多个消费者，但是对一个消息而言，只会有一个消费者可以消费。 (2)发布/订阅模式(一对多，消费者消费数据之后不会清除消息) + 消息生产者(发布)将消息发布到topic中，同时有多个消息消费者(订阅)消费该消 息。和点对点方式不同，发布到topic的消息会被所有订阅者消费。  mq主动推出。如公众号。在大数据量下容易使得消费者方崩溃或资源浪费，如生产者消息生产速度是50m/s，消费者A接收消息速度是100m/s，消费者B是10m/s，则B崩溃，A浪费 消费者主动拉取。缺点是消费者需要维护一个长轮询来查看mq中是否有新消息可以拉取。kafka就是如此。        架构：图1\n topic主题，有分区partition，每个分区（一个leader）都有备份（若干follower）。kafka集群中follower只是leader的备份，不能在同一台机器上，消息生产者和消息消费者只找leader，而不是像其它一些集群随机从leader和follower中择其一访问。 消费者可以分组，一组中的消费者是竞争关系，一个分区的一条消息只能被一个组中的一个消费者消费一次，如果一个topic有两个分区，但是订阅它的消费者只有一个，则该topic发布的一条消息到两个分区中，这一个消费者可以从两个分区中各消费一次该消息，即该消费者可以消费两次同一条消息，提高了消费能力。一个topic的分区与订阅该topic的消费者是m对n的关系，那么最好的情况是m=n 消费者消费到一半挂掉，重启需要接着上一次消费mq中的消息，那么消费者就需要记录上一次消费到mq的什么位置offset，内存中当然有一份记录以便正常运转使用，但是挂掉内存就释放了，而kafka0.9版本之前的外部存储就是保存在zookeeper中的，0.9及版本之后保存在kafka本地某个topic里面（不是本地磁盘，kafka的消息是存在本地磁盘的，缺省保留168小时即7天）。因为消费者本来就要维护与kafka集群的连接，如果offset存储在zookeeper中则还需要维护与zookeeper的连接，并且交互频率与消息拉取频率一致，太过于频繁    ","date":"0001-01-01","permalink":"https://yuanyatianchi.github.io/post/it.mq.kafka/","tags":["it","mq"],"title":"Kafka"},{"content":"Kubernetes 架构设计与实现原理 Kubernetes 为软件工程师提供了强大的容器编排能力，模糊了开发和运维之间的边界，让我们开发、管理和维护一个大型的分布式系统和项目变得更加容易\n介绍 Kubernetes 被定义成一个用于自动化部署、扩容和管理容器应用的开源系统；它将一个分布式软件的一组容器打包成一个个更容易管理和发现的逻辑单元。Kubernetes 目前是 Cloud Native Computing Foundation (CNCF) 的项目并且是很多公司管理分布式系统的解决方案。\nKubernetes 将已经打包好的应用镜像进行编排，所以如果没有容器技术的发展和微服务架构中复杂的应用关系，其实也很难找到合适的应用场景去使用，所以在这里我们会简单介绍 Kubernetes 的两大『依赖』——容器技术和微服务架构\n容器技术 Docker 已经是容器技术的事实标准了，作者在前面的文章中 Docker 核心技术与实现原理 曾经介绍过 Docker 的实现主要依赖于 Linux 的 namespace、cgroups 和 UnionFS\n它让开发者将自己的应用以及依赖打包到一个可移植的容器中，让应用程序的运行可以实现环境无关。\n我们能够通过 Docker 实现进程、网络以及挂载点和文件系统隔离的环境，并且能够对宿主机的资源进行分配，这能够让我们在同一个机器上运行多个不同的 Docker 容器，任意一个 Docker 的进程都不需要关心宿主机的依赖，都各自在镜像构建时完成依赖的安装和编译等工作，这也是为什么 Docker 是 Kubernetes 项目的一个重要依赖。\n微服务架构 如果今天的软件并不是特别复杂并且需要承载的峰值流量不是特别多，那么后端项目的部署其实也只需要在虚拟机上安装一些简单的依赖，将需要部署的项目编译后运行就可以了。\n但是随着软件变得越来越复杂，一个完整的后端服务不再是单体服务，而是由多个职责和功能不同的服务组成，服务之间复杂的拓扑关系以及单机已经无法满足的性能需求使得软件的部署和运维工作变得非常复杂，这也就使得部署和运维大型集群变成了非常迫切的需求。\n小结 Kubernetes 的出现不仅主宰了容器编排的市场，更改变了过去的运维方式，不仅将开发与运维之间边界变得更加模糊，而且让 DevOps 这一角色变得更加清晰，每一个软件工程师都可以通过 Kubernetes 来定义服务之间的拓扑关系、线上的节点个数、资源使用量并且能够快速实现水平扩容、蓝绿部署等在过去复杂的运维操作。\n设计 这一小节我们将介绍 Kubernetes 的一些设计理念，这些关键字能够帮助了解 Kubernetes 在设计时所做的一些选择：\n这里将按照顺序分别介绍声明式、显式接口、无侵入性和可移植性这几个设计的选择能够为我们带来什么\n声明式 声明式（Declarative）的编程方式一直都会被工程师们拿来与命令式（Imperative）进行对比，这两者是完全不同的编程方法。我们最常接触的其实是命令式编程，它要求我们描述为了达到某一个效果或者目标所需要完成的指令，常见的编程语言 Go、Ruby、C++ 其实都为开发者了命令式的编程方法，\n在 Kubernetes 中，我们可以直接使用 YAML 文件定义服务的拓扑结构和状态：\napiVersion: v1\rkind: Pod\rmetadata:\rname: rss-site\rlabels:\rapp: web\rspec:\rcontainers:\r- name: front-end\rimage: nginx\rports:\r- containerPort: 80\r- name: rss-reader\rimage: nickchase/rss-php-nginx:v1\rports:\r- containerPort: 88\r 这种声明式的方式能够大量地减少使用者的工作量，极大地增加开发的效率，这是因为声明式能够简化需要的代码，减少开发人员的工作，如果我们使用命令式的方式进行开发，虽然在配置上比较灵活，但是带来了更多的工作\nSELECT * FROM posts WHERE user_id = 1 AND title LIKE 'hello%';\r SQL 其实就是一种常见的声明式『编程语言』，它能够让开发者自己去指定想要的数据是什么，Kubernetes 中的 YAML 文件也有着相同的原理，我们可以告诉 Kubernetes 想要的最终状态是什么，而它会帮助我们从现有的状态进行迁移。\n如果 Kubernetes 采用命令式编程的方式提供接口，那么工程师可能就需要通过代码告诉 Kubernetes 要达到某个状态需要通过哪些操作，相比于更关注状态和结果声明式的编程方式，命令式的编程方式更强调过程。\n总而言之，Kubernetes 中声明式的 API 其实指定的是集群期望的运行状态，所以在出现任何不一致问题时，它本身都可以通过指定的 YAML 文件对线上集群进行状态的迁移，就像一个水平触发的系统，哪怕系统错过了相应的事件，最终也会根据当前的状态自动做出做合适的操作。\n显式接口 第二个 Kubernetes 的设计规范其实就是 —— 不存在内部的私有接口，所有的接口都是显示定义的，组件之间通信使用的接口对于使用者来说都是显式的，我们都可以直接调用。\n当 Kubernetes 的接口不能满足工程师的复杂需求时，我们需要利用已有的接口实现更复杂的特性，在这时 Kubernetes 的这一设计就不会成为自定义需求的障碍。\n无侵入性 为了尽可能满足用户（工程师）的需求，减少工程师的工作量与任务并增强灵活性，Kubernetes 为工程师提供了无侵入式的接入方式，每一个应用或者服务一旦被打包成了镜像就可以直接在 Kubernetes 中无缝使用，不需要修改应用程序中的任何代码。\nDocker 和 Kubernetes 就像包裹在应用程序上的两层，它们两个为应用程序提供了容器化以及编排的能力，在应用程序内部却不需要任何的修改就能够在 Docker 和 Kubernetes 集群中运行，这是 Kubernetes 在设计时选择无侵入带来最大的好处，同时无侵入的接入方式也是目前几乎所有应用程序或者服务都必须考虑的一点。\n可移植性 在微服务架构中，我们往往都会让所有处理业务的服务变成无状态的服务，以前在内存中存储的数据、Session 等缓存，现在都会放到 Redis、ETCD 等数据库中存储，微服务架构要求我们对业务进行拆分并划清服务之间的边界，所以有状态的服务往往会对架构的水平迁移带来障碍。\n然而有状态的服务其实是无可避免的，我们将每一个基础服务或者业务服务都变成了一个个只负责计算的进程，但是仍然需要有其他的进程负责存储易失的缓存和持久的数据，Kubernetes 对这种有状态的服务也提供了比较好的支持。\nPersistentVolume：Kubernetes 引入了 PersistentVolume 和 PersistentVolumeClaim 的概念用来屏蔽底层存储的差异性，目前的 Kubernetes 支持下列类型的 PersistentVolume：\n这些不同的 PersistentVolume 会被开发者声明的 PersistentVolumeClaim 分配到不同的服务中，对于上层来讲所有的服务都不需要接触 PersistentVolume，只需要直接使用 PersistentVolumeClaim 得到的卷就可以了。\n架构 架构：Kubernetes 遵循非常传统的客户端服务端架构，客户端通过 RESTful 接口或者直接使用 kubectl 与 Kubernetes 集群进行通信，这两者在实际上并没有太多的区别，后者也只是对 Kubernetes 提供的 RESTful API 进行封装并提供出来\n组成：每一个 Kubernetes 集群都由一组 Master 节点和一系列的 Worker 节点组成，其中 Master 节点主要负责存储集群的状态并为 Kubernetes 对象分配和调度资源。\nMaster 作为管理集群状态的 Master 节点，它主要负责接收客户端的请求，安排容器的执行并且运行控制循环，将集群的状态向目标状态进行迁移，Master 节点内部由三个组件构成：\nAPI Server：负责处理来自用户的请求，其主要作用就是对外提供 RESTful 的接口，包括用于查看集群状态的读请求以及改变集群状态的写请求，也是唯一一个与 etcd 集群通信的组件。\nController：管理器运行了一系列的控制器进程，这些进程会按照用户的期望状态，在后台不断地调节整个集群中的对象，当服务的状态发生了改变，控制器就会发现这个改变并且开始向目标状态迁移。\nScheduler：调度器为 Kubernetes 中运行的 Pod 选择部署的 Worker 节点，它会根据用户的需要选择最能满足请求的节点来运行 Pod，它会在每次需要调度 Pod 时执行。\nWorker 其它的 Worker 节点实现就相对比较简单了，它主要由 kubelet 和 kube-proxy 两部分组成：\nkubelet：是一个节点上的主要服务，它周期性地从 API Server 接受新的或者修改的 Pod 规范，并且保证节点上的 Pod 和其中容器的正常运行，还会保证节点会向目标状态迁移，该节点仍然会向 Master 节点发送宿主机的健康状况。\nkube-proxy：是一个运行在各个节点上的代理服务，负责宿主机的子网管理，同时也能将服务暴露给外部，其原理就是在多个隔离的网络中把请求转发给正确的 Pod 或者容器。\n实现原理 到现在，我们已经对 Kubernetes 有了一些简单的认识和了解，也大概清楚了 Kubernetes 的架构，在这一小节中我们将介绍 Kubernetes 中的一些重要概念和实现原理。\n对象 Kubernetes 对象：是系统中的持久实体，它使用这些对象来表示集群中的状态，这些对象能够描述：容器化应用，应用资源，重启/升级 策略。\n这些对象描述了哪些应用应该运行在集群中，它们请求的资源下限和上限以及重启、升级和容错的策略。每一个创建的对象其实都是我们对集群状态的改变，这些对象描述的其实就是集群的期望状态，Kubernetes 会根据我们指定的期望状态不断检查对当前的集群状态进行迁移。\ntype Deployment struct {\rmetav1.TypeMeta `json:\u0026quot;,inline\u0026quot;`\rmetav1.ObjectMeta `json:\u0026quot;metadata,omitempty\u0026quot; protobuf:\u0026quot;bytes,1,opt,name=metadata\u0026quot;`\rSpec DeploymentSpec `json:\u0026quot;spec,omitempty\u0026quot; protobuf:\u0026quot;bytes,2,opt,name=spec\u0026quot;`\rStatus DeploymentStatus `json:\u0026quot;status,omitempty\u0026quot; protobuf:\u0026quot;bytes,3,opt,name=status\u0026quot;`\r}\r 每一个对象都包含两个嵌套对象来描述规格（Spec）和状态（Status），对象的规格其实就是我们期望的目标状态，而状态描述了对象的当前状态，这部分一般由 Kubernetes 系统本身提供和管理，是我们观察集群本身的一个接口。\nPod Pod 是 Kubernetes 中最基本的概念，它也是 Kubernetes 对象模型中我们可以创建或者部署的最小并且最简单的单元。\n它将应用的容器、存储资源以及独立的网络 IP 地址等资源打包到了一起，表示一个最小的部署单元，但是每一个 Pod 中的运行的容器可能不止一个，这是因为 Pod 最开始设计时就能够在多个进程之间进行协调，构建一个高内聚的服务单元，这些容器能够共享存储和网络，非常方便地进行通信。\nController 最后要介绍的就是 Kubernetes 中的控制器，它们其实是用于创建和管理 Pod 的实例，能够在集群的层级提供复制、发布以及健康检查的功能，这些控制器其实都运行在 Kubernetes 集群的主节点上。\n在 Kubernetes 的 kubernetes/pkg/controller/ 目录中包含了官方提供的一些常见控制器，我们可以通过下面这个函数看到所有需要运行的控制器：\nfunc NewControllerInitializers(loopMode ControllerLoopMode) map[string]InitFunc {\rcontrollers := map[string]InitFunc{}\rcontrollers[\u0026quot;endpoint\u0026quot;] = startEndpointController\rcontrollers[\u0026quot;replicationcontroller\u0026quot;] = startReplicationController\rcontrollers[\u0026quot;podgc\u0026quot;] = startPodGCController\rcontrollers[\u0026quot;resourcequota\u0026quot;] = startResourceQuotaController\rcontrollers[\u0026quot;namespace\u0026quot;] = startNamespaceController\rcontrollers[\u0026quot;serviceaccount\u0026quot;] = startServiceAccountController\rcontrollers[\u0026quot;garbagecollector\u0026quot;] = startGarbageCollectorController\rcontrollers[\u0026quot;daemonset\u0026quot;] = startDaemonSetController\rcontrollers[\u0026quot;job\u0026quot;] = startJobController\rcontrollers[\u0026quot;deployment\u0026quot;] = startDeploymentController\rcontrollers[\u0026quot;replicaset\u0026quot;] = startReplicaSetController\rcontrollers[\u0026quot;horizontalpodautoscaling\u0026quot;] = startHPAController\rcontrollers[\u0026quot;disruption\u0026quot;] = startDisruptionController\rcontrollers[\u0026quot;statefulset\u0026quot;] = startStatefulSetController\rcontrollers[\u0026quot;cronjob\u0026quot;] = startCronJobController\r// ...\rreturn controllers\r}\r 这些控制器会随着控制器管理器的启动而运行，它们会监听集群状态的变更来调整集群中的 Kubernetes 对象的状态，在后面的文章中我们会展开介绍一些常见控制器的实现原理。\n总结 作为 Kubernetes 系列文章的开篇，我们已经了解了它出现的背景、依赖的关键技术，同时我们也介绍了 Kubernetes 的架构设计，主节点负责处理客户端的请求、节点的调度，最后我们提到了几个 Kubernetes 中非常重要的概念：对象、Pod 和控制器，在接下来的文章中我们会深入介绍 Kubernetes 的实现原理。\nKubernetes 对象 上一篇文章中，我们其实介绍了 Kubernetes 的对象其实就是系统中持久化的实体，Kubernetes 用这些实体来表示集群中的状态，它们描述了集群中运行的容器化应用以及这些对象占用的资源和行为。\n不过当我们想要了解 Kubernetes 的实现原理时，绕不开的其实就是 Kubernetes 中的对象，而在 Kubernetes 中，规格（Spec）和状态（Status）是用于描述 Kubernetes 对象的两个最重要的嵌套对象，在这篇文章中会重点介绍对象的规格和状态的使用方式和实现原理。\n简介 在真正展开介绍对象的规格和状态之前，我们首先需要介绍 Kubernetes 中所有对象都有的三个字段 apiVersion、kind 和 metadata，我们从一个常见的对象描述文件来展开介绍：\napiVersion: v1\rkind: Pod\rmetadata:\rname: nginx\rlabels:\rname: nginx\rspec:\r# ...\r YAML\nAPI 组和类型 apiVersion 和 kind 共同决定了当前的 YAML 配置文件应该由谁来进行处理，前者表示描述文件使用的 API 组，后者表示一个 API 组中的一个资源类型，这里的 v1 和 Pod 表示的就是核心 API 中组 api/v1 中的 Pod 类型对象。\n除了一些 Kubernetes 的核心 API 组和资源之外，还有一些 Kubernetes 官方提供的扩展 API 组 apis/batch/v1、apis/extensions/v1beta1 等等，除此之外，我们也可以通过 CustomResourceDefinition 或者实现 apiserver 来定义新的对象类型。\n元数据 apiVersion 和 kind 描述了当前对象的一些最根本信息，而 metadata 能够为我们提供一些用于唯一识别对象的数据，包括在虚拟集群 namespace 中唯一的 name 字段，用于组织和分类的 labels 以及用于扩展功能的注解 annotations。\ntype ObjectMeta struct {\rName string\rNamespace string\rLabels map[string]string\rAnnotations map[string]string\r// ...\r}\r Go\n上述的结构体嵌入在 Kubernetes 的每一个对象中，为所有的对象提供类似命名、命名空间、标签和注解等最基本的支持，让开发者能够更好地管理 Kubernetes 集群。\n标签和选择器 每一个 Kubernetes 对象都可以被打上多个标签，这些标签可以作为过滤项帮助我们选择和管理集群内部的 Kubernetes 对象，下面的命令可以获取到生产环境中的所有前端项目：\nkubectl get pods -l environment=production,tier=frontend\r Bash\n除了使用 kubectl 直接与 Kubernetes 集群通信获取其中的对象信息之外，我们也可以在 YAML 文件中使用选择器来达到相同的效果：\nselector:\rmatchLabels:\renvironment: production\rtier: frontend\r YAML\n标签的主要作用就是对 Kubernetes 对象进行分类，这里我们可以简单给一些常见的分类方法：\n这些标签能够帮助我们在复杂的集群中快速选择一系列的 Kubernetes 对象，用好标签能够为管理集群带来非常大的帮助。\n命名空间 Kubernetes 支持通过命名空间在一个物理集群中划分出多个虚拟集群，这些虚拟集群就是单独的命名空间。不同的命名空间可以对资源进行隔离，它们没有办法直接使用 name 访问其他命名空间的服务，而是需要使用 FQDN(fully qualified domain name)。\n也就是说当我们创建一个 Service 时，它实际上在集群中加入了类似 \u0026lt;service\u0026gt;.\u0026lt;namespace\u0026gt;.svc.cluster.local 的 DNS 记录，在同一个命名空间中，我们可以直接使用 service 来访问目标的服务，但是在访问其他命名空间中的服务时却没有办法这么做。\n对象接口 Kubernetes 中的对象其实并不是 struct，而是一个 interface，其中定义了一系列的 Getter/Setter 接口：\ntype Object interface {\rGetNamespace() string\rSetNamespace(namespace string)\rGetName() string\rSetName(name string)\rGetGenerateName() string\rSetGenerateName(name string)\rGetUID() types.UID\rSetUID(uid types.UID)\r// ...\r}\r Go\n这些 Getter/Setter 接口获取的字段基本都是 ObjectMeta 结构体中定义的一些字段，这也是为什么 Kubernetes 对象都需要嵌入一个 ObjectMeta 结构体。\nSpec 对象的规格（Spec）描述了某一个实体的期望状态，每一个对象的 Spec 基本都是由开发或者维护当前对象的工程师指定的，以下面的 busybox 举例：\napiVersion: v1\rkind: Pod\rmetadata:\rname: busybox\rlabels:\rapp: busybox\rspec:\rcontainers:\r- image: busybox\rcommand:\r- sleep\r- \u0026quot;3600\u0026quot;\rimagePullPolicy: IfNotPresent\rname: busybox\rrestartPolicy: Always\r YAML\n作为一个 Pod 对象，它其实就是一个在 Kubernetes 中运行的最小、最简单的单元，所以它的 Spec 声明的就是其中包含的容器以及容器的镜像、启动命令等信息。\n但是另一种对象 Service 就有着完全不同的 Spec 参数，作为一个一组 Pod 访问方式的抽象，它需要指定流量转发的 Pod 以及目前的端口号：\nkind: Service\rapiVersion: v1\rmetadata:\rname: nginx\rspec:\rselector:\rapp: nginx\rports:\r- protocol: TCP\rport: 80\rtargetPort: 80\r YAML\n我们可以看出，不同的 Kubernetes 对象基本上有着完全不同的 Spec，接下来我们按照 Kubernetes 项目中的源代码分别介绍如何描述几种不同的 Kubernetes 对象。\nPod 作为一个 Kubernetes 对象，结构体 Pod 中嵌入了 metav1.TypeMeta 和 metav1.ObjectMeta 两个结构，这两个结构体中包含了 Object 接口中需要的函数：\ntype Pod struct {\rmetav1.TypeMeta `json:\u0026quot;,inline\u0026quot;`\rmetav1.ObjectMeta `json:\u0026quot;metadata,omitempty\u0026quot; protobuf:\u0026quot;bytes,1,opt,name=metadata\u0026quot;`\rSpec PodSpec `json:\u0026quot;spec,omitempty\u0026quot; protobuf:\u0026quot;bytes,2,opt,name=spec\u0026quot;`\rStatus PodStatus `json:\u0026quot;status,omitempty\u0026quot; protobuf:\u0026quot;bytes,3,opt,name=status\u0026quot;`\r}\rtype PodSpec struct {\rInitContainers []Container `json:\u0026quot;initContainers,omitempty\u0026quot; patchStrategy:\u0026quot;merge\u0026quot; patchMergeKey:\u0026quot;name\u0026quot; protobuf:\u0026quot;bytes,20,rep,name=initContainers\u0026quot;`\rContainers []Container `json:\u0026quot;containers\u0026quot; patchStrategy:\u0026quot;merge\u0026quot; patchMergeKey:\u0026quot;name\u0026quot; protobuf:\u0026quot;bytes,2,rep,name=containers\u0026quot;`\rRestartPolicy RestartPolicy `json:\u0026quot;restartPolicy,omitempty\u0026quot; protobuf:\u0026quot;bytes,3,opt,name=restartPolicy,casttype=RestartPolicy\u0026quot;`\r// ...\r}\r Go\nPod 结构体中的 PodSpec 就是我们在 YAML 文件中定义的嵌套对象了，由于该结构体非常复杂（加上注释有 180 行），在这一节中我们只会简单介绍其中的几个字段。\nInitContainers 是当前 Pod 启动时需要首先执行的一系列容器，这些容器没有生命周期，也没有探针，它们的主要作用就是在容器启动时进行一些资源和依赖的初始化配置；接下来的 Containers 数组就是 Pod 正常运行时包含的一系列容器了，这些容器会共享网络和进程，运行在同一个『虚拟机』上，所以也可以相互通信。\n最后的 RestartPolicy 其实就整个 Pod 的重启策略，我们可以选择不重启 Never、在出现错误时重启 OnFailure 或者总是重启 Always。\nService Kubernetes 中另一个常见对象 Service 的规格就有很大的不同了，虽然他们两者有着完全相同的嵌入结构 metav1.TypeMeta 和 metav1.ObjectMeta 以及字段 Spec 和 Status，但是它们的规格和状态却完全不同：\ntype Service struct {\rmetav1.TypeMeta `json:\u0026quot;,inline\u0026quot;`\rmetav1.ObjectMeta `json:\u0026quot;metadata,omitempty\u0026quot; protobuf:\u0026quot;bytes,1,opt,name=metadata\u0026quot;`\rSpec ServiceSpec `json:\u0026quot;spec,omitempty\u0026quot; protobuf:\u0026quot;bytes,2,opt,name=spec\u0026quot;`\rStatus ServiceStatus `json:\u0026quot;status,omitempty\u0026quot; protobuf:\u0026quot;bytes,3,opt,name=status\u0026quot;`\r}\rtype ServiceSpec struct {\rPorts []ServicePort `json:\u0026quot;ports,omitempty\u0026quot; patchStrategy:\u0026quot;merge\u0026quot; patchMergeKey:\u0026quot;port\u0026quot; protobuf:\u0026quot;bytes,1,rep,name=ports\u0026quot;`\rSelector map[string]string `json:\u0026quot;selector,omitempty\u0026quot; protobuf:\u0026quot;bytes,2,rep,name=selector\u0026quot;`\r// ...\r}\r Go\nService 都会通过选择器 Selector 将流量导入对应的 Pod 的指定 Ports 端口，这两个字段也是使用 Service 时最常用的两个字段，前者能够根据 Pod 的标签选择 Service 背后的一组 Pod，而 Ports 会将端口的流量转发到目标 Pod 的指定端口上。\n小结 Kubernetes 中对象的 Spec 其实描述了对象的期望状态，也就是工程师直接指定运行在 Kubernetes 集群中对象的表现和行为，同时 Kubernetes 会通过控制器不断帮助对象向期望状态迁移。\nStatus 对于很多使用 Kubernetes 的工程师来说，它们都会对对象的 Spec 比较了解，但是很多人都不太会了解对象的状态（Status）；对象的 Spec 是工程师向 Kubernetes 描述期望的集群状态，而 Status 其实就是 Kubernetes 集群对外暴露集群内对象运行状态的一个接口：\napiVersion: v1\rkind: Pod\rmetadata:\r// ...\rspec:\r// ...\rstatus:\rconditions:\r- lastProbeTime: null\rlastTransitionTime: 2018-12-09T02:40:37Z\rstatus: \u0026quot;True\u0026quot;\rtype: Initialized\r- lastProbeTime: null\rlastTransitionTime: 2018-12-09T02:40:38Z\rstatus: \u0026quot;True\u0026quot;\rtype: Ready\r- lastProbeTime: null\rlastTransitionTime: 2018-12-09T02:40:33Z\rstatus: \u0026quot;True\u0026quot;\rtype: PodScheduled\rcontainerStatuses:\r- containerID: docker://99f668a89db97342d7bd603471dfad5be262d7708b48cb6c5c8e374e9a13cf4f\rimage: busybox:latest\rimageID: docker-pullable://busybox@sha256:915f390a8912e16d4beb8689720a17348f3f6d1a7b659697df850ab625ea29d5\rlastState: {}\rname: busybox\rready: true\rrestartCount: 0\rstate:\rrunning:\rstartedAt: 2018-12-09T02:40:37Z\rhostIP: 10.128.0.18\rphase: Running\rpodIP: 10.4.0.28\rqosClass: Burstable\rstartTime: 2018-12-09T02:40:33Z\r YAML\n当我们将对象运行到 Kubernetes 集群中时，Kubernetes 会将 Pod 的运行信息展示到 Status 上，接下来我们分别介绍 Pod 和 Service 的 Status 都包含哪些数据。\nPod 每一个 Pod 的 Status 其实包含了阶段、当前服务状态、宿主机和 Pod IP 地址以及其中内部所有容器的状态信息：\ntype PodStatus struct {\rPhase PodPhase `json:\u0026quot;phase,omitempty\u0026quot; protobuf:\u0026quot;bytes,1,opt,name=phase,casttype=PodPhase\u0026quot;`\rConditions []PodCondition `json:\u0026quot;conditions,omitempty\u0026quot; patchStrategy:\u0026quot;merge\u0026quot; patchMergeKey:\u0026quot;type\u0026quot; protobuf:\u0026quot;bytes,2,rep,name=conditions\u0026quot;`\rMessage string `json:\u0026quot;message,omitempty\u0026quot; protobuf:\u0026quot;bytes,3,opt,name=message\u0026quot;`\rReason string `json:\u0026quot;reason,omitempty\u0026quot; protobuf:\u0026quot;bytes,4,opt,name=reason\u0026quot;`\rHostIP string `json:\u0026quot;hostIP,omitempty\u0026quot; protobuf:\u0026quot;bytes,5,opt,name=hostIP\u0026quot;`\rPodIP string `json:\u0026quot;podIP,omitempty\u0026quot; protobuf:\u0026quot;bytes,6,opt,name=podIP\u0026quot;`\rStartTime *metav1.Time `json:\u0026quot;startTime,omitempty\u0026quot; protobuf:\u0026quot;bytes,7,opt,name=startTime\u0026quot;`\rInitContainerStatuses []ContainerStatus `json:\u0026quot;initContainerStatuses,omitempty\u0026quot; protobuf:\u0026quot;bytes,10,rep,name=initContainerStatuses\u0026quot;`\rContainerStatuses []ContainerStatus `json:\u0026quot;containerStatuses,omitempty\u0026quot; protobuf:\u0026quot;bytes,8,rep,name=containerStatuses\u0026quot;`\r// ...\r}\r Go\n上述的信息中，PodCondition 数组包含了一系列关于当前 Pod 状态的详情，其中包含了 Pod 处于当前状态的类型和原因以及 Kubernetes 获取该信息的时间；而另一个比较重要的 ContainerStatus 结构体中包含了容器的镜像、重启次数等信息。\nService Service 的状态其实就更加的简单了，只有在当前的 Service 类型是 LoadBalancer 的时候 Status 字段才不会为空：\ntype ServiceStatus struct {\rLoadBalancer LoadBalancerStatus `json:\u0026quot;loadBalancer,omitempty\u0026quot; protobuf:\u0026quot;bytes,1,opt,name=loadBalancer\u0026quot;`\r}\rtype LoadBalancerStatus struct {\rIngress []LoadBalancerIngress `json:\u0026quot;ingress,omitempty\u0026quot; protobuf:\u0026quot;bytes,1,rep,name=ingress\u0026quot;`\r}\rtype LoadBalancerIngress struct {\rIP string `json:\u0026quot;ip,omitempty\u0026quot; protobuf:\u0026quot;bytes,1,opt,name=ip\u0026quot;`\rHostname string `json:\u0026quot;hostname,omitempty\u0026quot; protobuf:\u0026quot;bytes,2,opt,name=hostname\u0026quot;`\r}\r Go\n其中可能会包含为当前负载均衡分配的 IP 地址或者 Hostname，并不会包含更加复杂的数据了。\n小结 Kubernetes 对象的 Status 不仅能够用来观察目前集群中对象的运行状态，还能帮助我们对集群中出现的问题进行排查以及修复，并能提供一些信息辅助优化集群中资源的利用率，我们在使用 Kubernetes 对象时也应该多多关注集群内的对象 Status 字段。\n总结 一个个 Kubernetes 对象组成了 Kubernetes 集群的期望状态，集群中的控制器会不断获取集群的运行状态与期望状态进行对比，保证集群向期望状态进行迁移，在接下来的文章中，我们会继续介绍 Kubernetes 集群是如何对常见的 Kubernetes 对象进行管理的。\nKubernetes Pod Pod、Service、Volume 和 Namespace 是 Kubernetes 集群中四大基本对象，它们能够表示系统中部署的应用、工作负载、网络和磁盘资源，共同定义了集群的状态。Kubernetes 中很多其他的资源其实只对这些基本的对象进行了组合。\nPod 是 Kubernetes 集群中能够被创建和管理的最小部署单元，想要彻底和完整的了解 Kubernetes 的实现原理，我们必须要清楚 Pod 的实现原理以及最佳实践。\n在这里，我们将分两个部分对 Pod 进行解析，第一部分主要会从概念入手介绍 Pod 中必须了解的特性，而第二部分会介绍 Pod 从创建到删除的整个生命周期内的重要事件在源码层面是如何实现的。\n概述 作为 Kubernetes 集群中的基本单元，Pod 就是最小并且最简单的 Kubernetes 对象，这个简单的对象其实就能够独立启动一个后端进程并在集群的内部为调用方提供服务。在上一篇文章 从 Kubernetes 中的对象谈起 中，我们曾经介绍过简单的 Kubernetes Pod 是如何使用 YAML 进行描述的：\napiVersion: v1\rkind: Pod\rmetadata:\rname: busybox\rlabels:\rapp: busybox\rspec:\rcontainers:\r- image: busybox\rcommand:\r- sleep\r- \u0026quot;3600\u0026quot;\rimagePullPolicy: IfNotPresent\rname: busybox\rrestartPolicy: Always\r YAML\n这个 YAML 文件描述了一个 Pod 启动时运行的容器和命令以及它的重启策略，在当前 Pod 出现错误或者执行结束后是否应该被 Kubernetes 的控制器拉起来，除了这些比较显眼的配置之外，元数据 metadata 的配置也非常重要，name 是当前对象在 Kubernetes 集群中的唯一标识符，而标签 labels 可以帮助我们快速选择对象。\n在同一个 Pod 中，有几个概念特别值得关注，首先就是容器，在 Pod 中其实可以同时运行一个或者多个容器，这些容器能够共享网络、存储以及 CPU、内存等资源。在这一小节中我们将关注 Pod 中的容器、卷和网络三大概念。\n容器 每一个 Kubernetes 的 Pod 其实都具有两种不同的容器，两种不同容器的职责其实十分清晰，一种是 InitContainer，这种容器会在 Pod 启动时运行，主要用于初始化一些配置，另一种是 Pod 在 Running 状态时内部存活的 Container，它们的主要作用是对外提供服务或者作为工作节点处理异步任务等等。\n通过对不同容器类型的命名我们也可以看出，InitContainer 会比 Container 优先启动，在 kubeGenericRuntimeManager.SyncPod 方法中会先后启动两种容器。\nfunc (m *kubeGenericRuntimeManager) SyncPod(pod *v1.Pod, _ v1.PodStatus, podStatus *kubecontainer.PodStatus, pullSecrets []v1.Secret, backOff *flowcontrol.Backoff) (result kubecontainer.PodSyncResult) {\r// Step 1: Compute sandbox and container changes.\r// Step 2: Kill the pod if the sandbox has changed.\r// Step 3: kill any running containers in this pod which are not to keep.\r// Step 4: Create a sandbox for the pod if necessary.\r// ...\r// Step 5: start the init container.\rif container := podContainerChanges.NextInitContainerToStart; container != nil {\rmsg, _ := m.startContainer(podSandboxID, podSandboxConfig, container, pod, podStatus, pullSecrets, podIP, kubecontainer.ContainerTypeInit)\r}\r// Step 6: start containers in podContainerChanges.ContainersToStart.\rfor _, idx := range podContainerChanges.ContainersToStart {\rcontainer := \u0026amp;pod.Spec.Containers[idx]\rmsg, _ := m.startContainer(podSandboxID, podSandboxConfig, container, pod, podStatus, pullSecrets, podIP, kubecontainer.ContainerTypeRegular)\r}\rreturn\r}\r Go\n通过分析私有方法 startContainer 的实现我们得出：容器的类型最终只会影响在 Debug 时创建的标签，所以对于 Kubernetes 来说两种容器的启动和执行也就只有顺序先后的不同。\n卷 每一个 Pod 中的容器是可以通过 卷（Volume） 的方式共享文件目录的，这些 Volume 能够存储持久化的数据；在当前 Pod 出现故障或者滚动更新时，对应 Volume 中的数据并不会被清除，而是会在 Pod 重启后重新挂载到期望的文件目录中：\nkubelet.go 文件中的私有方法 syncPod 会调用 WaitForAttachAndMount 方法为等待当前 Pod 启动需要的挂载文件：\nfunc (vm *volumeManager) WaitForAttachAndMount(pod *v1.Pod) error {\rexpectedVolumes := getExpectedVolumes(pod)\runiquePodName := util.GetUniquePodName(pod)\rvm.desiredStateOfWorldPopulator.ReprocessPod(uniquePodName)\rwait.PollImmediate(\rpodAttachAndMountRetryInterval,\rpodAttachAndMountTimeout,\rvm.verifyVolumesMountedFunc(uniquePodName, expectedVolumes))\rreturn nil\r}\r Go\n我们会在 后面的章节 详细地介绍 Kubernetes 中卷的创建、挂载是如何进行的，在这里我们需要知道的是卷的挂载是 Pod 启动之前必须要完成的工作：\nfunc (kl *Kubelet) syncPod(o syncPodOptions) error {\r// ...\rif !kl.podIsTerminated(pod) {\rkl.volumeManager.WaitForAttachAndMount(pod)\r}\rpullSecrets := kl.getPullSecretsForPod(pod)\rresult := kl.containerRuntime.SyncPod(pod, apiPodStatus, podStatus, pullSecrets, kl.backOff)\rkl.reasonCache.Update(pod.UID, result)\rreturn nil\r}\r Go\n在当前 Pod 的卷创建完成之后，就会调用上一节中提到的 SyncPod 公有方法继续进行同步 Pod 信息和创建、启动容器的工作。\n网络 同一个 Pod 中的多个容器会被共同分配到同一个 Host 上并且共享网络栈，也就是说这些 Pod 能够通过 localhost 互相访问到彼此的端口和服务，如果使用了相同的端口也会发生冲突，同一个 Pod 上的所有容器会连接到同一个网络设备上，这个网络设备就是由 Pod Sandbox 中的沙箱容器在 RunPodSandbox 方法中启动时创建的：\nfunc (ds *dockerService) RunPodSandbox(ctx context.Context, r *runtimeapi.RunPodSandboxRequest) (*runtimeapi.RunPodSandboxResponse, error) {\rconfig := r.GetConfig()\r// Step 1: Pull the image for the sandbox.\rimage := defaultSandboxImage\r// Step 2: Create the sandbox container.\rcreateConfig, _ := ds.makeSandboxDockerConfig(config, image)\rcreateResp, _ := ds.client.CreateContainer(*createConfig)\rresp := \u0026amp;runtimeapi.RunPodSandboxResponse{PodSandboxId: createResp.ID}\rds.setNetworkReady(createResp.ID, false)\r// Step 3: Create Sandbox Checkpoint.\rds.checkpointManager.CreateCheckpoint(createResp.ID, constructPodSandboxCheckpoint(config))\r// Step 4: Start the sandbox container.\rds.client.StartContainer(createResp.ID)\r// Step 5: Setup networking for the sandbox.\rcID := kubecontainer.BuildContainerID(runtimeName, createResp.ID)\rnetworkOptions := make(map[string]string)\rds.network.SetUpPod(config.GetMetadata().Namespace, config.GetMetadata().Name, cID, config.Annotations, networkOptions)\rreturn resp, nil\r}\r Go\n沙箱容器其实就是 pause 容器，上述方法引用的 defaultSandboxImage 其实就是官方提供的 k8s.gcr.io/pause:3.1 镜像，这里会创建沙箱镜像和检查点并启动容器。\n每一个节点上都会由 Kubernetes 的网络插件 Kubenet 创建一个基本的 cbr0 网桥并为每一个 Pod 创建 veth 虚拟网络设备，同一个 Pod 中的所有容器就会通过这个网络设备共享网络，也就是能够通过 localhost 互相访问彼此暴露的端口和服务。\n小结 Kubernetes 中的每一个 Pod 都包含多个容器，这些容器在通过 Kubernetes 创建之后就能共享网络和存储，这其实是 Pod 非常重要的特性，我们能通过这个特性构建比较复杂的服务拓扑和依赖关系。\n生命周期 想要深入理解 Pod 的实现原理，最好最快的办法就是从 Pod 的生命周期入手，通过理解 Pod 创建、重启和删除的原理我们最终就能够系统地掌握 Pod 的生命周期与核心原理。\n当 Pod 被创建之后，就会进入健康检查状态，当 Kubernetes 确定当前 Pod 已经能够接受外部的请求时，才会将流量打到新的 Pod 上并继续对外提供服务，在这期间如果发生了错误就可能会触发重启机制，在 Pod 被删除之前都会触发一个 PreStop 的钩子，其中的方法完成之后 Pod 才会被删除，接下来我们就会按照这里的顺序依次介绍 Pod 『从生到死』的过程。\n创建 Pod 的创建都是通过 SyncPod 来实现的，创建的过程大体上可以分为六个步骤：\n 计算 Pod 中沙盒和容器的变更； 强制停止 Pod 对应的沙盒； 强制停止所有不应该运行的容器； 为 Pod 创建新的沙盒； 创建 Pod 规格中指定的初始化容器； 依次创建 Pod 规格中指定的常规容器；  我们可以看到 Pod 的创建过程其实是比较简单的，首先计算 Pod 规格和沙箱的变更，然后停止可能影响这一次创建或者更新的容器，最后依次创建沙盒、初始化容器和常规容器。\nfunc (m *kubeGenericRuntimeManager) SyncPod(pod *v1.Pod, _ v1.PodStatus, podStatus *kubecontainer.PodStatus, pullSecrets []v1.Secret, backOff *flowcontrol.Backoff) (result kubecontainer.PodSyncResult) {\rpodContainerChanges := m.computePodActions(pod, podStatus)\rif podContainerChanges.CreateSandbox {\rref, _ := ref.GetReference(legacyscheme.Scheme, pod)\r}\rif podContainerChanges.KillPod {\rif podContainerChanges.CreateSandbox {\rm.purgeInitContainers(pod, podStatus)\r}\r} else {\rfor containerID, containerInfo := range podContainerChanges.ContainersToKill {\rm.killContainer(pod, containerID, containerInfo.name, containerInfo.message, nil)\t}\r}\r}\rpodSandboxID := podContainerChanges.SandboxID\rif podContainerChanges.CreateSandbox {\rpodSandboxID, _, _ = m.createPodSandbox(pod, podContainerChanges.Attempt)\r}\rpodSandboxConfig, _ := m.generatePodSandboxConfig(pod, podContainerChanges.Attempt)\rif container := podContainerChanges.NextInitContainerToStart; container != nil {\rmsg, _ := m.startContainer(podSandboxID, podSandboxConfig, container, pod, podStatus, pullSecrets, podIP, kubecontainer.ContainerTypeInit)\r}\rfor _, idx := range podContainerChanges.ContainersToStart {\rcontainer := \u0026amp;pod.Spec.Containers[idx]\rmsg, _ := m.startContainer(podSandboxID, podSandboxConfig, container, pod, podStatus, pullSecrets, podIP, kubecontainer.ContainerTypeRegular)\r}\rreturn\r}\r Go\n简化后的 SyncPod 方法的脉络非常清晰，可以很好地理解整个创建 Pod 的工作流程；而初始化容器和常规容器被调用 startContainer 来启动：\nfunc (m *kubeGenericRuntimeManager) startContainer(podSandboxID string, podSandboxConfig *runtimeapi.PodSandboxConfig, container *v1.Container, pod *v1.Pod, podStatus *kubecontainer.PodStatus, pullSecrets []v1.Secret, podIP string, containerType kubecontainer.ContainerType) (string, error) {\rimageRef, _, _ := m.imagePuller.EnsureImageExists(pod, container, pullSecrets)\r// ...\rcontainerID, _ := m.runtimeService.CreateContainer(podSandboxID, containerConfig, podSandboxConfig)\rm.internalLifecycle.PreStartContainer(pod, container, containerID)\rm.runtimeService.StartContainer(containerID)\rif container.Lifecycle != nil \u0026amp;\u0026amp; container.Lifecycle.PostStart != nil {\rkubeContainerID := kubecontainer.ContainerID{\rType: m.runtimeName,\rID: containerID,\r}\rmsg, _ := m.runner.Run(kubeContainerID, pod, container, container.Lifecycle.PostStart)\r}\rreturn \u0026quot;\u0026quot;, nil\r}\r Go\n在启动每一个容器的过程中也都按照相同的步骤进行操作：\n 通过镜像拉取器获得当前容器中使用镜像的引用； 调用远程的 runtimeService 创建容器； 调用内部的生命周期方法 PreStartContainer 为当前的容器设置分配的 CPU 等资源； 调用远程的 runtimeService 开始运行镜像； 如果当前的容器包含 PostStart 钩子就会执行该回调；  每次 SyncPod 被调用时不一定是创建新的 Pod 对象，它还会承担更新、删除和同步 Pod 规格的职能，根据输入的新规格执行相应的操作。\n健康检查 如果我们遵循 Pod 的最佳实践，其实应该尽可能地为每一个 Pod 添加 livenessProbe 和 readinessProbe 的健康检查，这两者能够为 Kubernetes 提供额外的存活信息，如果我们配置了合适的健康检查方法和规则，那么就不会出现服务未启动就被打入流量或者长时间未响应依然没有重启等问题。\n在 Pod 被创建或者被移除时，会被加入到当前节点上的 ProbeManager 中，ProbeManager 会负责这些 Pod 的健康检查：\nfunc (kl *Kubelet) HandlePodAdditions(pods []*v1.Pod) {\rstart := kl.clock.Now()\rfor _, pod := range pods {\rkl.podManager.AddPod(pod)\rkl.dispatchWork(pod, kubetypes.SyncPodCreate, mirrorPod, start)\rkl.probeManager.AddPod(pod)\r}\r}\rfunc (kl *Kubelet) HandlePodRemoves(pods []*v1.Pod) {\rstart := kl.clock.Now()\rfor _, pod := range pods {\rkl.podManager.DeletePod(pod)\rkl.deletePod(pod)\rkl.probeManager.RemovePod(pod)\r}\r}\r Go\n简化后的 HandlePodAdditions 和 HandlePodRemoves 方法非常直白，我们可以直接来看 ProbeManager 如何处理不同节点的健康检查。\n每一个新的 Pod 都会被调用 ProbeManager 的AddPod 函数，这个方法会初始化一个新的 Goroutine 并在其中运行对当前 Pod 进行健康检查：\nfunc (m *manager) AddPod(pod *v1.Pod) {\rkey := probeKey{podUID: pod.UID}\rfor _, c := range pod.Spec.Containers {\rkey.containerName = c.Name\rif c.ReadinessProbe != nil {\rkey.probeType = readiness\rw := newWorker(m, readiness, pod, c)\rm.workers[key] = w\rgo w.run()\r}\rif c.LivenessProbe != nil {\rkey.probeType = liveness\rw := newWorker(m, liveness, pod, c)\rm.workers[key] = w\rgo w.run()\r}\r}\r}\r Go\n在执行健康检查的过程中，Worker 只是负责根据当前 Pod 的状态定期触发一次 Probe，它会根据 Pod 的配置分别选择调用 Exec、HTTPGet 或 TCPSocket 三种不同的 Probe 方式：\nfunc (pb *prober) runProbe(probeType probeType, p *v1.Probe, pod *v1.Pod, status v1.PodStatus, container v1.Container, containerID kubecontainer.ContainerID) (probe.Result, string, error) {\rtimeout := time.Duration(p.TimeoutSeconds) * time.Second\rif p.Exec != nil {\rcommand := kubecontainer.ExpandContainerCommandOnlyStatic(p.Exec.Command, container.Env)\rreturn pb.exec.Probe(pb.newExecInContainer(container, containerID, command, timeout))\r}\rif p.HTTPGet != nil {\rscheme := strings.ToLower(string(p.HTTPGet.Scheme))\rhost := p.HTTPGet.Host\rport, _ := extractPort(p.HTTPGet.Port, container)\rpath := p.HTTPGet.Path\rurl := formatURL(scheme, host, port, path)\rheaders := buildHeader(p.HTTPGet.HTTPHeaders)\rif probeType == liveness {\rreturn pb.livenessHttp.Probe(url, headers, timeout)\r} else { // readiness\rreturn pb.readinessHttp.Probe(url, headers, timeout)\r}\r}\rif p.TCPSocket != nil {\rport, _ := extractPort(p.TCPSocket.Port, container)\rhost := p.TCPSocket.Host\rreturn pb.tcp.Probe(host, port, timeout)\r}\rreturn probe.Unknown, \u0026quot;\u0026quot;, fmt.Errorf(\u0026quot;Missing probe handler for %s:%s\u0026quot;, format.Pod(pod), container.Name)\r}\r Go\nKubernetes 在 Pod 启动后的 InitialDelaySeconds 时间内会等待 Pod 的启动和初始化，在这之后会开始健康检查，默认的健康检查重试次数是三次，如果健康检查正常运行返回了一个确定的结果，那么 Worker 就是记录这次的结果，在连续失败 FailureThreshold 次或者成功 SuccessThreshold 次，那么就会改变当前 Pod 的状态，这也是为了避免由于服务不稳定带来的抖动。\n删除 当 Kubelet 在 HandlePodRemoves 方法中接收到来自客户端的删除请求时，就会通过一个名为 deletePod 的私有方法中的 Channel 将这一事件传递给 PodKiller 进行处理：\nfunc (kl *Kubelet) deletePod(pod *v1.Pod) error {\rkl.podWorkers.ForgetWorker(pod.UID)\rrunningPods, _ := kl.runtimeCache.GetPods()\rrunningPod := kubecontainer.Pods(runningPods).FindPod(\u0026quot;\u0026quot;, pod.UID)\rpodPair := kubecontainer.PodPair{APIPod: pod, RunningPod: \u0026amp;runningPod}\rkl.podKillingCh \u0026lt;- \u0026amp;podPair\rreturn nil\r}\r Go\nKubelet 除了将事件通知给 PodKiller 之外，还需要将当前 Pod 对应的 Worker 从持有的 podWorkers 中删除；PodKiller 其实就是 Kubelet 持有的一个 Goroutine，它会在后台持续运行并监听来自 podKillingCh 的事件：\n经过一系列的方法调用之后，最终调用容器运行时的 killContainersWithSyncResult 方法，这个方法会同步地杀掉当前 Pod 中全部的容器：\nfunc (m *kubeGenericRuntimeManager) killContainersWithSyncResult(pod *v1.Pod, runningPod kubecontainer.Pod, gracePeriodOverride *int64) (syncResults []*kubecontainer.SyncResult) {\rcontainerResults := make(chan *kubecontainer.SyncResult, len(runningPod.Containers))\rfor _, container := range runningPod.Containers {\rgo func(container *kubecontainer.Container) {\rkillContainerResult := kubecontainer.NewSyncResult(kubecontainer.KillContainer, container.Name)\rm.killContainer(pod, container.ID, container.Name, \u0026quot;Need to kill Pod\u0026quot;, gracePeriodOverride)\rcontainerResults \u0026lt;- killContainerResult\r}(container)\r}\rclose(containerResults)\rfor containerResult := range containerResults {\rsyncResults = append(syncResults, containerResult)\r}\rreturn\r}\r Go\n对于每一个容器来说，它们在被停止之前都会先调用 PreStop 的钩子方法，让容器中的应用程序能够有时间完成一些未处理的操作，随后调用远程的服务停止运行的容器：\nfunc (m *kubeGenericRuntimeManager) killContainer(pod *v1.Pod, containerID kubecontainer.ContainerID, containerName string, reason string, gracePeriodOverride *int64) error {\rcontainerSpec := kubecontainer.GetContainerSpec(pod, containerName);\rgracePeriod := int64(minimumGracePeriodInSeconds)\rswitch {\rcase pod.DeletionGracePeriodSeconds != nil:\rgracePeriod = *pod.DeletionGracePeriodSeconds\rcase pod.Spec.TerminationGracePeriodSeconds != nil:\rgracePeriod = *pod.Spec.TerminationGracePeriodSeconds\r}\rm.executePreStopHook(pod, containerID, containerSpec, gracePeriod)\rm.internalLifecycle.PreStopContainer(containerID.ID)\rm.runtimeService.StopContainer(containerID.ID, gracePeriod)\rm.containerRefManager.ClearRef(containerID)\rreturn err\r}\r Go\n从这个简化版本的 killContainer 方法中，我们可以大致看出停止运行容器的大致逻辑，先从 Pod 的规格中计算出当前停止所需要的时间，然后运行钩子方法和内部的生命周期方法，最后将容器停止并清除引用。\n总结 在这篇文章中，我们已经介绍了 Pod 中的几个重要概念 — 容器、卷和网络以及从创建到删除整个过程是如何实现的。\nKubernetes 中 Pod 的运行和管理总是与 kubelet 以及它的组件密不可分，后面的文章中也会介绍 kubelet 究竟是什么，它在整个 Kubernetes 中扮演什么样的角色。\nKubernetes Service 在上一篇文章中，我们介绍了 Kubernetes 中 Pod 的实现原理，Pod 是 Kubernetes 中非常轻量的对象。\n集群中的每一个 Pod 都可以通过 podIP 被直接访问的，但是正如我们所看到的，Kubernetes 中的 Pod 是有生命周期的对象，尤其是被 ReplicaSet、Deployment 等对象管理的 Pod，随时都有可能由于集群的状态变化被销毁和创建。\n这也就造成了一个非常有意思的问题，当 Kubernetes 集群中的一些 Pod 需要为另外的一些 Pod 提供服务时，我们如何为提供同一功能服务的一组 Pod 建立一个抽象并追踪这组服务中节点的健康状态。\n这一个抽象在 Kubernetes 中其实就是 Service，每一个 Kubernetes 的 Service 都是一组 Pod 的逻辑集合和访问方式的抽象，我也可以把 Service 加上的一组 Pod 称作是一个微服务。\n在这篇文章中，我们将分两个部分介绍 Kubernetes 中 Service 的实现原理，在第一部分我们将介绍 Kubernetes 如何处理服务的创建，第二部分会介绍它是如何转发来自节点内部和外部的流量。\n创建服务 在 Kubernetes 中创建一个新的 Service 对象需要两大模块同时协作，其中一个模块是控制器，它需要在每次客户端创建新的 Service 对象时，生成其他用于暴露一组 Pod 的 Kubernetes 对象，也就是 Endpoint 对象；另一个模块是 kube-proxy，它运行在 Kubernetes 集群中的每一个节点上，会根据 Service 和 Endpoint 的变动改变节点上 iptables 或者 ipvs 中保存的规则。\n控制器 控制器模块其实总共有两个部分监听了 Service 变动的事件，其中一个是 ServiceController、另一个是 EndpointController，我们分别来看两者如何应对 Service 的变动。\nService\n我们可以先来看一下 ServiceController 在 Service 对象变动时发生了什么事情，每当有服务被创建或者销毁时，Informer 都会通知 ServiceController，它会将这些任务投入工作队列中并由其本身启动的 Worker 协程消费：\nsequenceDiagram\rparticipant I as Informer\rparticipant SC as ServiceController\rparticipant Q as WorkQueue\rparticipant B as Balancer\rI-\u0026gt;\u0026gt;+SC: Add/Update/DeleteService\rSC-\u0026gt;\u0026gt;Q: Add\rQ--\u0026gt;\u0026gt;SC: return\rdeactivate SC\rloop Worker\rSC-\u0026gt;\u0026gt;+Q: Get\rQ--\u0026gt;\u0026gt;-SC: key\rSC-\u0026gt;\u0026gt;SC: syncService\rSC-\u0026gt;\u0026gt;+B: EnsureLoadBalancer\rB--\u0026gt;\u0026gt;-SC: LoadBalancerStatus\rend\r Mermaid\n不过 ServiceController 其实只处理了负载均衡类型的 Service 对象，它会调用云服务商的 API 接口，不同的云服务商会实现不同的适配器来创建 LoadBalancer 类型的资源。\n我们以 GCE 为例简单介绍一下 Google Cloud 是如何对实现负载均衡类型的 Service：\nfunc (g *Cloud) EnsureLoadBalancer(ctx context.Context, clusterName string, svc *v1.Service, nodes []*v1.Node) (*v1.LoadBalancerStatus, error) {\rloadBalancerName := g.GetLoadBalancerName(ctx, clusterName, svc)\rdesiredScheme := getSvcScheme(svc)\rclusterID, _ := g.ClusterID.GetID()\rexistingFwdRule, _ := g.GetRegionForwardingRule(loadBalancerName, g.region)\rif existingFwdRule != nil {\rexistingScheme := cloud.LbScheme(strings.ToUpper(existingFwdRule.LoadBalancingScheme))\rif existingScheme != desiredScheme {\rswitch existingScheme {\rcase cloud.SchemeInternal:\rg.ensureInternalLoadBalancerDeleted(clusterName, clusterID, svc)\rdefault:\rg.ensureExternalLoadBalancerDeleted(clusterName, clusterID, svc)\r}\rexistingFwdRule = nil\r}\r}\rvar status *v1.LoadBalancerStatus\rswitch desiredScheme {\rcase cloud.SchemeInternal:\rstatus, err = g.ensureInternalLoadBalancer(clusterName, clusterID, svc, existingFwdRule, nodes)\rdefault:\rstatus, err = g.ensureExternalLoadBalancer(clusterName, clusterID, svc, existingFwdRule, nodes)\r}\rreturn status, err\r}\r Go\n上述代码会先判断是否应该先删除已经存在的负载均衡资源，随后会调用一个内部的方法 ensureExternalLoadBalancer 在 Google Cloud 上创建一个新的资源，这个方法的调用过程比较复杂：\n 检查转发规则是否存在并获取它的 IP 地址； 确定当前 LoadBalancer 使用的 IP 地址； 处理防火墙的规则的创建和更新； 创建和删除指定的健康检查；  想要了解 GCE 是如何对 LoadBalancer 进行支持的可以在 Kubernetes 中的 gce package 中阅读相关的代码，这里面就是 gce 对于云服务商特定资源的实现方式。\nEndpoint\nServiceController 主要处理的还是与 LoadBalancer 相关的逻辑，但是 EndpointController 的作用就没有这么简单了，我们在使用 Kubernetes 时虽然很少会直接与 Endpoint 资源打交道，但是它却是 Kubernetes 中非常重要的组成部分。\nEndpointController 本身并没有通过 Informer 监听 Endpoint 资源的变动，但是它却同时订阅了 Service 和 Pod 资源的增删事件，对于 Service 资源来讲，EndpointController 会通过以下的方式进行处理：\nsequenceDiagram\rparticipant I as Informer\rparticipant EC as EndpointController\rparticipant Q as WorkQueue\rparticipant PL as PodLister\rparticipant C as Client\rI-\u0026gt;\u0026gt;+EC: Add/Update/DeleteService\rEC-\u0026gt;\u0026gt;Q: Add\rQ--\u0026gt;\u0026gt;EC: return\rdeactivate EC\rloop Worker\rEC-\u0026gt;\u0026gt;+Q: Get\rQ--\u0026gt;\u0026gt;-EC: key\rEC-\u0026gt;\u0026gt;+EC: syncService\rEC-\u0026gt;\u0026gt;+PL: ListPod(service.Spec.Selector)\rPL--\u0026gt;\u0026gt;-EC: Pods\rloop Every Pod\rEC-\u0026gt;\u0026gt;EC: addEndpointSubset\rend\rEC-\u0026gt;\u0026gt;C: Create/UpdateEndpoint\rC--\u0026gt;\u0026gt;-EC: result\rend\r Mermaid\nEndpointController 中的 syncService 方法是用于创建和删除 Endpoint 资源最重要的方法，在这个方法中我们会根据 Service 对象规格中的选择器 Selector 获取集群中存在的所有 Pod，并将 Service 和 Pod 上的端口进行映射生成一个 EndpointPort 结构体：\nfunc (e *EndpointController) syncService(key string) error {\rnamespace, name, _ := cache.SplitMetaNamespaceKey(key)\rservice, _ := e.serviceLister.Services(namespace).Get(name)\rpods, _ := e.podLister.Pods(service.Namespace).List(labels.Set(service.Spec.Selector).AsSelectorPreValidated())\rsubsets := []v1.EndpointSubset{}\rfor _, pod := range pods {\repa := *podToEndpointAddress(pod)\rfor i := range service.Spec.Ports {\rservicePort := \u0026amp;service.Spec.Ports[i]\rportName := servicePort.Name\rportProto := servicePort.Protocol\rportNum, _ := podutil.FindPort(pod, servicePort)\repp := \u0026amp;v1.EndpointPort{Name: portName, Port: int32(portNum), Protocol: portProto}\rsubsets, _, _ = addEndpointSubset(subsets, pod, epa, epp, tolerateUnreadyEndpoints)\r}\r}\rsubsets = endpoints.RepackSubsets(subsets)\rcurrentEndpoints = \u0026amp;v1.Endpoints{\rObjectMeta: metav1.ObjectMeta{\rName: service.Name,\rLabels: service.Labels,\r},\r}\rnewEndpoints := currentEndpoints.DeepCopy()\rnewEndpoints.Subsets = subsets\rnewEndpoints.Labels = service.Labels\re.client.CoreV1().Endpoints(service.Namespace).Create(newEndpoints)\rreturn nil\r}\r Go\n对于每一个 Pod 都会生成一个新的 EndpointSubset，其中包含了 Pod 的 IP 地址和端口和 Service 的规格中指定的输入端口和目标端口，在最后 EndpointSubset 的数据会被重新打包并通过客户端创建一个新的 Endpoint 资源。\n在上面我们已经提到过，除了 Service 的变动会触发 Endpoint 的改变之外，Pod 对象的增删也会触发 EndpointController 中的回调函数。\nsequenceDiagram\rparticipant I as Informer\rparticipant EC as EndpointController\rparticipant Q as WorkQueue\rparticipant SL as ServiceLister\rI-\u0026gt;\u0026gt;+EC: Add/Update/DeletePod\rEC-\u0026gt;\u0026gt;+SL: GetPodServices\rSL--\u0026gt;\u0026gt;-EC: []Service\rEC-\u0026gt;\u0026gt;Q: Add\rQ--\u0026gt;\u0026gt;EC: return\rdeactivate EC\r Mermaid\ngetPodServiceMemberships 会获取跟当前 Pod 有关的 Service 对象并将所有的 Service 对象都转换成 \u0026lt;namespace\u0026gt;/\u0026lt;name\u0026gt; 的字符串：\nfunc (e *EndpointController) getPodServiceMemberships(pod *v1.Pod) (sets.String, error) {\rset := sets.String{}\rservices, _ := e.serviceLister.GetPodServices(pod)\rfor i := range services {\rkey, _ := controller.KeyFunc(services[i])\rset.Insert(key)\r}\rreturn set, nil\r}\r Go\n这些服务最后会被加入 EndpointController 的队列中，等待它持有的几个 Worker 对 Service 进行同步。\n这些其实就是 EndpointController 的作用，订阅 Pod 和 Service 对象的变更，并根据当前集群中的对象生成 Endpoint 对象将两者进行关联。\n代理 在整个集群中另一个订阅 Service 对象变动的组件就是 kube-proxy 了，每当 kube-proxy 在新的节点上启动时都会初始化一个 ServiceConfig 对象，就像介绍 iptables 代理模式时提到的，这个对象会接受 Service 的变更事件：\nsequenceDiagram\rparticipant SCT as ServiceChangeTracker\rparticipant SC as ServiceConfig\rparticipant P as Proxier\rparticipant EC as EndpointConfig\rparticipant ECT as EndpointChangeTracker\rparticipant SR as SyncRunner\rSC-\u0026gt;\u0026gt;+P: OnServiceAdd/Update/Delete/Synced\rP-\u0026gt;\u0026gt;SCT: Update\rSCT--\u0026gt;\u0026gt;P: Return ServiceMap\rdeactivate P\rEC-\u0026gt;\u0026gt;+P: OnEndpointsAdd/Update/Delete/Synced\rECT--\u0026gt;\u0026gt;P: Return EndpointMap\rP-\u0026gt;\u0026gt;ECT: Update\rdeactivate P\rloop Every minSyncPeriod ~ syncPeriod\rSR-\u0026gt;\u0026gt;P: syncProxyRules\rend\r Mermaid\n这些变更事件都会被订阅了集群中对象变动的 ServiceConfig 和 EndpointConfig 对象推送给启动的 Proxier 实例：\nfunc (c *ServiceConfig) handleAddService(obj interface{}) {\rservice, ok := obj.(*v1.Service)\rif !ok {\rreturn\r}\rfor i := range c.eventHandlers {\rc.eventHandlers[i].OnServiceAdd(service)\r}\r}\r Go\n收到事件变动的 Proxier 实例随后会根据启动时的配置更新 iptables 或者 ipvs 中的规则，这些应用最终会负责对进出的流量进行转发并完成一些负载均衡相关的任务。\n代理模式 在 Kubernetes 集群中的每一个节点都运行着一个 kube-proxy 进程，这个进程会负责监听 Kubernetes 主节点中 Service 的增加和删除事件并修改运行代理的配置，为节点内的客户端提供流量的转发和负载均衡等功能，但是当前 kube-proxy 的代理模式目前来看有三种：\n这三种代理模式中的第一种 userspace 其实就是运行在用户空间代理，所有的流量最终都会通过 kube-proxy 本身转发给其他的服务，后两种 iptable 和 ipvs 都运行在内核空间能够为 Kubernetes 集群提供更加强大的性能支持。\nuserspace 作为运行在用户空间的代理，对于每一个 Service 都会在当前的节点上开启一个端口，所有连接到当前代理端口的请求都会被转发到 Service 背后的一组 Pod 上，它其实会在节点上添加 iptables 规则，通过 iptables 将流量转发给 kube-proxy 处理。\n如果当前节点上的 kube-proxy 在启动时选择了 userspace 模式，那么每当有新的 Service 被创建时，kube-proxy 就会增加一条 iptables 记录并启动一个 Goroutine，前者用于将节点中服务对外发出的流量转发给 kube-proxy，再由后者持有的一系列 Goroutine 将流量转发到目标的 Pod 上。\n这一系列的工作大都是在 OnServiceAdd 被触发时中完成的，正如上面所说的，该方法会调用 mergeService 将传入服务 Service 的端口变成一条 iptables 的配置命令为当前节点增加一条规则，同时在 addServiceOnPort 方法中启动一个 TCP 或 UDP 的 Socket：\nfunc (proxier *Proxier) mergeService(service *v1.Service) sets.String {\rsvcName := types.NamespacedName{Namespace: service.Namespace, Name: service.Name}\rexistingPorts := sets.NewString()\rfor i := range service.Spec.Ports {\rservicePort := \u0026amp;service.Spec.Ports[i]\rserviceName := proxy.ServicePortName{NamespacedName: svcName, Port: servicePort.Name}\rexistingPorts.Insert(servicePort.Name)\rinfo, exists := proxier.getServiceInfo(serviceName)\rif exists {\rproxier.closePortal(serviceName, info)\rproxier.stopProxy(serviceName, info)\r}\rproxyPort, := proxier.proxyPorts.AllocateNext()\rserviceIP := net.ParseIP(service.Spec.ClusterIP)\rinfo, _ = proxier.addServiceOnPort(serviceName, servicePort.Protocol, proxyPort, proxier.udpIdleTimeout)\rinfo.portal.ip = serviceIP\rinfo.portal.port = int(servicePort.Port)\rinfo.externalIPs = service.Spec.ExternalIPs\rinfo.loadBalancerStatus = *service.Status.LoadBalancer.DeepCopy()\rinfo.nodePort = int(servicePort.NodePort)\rinfo.sessionAffinityType = service.Spec.SessionAffinity\rproxier.openPortal(serviceName, info)\rproxier.loadBalancer.NewService(serviceName, info.sessionAffinityType, info.stickyMaxAgeSeconds)\r}\rreturn existingPorts\r}\r Go\n这个启动的进程会监听同一个节点上，转发自所有进程的 TCP 和 UDP 请求并将这些数据包发送给目标的 Pod 对象。\n在用户空间模式中，如果一个连接被目标服务拒绝，我们的代理服务能够重新尝试连接其他的服务，除此之外用户空间模式并没有太多的优势。\niptables 另一种常见的代理模式就是直接使用 iptables 转发当前节点上的全部流量，这种脱离了用户空间在内核空间中实现转发的方式能够极大地提高 proxy 的效率，增加 kube-proxy 的吞吐量。\niptables 作为一种代理模式，它同样实现了 OnServiceUpdate、OnEndpointsUpdate 等方法，这两个方法会分别调用相应的变更追踪对象。\nsequenceDiagram\rparticipant SC as ServiceConfig\rparticipant P as Proxier\rparticipant SCT as ServiceChangeTracker\rparticipant SR as SyncRunner\rparticipant I as iptable\rSC-\u0026gt;\u0026gt;+P: OnServiceAdd\rP-\u0026gt;\u0026gt;P: OnServiceUpdate\rP-\u0026gt;\u0026gt;SCT: Update\rSCT--\u0026gt;\u0026gt;P: Return ServiceMap\rdeactivate P\rloop Every minSyncPeriod ~ syncPeriod\rSR-\u0026gt;\u0026gt;+P: syncProxyRules\rP-\u0026gt;\u0026gt;I: UpdateChain\rP-\u0026gt;\u0026gt;P: writeLine x N\rP-\u0026gt;\u0026gt;I: RestoreAll\rdeactivate P\rend\r Mermaid\n变更追踪对象会根据 Service 或 Endpoint 对象的前后变化改变 ServiceChangeTracker 本身的状态，这些变更会每隔一段时间通过一个 700 行的巨大方法 syncProxyRules 同步，在这里就不介绍这个方法的具体实现了，它的主要功能就是根据 Service 和 Endpoint 对象的变更生成一条一条的 iptables 规则，比较感兴趣的读者，可以点击 proxier.go#L640-1379 查看代码。\n当我们使用 iptables 的方式启动节点上的代理时，所有的流量都会先经过 PREROUTING 或者 OUTPUT 链，随后进入 Kubernetes 自定义的链入口 KUBE-SERVICES、单个 Service 对应的链 KUBE-SVC-XXXX 以及每个 Pod 对应的链 KUBE-SEP-XXXX，经过这些链的处理，最终才能够访问当一个服务的真实 IP 地址。\n虽然相比于用户空间来说，直接运行在内核态的 iptables 能够增加代理的吞吐量，但是当集群中的节点数量非常多时，iptables 并不能达到生产级别的可用性要求，每次对规则进行匹配时都会遍历 iptables 中的所有 Service 链。\n规则的更新也不是增量式的，当集群中的 Service 达到 5,000 个，每增加一条规则都需要耗时 11min，当集群中的 Service 达到 20,000 个时，每增加一条规则都需要消耗 5h 的时间，这也就是告诉我们在大规模集群中使用 iptables 作为代理模式是完全不可用的。\nipvs ipvs 就是用于解决在大量 Service 时，iptables 规则同步变得不可用的性能问题。与 iptables 比较像的是，ipvs 的实现虽然也基于 netfilter 的钩子函数，但是它却使用哈希表作为底层的数据结构并且工作在内核态，这也就是说 ipvs 在重定向流量和同步代理规则有着更好的性能。\n在处理 Service 的变化时，ipvs 包和 iptables 其实就有非常相似了，它们都同样使用 ServiceChangeTracker 对象来追踪变更，只是两者对于同步变更的方法 syncProxyRules 实现上有一些不同。\nsequenceDiagram\rparticipant P as Proxier\rparticipant SR as SyncRunner\rparticipant IP as ipvs\rparticipant I as iptable\rloop Every minSyncPeriod ~ syncPeriod\rSR-\u0026gt;\u0026gt;+P: syncProxyRules\rP-\u0026gt;\u0026gt;P: writeLine(iptable)\rP-\u0026gt;\u0026gt;IP: Add/UpdateVirtualServer(syncService)\rIP--\u0026gt;\u0026gt;P: result\rP-\u0026gt;\u0026gt;IP: AddRealServer(syncEndpoint)\rIP--\u0026gt;\u0026gt;P: result\rP-\u0026gt;\u0026gt;I: RestoreAll\rdeactivate P\rend\r Mermaid\n我们从 ipvs 的源代码和上述的时序图中可以看到，Kubernetes ipvs 的实现其实是依赖于 iptables 的，后者能够辅助它完成一些功能，使用 ipvs 相比 iptables 能够减少节点上的 iptables 规则数量，这也是因为 ipvs 接管了原来存储在 iptables 中的规则。\n除了能够提升性能之外，ipvs 也提供了多种类型的负载均衡算法，除了最常见的 Round-Robin 之外，还支持最小连接、目标哈希、最小延迟等算法，能够很好地提升负载均衡的效率。\n小结 三种不同的代理模式其实是一个逐渐演化的过程，从最开始运行在用户空间需要『手动』监听端口并对数据包进行转发的用户空间模式，到之后使用运行在内核空间的 iptables 模式，再到 Kubernetes 1.9 版本中出现的 ipvs 模式，几种不同的模式在大量 Service 存在时有数量级别效率差异。\n总结 Kubernetes 中的 Service 将一组 Pod 以统一的形式对外暴露成一个服务，它利用运行在内核空间的 iptables 或者 ipvs 高效地转发来自节点内部和外部的流量。除此之外，作为非常重要的 Kubernetes 对象，Service 不仅在逻辑上提供了微服务的概念，还引入 LoadBalancer 类型的 Service 无缝对接云服务商提供的复杂资源。\n理解 Kubernetes 的 Service 对象能够帮助我们梳理集群内部的网络拓扑关系，也能让我们更清楚它是如何在集群内部实现服务发现、负载均衡等功能的，在后面的文章中我们会展开介绍 kube-proxy 的作用和实现。\nKubernetes Volume 在 Kubernetes 集群中，虽然无状态的服务非常常见，但是在实际的生产中仍然会需要在集群中部署一些有状态的节点，比如一些存储中间件、消息队列等等。\n然而 Kubernetes 中的每一个容器随时都可能因为某些原因而被删除和重启，容器中的文件也会随着它的删除而丢失，所以我们需要对集群中的某些文件和数据进行『持久化』；除此之外，由于同一个 Pod 中的多个 Container 可能也会有共享文件的需求，比如通过共享文件目录的方式为 nginx 生成需要代理的静态文件，所以我们需要一种方式来解决这两个问题。\n作为 Kubernetes 集群中除了 Pod 和 Service 之外最常见的基本对象，Volume 不仅能够解决 Container 中文件的临时性问题，也能够让同一个 Pod 中的多个 Container 共享文件。\n 这篇文章并不会介绍 Kubernetes 中 Volume 的使用方法和 API，而是会着重介绍 Volume 的工作原理，包含其创建过程、多种 Volume 实现的异同以及如何与云服务提供商进行适配。\n 概述 Kubernetes 中的 Volume 种类非常多，它不仅要支持临时的、易失的磁盘文件，还需要解决持久存储的问题；第一个问题往往都比较容易解决，后者作为持久存储在很多时候都需要与云服务商提供的存储方案打交道，如果是 Kubernetes 中已经支持的存储类型倒是还好，遇到不支持的类型还是比较麻烦的。\n除了卷和持久卷之外，Kubernetes 还有另外一种更加复杂的概念 - 动态存储供应，它能够允许存储卷按需进行创建，不再需要集群的管理员手动调用云服务商提供的接口或者界面创建新的存储卷。\n集群中的每一个卷在被 Pod 使用时都会经历四个操作，也就是附着（Attach）、挂载（Mount）、卸载（Unmount）和分离（Detach）。\n如果 Pod 中使用的是 EmptyDir、HostPath 这种类型的卷，那么这些卷并不会经历附着和分离的操作，它们只会被挂载和卸载到某一个的 Pod 中，不过如果使用的云服务商提供的存储服务，这些持久卷只有附着到某一个节点之后才可以被挂在到相应的目录下，不过在其他节点使用这些卷时，该存储资源也需要先与当前的节点分离。\n卷 在这一节中提到的卷（Volume）其实是一个比较特定的概念，它并不是一个持久化存储，可能会随着 Pod 的删除而删除，常见的卷就包括 EmptyDir、HostPath、ConfigMap 和 Secret，这些卷与所属的 Pod 具有相同的生命周期，它们可以通过如下的方式挂载到 Pod 下面的某一个目录中：\napiVersion: v1\rkind: Pod\rmetadata:\rname: test-pod\rspec:\rcontainers:\r- name: test-container\rimage: k8s.gcr.io/busybox\rvolumeMounts:\r- name: cache-volume\rmountPath: /cache\r- name: test-volume\rmountPath: /hostpath\r- name: config-volume\rmountPath: /data/configmap\r- name: special-volume\rmountPath: /data/secret\rvolumes:\r- name: cache-volume\remptyDir: {}\r- name: hostpath-volume\rhostPath:\rpath: /data/hostpath\rtype: Directory\r- name: config-volume\rconfigMap:\rname: special-config\r- name: secret-volume\rsecret:\rsecretName: secret-config\r YAML\n需要注意的是，当我们将 ConfigMap 或者 Secret 『包装』成卷并挂载到某个目录时，我们其实创建了一些新的 Volume，这些 Volume 并不是 Kubernetes 中的对象，它们只存在于当前 Pod 中，随着 Pod 的删除而删除，但是需要注意的是这些『临时卷』的删除并不会导致相关 ConfigMap 或者 Secret 对象的删除。\n从上面我们其实可以看出 Volume 没有办法脱离 Pod 而生存，它与 Pod 拥有完全相同的生命周期，而且它们也不是 Kubernetes 对象，所以 Volume 的主要作用还是用于跨节点或者容器对数据进行同步和共享。\n持久卷 临时的卷没有办法解决数据持久存储的问题，想要让数据能够持久化，首先就需要将 Pod 和卷的声明周期分离，这也就是引入持久卷 PersistentVolume(PV) 的原因。我们可以将 PersistentVolume 理解为集群中资源的一种，它与集群中的节点 Node 有些相似，PV 为 Kubernete 集群提供了一个如何提供并且使用存储的抽象，与它一起被引入的另一个对象就是 PersistentVolumeClaim(PVC)，这两个对象之间的关系与节点和 Pod 之间的关系差不多：\nPersistentVolume 是集群中的一种被管理员分配的存储资源，而 PersistentVolumeClaim 表示用户对存储资源的申请，它与 Pod 非常相似，PVC 消耗了持久卷资源，而 Pod 消耗了节点上的 CPU 和内存等物理资源。\n因为 PVC 允许用户消耗抽象的存储资源，所以用户需要不同类型、属性和性能的 PV 就是一个比较常见的需求了，在这时我们可以通过 StorageClass 来提供不同种类的 PV 资源，上层用户就可以直接使用系统管理员提供好的存储类型。\n访问模式 Kubernetes 中的 PV 提供三种不同的访问模式，分别是 ReadWriteOnce、ReadOnlyMany 和 ReadWriteMany，这三种模式的含义和用法我们可以通过它们的名字推测出来：\n ReadWriteOnce 表示当前卷可以被一个节点使用读写模式挂载； ReadOnlyMany 表示当前卷可以被多个节点使用只读模式挂载； ReadWriteMany 表示当前卷可以被多个节点使用读写模式挂载；  不同的卷插件对于访问模式其实有着不同的支持，AWS 上的 AWSElasticBlockStore 和 GCP 上的 GCEPersistentDisk 就只支持 ReadWriteOnce 方式的挂载，不能同时挂载到多个节点上，但是 CephFS 就同时支持这三种访问模式。\n回收策略 当某个服务使用完某一个卷之后，它们会从 apiserver 中删除 PVC 对象，这时 Kubernetes 就需要对卷进行回收（Reclaim），持久卷也同样包含三种不同的回收策略，这三种回收策略会指导 Kubernetes 选择不同的方式对使用过的卷进行处理。\n第一种回收策略就是保留（Retain）PV 中的数据，如果希望 PV 能够被重新使用，系统管理员需要删除被使用的 PersistentVolume 对象并手动清除存储和相关存储上的数据。\n另一种常见的回收策略就是删除（Delete），当 PVC 被使用者删除之后，如果当前卷支持删除的回收策略，那么 PV 和相关的存储会被自动删除，如果当前 PV 上的数据确实不再需要，那么将回收策略设置成 Delete 能够节省手动处理的时间并快速释放无用的资源。\n存储供应 Kubernetes 集群中包含了很多的 PV 资源，而 PV 资源有两种供应的方式，一种是静态的，另一种是动态的，静态存储供应要求集群的管理员预先创建一定数量的 PV，然后使用者通过 PVC 的方式对 PV 资源的使用进行声明和申请；但是当系统管理员创建的 PV 对象不能满足使用者的需求时，就会进入动态存储供应的逻辑，供应的方式是基于集群中的 StorageClass 对象，当然这种动态供应的方式也可以通过配置进行关闭。\n管理 Volume 的创建和管理在 Kubernetes 中主要由卷管理器 VolumeManager 和 AttachDetachController 和 PVController 三个组件负责。其中卷管理器会负责卷的创建和管理的大部分工作，而 AttachDetachController 主要负责对集群中的卷进行 Attach 和 Detach，PVController 负责处理持久卷的变更，文章接下来的内容会详细介绍这几部分之间的关系、工作原理以及它们是如何协作的。\nkubelet 作者在 详解 Kubernetes Pod 的实现原理 一文中曾简单介绍过 kubelet 和 Pod 的关系，前者会负责后者的创建和管理，kubelet 中与 Pod 相关的信息都是从 apiserver 中获取的：\ngraph LR\rapiserver-.-\u0026gt;u\ru((updates))-.-\u0026gt;kubelet\rkubelet-.-\u0026gt;podWorkers\rpodWorkers-.-\u0026gt;worker1\rpodWorkers-.-\u0026gt;worker2\rstyle u fill:#fffede,stroke:#ebebb7\r Mermaid\n两者的通信会使用一个 kubetypes.PodUpdate 类型的 Channel，kubelet 从 apiserver 中获取 Pod 时也会通过字段过滤器 fields.OneTermEqualSelector(api.PodHostField, string(nodeName)) 仅选择被调度到 kubelet 所在节点上的 Pod：\nfunc NewSourceApiserver(c clientset.Interface, nodeName types.NodeName, updates chan\u0026lt;- interface{}) {\rlw := cache.NewListWatchFromClient(c.CoreV1().RESTClient(), \u0026quot;pods\u0026quot;, metav1.NamespaceAll, fields.OneTermEqualSelector(api.PodHostField, string(nodeName)))\rnewSourceApiserverFromLW(lw, updates)\r}\r Go\n所有对 Pod 的变更最终都会通知给具体的 PodWorker，这些 Worker 协程会调用 kubelet syncPod 函数完成对 Pod 的同步：\nsequenceDiagram\rparticipant PW as PodWorker\rparticipant K as Kubelet\rparticipant VL as VolumeManager\rparticipant DSOWP as DesiredStateOfWorldPopulator\rparticipant ASOW as ActualStateOfWorld\rPW-\u0026gt;\u0026gt;+K: syncPod\rK-\u0026gt;\u0026gt;+VL: WaitForAttachAndMount\rVL-xDSOWP: ReprocessPod\rloop verifyVolumesMounted\rVL-\u0026gt;\u0026gt;+ASOW: getUnmountedVolumes\rASOW--\u0026gt;\u0026gt;-VL: Volumes\rend\rVL--\u0026gt;\u0026gt;-K: Attached/Timeout\rK--\u0026gt;\u0026gt;-PW: return\r Mermaid\n在一个 100 多行的 syncPod 方法中，kubelet 会调用 WaitForAttachAndMount 方法，等待某一个 Pod 中的全部卷已经被成功地挂载：\nfunc (kl *Kubelet) syncPod(o syncPodOptions) error {\rpod := o.pod\r// ...\rif !kl.podIsTerminated(pod) {\rkl.volumeManager.WaitForAttachAndMount(pod)\r}\r// ...\rreturn nil\r}\r Go\n这个方法会将当前的 Pod 加入需要重新处理卷挂载的队列并在循环中持续调用 verifyVolumesMounted 方法来比较期望挂载的卷和实际挂载卷的区别，这个循环会等待两者变得完全相同或者超时后才会返回，当前方法的返回一般也意味着 Pod 中的全部卷已经挂载成功了。\n卷管理器 当前节点卷的管理就都是由 VolumeManager 来负责了，在 Kubernetes 集群中的每一个节点（Node）上的 kubelet 启动时都会运行一个 VolumeManager Goroutine，它会负责在当前节点上的 Pod 和 Volume 发生变动时对 Volume 进行挂载和卸载等操作。\ngraph TD\rsubgraph Node\rVolumeManager-.-\u0026gt;Kubelet\rDesiredStateOfWorldPopulator-.-\u0026gt;VolumeManager\rReconciler-.-\u0026gt;VolumeManager\rend\r Mermaid\n这个组件会在运行时启动两个 Goroutine 来管理节点中的卷，其中一个是 DesiredStateOfWorldPopulator，另一个是 Reconciler：\ngraph LR\rVM(VolumeManager)-. run .-\u0026gt;R(Reconciler)\rVM-. run .-\u0026gt;DSWP(DesiredStateOfWorldPopulator)\rDSWP-. update .-\u0026gt;DSW[DesiredStateOfWorld]\rASW[ActualStateOfWorld]-. get .-\u0026gt;DSWP\rDSW-. get .-\u0026gt;R\rR-. update .-\u0026gt;ASW\rDSWP-. getpods .-\u0026gt;PodManager\rstyle ASW fill:#fffede,stroke:#ebebb7\rstyle DSW fill:#fffede,stroke:#ebebb7\r Mermaid\n如上图所示，这里的 DesiredStateOfWorldPopulator 和 Reconciler 两个 Goroutine 会通过图中两个的 XXXStateOfWorld 状态进行通信，DesiredStateOfWorldPopulator 主要负责从 Kubernetes 节点中获取新的 Pod 对象并更新 DesiredStateOfWorld 结构；而后者会根据实际状态和当前状态的区别对当前节点的状态进行迁移，也就是通过 DesiredStateOfWorld 中状态的变更更新 ActualStateOfWorld 中的内容。\n卷管理器中的两个 Goroutine，一个根据工程师的需求更新节点的期望状态 DesiredStateOfWorld，另一个 Goroutine 保证节点向期望状态『迁移』，也就是说 DesiredStateOfWorldPopulator 是卷管理器中的生产者，而 Reconciler 是消费者，接下来我们会分别介绍这两个 Goroutine 的工作和实现。\nDesiredStateOfWorldPopulator 作为卷管理器中的消费者，DesiredStateOfWorldPopulator 会根据工程师的请求不断修改当前节点的期望状态，我们可以通过以下的时序图来了解它到底做了哪些工作：\nsequenceDiagram\rparticipant DSOWP as DesiredStateOfWorldPopulator\rparticipant ASOW as ActualStateOfWorld\rparticipant DSOW as DesiredStateOfWorld\rparticipant PM as PodManager\rparticipant VPM as VolumePluginManager\rloop populatorLoop\rDSOWP-\u0026gt;\u0026gt;+DSOWP: findAndAddNewPods\rDSOWP-\u0026gt;\u0026gt;+ASOW: GetMountedVolumes\rASOW--\u0026gt;\u0026gt;-DSOWP: mountedVolume\rDSOWP-\u0026gt;\u0026gt;+PM: GetPods\rPM--\u0026gt;\u0026gt;-DSOWP: pods\rloop Every Pod\rDSOWP-\u0026gt;\u0026gt;+DSOW: AddPodToVolume\rDSOW-\u0026gt;\u0026gt;+VPM: FindPluginBySpec\rVPM--\u0026gt;\u0026gt;-DSOW: volumePlugin\rDSOW--\u0026gt;\u0026gt;-DSOWP: volumeName\rend\rdeactivate DSOWP\rDSOWP-\u0026gt;\u0026gt;+DSOWP: findAndRemoveDeletedPods\rDSOWP-\u0026gt;\u0026gt;+DSOW: GetVolumesToMount\rDSOW--\u0026gt;\u0026gt;-DSOWP: volumeToMount\rloop Every Volume\rDSOWP-\u0026gt;\u0026gt;+PM: GetPodByUID\rPM--\u0026gt;\u0026gt;-DSOWP: pods\rDSOWP-\u0026gt;\u0026gt;DSOW: DeletePodFromVolume\rend\rdeactivate DSOWP\rend\r Mermaid\n整个 DesiredStateOfWorldPopulator 运行在一个大的循环 populatorLoop 中，当前循环会通过两个方法 findAndAddNewPods 和 findAndRemoveDeletedPods 分别获取节点中被添加的新 Pod 或者已经被删除的老 Pod，获取到 Pod 之后会根据当前的状态修改期望状态：\nfunc (dswp *desiredStateOfWorldPopulator) findAndAddNewPods() {\rmountedVolumesForPod := make(map[volumetypes.UniquePodName]map[string]cache.MountedVolume)\rprocessedVolumesForFSResize := sets.NewString()\rfor _, pod := range dswp.podManager.GetPods() {\rif dswp.isPodTerminated(pod) {\rcontinue\r}\rdswp.processPodVolumes(pod, mountedVolumesForPod, processedVolumesForFSResize)\r}\r}\r Go\n就像时序图和代码中所描述的，DesiredStateOfWorldPopulator 会从 PodManager 中获取当前节点中的 Pod，随后调用 processPodVolumes 方法为将所有的 Pod 对象加入 DesiredStateOfWorld 结构中：\nfunc (dswp *desiredStateOfWorldPopulator) processPodVolumes(pod *v1.Pod, mountedVolumesForPod map[volumetypes.UniquePodName]map[string]cache.MountedVolume, processedVolumesForFSResize sets.String) {\runiquePodName := util.GetUniquePodName(pod)\rif dswp.podPreviouslyProcessed(uniquePodName) {\rreturn\r}\rmountsMap, devicesMap := dswp.makeVolumeMap(pod.Spec.Containers)\rfor _, podVolume := range pod.Spec.Volumes {\rpvc, volumeSpec, volumeGidValue, _ := dswp.createVolumeSpec(podVolume, pod.Name, pod.Namespace, mountsMap, devicesMap)\rdswp.desiredStateOfWorld.AddPodToVolume(uniquePodName, pod, volumeSpec, podVolume.Name, volumeGidValue)\r}\rdswp.markPodProcessed(uniquePodName)\rdswp.actualStateOfWorld.MarkRemountRequired(uniquePodName)\r}\r Go\nfindAndAddNewPods 方法做的主要就是将节点中加入的新 Pod 添加到 DesiredStateOfWorld 中，而另一个方法 findAndRemoveDeletedPods 其实也做着类似的事情，它会将已经被删除的节点从 DesiredStateOfWorld 中剔除，总而言之 DesiredStateOfWorldPopulator 就是将当前节点的期望状态同步到 DesiredStateOfWorld 中，等待消费者的处理。\nReconciler VolumeManager 持有的另一个 Goroutine Reconciler 会负责对当前节点上的 Volume 进行管理，它在正常运行时会启动 reconcile 循环，在这个方法中会分三次对当前状态和期望状态不匹配的卷进行卸载、挂载等操作：\nsequenceDiagram\rparticipant R as Reconciler\rparticipant ASOW as ActualStateOfWorld\rparticipant DSOW as DesiredStateOfWorld\rparticipant OE as OperationExecutor\rloop reconcile\rR-\u0026gt;\u0026gt;+ASOW: GetMountedVolumes\ractivate R\rASOW--\u0026gt;\u0026gt;-R: MountedVolumes\rR-\u0026gt;\u0026gt;DSOW: PodExistsInVolume\rR-\u0026gt;\u0026gt;OE: UnmountVolume\rdeactivate R\rR-\u0026gt;\u0026gt;+DSOW: GetVolumesToMount\ractivate R\rDSOW--\u0026gt;\u0026gt;-R: volumeToMount\rR-\u0026gt;\u0026gt;ASOW: PodExistsInVolume\rR-\u0026gt;\u0026gt;OE: AttachVolume/MountVolume\rdeactivate R\rR-\u0026gt;\u0026gt;+ASOW: GetUnmountedVolumes\ractivate R\rR-\u0026gt;\u0026gt;DSOW: VolumeExists\rR-\u0026gt;\u0026gt;OE: UnmountDevice/DetachVolume\rdeactivate R\rend\r Mermaid\n在当前的循环中首先会保证应该被卸载但是仍然在节点中存在的卷被卸载，然后将应该挂载的卷挂载到合适的位置，最后将设备与节点分离或者卸载，所有挂载和卸载的操作都是通过 OperationExecutor 完成的，这个结构体负责调用相应的插件执行操作，我们会在文章的后面展开进行介绍。\n附着分离控制器 除了 VolumeManager 之外，另一个负责管理 Kubernetes 卷的组件就是 AttachDetachController 了，引入这个组件的目的主要是：\n 让卷的挂载和卸载能够与节点的可用性脱离；  一旦节点或者 kubelet 宕机，附着（Attach）在当前节点上的卷应该能够被分离（Detach），分离之后的卷就能够再次附着到其他节点上；   保证云服务商秘钥的安全；  如果每一个 kubelet 都需要触发卷的附着和分离逻辑，那么每一个节点都应该有操作卷的权限，但是这些权限应该只由主节点掌握，这样能够降低秘钥泄露的风险；   提高卷附着和分离部分代码的稳定性；   这些内容都是在 Kubernetes 官方项目的 GitHub issue Detailed Design for Volume Attach/Detach Controller #20262 中讨论的，想要了解 AttachDetachController 出现的原因可以阅读相关的内容。\n 每一个 AttachDetachController 其实也包含 Reconciler 和 DesiredStateOfWorldPopulator 两个组件，这两个组件虽然与 VolumeManager 中的两个组件重名，实现的功能也非常类似，与 VolumeManager 具有几乎相同的数据流向，但是这两个 Goroutine 是跑在 Kubernetes 主节点中的，所以实现上可能一些差异：\ngraph LR\rADC(AttachDetachController)-. run .-\u0026gt;R(Reconciler)\rADC-. run .-\u0026gt;DSWP(DesiredStateOfWorldPopulator)\rDSWP-. update .-\u0026gt;DSW[DesiredStateOfWorld]\rASW[ActualStateOfWorld]-. get .-\u0026gt;DSWP\rDSW-. get .-\u0026gt;R\rR-. update .-\u0026gt;ASW\rDSWP-. getpods .-\u0026gt;PodManager\rstyle ASW fill:#fffede,stroke:#ebebb7\rstyle DSW fill:#fffede,stroke:#ebebb7\r Mermaid\n首先，无论是 Reconciler 还是 DesiredStateOfWorldPopulator，它们同步的就不再只是某个节点上 Pod 的信息了，它们需要对整个集群中的 Pod 对象负责，相关数据也不再是通过 apiserver 拉取了，而是使用 podInformer 在 Pod 对象发生变更时调用相应的方法。\nDesiredStateOfWorldPopulator 作为 AttachDetachController 启动的 Goroutine，DesiredStateOfWorldPopulator 的主要作用是从当前集群的状态中获取 Pod 对象并修改 DesiredStateOfWorld 结构，与 VolumeManager 中的同名 Goroutine 起到相同的作用，作为整个链路的生产者，它们只是在实现上由于处理 Pod 范围的不同有一些区别：\nsequenceDiagram\rparticipant DSOWP as DesiredStateOfWorldPopulator\rparticipant ASOW as ActualStateOfWorld\rparticipant DSOW as DesiredStateOfWorld\rparticipant PL as PodLister\rparticipant VPM as VolumePluginManager\rloop populatorLoopFunc\rDSOWP-\u0026gt;\u0026gt;+DSOWP: findAndRemoveDeletedPods\rDSOWP-\u0026gt;\u0026gt;+DSOW: GetPodToAdd\rDSOW--\u0026gt;\u0026gt;-DSOWP: podsToAdd\rloop Every Pod\rDSOWP-\u0026gt;\u0026gt;+PL: GetPod\ralt PodNotFound\rPL--\u0026gt;\u0026gt;-DSOWP: return\rDSOWP-\u0026gt;\u0026gt;DSOW: DeletePod\relse\rend\rend\rdeactivate DSOWP\rDSOWP-\u0026gt;\u0026gt;+DSOWP: findAndAddActivePods\rDSOWP-\u0026gt;\u0026gt;+PL: List\rPL--\u0026gt;\u0026gt;-DSOWP: pods\rloop Every Pod\rDSOWP-\u0026gt;\u0026gt;+VPM: FindAttachablePluginBySpec\rVPM--\u0026gt;\u0026gt;-DSOW: attachableVolumePlugin\rDSOWP-\u0026gt;\u0026gt;+DSOW: AddPod/DeletePod\rDSOW--\u0026gt;\u0026gt;-DSOWP: volumeName\rend\rdeactivate DSOWP\rend\r Mermaid\nAttachDetachController 中的 DesiredStateOfWorldPopulator 协程就主要会先处理 Pod 的删除逻辑，添加 Pod 的逻辑都是根据 listPodsRetryDuration 的设置周期性被触发的，所以从这里我们就能看到 AttachDetachController 其实主要还是处理被删除 Pod 中 Volume 的分离工作，当节点或者 kubelet 宕机时能够将节点中的卷进行分离，保证 Pod 在其他节点重启时不会出现问题。\nReconciler 另一个用于调节当前状态与期望状态的 Goroutine 在执行它内部的循环时，也会优先处理分离卷的逻辑，后处理附着卷的工作，整个时序图与 VolumeManager 中的 Reconciler 非常相似：\nsequenceDiagram\rparticipant R as Reconciler\rparticipant ASOW as ActualStateOfWorld\rparticipant DSOW as DesiredStateOfWorld\rparticipant OE as OperationExecutor\rloop reconcile\rR-\u0026gt;\u0026gt;+ASOW: GetAttachedVolumes\ractivate R\rASOW--\u0026gt;\u0026gt;-R: attachedVolumes\rR-\u0026gt;\u0026gt;+DSOW: VolumeExists\ralt VolumeNotExists\rDSOW--\u0026gt;\u0026gt;-R: return\rR-\u0026gt;\u0026gt;OE: DetachVolume\rdeactivate R\relse\rend\rR-\u0026gt;\u0026gt;+DSOW: GetVolumesToAttach\ractivate R\rDSOW--\u0026gt;\u0026gt;-R: volumeToAttach\rR-\u0026gt;\u0026gt;+ASOW: VolumeNodeExists\ralt VolumeNotExists\rASOW--\u0026gt;\u0026gt;-R: return\rR-\u0026gt;\u0026gt;OE: AttachVolume\relse\rend\rdeactivate R\rend\r Mermaid\n这里处理的工作其实相对更少一些，Reconciler 会将期望状态中的卷与实际状态进行比较，然后分离需要分离的卷、附着需要附着的卷，逻辑非常的清晰和简单。\n持久卷控制器 作为集群中与 PV 和 PVC 打交道的控制器，持久卷控制器同时运行着三个 Goroutine 用于处理相应的逻辑，其中 Resync 协程负责从 Kubernetes 集群中同步 PV 和 PVC 的信息，而另外两个工作协程主要负消费队列中的任务：\ngraph LR\rPVC(PVController)-.-\u0026gt;R(Resync)\rPVC-.-\u0026gt;VW(VolumeWorker)\rR-. enqueue .-\u0026gt;VQ(VolumeQueue)\rR-. enqueue .-\u0026gt;CQ(ClaimQueue)\rVQ-. dequeue .-\u0026gt;VW\rCQ-. dequeue .-\u0026gt;CW\rPVC-.-\u0026gt;CW(ClaimWorker)\rstyle VQ fill:#fffede,stroke:#ebebb7\rstyle CQ fill:#fffede,stroke:#ebebb7\r Mermaid\n这两个工作协程主要负责对需要绑定或者解绑的 PV 和 PVC 进行处理，例如，当用户创建了新的 PVC 对象时，从集群中查找该 PVC 选择的 PV 并绑定到当前的 PVC 上。\nVolumeWorker VolumeWorker 协程中执行的最重要的方法其实就是 syncVolume，在这个方法中会根据当前 PV 对象的规格对 PV 和 PVC 进行绑定或者解绑：\nfunc (ctrl *PersistentVolumeController) syncVolume(volume *v1.PersistentVolume) error {\rif volume.Spec.ClaimRef == nil {\rreturn nil\r} else {\rif volume.Spec.ClaimRef.UID == \u0026quot;\u0026quot; {\rreturn nil\r}\rvar claim *v1.PersistentVolumeClaim\rclaimName := claimrefToClaimKey(volume.Spec.ClaimRef)\robj, _, _ := ctrl.claims.GetByKey(claimName)\rclaim, _ = obj.(*v1.PersistentVolumeClaim)\rif claim != nil \u0026amp;\u0026amp; claim.UID != volume.Spec.ClaimRef.UID {\rclaim = nil\r}\rif claim == nil {\rctrl.reclaimVolume(volume)\r} else if claim.Spec.VolumeName == \u0026quot;\u0026quot; {\rctrl.claimQueue.Add(claimToClaimKey(claim))\r} else if claim.Spec.VolumeName == volume.Name {\r} else {\rif metav1.HasAnnotation(volume.ObjectMeta, annDynamicallyProvisioned) \u0026amp;\u0026amp; volume.Spec.PersistentVolumeReclaimPolicy == v1.PersistentVolumeReclaimDelete {\rctrl.reclaimVolume(volume)\r} else {\rctrl.unbindVolume(volume)\r}\r}\r}\rreturn nil\r}\r Go\n如果当前 PV 没有绑定的 PVC 对象，那么这里的 reclaimVolume 可能会将当前的 PV 对象根据回收策略将其放回资源池等待重用、回收或者保留；而 unbindVolume 会删除 PV 与 PVC 之间的关系并更新 apiserver 中保存的 Kubernetes 对象数据。\nClaimWorker ClaimWorker 就是控制器用来决定如何处理一个 PVC 对象的方法了，它会在一个 PVC 对象被创建、更新或者同步时被触发，syncClaim 会根据当前对象中的注解决定调用 syncUnboundClaim 或者 syncBoundClaim 方法来处理相应的逻辑：\nfunc (ctrl *PersistentVolumeController) syncClaim(claim *v1.PersistentVolumeClaim) error {\rif !metav1.HasAnnotation(claim.ObjectMeta, annBindCompleted) {\rreturn ctrl.syncUnboundClaim(claim)\r} else {\rreturn ctrl.syncBoundClaim(claim)\r}\r}\r Go\nsyncUnboundClaim 会处理绑定没有结束的 PVC 对象，如果当前 PVC 对象没有对应合适的 PV 存在，那么就会调用 provisionClaim 尝试从集群中获取新的 PV 供应，如果能够找到 PV 对象，就会通过 bind 方法将两者绑定：\nfunc (ctrl *PersistentVolumeController) syncUnboundClaim(claim *v1.PersistentVolumeClaim) error {\rif claim.Spec.VolumeName == \u0026quot;\u0026quot; {\rdelayBinding, err := ctrl.shouldDelayBinding(claim)\rvolume, err := ctrl.volumes.findBestMatchForClaim(claim, delayBinding)\rif volume == nil {\rswitch {\rcase delayBinding:\rcase v1helper.GetPersistentVolumeClaimClass(claim) != \u0026quot;\u0026quot;:\rctrl.provisionClaim(claim)\r}\r} else {\rctrl.bind(volume, claim)\r}\r} else {\robj, found, _ := ctrl.volumes.store.GetByKey(claim.Spec.VolumeName)\rif found {\rvolume, _ := obj.(*v1.PersistentVolume)\rif volume.Spec.ClaimRef == nil {\rctrl.bind(volume, claim)\r} else if isVolumeBoundToClaim(volume, claim) {\rctrl.bind(volume, claim)\r}\r}\r}\rreturn nil\r}\r Go\n绑定的过程其实就是将 PV 和 PVC 之间建立起新的关系，更新 Spec 中的数据让两者能够通过引用 Ref 找到另一个对象并将更新后的 Kubernetes 对象存储到 apiserver 中。\n另一个用于绑定 PV 和 PVC 对象的方法就是 syncBoundClaim 了，相比于 syncUnboundClaim 方法，该方法的实现更为简单，直接从缓存中尝试获取对应的 PV 对象：\nfunc (ctrl *PersistentVolumeController) syncBoundClaim(claim *v1.PersistentVolumeClaim) error {\rif claim.Spec.VolumeName == \u0026quot;\u0026quot; {\rreturn nil\r}\robj, found, _ := ctrl.volumes.store.GetByKey(claim.Spec.VolumeName)\rif found {\rvolume, _ := obj.(*v1.PersistentVolume)\rif volume.Spec.ClaimRef == nil {\rctrl.bind(volume, claim)\r} else if volume.Spec.ClaimRef.UID == claim.UID {\rctrl.bind(volume, claim)\r}\r}\rreturn nil\r}\r Go\n如果找到了 PV 对象并且该对象没有绑定的 PVC 或者当前 PV 和 PVC 已经存在了引用就会调用 bind 方法对两者进行绑定。\n小结 无论是 VolumeWorker 还是 ClaimWorker 最终都可能会通过 apiserver 更新集群中 etcd 的数据，当然它们也会调用一些底层的插件获取新的存储供应、删除或者重用一些持久卷，我们会在下面介绍插件的工作原理。\n插件 Kubernetes 中的所有对卷的操作最终基本都是通过 OperationExecutor 来完成的，这个组件包含了用于附着、挂载、卸载和分离几个常见的操作以及对设备进行操作的一些方法：\ntype OperationExecutor interface {\rAttachVolume(volumeToAttach VolumeToAttach, actualStateOfWorld ActualStateOfWorldAttacherUpdater) error\rDetachVolume(volumeToDetach AttachedVolume, verifySafeToDetach bool, actualStateOfWorld ActualStateOfWorldAttacherUpdater) error\rMountVolume(waitForAttachTimeout time.Duration, volumeToMount VolumeToMount, actualStateOfWorld ActualStateOfWorldMounterUpdater, isRemount bool) error\rUnmountVolume(volumeToUnmount MountedVolume, actualStateOfWorld ActualStateOfWorldMounterUpdater, podsDir string) error\r// ...\r}\r Go\n实现 OperationExecutor 接口的私有结构体会通过 OperatorGenerator 来生成一个用于挂载和卸载卷的方法，并将这个方法包装在一个 GeneratedOperations 结构中，在这之后操作执行器会启动一个新的 Goroutine 用于执行生成好的方法：\ngraph LR\rOE(OperationExexutor)-. 1. 获取相关方法 .-\u0026gt;OG(OperationGenerator)\rOG-. 2. 根据 Spec 获取插件 .-\u0026gt;VM(VolumePluginManager)\rVM-. 3. 返回 VolumePlugin .-\u0026gt;OG\rOG-. 4. 构建方法 .-\u0026gt;OG\rOG-. 5. 生成一个 Operation 结构 .-\u0026gt;OE\rOE-. 6. 运行 Operation .-\u0026gt;NPO(NestedPendingOperations)\rNPO-. 7. 启动 Goroutine 运行生成的方法 .-\u0026gt;Goroutine\r Mermaid\nVolumePluginManager 和 VolumePlugin 这两个组件在整个流程中帮我们屏蔽了底层不同类型卷的实现差异，我们能直接在上层调用完全相同的接口，剩下的逻辑都由底层的插件来负责。\nKubernetes 提供了插件的概念，通过 Volume 和 Mounter 两个接口支持卷类型的扩展，作为存储提供商或者不同类型的文件系统，我们都可以通过实现以上的两个接口成为 Kubernetes 存储系统中一个新的存储类型：\ntype VolumePlugin interface {\rInit(host VolumeHost) error\rGetPluginName() string\rGetVolumeName(spec *Spec) (string, error)\rNewMounter(spec *Spec, podRef *v1.Pod, opts VolumeOptions) (Mounter, error)\r// ...\r}\rtype Mounter interface {\rVolume\rCanMount() error\rSetUp(fsGroup *int64) error\rSetUpAt(dir string, fsGroup *int64) error\rGetAttributes() Attributes\r}\r Go\n在这一节中我们将介绍几种不同卷插件的实现，包括最常见的 EmptyDir、ConfigMap、Secret 和 Google 云上的 GCEPersistentDisk，这一节会简单介绍不同卷插件的实现方式，想要了解详细实现的读者可以阅读相关的源代码。\nEmptyDir EmptyDir 是 Kubernetes 中最简单的卷了，当我们为一个 Pod 设置一个 EmptyDir 类型的卷时，其实就是在当前 Pod 对应的目录创建了一个空的文件夹，这个文件夹会随着 Pod 的删除而删除。\nfunc (ed *emptyDir) SetUpAt(dir string, fsGroup *int64) error {\red.setupDir(dir)\rvolume.SetVolumeOwnership(ed, fsGroup)\rvolumeutil.SetReady(ed.getMetaDir())\rreturn nil\r}\rfunc (ed *emptyDir) setupDir(dir string) error {\rif err := os.MkdirAll(dir, perm); err != nil {\rreturn err\r}\r// ...\rreturn nil\r}\r Go\nSetUpAt 方法其实就实现了对这种类型卷的创建工作，每当 Pod 被分配到了某个节点上，对应的文件目录就会通过 MkdirAll 方法创建，如果使用者配置了 medium 字段，也会选择使用相应的文件系统挂载到当前目录上，例如：tmpfs、nodev 等。\napiVersion: v1\rkind: Pod\rmetadata:\rname: test-pd\rspec:\rcontainers:\r- image: k8s.gcr.io/test-webserver\rname: test-container\rvolumeMounts:\r- mountPath: /cache\rname: cache-volume\rvolumes:\r- name: cache-volume\remptyDir: {}\r YAML\n我们经常会使用 EmptyDir 类型的卷在多个容器之间共享文件、充当缓存或者保留一些临时的日志，总而言之，这是一种经常被使用的卷类型。\nConfigMap 和 Secret 另一种比较常见的卷就是 ConfigMap 了，首先，ConfigMap 本身就是 Kubernetes 中常见的对象了，其中的 data 就是一个存储了从文件名到文件内容的字段，这里的 ConfigMap 对象被挂载到文件目录时就会创建一个名为 redis-config 的文件，然后将文件内容写入该文件：\napiVersion: v1\rkind: ConfigMap\rdata:\rredis-config: |\rmaxmemory 2mb\rmaxmemory-policy allkeys-lru  YAML\n在对 ConfigMap 类型的卷进行挂载时，总共需要完成三部分工作，首先从 apiserver 中获取当前 ConfigMap 对象，然后根据当前的 ConfigMap 生成一个从文件名到文件内容的键值对，最后构造一个 Writer 并执行 Write 方法写入内容：\nfunc (b *configMapVolumeMounter) SetUpAt(dir string, fsGroup *int64) error {\rconfigMap, _ := b.getConfigMap(b.pod.Namespace, b.source.Name)\rtotalBytes := totalBytes(configMap)\rpayload, _ := MakePayload(b.source.Items, configMap, b.source.DefaultMode, false)\rwriterContext := fmt.Sprintf(\u0026quot;pod %v/%v volume %v\u0026quot;, b.pod.Namespace, b.pod.Name, b.volName)\rwriter, _ := volumeutil.NewAtomicWriter(dir, writerContext)\rwriter.Write(payload)\rreturn nil\r}\r Go\n在涉及挂载的函数几个中，作者想要着重介绍的也就是在底层直接与文件系统打交道的 writePayloadToDir 方法：\nfunc (w *AtomicWriter) writePayloadToDir(payload map[string]FileProjection, dir string) error {\rfor userVisiblePath, fileProjection := range payload {\rcontent := fileProjection.Data\rmode := os.FileMode(fileProjection.Mode)\rfullPath := path.Join(dir, userVisiblePath)\rbaseDir, _ := filepath.Split(fullPath)\ros.MkdirAll(baseDir, os.ModePerm)\rioutil.WriteFile(fullPath, content, mode)\ros.Chmod(fullPath, mode)\r}\rreturn nil\r}\r Go\n这个方法使用了 os 包提供的接口完成了拼接文件名、创建相应文件目录、写入文件并且修改文件模式的工作，将 ConfigMap data 中的数据映射到了一个文件夹中，达到了让 Pod 中的容器可以直接通过文件系统获取内容的目的。\n对于另一个非常常见的卷类型 Secret，Kubernetes 其实也做了几乎完全相同的工作，也是先获取 Secret 对象，然后构建最终写入到文件的键值对，最后初始化一个 Writer 并调用它的 Write 方法，从这里我们也能看出在卷插件这一层对于 ConfigMap 和 Secret 的处理几乎完全相同，并没有出现需要对 Secret 对象中的内容进行解密的工作。\nGCEPersistentDisk 最后一个要介绍的卷与上面的几种都非常的不同，它在底层使用的是云服务商提供的网络磁盘，想要在一个节点上使用云磁盘其实总共需要两个步骤，首先是要将云磁盘附着到当前的节点上，这部分的工作其实就是由 gcePersistentDiskAttacher 完成的，每当调用 AttachDisk 方法时，最终都会执行云服务商提供的接口，将磁盘附着到相应的节点实例上：\nsequenceDiagram\rparticipant GPDA as gcePersistentDiskAttacher\rparticipant C as Cloud\rparticipant GCESM as gceServiceManager\rparticipant I as GCEInstances\rGPDA-\u0026gt;\u0026gt;+C: DiskIsAttached\ralt NotAttached\rC--\u0026gt;\u0026gt;-GPDA: return NotAttached\rGPDA-\u0026gt;\u0026gt;+C: AttachDisk\rC-\u0026gt;\u0026gt;+GCESM: AttachDiskOnCloudProvider\rGCESM-\u0026gt;\u0026gt;+I: AttachDisk\rI--\u0026gt;\u0026gt;-GCESM: return\rGCESM--\u0026gt;\u0026gt;-C: return\rC--\u0026gt;\u0026gt;-GPDA: return\relse\rend\r Mermaid\n在方法的最后会将该请求包装成一个 HTTP 的方法调用向 https://www.googleapis.com/compute/v1/projects/{project}/zones/{zone}/instances/{resourceId}/attachDisk 链接发出一个 POST 请求，这个请求会将某个 GCE 上的磁盘附着到目标实例上，详细的内容可以阅读 相关文档。\n一旦当前的磁盘被附着到了当前节点上，我们就能跟使用其他的插件一样，把磁盘挂载到某个目录上，完成从附着到挂载的全部操作。\n总结 Volume 和存储系统是 Kubernetes 非常重要的一部分，它能够帮助我们在多个容器之间共享文件，同时也能够为集群提供持久存储的功能，假如 Kubernetes 没有用于持久存储的对象，我们也很难在集群中运行有状态的服务，例如：消息队列、分布式存储等。\n对于刚刚使用 Kubernetes 的开发者来说，Volume、PV 和 PVC 确实是比较难以理解的概念，但是这却是深入使用 Kubernetes 必须要了解和掌握的，希望这篇文章能够帮助各位读者更好地理解存储系统底层的实现原理。\nKubernetes ReplicaSet Kubernetes 中的 ReplicaSet 主要的作用是维持一组 Pod 副本的运行，它的主要作用就是保证一定数量的 Pod 能够在集群中正常运行，它会持续监听这些 Pod 的运行状态，在 Pod 发生故障重启数量减少时重新运行新的 Pod 副本。\n这篇文章会介绍 ReplicaSet 的工作原理，其中包括在 Kubernetes 中是如何被创建的、如何创建并持有 Pod 并在出现问题时重启它们。\n概述 在具体介绍 ReplicaSet 的实现原理之前，我们还是会先简单介绍它的使用，与其他的 Kubernetes 对象一样，我们会在 Kubernetes 集群中使用 YAML 文件创建新的 ReplicaSet 对象，一个常见的 ReplicaSet 的定义其实是这样的：\napiVersion: apps/v1\rkind: ReplicaSet\rmetadata:\rname: frontend\rlabels:\rapp: guestbook\rtier: frontend\rspec:\rreplicas: 3\rselector:\rmatchLabels:\rtier: frontend\rtemplate:\rmetadata:\rlabels:\rtier: frontend\rspec:\rcontainers:\r- name: php-redis\rimage: gcr.io/google_samples/gb-frontend:v3\r YAML\n这里的 YAML 文件除了常见的 apiVersion、kind 和 metadata 属性之外，规格中总共包含三部分重要内容，也就是 Pod 副本数目 replicas、选择器 selector 和 Pod 模板 template，这三个部分共同定义了 ReplicaSet 的规格：\n同一个 ReplicaSet 会使用选择器 selector 中的定义查找集群中自己持有的 Pod 对象，它们会根据标签的匹配获取能够获得的 Pod，下面就是持有三个 Pod 对象的 Replica 拓扑图：\ngraph TD\rReplicaSet-.-\u0026gt;Pod1\rReplicaSet-.-\u0026gt;Pod2\rReplicaSet-.-\u0026gt;Pod3\r Mermaid\n被 ReplicaSet 持有的 Pod 有一个 metadata.ownerReferences 指针指向当前的 ReplicaSet，表示当前 Pod 的所有者，这个引用主要会被集群中的 垃圾收集器 使用以清理失去所有者的 Pod 对象。\n实现原理 所有 ReplicaSet 对象的增删改查都是由 ReplicaSetController 控制器完成的，该控制器会通过 Informer 监听 ReplicaSet 和 Pod 的变更事件并将其加入持有的待处理队列:\ngraph TD\rPI[PodInformer]-. Event .-\u0026gt;RSC[ReplicaSetController]\rRSI[ReplicaSetInformer]-. Event .-\u0026gt;RSC\rRSC--\u0026gt;worker1\rRSC-. Add ReplicaSet .-\u0026gt;queue\rworker1-. Loop .-\u0026gt;worker1\rqueue-. Get ReplicaSet .-\u0026gt;worker1\rRSC--\u0026gt;worker2\rworker2-. Loop .-\u0026gt;worker2\rqueue-. Get ReplicaSet .-\u0026gt;worker2\rstyle worker1 fill:#fffede,stroke:#ebebb7\rstyle worker2 fill:#fffede,stroke:#ebebb7\r Mermaid\nReplicaSetController 中的 queue 其实就是一个存储待处理 ReplicaSet 的『对象池』，它运行的几个 Goroutine 会从队列中取出最新的数据进行处理，上图展示了事件从发生到被处理的流向，我们接下来将分别介绍 ReplicaSet 中常见的同步过程。\n同步 ReplicaSetController 启动的多个 Goroutine 会从队列中取出待处理的任务，然后调用 syncReplicaSet 进行同步，这个方法会按照传入的 key 从 etcd 中取出 ReplicaSet 对象，然后取出全部 Active 的 Pod：\nfunc (rsc *ReplicaSetController) syncReplicaSet(key string) error {\rnamespace, name, _ := cache.SplitMetaNamespaceKey(key)\rrs, _ := rsc.rsLister.ReplicaSets(namespace).Get(name)\rrsNeedsSync := rsc.expectations.SatisfiedExpectations(key)\rselector, _ := metav1.LabelSelectorAsSelector(rs.Spec.Selector)\rallPods, _ := rsc.podLister.Pods(rs.Namespace).List(labels.Everything())\rfilteredPods := controller.FilterActivePods(allPods)\rfilteredPods, _ = rsc.claimPods(rs, selector, filteredPods)\rvar manageReplicasErr error\rif rsNeedsSync \u0026amp;\u0026amp; rs.DeletionTimestamp == nil {\rmanageReplicasErr = rsc.manageReplicas(filteredPods, rs)\r}\rnewStatus := calculateStatus(rs, filteredPods, manageReplicasErr)\rupdatedRS, _ := updateReplicaSetStatus(rsc.kubeClient.AppsV1().ReplicaSets(rs.Namespace), rs, newStatus)\rreturn manageReplicasErr\r}\r Go\n随后执行的 ClaimPods 方法会获取一系列 Pod 的所有权，如果当前的 Pod 与 ReplicaSet 的选择器匹配就会建立从属关系，否则就会释放持有的对象，或者直接忽视无关的 Pod，建立和释放关系的方法就是 AdoptPod 和 ReleasePod，AdoptPod 会设置目标对象的 metadata.OwnerReferences 字段：\n// AdoptPod\r{\r\u0026quot;metadata\u0026quot;: {\r\u0026quot;ownerReferences\u0026quot;: [\r{\r\u0026quot;apiVersion\u0026quot;: m.controllerKind.GroupVersion(),\r\u0026quot;kind\u0026quot;: m.controllerKind.Kind,\r\u0026quot;name\u0026quot;: m.Controller.GetName(),\r\u0026quot;uid\u0026quot;: m.Controller.GetUID(),\r\u0026quot;controller\u0026quot;: true,\r\u0026quot;blockOwnerDeletion\u0026quot;: true\r}\r],\r\u0026quot;uid\u0026quot;: pod.UID\r}\r}\r JSON\n而 ReleasePod 会使用如下的 JSON 数据删除目标 Pod 中的 metadata.OwnerReferences 属性：\n// ReleasePod\r{\r\u0026quot;metadata\u0026quot;: {\r\u0026quot;ownerReferences\u0026quot;: [\r{\r\u0026quot;$patch\u0026quot;:\u0026quot;delete\u0026quot;,\r\u0026quot;uid\u0026quot;: m.Controller.GetUID()\r}\r],\r\u0026quot;uid\u0026quot;: pod.UID\r}\r}\r JSON\n无论是建立还是释放从属关系，都是根据 ReplicaSet 的选择器配置进行的，它们根据匹配的标签执行不同的操作。\n在对已经存在 Pod 进行更新之后，manageReplicas 方法会检查并更新当前 ReplicaSet 持有的副本，如果已经存在的 Pod 数量小于 ReplicaSet 的期望数量，那么就会根据模板的配置创建一些新的 Pod 并与这些 Pod 建立从属关系，创建使用 slowStartBatch 方法分组批量创建 Pod 以减少失败的次数：\nfunc (rsc *ReplicaSetController) manageReplicas(filteredPods []*v1.Pod, rs *apps.ReplicaSet) error {\rdiff := len(filteredPods) - int(*(rs.Spec.Replicas))\rrsKey, _ := controller.KeyFunc(rs)\rif diff \u0026lt; 0 {\rdiff *= -1\rif diff \u0026gt; rsc.burstReplicas {\rdiff = rsc.burstReplicas\r}\rsuccessfulCreations, err := slowStartBatch(diff, controller.SlowStartInitialBatchSize, func() error {\rboolPtr := func(b bool) *bool { return \u0026amp;b }\rcontrollerRef := \u0026amp;metav1.OwnerReference{\rAPIVersion: rsc.GroupVersion().String(),\rKind: rsc.Kind,\rName: rs.Name,\rUID: rs.UID,\rBlockOwnerDeletion: boolPtr(true),\rController: boolPtr(true),\r}\rrsc.podControl.CreatePodsWithControllerRef(rs.Namespace, \u0026amp;rs.Spec.Template, rs, controllerRef)\rreturn nil\r})\rreturn err\r Go\n删除 Pod 的方式就是并发进行的了，代码使用 WaitGroup 等待全部的删除任务运行结束才会返回：\n} else if diff \u0026gt; 0 {\rif diff \u0026gt; rsc.burstReplicas {\rdiff = rsc.burstReplicas\r}\rpodsToDelete := getPodsToDelete(filteredPods, diff)\rvar wg sync.WaitGroup\rwg.Add(diff)\rfor _, pod := range podsToDelete {\rgo func(targetPod *v1.Pod) {\rdefer wg.Done()\rrsc.podControl.DeletePod(rs.Namespace, targetPod.Name, rs)\t}\r}(pod)\r}\rwg.Wait()\r}\rreturn nil\r}\r Go\n如果需要删除全部的 Pod 就不对传入的 filteredPods 进行排序，否则就会按照三个不同的维度对 Pod 进行排序：\n NotReady \u0026lt; Ready Unscheduled \u0026lt; Scheduled Pending \u0026lt; Running  按照上述规则进行排序的 Pod 能够保证删除在早期阶段的 Pod 对象，简单总结一下，manageReplicas 方法会在与已经存在的 Pod 建立关系之后，对持有的数量和期望的数量进行比较之后，会根据 Pod 模板创建或者删除 Pod:\n到这里整个处理 ReplicaSet 的主要工作就结束了，syncReplicaSet 中剩下的代码会更新 ReplicaSet 的状态并结束同步 ReplicaSet 的工作。\n删除 如果我们在 Kubernetes 集群中删除一个 ReplicaSet 持有的 Pod，那么控制器会重新同步 ReplicaSet 的状态并启动一个新的 Pod，但是如果删除集群中的 ReplicaSet 所有相关的 Pod 也都会被删除：\n$ kubectl delete rs example\rreplicaset.extensions \u0026quot;example\u0026quot; deleted\r$ kubectl get pods --watch\rexample-z4fvc 0/1 Terminating 0 54s\rexample-zswpk 0/1 Terminating 0 54s\rexample-v8wwn 0/1 Terminating 0 54s\r Bash\n删除相关 Pod 的工作并不是 ReplicaSetController 负责的，而是由集群中的 垃圾收集器，也就是 GarbageCollector 实现的。\nKubernetes 中的垃圾收集器会负责删除以前有所有者但是现在没有的对象，metadata.ownerReference 属性标识了一个对象的所有者，当垃圾收集器发现对象的所有者被删除时，就会自动删除这些无用的对象，这也是 ReplicaSet 持有的 Pod 被自动删除的原因，我们会在 垃圾收集器 一节中具体介绍垃圾收集器的原理。\n总结 Kubernetes 中的 ReplicaSet 并不是一个工程师经常需要直接接触的对象，常用的 Deployment 其实使用 ReplicaSet 实现了很多复杂的特性，例如滚动更新，虽然作为使用者我们并不会经常直接与 ReplicaSet 这一对象打交道，但是如果需要对 Kubernetes 进行一些定制化开发，可能会用 ReplicaSet 和其他对象实现一些更复杂的功能。\nKubernetes 垃圾收集器 垃圾收集器在 Kubernetes 中的作用就是删除之前有所有者但是现在所有者已经不存在的对象，例如删除 ReplicaSet 时会删除它依赖的 Pod，虽然它的名字是垃圾收集器，但是它在 Kubernetes 中还是以控制器的形式进行设计和实现的。\n在 Kubernetes 引入垃圾收集器之前，所有的级联删除逻辑都是在客户端完成的，kubectl 会先删除 ReplicaSet 持有的 Pod 再删除 ReplicaSet，但是垃圾收集器的引入就让级联删除的实现移到了服务端，我们在这里就会介绍垃圾收集器的设计和实现原理。\n概述 垃圾收集主要提供的功能就是级联删除，它向对象的 API 中加入了 metadata.ownerReferences 字段，这一字段会包含当前对象的所有依赖者，在默认情况下，如果当前对象的所有依赖者都被删除，那么当前对象就会被删除：\ntype ObjectMeta struct {\r...\rOwnerReferences []OwnerReference\r}\rtype OwnerReference struct {\rAPIVersion string\rKind string\rName string\rUID types.UID\r}\r Go\nOwnerReference 包含了足够的信息来标识当前对象的依赖者，对象的依赖者必须与当前对象位于同一个命名空间 namespace，否则两者就无法建立起依赖关系。\n通过引入 metadata.ownerReferences 能够建立起不同对象的关系，但是我们依然需要其他的组件来负责处理对象之间的联系并在所有依赖者不存在时将对象删除，这个处理不同对象联系的组件就是 GarbageCollector，也是 Kubernetes 控制器的一种。\n实现原理 GarbageCollector 中包含一个 GraphBuilder 结构体，这个结构体会以 Goroutine 的形式运行并使用 Informer 监听集群中几乎全部资源的变动，一旦发现任何的变更事件 — 增删改，就会将事件交给主循环处理，主循环会根据事件的不同选择将待处理对象加入不同的队列，与此同时 GarbageCollector 持有的另外两组队列会负责删除或者孤立目标对象。\ngraph TD\rM1[PodMonitor]-. event .-\u0026gt;WQ[WorkQueue]\rM2[ReplicaSetMonitor]-. event .-\u0026gt;WQ[WorkQueue]\rGB[GraphBuilder]-- owns --\u0026gt;M1\rGB-- owns --\u0026gt;M2\rWQ-. event .-\u0026gt;PGC(ProcessGraphChanges)\rGB-- owns --\u0026gt;PGC\rPGC-. item .-\u0026gt;ATD[AttemptsToDelete]\rPGC-. item .-\u0026gt;ATO[AttemptsToOrphan]\rGC[GarbageCollector]-- owns --\u0026gt;DW[DeleteWorker]\rATD-.-\u0026gt;DW\rATO-.-\u0026gt;OW\rGC[GarbageCollector]-- owns --\u0026gt;OW[OrphanWorker]\rGC-- owns --\u0026gt;GB\rstyle M1 fill:#fffede,stroke:#ebebb7\rstyle M2 fill:#fffede,stroke:#ebebb7\rstyle GB fill:#fffede,stroke:#ebebb7\rstyle PGC fill:#fffede,stroke:#ebebb7\rstyle DW fill:#fffede,stroke:#ebebb7\rstyle OW fill:#fffede,stroke:#ebebb7\r Mermaid\n接下来我们会从几个关键点介绍垃圾收集器是如何删除 Kubernetes 集群中的对象以及它们的依赖的。\n删除策略 多个资源的 Informer 共同构成了垃圾收集器中的 Propagator，它监听所有的资源更新事件并将它们投入到工作队列中，这些事件会更新内存中的 DAG，这个 DAG 表示了集群中不同对象之间的从属关系，垃圾收集器的多个 Worker 会从两个队列中获取待处理的对象并调用 attemptToDeleteItem 和 attempteToOrphanItem 方法，这里我们主要介绍 attemptToDeleteItem 的实现：\nfunc (gc *GarbageCollector) attemptToDeleteItem(item *node) error {\rlatest, _ := gc.getObject(item.identity)\rownerReferences := latest.GetOwnerReferences()\rsolid, dangling, waitingForDependentsDeletion, _ := gc.classifyReferences(item, ownerReferences)\r Go\n该方法会先获取待处理的对象以及所有者的引用列表，随后使用 classifyReferences 方法将引用进行分类并按照不同的条件分别进行处理：\nswitch {\rcase len(solid) != 0:\rownerUIDs := append(ownerRefsToUIDs(dangling), ownerRefsToUIDs(waitingForDependentsDeletion)...)\rpatch := deleteOwnerRefStrategicMergePatch(item.identity.UID, ownerUIDs...)\rgc.patch(item, patch, func(n *node) ([]byte, error) {\rreturn gc.deleteOwnerRefJSONMergePatch(n, ownerUIDs...)\r})\rreturn err\r Go\n如果当前对象的所有者还有存在于集群中的，那么当前的对象就不会被删除，上述代码会将已经被删除或等待删除的引用从对象中删掉。\n当正在被删除的所有者不存在任何的依赖并且该对象的 ownerReference.blockOwnerDeletion 属性为 true 时会阻止依赖方的删除，所以当前的对象会等待属性 ownerReference.blockOwnerDeletion=true 的所有对象的删除后才会被删除。\n// ...\rcase len(waitingForDependentsDeletion) != 0 \u0026amp;\u0026amp; item.dependentsLength() != 0:\rdeps := item.getDependents()\rfor _, dep := range deps {\rif dep.isDeletingDependents() {\rpatch, _ := item.unblockOwnerReferencesStrategicMergePatch()\rgc.patch(item, patch, gc.unblockOwnerReferencesJSONMergePatch)\tbreak\r}\r}\rpolicy := metav1.DeletePropagationForeground\rreturn gc.deleteObject(item.identity, \u0026amp;policy)\r// ...\t Go\n在默认情况下，也就是当前对象已经不包含任何依赖，那么如果当前对象可能会选择三种不同的策略处理依赖：\n// ...\rdefault:\rvar policy metav1.DeletionPropagation\rswitch {\rcase hasOrphanFinalizer(latest):\rpolicy = metav1.DeletePropagationOrphan\rcase hasDeleteDependentsFinalizer(latest):\rpolicy = metav1.DeletePropagationForeground\rdefault:\rpolicy = metav1.DeletePropagationBackground\r}\rreturn gc.deleteObject(item.identity, \u0026amp;policy)\r}\r}\r Go\n 如果当前对象有 FinalizerOrphanDependents 终结器，DeletePropagationOrphan 策略会让对象所有的依赖变成孤立的； 如果当前对象有 FinalizerDeleteDependents 终结器，DeletePropagationBackground 策略在前台等待所有依赖被删除后才会删除，整个删除过程都是同步的； 默认情况下会使用 DeletePropagationDefault 策略在后台删除当前对象的全部依赖；  终结器 对象的终结器是在对象删除之前需要执行的逻辑，所有的对象在删除之前，它的终结器字段必须为空，终结器提供了一个通用的 API，它的功能不只是用于阻止级联删除，还能过通过它在对象删除之前加入钩子：\ntype ObjectMeta struct {\r// ...\rFinalizers []string\r}\r Go\n终结器在对象被删之前运行，每当终结器成功运行之后，就会将它自己从 Finalizers 数组中删除，当最后一个终结器被删除之后，API Server 就会删除该对象。\n在默认情况下，删除一个对象会删除它的全部依赖，但是我们在一些特定情况下我们只是想删除当前对象本身并不想造成复杂的级联删除，垃圾回收机制在这时引入了 OrphanFinalizer，它会在对象被删除之前向 Finalizers 数组添加或者删除 OrphanFinalizer。\n该终结器会监听对象的更新事件并将它自己从它全部依赖对象的 OwnerReferences 数组中删除，与此同时会删除所有依赖对象中已经失效的 OwnerReferences 并将 OrphanFinalizer 从 Finalizers 数组中删除。\n通过 OrphanFinalizer 我们能够在删除一个 Kubernetes 对象时保留它的全部依赖，为使用者提供一种更灵活的办法来保留和删除对象。\n总结 Kubernetes 中垃圾收集器的实现还是比较容易理解的，它的主要作用就是监听集群中对象的变更事件并根据两个字段 OwnerReferences 和 Finalizers 确定对象的删除策略，其中包括同步和后台的选择、是否应该触发级联删除移除当前对象的全部依赖；在默认情况下，当我们删除 Kubernetes 集群中的 ReplicaSet、Deployment 对象时都会删除这些对象的全部依赖，不过我们也可以通过 OrphanFinalizer 终结器删除单独的对象。\nKubernetes Deployment 如果你在生产环境中使用过 Kubernetes，那么相信你对 Deployment 一定不会陌生，Deployment 提供了一种对 Pod 和 ReplicaSet 的管理方式，每一个 Deployment 都对应集群中的一次部署，是非常常见的 Kubernetes 对象。\n我们在这篇文章中就会介绍 Deployment 的实现原理，包括它是如何处理 Pod 的滚动更新、回滚以及支持副本的水平扩容。\n概述 作为最常用的 Kubernetes 对象，Deployment 经常会用来创建 ReplicaSet 和 Pod，我们往往不会直接在集群中使用 ReplicaSet 部署一个新的微服务，一方面是因为 ReplicaSet 的功能其实不够强大，一些常见的更新、扩容和缩容运维操作都不支持，Deployment 的引入就是为了就是为了支持这些复杂的操作。\napiVersion: apps/v1\rkind: Deployment\rmetadata:\rname: nginx-deployment\rlabels:\rapp: nginx\rspec:\rreplicas: 3\rselector:\rmatchLabels:\rapp: nginx\rtemplate:\rmetadata:\rlabels:\rapp: nginx\rspec:\rcontainers:\r- name: nginx\rimage: nginx:1.7.9\rports:\r- containerPort: 80\r YAML\n当我们在 Kubernetes 集群中创建上述 Deployment 对象时，它不只会创建 Deployment 资源，还会创建另外的 ReplicaSet 以及三个 Pod 对象：\n$ kubectl get deployments.apps\rNAME READY UP-TO-DATE AVAILABLE AGE\rnginx-deployment 3/3 3 3 6m55s\r$ kubectl get replicasets.apps\rNAME DESIRED CURRENT READY AGE\rnginx-deployment-76bf4969df 3 3 3 7m27s\r$ kubectl get pods\rNAME READY STATUS RESTARTS AGE\rnginx-deployment-76bf4969df-58gxj 1/1 Running 0 7m42s\rnginx-deployment-76bf4969df-9jgk9 1/1 Running 0 7m42s\rnginx-deployment-76bf4969df-m4pkg 1/1 Running 0 7m43s\r Bash\n每一个 Deployment 都会和它的依赖组成以下的拓扑结构，在这个拓扑结构中的子节点都是『稳定』的，任意节点的删除都会被 Kubernetes 的控制器重启：\ngraph TD\rDeployment-.-\u0026gt;ReplicaSet\rReplicaSet-.-\u0026gt;Pod1\rReplicaSet-.-\u0026gt;Pod2\rReplicaSet-.-\u0026gt;Pod3\r Mermaid\n所有的 Deployment 对象都是由 Kubernetes 集群中的 DeploymentController 进行管理，家下来我们将开始介绍该控制器的实现原理。\n实现原理 DeploymentController 作为管理 Deployment 资源的控制器，会在启动时通过 Informer 监听三种不同资源的通知，Pod、ReplicaSet 和 Deployment，这三种资源的变动都会触发 DeploymentController 中的回调。\ngraph TD\rDI[DeploymentInformer]-. Add/Update/Delete .-\u0026gt;DC[DeploymentController]\rReplicaSetInformer-. Add/Update/Delete .-\u0026gt;DC\rPodInformer-. Delete .-\u0026gt;DC\r Mermaid\n不同的事件最终都会在被过滤后进入控制器持有的队列，等待工作进程的消费，下面的这些事件都会触发 Deployment 的同步：\n Deployment 的变动； Deployment 相关的 ReplicaSet 变动； Deployment 相关的 Pod 数量为 0 时，Pod 的删除事件；  DeploymentController 会在调用 Run 方法时启动多个工作进程，这些工作进程会运行 worker 方法从队列中读取最新的 Deployment 对象进行同步。\n同步 Deployment 对象的同步都是通过以下的 syncDeployment 方法进行的，该方法包含了同步、回滚以及更新的逻辑，是同步 Deployment 资源的唯一入口：\nfunc (dc *DeploymentController) syncDeployment(key string) error {\rnamespace, name, _ := cache.SplitMetaNamespaceKey(key)\rdeployment, _ := dc.dLister.Deployments(namespace).Get(name)\rd := deployment.DeepCopy()\rrsList, _ := dc.getReplicaSetsForDeployment(d)\rpodMap, _ := dc.getPodMapForDeployment(d, rsList)\rdc.checkPausedConditions(d)\rif d.Spec.Paused {\rreturn dc.sync(d, rsList)\r}\rscalingEvent, _ := dc.isScalingEvent(d, rsList)\rif scalingEvent {\rreturn dc.sync(d, rsList)\r}\rswitch d.Spec.Strategy.Type {\rcase apps.RecreateDeploymentStrategyType:\rreturn dc.rolloutRecreate(d, rsList, podMap)\rcase apps.RollingUpdateDeploymentStrategyType:\rreturn dc.rolloutRolling(d, rsList)\r}\rreturn fmt.Errorf(\u0026quot;unexpected deployment strategy type: %s\u0026quot;, d.Spec.Strategy.Type)\r}\r Go\n  根据传入的键获取 Deployment 资源；\n  调用\ngetReplicaSetsForDeployment\r 获取集群中与 Deployment 相关的全部 ReplicaSet；\n 查找集群中的全部 ReplicaSet； 根据 Deployment 的选择器对 ReplicaSet 建立或者释放从属关系；    调用\ngetPodMapForDeployment\r 获取当前 Deployment 对象相关的从 ReplicaSet 到 Pod 的映射；\n 根据选择器查找全部的 Pod； 根据 Pod 的控制器 ReplicaSet 对上述 Pod 进行分类；    如果当前的 Deployment 处于暂停状态或者需要进行扩容，就会调用 sync 方法同步 Deployment;\n  在正常情况下会根据规格中的策略对 Deployment 进行更新；\n Recreate 策略会调用 rolloutRecreate 方法，它会先杀掉所有存在的 Pod 后启动新的 Pod 副本； RollingUpdate 策略会调用 rolloutRolling 方法，根据 maxSurge 和 maxUnavailable 配置对 Pod 进行滚动更新；    这就是 Deployment 资源同步的主要流程，我们在这里可以关注一下 getReplicaSetsForDeployment 方法：\nfunc (dc *DeploymentController) getReplicaSetsForDeployment(d *apps.Deployment) ([]*apps.ReplicaSet, error) {\rrsList, _ := dc.rsLister.ReplicaSets(d.Namespace).List(labels.Everything())\rdeploymentSelector, _ := metav1.LabelSelectorAsSelector(d.Spec.Selector)\rcanAdoptFunc := controller.RecheckDeletionTimestamp(func() (metav1.Object, error) {\rreturn dc.client.AppsV1().Deployments(d.Namespace).Get(d.Name, metav1.GetOptions{})\r})\rcm := controller.NewReplicaSetControllerRefManager(dc.rsControl, d, deploymentSelector, controllerKind, canAdoptFunc)\rreturn cm.ClaimReplicaSets(rsList)\r}\r Go\n该方法获取 Deployment 持有的 ReplicaSet 时会重新与集群中符合条件的 ReplicaSet 通过 ownerReferences 建立关系，执行的逻辑与 ReplicaSet 调用 AdoptPod/ReleasePod 几乎完全相同。\n扩容 如果当前需要更新的 Deployment 经过 isScalingEvent 的检查发现更新事件实际上是一次扩容或者缩容，也就是 ReplicaSet 持有的 Pod 数量和规格中的 Replicas 字段并不一致，那么就会调用 sync 方法对 Deployment 进行同步：\nfunc (dc *DeploymentController) sync(d *apps.Deployment, rsList []*apps.ReplicaSet) error {\rnewRS, oldRSs, _ := dc.getAllReplicaSetsAndSyncRevision(d, rsList, false)\rdc.scale(d, newRS, oldRSs)\rallRSs := append(oldRSs, newRS)\rreturn dc.syncDeploymentStatus(allRSs, newRS, d)\r}\r Go\n同步的过程其实比较简单，该方法会从 apiserver 中拿到当前 Deployment 对应的最新 ReplicaSet 和历史的 ReplicaSet 并调用 scale 方法开始扩容，scale 就是扩容需要执行的主要方法，我们将下面的方法分成几部分依次进行介绍：\nfunc (dc *DeploymentController) scale(deployment *apps.Deployment, newRS *apps.ReplicaSet, oldRSs []*apps.ReplicaSet) error {\rif activeOrLatest := deploymentutil.FindActiveOrLatest(newRS, oldRSs); activeOrLatest != nil {\rif *(activeOrLatest.Spec.Replicas) == *(deployment.Spec.Replicas) {\rreturn nil\r}\rdc.scaleReplicaSetAndRecordEvent(activeOrLatest, *(deployment.Spec.Replicas), deployment)\rreturn nil\r}\rif deploymentutil.IsSaturated(deployment, newRS) {\rfor _, old := range controller.FilterActiveReplicaSets(oldRSs) {\rdc.scaleReplicaSetAndRecordEvent(old, 0, deployment)\r}\rreturn nil\r}\r Go\n如果集群中只有一个活跃的 ReplicaSet，那么就会对该 ReplicaSet 进行扩缩容，但是如果不存在活跃的 ReplicaSet 对象，就会选择最新的 ReplicaSet 进行操作，这部分选择 ReplicaSet 的工作都是由 FindActiveOrLatest 和 scaleReplicaSetAndRecordEvent 共同完成的。\n当调用 IsSaturated 方法发现当前的 Deployment 对应的副本数量已经饱和时就会删除所有历史版本 ReplicaSet 持有的 Pod 副本。\n但是在 Deployment 使用滚动更新策略时，如果发现当前的 ReplicaSet 并没有饱和并且存在多个活跃的 ReplicaSet 对象就会按照比例分别对各个活跃的 ReplicaSet 进行扩容或者缩容：\nif deploymentutil.IsRollingUpdate(deployment) {\rallRSs := controller.FilterActiveReplicaSets(append(oldRSs, newRS))\rallRSsReplicas := deploymentutil.GetReplicaCountForReplicaSets(allRSs)\rallowedSize := int32(0)\rif *(deployment.Spec.Replicas) \u0026gt; 0 {\rallowedSize = *(deployment.Spec.Replicas) + deploymentutil.MaxSurge(*deployment)\r}\rdeploymentReplicasToAdd := allowedSize - allRSsReplicas\rvar scalingOperation string\rswitch {\rcase deploymentReplicasToAdd \u0026gt; 0:\rsort.Sort(controller.ReplicaSetsBySizeNewer(allRSs))\rscalingOperation = \u0026quot;up\u0026quot;\rcase deploymentReplicasToAdd \u0026lt; 0:\rsort.Sort(controller.ReplicaSetsBySizeOlder(allRSs))\rscalingOperation = \u0026quot;down\u0026quot;\r}\r Go\n  通过 FilterActiveReplicaSets 获取所有活跃的 ReplicaSet 对象；\n  调用 GetReplicaCountForReplicaSets 计算当前 Deployment 对应 ReplicaSet 持有的全部 Pod 副本个数；\n  根据 Deployment 对象配置的 Replicas 和最大额外可以存在的副本数 maxSurge 以计算 Deployment 允许创建的 Pod 数量；\n  通过 allowedSize 和 allRSsReplicas 计算出需要增加或者删除的副本数；\n  根据\ndeploymentReplicasToAdd\r 变量的符号对 ReplicaSet 数组进行排序并确定当前的操作时扩容还是缩容；\n 如果 deploymentReplicasToAdd \u0026gt; 0，ReplicaSet 将按照从新到旧的顺序依次进行扩容； 如果 deploymentReplicasToAdd \u0026lt; 0，ReplicaSet 将按照从旧到新的顺序依次进行缩容；     maxSurge、maxUnavailable 是两个处理滚动更新时需要关注的参数，我们会在滚动更新一节中具体介绍。\n deploymentReplicasAdded := int32(0)\rnameToSize := make(map[string]int32)\rfor i := range allRSs {\rrs := allRSs[i]\rif deploymentReplicasToAdd != 0 {\rproportion := deploymentutil.GetProportion(rs, *deployment, deploymentReplicasToAdd, deploymentReplicasAdded)\rnameToSize[rs.Name] = *(rs.Spec.Replicas) + proportion\rdeploymentReplicasAdded += proportion\r} else {\rnameToSize[rs.Name] = *(rs.Spec.Replicas)\r}\r}\r Go\n因为当前的 Deployment 持有了多个活跃的 ReplicaSet，所以在计算了需要增加或者删除的副本个数 deploymentReplicasToAdd 之后，就会为多个活跃的 ReplicaSet 分配每个 ReplicaSet 需要改变的副本数，GetProportion 会根据以下几个参数决定最后的结果:\n Deployment 期望的 Pod 副本数量； 需要新增或者减少的副本数量； Deployment 当前通过 ReplicaSet 持有 Pod 的总数量；  Kubernetes 会在 getReplicaSetFraction 使用下面的公式计算每一个 ReplicaSet 在 Deployment 资源中的占比，最后会返回该 ReplicaSet 需要改变的副本数：\n该结果又会与目前期望的剩余变化量进行对比，保证变化的副本数量不会超过期望值。\nfor i := range allRSs {\rrs := allRSs[i]\r// ...\rdc.scaleReplicaSet(rs, nameToSize[rs.Name], deployment, scalingOperation)\r}\r}\rreturn nil\r}\r Go\n在 scale 方法的最后会直接调用 scaleReplicaSet 将每一个 ReplicaSet 都扩容或者缩容到我们期望的副本数：\nfunc (dc *DeploymentController) scaleReplicaSet(rs *apps.ReplicaSet, newScale int32, deployment *apps.Deployment, scalingOperation string) (bool, *apps.ReplicaSet, error) {\rsizeNeedsUpdate := *(rs.Spec.Replicas) != newScale\rannotationsNeedUpdate := deploymentutil.ReplicasAnnotationsNeedUpdate(rs, *(deployment.Spec.Replicas), *(deployment.Spec.Replicas)+deploymentutil.MaxSurge(*deployment))\rif sizeNeedsUpdate || annotationsNeedUpdate {\rrsCopy := rs.DeepCopy()\r*(rsCopy.Spec.Replicas) = newScale\rdeploymentutil.SetReplicasAnnotations(rsCopy, *(deployment.Spec.Replicas), *(deployment.Spec.Replicas)+deploymentutil.MaxSurge(*deployment))\rrs, _ = dc.client.AppsV1().ReplicaSets(rsCopy.Namespace).Update(rsCopy)\r}\rreturn true, rs, err\r}\r Go\n这里会直接修改目标 ReplicaSet 规格中的 Replicas 参数和注解 deployment.kubernetes.io/desired-replicas 的值并通过 API 请求更新当前的 ReplicaSet 对象：\n$ kubectl describe rs nginx-deployment-76bf4969df\rName: nginx-deployment-76bf4969df\rNamespace: default\rSelector: app=nginx,pod-template-hash=76bf4969df\rLabels: app=nginx\rpod-template-hash=76bf4969df\rAnnotations: deployment.kubernetes.io/desired-replicas=3\rdeployment.kubernetes.io/max-replicas=4\r...\r Bash\n我们可以通过 describe 命令查看 ReplicaSet 的注解，其实能够发现当前 ReplicaSet 的期待副本数和最大副本数，deployment.kubernetes.io/desired-replicas 注解就是在上述方法中被 Kubernetes 的 DeploymentController 更新的。\n重新创建 当 Deployment 使用的更新策略类型是 Recreate 时，DeploymentController 就会使用如下的 rolloutRecreate 方法对 Deployment 进行更新：\nfunc (dc *DeploymentController) rolloutRecreate(d *apps.Deployment, rsList []*apps.ReplicaSet, podMap map[types.UID]*v1.PodList) error {\rnewRS, oldRSs, _ := dc.getAllReplicaSetsAndSyncRevision(d, rsList, false)\rallRSs := append(oldRSs, newRS)\ractiveOldRSs := controller.FilterActiveReplicaSets(oldRSs)\rscaledDown, _ := dc.scaleDownOldReplicaSetsForRecreate(activeOldRSs, d)\rif scaledDown {\rreturn dc.syncRolloutStatus(allRSs, newRS, d)\r}\rif oldPodsRunning(newRS, oldRSs, podMap) {\rreturn dc.syncRolloutStatus(allRSs, newRS, d)\r}\rif newRS == nil {\rnewRS, oldRSs, _ = dc.getAllReplicaSetsAndSyncRevision(d, rsList, true)\rallRSs = append(oldRSs, newRS)\r}\rdc.scaleUpNewReplicaSetForRecreate(newRS, d)\rif util.DeploymentComplete(d, \u0026amp;d.Status) {\rdc.cleanupDeployment(oldRSs, d)\r}\rreturn dc.syncRolloutStatus(allRSs, newRS, d)\r}\r Go\n 利用 getAllReplicaSetsAndSyncRevision 和 FilterActiveReplicaSets 两个方法获取 Deployment 中所有的 ReplicaSet 以及其中活跃的 ReplicaSet 对象； 调用 scaleDownOldReplicaSetsForRecreate 方法将所有活跃的历史 ReplicaSet 持有的副本 Pod 数目降至 0； 同步 Deployment 的最新状态并等待 Pod 的终止； 在需要时通过 getAllReplicaSetsAndSyncRevision 方法创建新的 ReplicaSet 并调用 scaleUpNewReplicaSetForRecreate 函数对 ReplicaSet 进行扩容； 更新完成之后会调用 cleanupDeployment 方法删除历史全部的 ReplicaSet 对象并更新 Deployment 的状态；  也就是说在更新的过程中，之前创建的 ReplicaSet 和 Pod 资源全部都会被删除，只是 Pod 会先被删除而 ReplicaSet 会后被删除；上述方法也会创建新的 ReplicaSet 和 Pod 对象，需要注意的是在这个过程中旧的 Pod 副本一定会先被删除，所以会有一段时间不存在可用的 Pod。\n滚动更新 Deployment 的另一个更新策略 RollingUpdate 其实更加常见，在具体介绍滚动更新的流程之前，我们首先需要了解滚动更新策略使用的两个参数 maxUnavailable 和 maxSurge：\n maxUnavailable 表示在更新过程中能够进入不可用状态的 Pod 的最大值； maxSurge 表示能够额外创建的 Pod 个数；  maxUnavailable 和 maxSurge 这两个滚动更新的配置都可以使用绝对值或者百分比表示，使用百分比时需要用 Replicas * Strategy.RollingUpdate.MaxSurge 公式计算相应的数值。\nrolloutRolling 方法就是 DeploymentController 用于处理滚动更新的方法：\nfunc (dc *DeploymentController) rolloutRolling(d *apps.Deployment, rsList []*apps.ReplicaSet) error {\rnewRS, oldRSs, _ := dc.getAllReplicaSetsAndSyncRevision(d, rsList, true)\rallRSs := append(oldRSs, newRS)\rscaledUp, _ := dc.reconcileNewReplicaSet(allRSs, newRS, d)\rif scaledUp {\rreturn dc.syncRolloutStatus(allRSs, newRS, d)\r}\rscaledDown, _ := dc.reconcileOldReplicaSets(allRSs, controller.FilterActiveReplicaSets(oldRSs), newRS, d)\rif scaledDown {\rreturn dc.syncRolloutStatus(allRSs, newRS, d)\r}\rif deploymentutil.DeploymentComplete(d, \u0026amp;d.Status) {\rdc.cleanupDeployment(oldRSs, d)\r}\rreturn dc.syncRolloutStatus(allRSs, newRS, d)\r}\r Go\n 首先获取 Deployment 对应的全部 ReplicaSet 资源； 通过 reconcileNewReplicaSet 调解新 ReplicaSet 的副本数，创建新的 Pod 并保证额外的副本数量不超过 maxSurge； 通过 reconcileOldReplicaSets 调解历史 ReplicaSet 的副本数，删除旧的 Pod 并保证不可用的部分数不会超过 maxUnavailable； 最后删除无用的 ReplicaSet 并更新 Deployment 的状态；  需要注意的是，在滚动更新的过程中，Kubernetes 并不是一次性就切换到期望的状态，即『新 ReplicaSet 运行指定数量的副本』，而是会先启动新的 ReplicaSet 以及一定数量的 Pod 副本，然后删除历史 ReplicaSet 中的副本，再启动一些新 ReplicaSet 的副本，不断对新 ReplicaSet 进行扩容并对旧 ReplicaSet 进行缩容最终达到了集群期望的状态。\n当我们使用如下的 reconcileNewReplicaSet 方法对新 ReplicaSet 进行调节时，我们会发现在新 ReplicaSet 中副本数量满足期望时会直接返回，在超过期望时会进行缩容：\nfunc (dc *DeploymentController) reconcileNewReplicaSet(allRSs []*apps.ReplicaSet, newRS *apps.ReplicaSet, deployment *apps.Deployment) (bool, error) {\rif *(newRS.Spec.Replicas) == *(deployment.Spec.Replicas) {\rreturn false, nil\r}\rif *(newRS.Spec.Replicas) \u0026gt; *(deployment.Spec.Replicas) {\rscaled, _, err := dc.scaleReplicaSetAndRecordEvent(newRS, *(deployment.Spec.Replicas), deployment)\rreturn scaled, err\r}\rnewReplicasCount, _ := deploymentutil.NewRSNewReplicas(deployment, allRSs, newRS)\rscaled, _, err := dc.scaleReplicaSetAndRecordEvent(newRS, newReplicasCount, deployment)\rreturn scaled, err\r}\r Go\n如果 ReplicaSet 的数量不够就会调用 NewRSNewReplicas 函数计算新的副本个数，计算的过程使用了如下所示的公式：\nmaxTotalPods = deployment.Spec.Replicas + currentPodCount = sum(deployement.ReplicaSets.Replicas)\rscaleUpCount = maxTotalPods - currentPodCount\rscaleUpCount = min(scaleUpCount, deployment.Spec.Replicas - newRS.Spec.Replicas))\rnewRSNewReplicas = newRS.Spec.Replicas + scaleUpCount\r Go\n该过程总共需要考虑 Deployment 期望的副本数量、当前可用的副本数量以及新 ReplicaSet 持有的副本，还有一些最大值和最小值的限制，例如额外 Pod 数量不能超过 maxSurge、新 ReplicaSet 的 Pod 数量不能超过 Deployment 的期望数量，遵循这些规则我们就能计算出 newRSNewReplicas。\n另一个滚动更新中使用的方法 reconcileOldReplicaSets 主要作用就是对历史 ReplicaSet 对象持有的副本数量进行缩容：\nfunc (dc *DeploymentController) reconcileOldReplicaSets(allRSs []*apps.ReplicaSet, oldRSs []*apps.ReplicaSet, newRS *apps.ReplicaSet, deployment *apps.Deployment) (bool, error) {\roldPodsCount := deploymentutil.GetReplicaCountForReplicaSets(oldRSs)\rif oldPodsCount == 0 {\rreturn false, nil\r}\rallPodsCount := deploymentutil.GetReplicaCountForReplicaSets(allRSs)\rmaxUnavailable := deploymentutil.MaxUnavailable(*deployment)\rminAvailable := *(deployment.Spec.Replicas) - maxUnavailable\rnewRSUnavailablePodCount := *(newRS.Spec.Replicas) - newRS.Status.AvailableReplicas\rmaxScaledDown := allPodsCount - minAvailable - newRSUnavailablePodCount\rif maxScaledDown \u0026lt;= 0 {\rreturn false, nil\r}\roldRSs, cleanupCount, _ := dc.cleanupUnhealthyReplicas(oldRSs, deployment, maxScaledDown)\rallRSs = append(oldRSs, newRS)\rscaledDownCount, _ := dc.scaleDownOldReplicaSetsForRollingUpdate(allRSs, oldRSs, deployment)\rtotalScaledDown := cleanupCount + scaledDownCount\rreturn totalScaledDown \u0026gt; 0, nil\r}\r Go\n 计算历史 ReplicaSet 持有的副本总数量； 计算全部 ReplicaSet 持有的副本总数量； 根据 Deployment 期望的副本数、最大不可用副本数以及新 ReplicaSet 中不可用的 Pod 数量计算最大缩容的副本个数； 通过 cleanupUnhealthyReplicas 方法清理 ReplicaSet 中处于不健康状态的副本； 调用 scaleDownOldReplicaSetsForRollingUpdate 方法对历史 ReplicaSet 中的副本进行缩容；  minAvailable = deployment.Spec.Replicas - maxUnavailable(deployment)\rmaxScaledDown = allPodsCount - minAvailable - newReplicaSetPodsUnavailable\r Go\n该方法会使用上述简化后的公式计算这次总共能够在历史 ReplicaSet 中删除的最大 Pod 数量，并调用 cleanupUnhealthyReplicas 和 scaleDownOldReplicaSetsForRollingUpdate 两个方法进行缩容，这两个方法的实现都相对简单，它们都对历史 ReplicaSet 按照创建时间进行排序依次对这些资源进行缩容，两者的区别在于前者主要用于删除不健康的副本。\n回滚 Kubernetes 中的每一个 Deployment 资源都包含有 revision 这个概念，版本的引入可以让我们在更新发生问题时及时通过 Deployment 的版本对其进行回滚，当我们在更新 Deployment 时，之前 Deployment 持有的 ReplicaSet 其实会被 cleanupDeployment 方法清理：\nfunc (dc *DeploymentController) cleanupDeployment(oldRSs []*apps.ReplicaSet, deployment *apps.Deployment) error {\raliveFilter := func(rs *apps.ReplicaSet) bool {\rreturn rs != nil \u0026amp;\u0026amp; rs.ObjectMeta.DeletionTimestamp == nil\r}\rcleanableRSes := controller.FilterReplicaSets(oldRSs, aliveFilter)\rdiff := int32(len(cleanableRSes)) - *deployment.Spec.RevisionHistoryLimit\rif diff \u0026lt;= 0 {\rreturn nil\r}\rsort.Sort(controller.ReplicaSetsByCreationTimestamp(cleanableRSes))\rfor i := int32(0); i \u0026lt; diff; i++ {\rrs := cleanableRSes[i]\rif rs.Status.Replicas != 0 || *(rs.Spec.Replicas) != 0 || rs.Generation \u0026gt; rs.Status.ObservedGeneration || rs.DeletionTimestamp != nil {\rcontinue\r}\rdc.client.AppsV1().ReplicaSets(rs.Namespace).Delete(rs.Name, nil)\r}\rreturn nil\r}\r Go\nDeployment 资源在规格中由一个 spec.revisionHistoryLimit 的配置，这个配置决定了 Kubernetes 会保存多少个 ReplicaSet 的历史版本，这些历史上的 ReplicaSet 并不会被删除，它们只是不再持有任何的 Pod 副本了，假设我们有一个 spec.revisionHistoryLimit=2 的 Deployment 对象，那么当前资源最多持有两个历史的 ReplicaSet 版本：\n这些资源的保留能够方便 Deployment 的回滚，而回滚其实是通过 kubectl 在客户端实现的，我们可以使用如下的命令将 Deployment 回滚到上一个版本：\n$ kubectl rollout undo deployment.v1.apps/nginx-deployment\rdeployment.apps/nginx-deployment\r Bash\n上述 kubectl 命令没有指定回滚到的版本号，所以在默认情况下会回滚到上一个版本，在回滚时会直接根据传入的版本查找历史的 ReplicaSet 资源，拿到这个 ReplicaSet 对应的 Pod 模板后会触发一个资源更新的请求：\nfunc (r *DeploymentRollbacker) Rollback(obj runtime.Object, updatedAnnotations map[string]string, toRevision int64, dryRun bool) (string, error) {\raccessor, _ := meta.Accessor(obj)\rname := accessor.GetName()\rnamespace := accessor.GetNamespace()\rdeployment, _ := r.c.AppsV1().Deployments(namespace).Get(name, metav1.GetOptions{})\rrsForRevision, _ := deploymentRevision(deployment, r.c, toRevision)\rannotations := ...\rpatchType, patch, _ := getDeploymentPatch(\u0026amp;rsForRevision.Spec.Template, annotations)\rr.c.AppsV1().Deployments(namespace).Patch(name, patchType, patch)\rreturn rollbackSuccess, nil\r}\r Go\n回滚对于 Kubernetes 服务端来说其实与其他的更新操作没有太多的区别，在每次更新时都会在 FindNewReplicaSet 函数中根据 Deployment 的 Pod 模板在历史 ReplicaSet 中查询是否有相同的 ReplicaSet 存在：\nfunc FindNewReplicaSet(deployment *apps.Deployment, rsList []*apps.ReplicaSet) *apps.ReplicaSet {\rsort.Sort(controller.ReplicaSetsByCreationTimestamp(rsList))\rfor i := range rsList {\rif EqualIgnoreHash(\u0026amp;rsList[i].Spec.Template, \u0026amp;deployment.Spec.Template) {\rreturn rsList[i]\r}\r}\rreturn nil\r}\r Go\n如果存在规格完全相同的 ReplicaSet，就会保留这个 ReplicaSet 历史上使用的版本号并对该 ReplicaSet 重新进行扩容并对正在工作的 ReplicaSet 进行缩容以实现集群的期望状态。\n$ k describe deployments.apps nginx-deployment\rName: nginx-deployment\rNamespace: default\rCreationTimestamp: Thu, 21 Feb 2019 10:14:29 +0800\rLabels: app=nginx\rAnnotations: deployment.kubernetes.io/revision: 11\rkubectl.kubernetes.io/last-applied-configuration:\r{\u0026quot;apiVersion\u0026quot;:\u0026quot;apps/v1\u0026quot;,\u0026quot;kind\u0026quot;:\u0026quot;Deployment\u0026quot;,\u0026quot;metadata\u0026quot;:{\u0026quot;annotations\u0026quot;:{},\u0026quot;labels\u0026quot;:{\u0026quot;app\u0026quot;:\u0026quot;nginx\u0026quot;},\u0026quot;name\u0026quot;:\u0026quot;nginx-deployment\u0026quot;,\u0026quot;namespace\u0026quot;:\u0026quot;d...\rSelector: app=nginx\rReplicas: 3 desired | 3 updated | 3 total | 3 available | 0 unavailable\rStrategyType: RollingUpdate\r...\rEvents:\rType Reason Age From Message\r---- ------ ---- ---- -------\rNormal ScalingReplicaSet 20s deployment-controller Scaled up replica set nginx-deployment-5cc74f885d to 1\rNormal ScalingReplicaSet 19s deployment-controller Scaled down replica set nginx-deployment-7c6cf994f6 to 2\rNormal ScalingReplicaSet 19s deployment-controller Scaled up replica set nginx-deployment-5cc74f885d to 2\rNormal ScalingReplicaSet 17s deployment-controller Scaled down replica set nginx-deployment-7c6cf994f6 to 1\rNormal ScalingReplicaSet 17s deployment-controller Scaled up replica set nginx-deployment-5cc74f885d to 3\rNormal ScalingReplicaSet 14s deployment-controller Scaled down replica set nginx-deployment-7c6cf994f6 to 0\r Bash\n在之前的 Kubernetes 版本中，客户端还会使用注解来实现 Deployment 的回滚，但是在最新的 kubectl 版本中这种使用注解的方式已经被废弃了。\n暂停和恢复 Deployment 中有一个不是特别常用的功能，也就是 Deployment 进行暂停，暂停之后的 Deployment 哪怕发生了改动也不会被 Kubernetes 更新，这时我们可以对 Deployment 资源进行更新或者修复，随后当重新恢复 Deployment 时，DeploymentController 才会重新对其进行滚动更新向期望状态迁移：\nfunc defaultObjectPauser(obj runtime.Object) ([]byte, error) {\rswitch obj := obj.(type) {\rcase *appsv1.Deployment:\rif obj.Spec.Paused {\rreturn nil, errors.New(\u0026quot;is already paused\u0026quot;)\r}\robj.Spec.Paused = true\rreturn runtime.Encode(scheme.Codecs.LegacyCodec(appsv1.SchemeGroupVersion), obj)\r// ...\rdefault:\rreturn nil, fmt.Errorf(\u0026quot;pausing is not supported\u0026quot;)\r}\r}\r Go\n暂停和恢复也都是由 kubectl 在客户端实现的，其实就是通过更改 spec.paused 属性，这里的更改会变成一个更新操作修改 Deployment 资源。\n$ kubectl rollout pause deployment.v1.apps/nginx-deployment\rdeployment.apps/nginx-deployment paused\r$ kubectl get deployments.apps nginx-deployment -o yaml\rapiVersion: apps/v1\rkind: Deployment\rmetadata:\r# ...\rname: nginx-deployment\rnamespace: default\rselfLink: /apis/apps/v1/namespaces/default/deployments/nginx-deployment\ruid: 6b44965f-357e-11e9-af24-0800275e8310\rspec:\rpaused: true\r# ...\r Bash\n如果我们使用 YAML 文件和 kubectl apply 命令来更新整个 Deployment 资源，那么其实用不到暂停这一功能，我们只需要在文件里对资源进行修改并进行一次更新就可以了，但是我们可以在出现问题时，暂停一次正在进行的滚动更新以防止错误的扩散。\n删除 如果我们在 Kubernetes 集群中删除了一个 Deployment 资源，那么 Deployment 持有的 ReplicaSet 以及 ReplicaSet 持有的副本都会被 Kubernetes 中的 垃圾收集器 删除：\n$ kubectl delete deployments.apps nginx-deployment\rdeployment.apps \u0026quot;nginx-deployment\u0026quot; deleted\r$ kubectl get replicasets --watch\rnginx-deployment-7c6cf994f6 0 0 0 2d1h\rnginx-deployment-5cc74f885d 0 0 0 2d1h\rnginx-deployment-c5d875444 3 3 3 30h\r$ kubectl get pods --watch\rnginx-deployment-c5d875444-6r4q6 1/1 Terminating 2 30h\rnginx-deployment-c5d875444-7ssgj 1/1 Terminating 2 30h\rnginx-deployment-c5d875444-4xvvz 1/1 Terminating 2 30h\r Go\n由于与当前 Deployment 有关的 ReplicaSet 历史和最新版本都会被删除，所以对应的 Pod 副本也都会随之被删除，这些对象之间的关系都是通过 metadata.ownerReference 这一字段关联的，垃圾收集器 一节详细介绍了它的实现原理。\n总结 Deployment 是 Kubernetes 中常用的对象类型，它解决了 ReplicaSet 更新的诸多问题，通过对 ReplicaSet 和 Pod 进行组装支持了滚动更新、回滚以及扩容等高级功能，通过对 Deployment 的学习既能让我们了解整个常见资源的实现也能帮助我们理解如何将 Kubernetes 内置的对象组合成更复杂的自定义资源。\nKubernetes StatefulSet 在 Kubernetes 的世界中，ReplicaSet 和 Deployment 主要用于处理无状态的服务，无状态服务的需求往往非常简单并且轻量，每一个无状态节点存储的数据在重启之后就会被删除，虽然这种服务虽然常见，但是我们仍然需要有状态的服务来实现一些特殊的需求，StatefulSet 就是 Kubernetes 为了运行有状态服务引入的资源，例如 Zookeeper、Kafka 等。\n这篇文章会介绍 Kubernetes 如何在集群中运行有状态服务，同时会分析这些有状态服务 StatefulSet 的同步过程以及实现原理。\n概述 StatefulSet 是用于管理有状态应用的工作负载对象，与 ReplicaSet 和 Deployment 这两个对象不同，StatefulSet 不仅能管理 Pod 的对象，还它能够保证这些 Pod 的顺序性和唯一性。\n与 Deployment 一样，StatefulSet 也使用规格中声明的 template 模板来创建 Pod 资源，但是这些 Pod 相互之间是不能替换的；除此之外 StatefulSet 会为每个 Pod 设置一个单独的持久标识符，这些用于标识序列的标识符在发生调度时也不会丢失。\napiVersion: apps/v1\rkind: StatefulSet\rmetadata:\rname: web\rspec:\rserviceName: \u0026quot;nginx\u0026quot;\rreplicas: 2\rselector:\rmatchLabels:\rapp: nginx\rtemplate:\rmetadata:\rlabels:\rapp: nginx\rspec:\rcontainers:\r- name: nginx\rimage: k8s.gcr.io/nginx-slim:0.8\rvolumeMounts:\r- name: www\rmountPath: /usr/share/nginx/html\rvolumeClaimTemplates:\r- metadata:\rname: www\rspec:\raccessModes: [ \u0026quot;ReadWriteOnce\u0026quot; ]\rresources:\rrequests:\rstorage: 1Gi\r YAML\n如果我们在 Kubernetes 集群中创建如上所示的 StatefulSet 对象，会得到以下结果，Kubernetes 不仅会创建 StatefulSet 对象，还会自动创建两个 Pod 副本：\n$ kubectl get statefulsets.apps\rkNAME READY AGE\rweb 2/2 2m27s\r$ kubectl get pods\rNAME READY STATUS RESTARTS AGE\rweb-0 1/1 Running 0 2m31s\rweb-1 1/1 Running 0 105s\r$ kubectl get persistentvolumes\rNAME CAPACITY ACCESS MODES RECLAIM POLICY STATUS CLAIM STORAGECLASS REASON AGE\rpvc-19ef374f-39d1-11e9-b870-9efb418608da 1Gi RWO Delete Bound default/www-web-1 do-block-storage 21m\rpvc-fe53d5f7-39d0-11e9-b870-9efb418608da 1Gi RWO Delete Bound default/www-web-0 do-block-storage 21m\r$ kubectl get persistentvolumeclaims\rNAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGE\rwww-web-0 Bound pvc-fe53d5f7-39d0-11e9-b870-9efb418608da 1Gi RWO do-block-storage 21m\rwww-web-1 Bound pvc-19ef374f-39d1-11e9-b870-9efb418608da 1Gi RWO do-block-storage 21m\r Go\n除此之外，上述 YAML 文件中的 volumeClaimTemplates 配置还会创建持久卷PersistentVolume 和用于绑定持久卷和 Pod 的 PersistentVolumeClaim 资源；两个 Pod 对象名中包含了它们的序列号，该序列号会在 StatefulSet 存在的时间内保持不变，哪怕 Pod 被重启或者重新调度，也不会出现任何的改变。\ngraph TD\rSS[StatefulSet]-.-\u0026gt;Pod1\rSS[StatefulSet]-.-\u0026gt;Pod2\rPod1-.-PersistentVolumeClaim1\rPod2-.-PersistentVolumeClaim2\rPersistentVolumeClaim1-.-\u0026gt;PersistentVolume1\rPersistentVolumeClaim2-.-\u0026gt;PersistentVolume2\r Mermaid\nStatefulSet 的拓扑结构和其他用于部署的资源其实比较类似，比较大的区别在于 StatefulSet 引入了 PV 和 PVC 对象来持久存储服务产生的状态，这样所有的服务虽然可以被杀掉或者重启，但是其中的数据由于 PV 的原因不会丢失。\n 这里不会展开介绍 PV 和 PVC，感兴趣的读者可以阅读 详解 Kubernetes Volume 的实现原理 了解 Kubernetes 存储系统的实现原理。\n 实现原理 与 ReplicaSet 和 Deployment 资源一样，StatefulSet 也使用控制器的方式实现，它主要由 StatefulSetController、StatefulSetControl 和 StatefulPodControl 三个组件协作来完成 StatefulSet 的管理，StatefulSetController 会同时从 PodInformer 和 ReplicaSetInformer 中接受增删改事件并将事件推送到队列中：\ngraph TD\rPI[PodInformer]-. Add/Update/Delete .-\u0026gt;SSC[StatefulSetController]\rRSI[ReplicaSetInformer]-. Add/Update/Delete .-\u0026gt;SSC\rSSC--\u0026gt;worker1\rSSC-. Add StatefulSet .-\u0026gt;queue\rworker1-. Loop .-\u0026gt;worker1\rqueue-. Get StatefulSet .-\u0026gt;worker1\rSSC--\u0026gt;worker2\rworker2-. Loop .-\u0026gt;worker2\rqueue-. Get StatefulSet .-\u0026gt;worker2\rstyle worker1 fill:#fffede,stroke:#ebebb7\rstyle worker2 fill:#fffede,stroke:#ebebb7\r Mermaid\n控制器 StatefulSetController 会在 Run 方法中启动多个 Goroutine 协程，这些协程会从队列中获取待处理的 StatefulSet 资源进行同步，接下来我们会先介绍 Kubernetes 同步 StatefulSet 的过程。\n同步 StatefulSetController 使用 sync 方法同步 StatefulSet 资源，这是同步该资源的唯一入口，下面是这个方法的具体实现：\nfunc (ssc *StatefulSetController) sync(key string) error {\rnamespace, name, _ := cache.SplitMetaNamespaceKey(key)\rset, _ := ssc.setLister.StatefulSets(namespace).Get(name)\rssc.adoptOrphanRevisions(set)\rselector, _ := metav1.LabelSelectorAsSelector(set.Spec.Selector)\rpods, _ := ssc.getPodsForStatefulSet(set, selector)\rreturn ssc.syncStatefulSet(set, pods)\r}\rfunc (ssc *StatefulSetController) syncStatefulSet(set *apps.StatefulSet, pods []*v1.Pod) error {\rssc.control.UpdateStatefulSet(set.DeepCopy(), pods); err != nil\rreturn nil\r}\r Go\n 先重新获取 StatefulSet 对象； 收养集群中与 StatefulSet 有关的孤立控制器版本； 获取当前 StatefulSet 对应的全部 Pod 副本； 调用 syncStatefulSet 方法同步资源；  syncStatefulSet 方法只是将方法的调用转发到了一个 StatefulSetControlInterface 的实现 defaultStatefulSetControl 上，StatefulSetControlInterface 定义了用与控制 StatefulSet 和 Pod 副本的接口，这里调用的 UpdateStatefulSet 函数执行了一个 StatefulSet 的核心逻辑，它会负责获取 StatefulSet 版本、更新 StatefulSet 以及它的状态和历史：\nfunc (ssc *defaultStatefulSetControl) UpdateStatefulSet(set *apps.StatefulSet, pods []*v1.Pod) error {\rrevisions, err := ssc.ListRevisions(set)\rhistory.SortControllerRevisions(revisions)\rcurrentRevision, updateRevision, collisionCount, err := ssc.getStatefulSetRevisions(set, revisions)\rstatus, err := ssc.updateStatefulSet(set, currentRevision, updateRevision, collisionCount, pods)\rssc.updateStatefulSetStatus(set, status)\rreturn ssc.truncateHistory(set, pods, revisions, currentRevision, updateRevision)\r}\r Go\n它会使用默认的单调递增策略，按照升序依次创建副本并按照降序删除副本，当出现 Pod 处于不健康的状态时，那么新的 Pod 就不会被创建，StatefulSetController 会等待 Pod 恢复后继续执行下面的逻辑。\n上述代码会在获取 StatefulSet 的历史版本之后调用 updateStatefulSet 方法开始更新 StatefulSet，这个将近 300 行的代码会按照执行以下的执行：\n 将当前 StatefulSet 持有的 Pod 副本按照序列号进行分组，超出数量的副本将被分入 condemned 中等待后续的删除操作，这次同步中需要保留的副本将进入 replicas 分组； 对当前的 StatefulSet 进行扩容，让集群达到目标的副本数； 获取副本数组中第一个不健康的 Pod； 根据副本的序列号检查各个副本的状态；  如果发现了失败的副本就会进行重启； 如果当前副本没有正常运行就会退出循环，直到当前副本达到正常运行的状态；   按照降序依次删除 condemned 数组中的副本； 按照降序依次更新 replicas 数组中的副本；  func (ssc *defaultStatefulSetControl) updateStatefulSet(set *apps.StatefulSet, currentRevision *apps.ControllerRevision, updateRevision *apps.ControllerRevision, collisionCount int32, pods []*v1.Pod) (*apps.StatefulSetStatus, error) {\rcurrentSet, _ := ApplyRevision(set, currentRevision)\rupdateSet, _ := ApplyRevision(set, updateRevision)\rstatus := apps.StatefulSetStatus{}\r// ...\rreplicaCount := int(*set.Spec.Replicas)\rreplicas := make([]*v1.Pod, replicaCount)\rcondemned := make([]*v1.Pod, 0, len(pods))\runhealthy := 0\rvar firstUnhealthyPod *v1.Pod\rfor i := range pods {\rif ord := getOrdinal(pods[i]); 0 \u0026lt;= ord \u0026amp;\u0026amp; ord \u0026lt; replicaCount {\rreplicas[ord] = pods[i]\r} else if ord \u0026gt;= replicaCount {\rcondemned = append(condemned, pods[i])\r}\r}\rsort.Sort(ascendingOrdinal(condemned))\rfor ord := 0; ord \u0026lt; replicaCount; ord++ {\rif replicas[ord] == nil {\rreplicas[ord] = newVersionedStatefulSetPod(currentSet, updateSet, currentRevision.Name, updateRevision.Name, ord)\r}\r}\r Go\n这里通过 StatefulSet 应该持有的副本数对当前的副本进行分组，一部分是需要保证存活的 replicas，另一部分是需要被终止的副本 condemned，如果分组后的 replicas 数量不足，就会通过 newVersionedStatefulSetPod 函数创建新的 Pod，不过这里的 Pod 也只是待创建的模板。\n拿到线上应该存在的 replicas 数组时，我们就可以进行通过 CreateStatefulPod 进行扩容了，每个 Pod 的更新和创建都会等待前面所有 Pod 正常运行，它会调用 isFailed、isCreated、isTerminating 等方法保证每一个 Pod 都正常运行时才会继续处理下一个 Pod，如果使用滚动更新策略，那么会在完成扩容之后才会对当前的 Pod 进行更新：\nfor i := range replicas {\rif isFailed(replicas[i]) {\rssc.podControl.DeleteStatefulPod(set, replicas[i])\rreplicas[i] = newVersionedStatefulSetPod(i)\r}\rif !isCreated(replicas[i]) {\rssc.podControl.CreateStatefulPod(set, replicas[i])\rreturn \u0026amp;status, nil\r}\rif isTerminating(replicas[i]) || !isRunningAndReady(replicas[i]) {\rreturn \u0026amp;status, nil\r}\rif identityMatches(set, replicas[i]) \u0026amp;\u0026amp; storageMatches(set, replicas[i]) {\rcontinue\r}\rreplica := replicas[i].DeepCopy()\rssc.podControl.UpdateStatefulPod(updateSet, replica)\r}\r Go\n当 StatefulSetController 处理完副本的创建和更新任务之后，就开始删除需要抛弃的节点了，节点的删除也需要确定按照降序依次进行：\nfor target := len(condemned) - 1; target \u0026gt;= 0; target-- {\rif isTerminating(condemned[target]) {\rreturn \u0026amp;status, nil\r}\rif !isRunningAndReady(condemned[target]) \u0026amp;\u0026amp; condemned[target] != firstUnhealthyPod {\rreturn \u0026amp;status, nil\r}\rssc.podControl.DeleteStatefulPod(set, condemned[target])\rreturn \u0026amp;status, nil\r}\rupdateMin := 0\rif set.Spec.UpdateStrategy.RollingUpdate != nil {\rupdateMin = int(*set.Spec.UpdateStrategy.RollingUpdate.Partition)\r}\rfor target := len(replicas) - 1; target \u0026gt;= updateMin; target-- {\rif getPodRevision(replicas[target]) != updateRevision.Name \u0026amp;\u0026amp; !isTerminating(replicas[target]) {\rssc.podControl.DeleteStatefulPod(set, replicas[target])\rreturn \u0026amp;status, err\r}\rif !isHealthy(replicas[target]) {\rreturn \u0026amp;status, nil\r}\r}\rreturn \u0026amp;status, nil\r}\r Go\n我们首先会删除待抛弃列表中的副本，其次根据滚动更新 RollingUpdate 的配置从高到低依次删除所有 Pod 版本已经过时的节点，所有删除节点的方式都会通过 DeleteStatefulPod 方法进行，该方法会通过客户端的接口直接根据 Pod 名称删除对应的资源。\n序列号 Pod 的序列号（Ordinal）是其唯一性和顺序性的保证，在创建和删除 StatefulSet 的副本时，我们都需要按照 Pod 的序列号对它们按照顺序操作，副本的创建会按照序列号升序处理，副本的更新和删除会按照序列号降序处理。\n创建 StatefulSet 中的副本时，就会在 newStatefulSetPod 函数中传入当前 Pod 的 ordinal 信息，该方法会调用 GetPodFromTemplate 获取 StatefulSet 中的 Pod 模板并且初始化 Pod 的 metadata 和引用等配置：\nfunc newStatefulSetPod(set *apps.StatefulSet, ordinal int) *v1.Pod {\rpod, _ := controller.GetPodFromTemplate(\u0026amp;set.Spec.Template, set, metav1.NewControllerRef(set, controllerKind))\rpod.Name = getPodName(set, ordinal)\rinitIdentity(set, pod)\rupdateStorage(set, pod)\rreturn pod\r}\rfunc getPodName(set *apps.StatefulSet, ordinal int) string {\rreturn fmt.Sprintf(\u0026quot;%s-%d\u0026quot;, set.Name, ordinal)\r}\r Go\ngetPodName 函数的实现非常简单，它将 StatefulSet 的名字和传入的序列号通过破折号连接起来组成我们经常见到的 web-0、web-1 等形式的副本名；initIdentity 会更新 Pod 的主机名、资源名、命名空间标签，而 updateStorage 会为待创建的副本设置卷：\nfunc updateStorage(set *apps.StatefulSet, pod *v1.Pod) {\rcurrentVolumes := pod.Spec.Volumes\rclaims := getPersistentVolumeClaims(set, pod)\rnewVolumes := make([]v1.Volume, 0, len(claims))\rfor name, claim := range claims {\rnewVolumes = append(newVolumes, v1.Volume{\rName: name,\rVolumeSource: v1.VolumeSource{\rPersistentVolumeClaim: \u0026amp;v1.PersistentVolumeClaimVolumeSource{\rClaimName: claim.Name,\rReadOnly: false,\r},\r},\r})\r}\rfor i := range currentVolumes {\rif _, ok := claims[currentVolumes[i].Name]; !ok {\rnewVolumes = append(newVolumes, currentVolumes[i])\r}\r}\rpod.Spec.Volumes = newVolumes\r}\r Go\n设置卷的配置主要来自于 StatefulSet 规格中的 volumeClaimTemplates 模板，所有卷相关的配置信息都会通过该方法传递过来。\nPod 通过当前名字存储自己对应的序列号，在 StatefulSetController 同步时就会从 Pod 的名字中取出序列号并进行排序，随后的各种循环就可以选择使用正序或者倒序的方式依次处理各个节点了。\n删除 当我们删除一个 Kubernetes 中的 StatefulSet 资源时，它对应的全部 Pod 副本都会被 垃圾收集器 自动删除，该收集器在检查到当前 Pod 的 metadata.ownerReferences 已经不再存在时就会删除 Pod 资源，读者可以阅读 垃圾收集器 了解具体的执行过程和实现原理。\n$ kubectl delete statefulsets.apps web\rstatefulset.apps \u0026quot;web\u0026quot; deleted\r$ kubectl get pods --watch\rNAME READY STATUS RESTARTS AGE\rweb-2 1/1 Terminating 0 14h\rweb-1 1/1 Terminating 0 14h\rweb-0 1/1 Terminating 0 14h\r Go\n我们会发现除了 StatefulSet 和 Pod 之外的任何其他资源都没有被删除，之前创建的 PersistentVolume 和 PersistentVolumeClaim 对象都没有发生任何的变化，这也是 StatefulSet 的行为，它会在服务被删除之后仍然保留其中的状态，也就是数据，这些数据就都存储在 PersistentVolume 中。\n如果我们重新创建相同的 StatefulSet，它还会使用之前的 PV 和 PVC 对象，不过也可以选择手动删除所有的 PV 和 PVC 来生成新的存储，这两个对象都属于 Kubernetes 的存储系统，感兴趣的读者可以通过 存储系统 了解 Kubernetes 中 Volume 的设计和实现。\n总结 StatefulSet 是 Kubernetes 为了处理有状态服务引入的概念，在有状态服务中，它为无序和短暂的 Pod 引入了顺序性和唯一性，使得 Pod 的创建和删除更容易被掌控和预测，同时加入 PV 和 PVC 对象来存储这些 Pod 的状态，我们可以使用 StatefulSet 实现一些偏存储的有状态系统，例如 Zookeeper、Kafka、MongoDB 等，这些系统大多数都需要持久化的存储数据，防止在服务宕机时发生数据丢失。\nKubernetes DaemonSet 不同的维度解决了集群中的问题 — 如何同时在集群中的所有节点上提供基础服务和守护进程。\n我们在这里将介绍 DaemonSet 如何进行状态的同步、Pod 与节点（Node）之间的调度方式和滚动更新的过程以及实现原理。\n概述 DaemonSet 可以保证集群中所有的或者部分的节点都能够运行同一份 Pod 副本，每当有新的节点被加入到集群时，Pod 就会在目标的节点上启动，如果节点被从集群中剔除，节点上的 Pod 也会被垃圾收集器清除；DaemonSet 的作用就像是计算机中的守护进程，它能够运行集群存储、日志收集和监控等『守护进程』，这些服务一般是集群中必备的基础服务。\nGoogle Cloud 的 Kubernetes 集群就会在所有的节点上启动 fluentd 和 Prometheus 来收集节点上的日志和监控数据，想要创建用于日志收集的守护进程其实非常简单，我们可以使用如下所示的代码：\napiVersion: apps/v1\rkind: DaemonSet\rmetadata:\rname: fluentd-elasticsearch\rnamespace: kube-system\rspec:\rselector:\rmatchLabels:\rname: fluentd-elasticsearch\rtemplate:\rmetadata:\rlabels:\rname: fluentd-elasticsearch\rspec:\rcontainers:\r- name: fluentd-elasticsearch\rimage: k8s.gcr.io/fluentd-elasticsearch:1.20\rvolumeMounts:\r- name: varlog\rmountPath: /var/log\r- name: varlibdockercontainers\rmountPath: /var/lib/docker/containers\rreadOnly: true\rvolumes:\r- name: varlog\rhostPath:\rpath: /var/log\r- name: varlibdockercontainers\rhostPath:\rpath: /var/lib/docker/containers\r YAML\n当我们使用 kubectl apply -f 创建上述的 DaemonSet 时，它会在 Kubernetes 集群的 kube-system 命名空间中创建 DaemonSet 资源并在所有的节点上创建新的 Pod：\n$ kubectl get daemonsets.apps fluentd-elasticsearch --namespace kube-system\rNAME DESIRED CURRENT READY UP-TO-DATE AVAILABLE NODE SELECTOR AGE\rfluentd-elasticsearch 1 1 1 1 1 \u0026lt;none\u0026gt; 19h\r$ kubectl get pods --namespace kube-system --label name=fluentd-elasticsearch\rNAME READY STATUS RESTARTS AGE\rfluentd-elasticsearch-kvtwj 1/1 Running 0 19h\r Go\n由于集群中只存在一个 Pod，所以 Kubernetes 只会在该节点上创建一个 Pod，如果我们向当前的集群中增加新的节点时，Kubernetes 就会创建在新节点上创建新的副本，总的来说，我们能够得到以下的拓扑结构：\n集群中的 Pod 和 Node 一一对应，而 DaemonSet 会管理全部机器上的 Pod 副本，负责对它们进行更新和删除。\n实现原理 所有的 DaemonSet 都是由控制器负责管理的，与其他的资源一样，用于管理 DaemonSet 的控制器是 DaemonSetsController，该控制器会监听 DaemonSet、ControllerRevision、Pod 和 Node 资源的变动。\n大多数的触发事件最终都会将一个待处理的 DaemonSet 资源入栈，下游 DaemonSetsController 持有的多个工作协程就会从队列里面取出资源进行消费和同步。\n同步 DaemonSetsController 同步 DaemonSet 资源使用的方法就是 syncDaemonSet，这个方法从队列中拿到 DaemonSet 的名字时，会先从集群中获取最新的 DaemonSet 对象并通过 constructHistory 方法查找当前 DaemonSet 全部的历史版本：\nfunc (dsc *DaemonSetsController) syncDaemonSet(key string) error {\rnamespace, name, _ := cache.SplitMetaNamespaceKey(key)\rds, _ := dsc.dsLister.DaemonSets(namespace).Get(name)\rdsKey, _ := controller.KeyFunc(ds)\rcur, old, _ := dsc.constructHistory(ds)\rhash := cur.Labels[apps.DefaultDaemonSetUniqueLabelKey]\rdsc.manage(ds, hash)\rswitch ds.Spec.UpdateStrategy.Type {\rcase apps.OnDeleteDaemonSetStrategyType:\rcase apps.RollingUpdateDaemonSetStrategyType:\rdsc.rollingUpdate(ds, hash)\r}\rdsc.cleanupHistory(ds, old)\rreturn dsc.updateDaemonSetStatus(ds, hash, true)\r}\r Go\n然后调用的 manage 方法会负责管理 DaemonSet 在节点上 Pod 的调度和运行，rollingUpdate 会负责 DaemonSet 的滚动更新；前者会先找出找出需要运行 Pod 和不需要运行 Pod 的节点，并调用 syncNodes 对这些需要创建和删除的 Pod 进行同步：\nfunc (dsc *DaemonSetsController) syncNodes(ds *apps.DaemonSet, podsToDelete, nodesNeedingDaemonPods []string, hash string) error {\rdsKey, _ := controller.KeyFunc(ds)\rgeneration, err := util.GetTemplateGeneration(ds)\rtemplate := util.CreatePodTemplate(ds.Spec.Template, generation, hash)\rcreateDiff := len(nodesNeedingDaemonPods)\rcreateWait := sync.WaitGroup{}\rcreateWait.Add(createDiff)\rfor i := 0; i \u0026lt; createDiff; i++ {\rgo func(ix int) {\rdefer createWait.Done()\rpodTemplate := template.DeepCopy()\rif utilfeature.DefaultFeatureGate.Enabled(features.ScheduleDaemonSetPods) {\rpodTemplate.Spec.Affinity = util.ReplaceDaemonSetPodNodeNameNodeAffinity(podTemplate.Spec.Affinity, nodesNeedingDaemonPods[ix])\rdsc.podControl.CreatePodsWithControllerRef(ds.Namespace, podTemplate, ds, metav1.NewControllerRef(ds, controllerKind))\r} else {\rpodTemplate.Spec.SchedulerName = \u0026quot;kubernetes.io/daemonset-controller\u0026quot;\rdsc.podControl.CreatePodsOnNode(nodesNeedingDaemonPods[ix], ds.Namespace, podTemplate, ds, metav1.NewControllerRef(ds, controllerKind))\r}\r}(i)\r}\rcreateWait.Wait()\r Go\n获取了 DaemonSet 中的模板之之后，就会开始并行地为节点创建 Pod 副本，并发创建的过程使用了 for 循环、Goroutine 和 WaitGroup 保证程序运行的正确，然而这里使用了特性开关来对调度新 Pod 的方式进行了控制，我们会在接下来的调度一节介绍 DaemonSet 调度方式的变迁和具体的执行过程。\n当 Kubernetes 创建了需要创建的 Pod 之后，就需要删除所有节点上不必要的 Pod 了，这里使用同样地方式并发地对 Pod 进行删除：\ndeleteDiff := len(podsToDelete)\rdeleteWait := sync.WaitGroup{}\rdeleteWait.Add(deleteDiff)\rfor i := 0; i \u0026lt; deleteDiff; i++ {\rgo func(ix int) {\rdefer deleteWait.Done()\rdsc.podControl.DeletePod(ds.Namespace, podsToDelete[ix], ds)\r}(i)\r}\rdeleteWait.Wait()\rreturn nil\r}\r Go\n到了这里我们就完成了节点上 Pod 的调度和运行，为一些节点创建 Pod 副本的同时删除另一部分节点上的副本，manage 方法执行完成之后就会调用 rollingUpdate 方法对 DaemonSet 的节点进行滚动更新并对控制器版本进行清理并更新 DaemonSet 的状态，文章后面的部分会介绍滚动更新的过程和实现。\n调度 在早期的 Kubernetes 版本中，所有 DaemonSet Pod 的创建都是由 DaemonSetsController 负责的，而其他的资源都是由 kube-scheduler 进行调度，这就导致了如下的一些问题：\n DaemonSetsController 没有办法在节点资源变更时收到通知 (#46935, #58868)； DaemonSetsController 没有办法遵循 Pod 的亲和性和反亲和性设置 (#29276)； DaemonSetsController 可能需要二次实现 Pod 调度的重要逻辑，造成了重复的代码逻辑 (#42028)； 多个组件负责调度会导致 Debug 和抢占等功能的实现非常困难；  设计文档 Schedule DaemonSet Pods by default scheduler, not DaemonSet controller 中包含了使用 DaemonSetsController 调度时遇到的问题以及新设计给出的解决方案。\n如果我们选择使用过去的调度方式，DeamonSetsController 就会负责在节点上创建 Pod，通过这种方式创建的 Pod 的 schedulerName 都会被设置成 kubernetes.io/daemonset-controller，但是在默认情况下这个字段一般为 default-scheduler，也就是使用 Kubernetes 默认的调度器 kube-scheduler 进行调度：\nfunc (dsc *DaemonSetsController) syncNodes(ds *apps.DaemonSet, podsToDelete, nodesNeedingDaemonPods []string, hash string) error {\r// ...\rfor i := 0; i \u0026lt; createDiff; i++ {\rgo func(ix int) {\rpodTemplate := template.DeepCopy()\rif utilfeature.DefaultFeatureGate.Enabled(features.ScheduleDaemonSetPods) {\r// ...\r} else {\rpodTemplate.Spec.SchedulerName = \u0026quot;kubernetes.io/daemonset-controller\u0026quot;\rdsc.podControl.CreatePodsOnNode(nodesNeedingDaemonPods[ix], ds.Namespace, podTemplate, ds, metav1.NewControllerRef(ds, controllerKind))\r}\r}(i)\r}\r// ...\r}\r Go\nDaemonSetsController 在调度 Pod 时都会使用 CreatePodsOnNode 方法，这个方法的实现非常简单，它会先对 Pod 模板进行验证，随后调用 createPods 方法通过 Kubernetes 提供的 API 创建新的副本：\nfunc (r RealPodControl) CreatePodsWithControllerRef(namespace string, template *v1.PodTemplateSpec, controllerObject runtime.Object, controllerRef *metav1.OwnerReference) error {\rif err := validateControllerRef(controllerRef); err != nil {\rreturn err\r}\rreturn r.createPods(\u0026quot;\u0026quot;, namespace, template, controllerObject, controllerRef)\r}\r Go\nDaemonSetsController 通过节点选择器和调度器的谓词对节点进行过滤，createPods 会直接为当前的 Pod 设置 spec.NodeName 属性，最后得到的 Pod 就会被目标节点上的 kubelet 创建。\n除了这种使用 DaemonSetsController 管理和调度 DaemonSet 的方法之外，我们还可以使用 Kubernetes 默认的方式 kube-scheduler 创建新的 Pod 副本：\nfunc (dsc *DaemonSetsController) syncNodes(ds *apps.DaemonSet, podsToDelete, nodesNeedingDaemonPods []string, hash string) error {\r// ...\rfor i := 0; i \u0026lt; createDiff; i++ {\rgo func(ix int) {\rpodTemplate := template.DeepCopy()\rif utilfeature.DefaultFeatureGate.Enabled(features.ScheduleDaemonSetPods) {\rpodTemplate.Spec.Affinity = util.ReplaceDaemonSetPodNodeNameNodeAffinity(podTemplate.Spec.Affinity, nodesNeedingDaemonPods[ix])\rdsc.podControl.CreatePodsWithControllerRef(ds.Namespace, podTemplate, ds, metav1.NewControllerRef(ds, controllerKind))\r} else {\r// ...\r}\r}(i)\r}\r// ...\r}\r Go\n这种情况会使用 NodeAffinity 特性来避免发生在 DaemonSetsController 中的调度：\n  DaemonSetsController 会在 podsShouldBeOnNode 方法中根据节点选择器过滤所有的节点；\n  对于每一个节点，控制器都会创建一个遵循以下节点亲和的 Pod；\nnodeAffinity:\rrequiredDuringSchedulingIgnoredDuringExecution:\r- nodeSelectorTerms:\rmatchExpressions:\r- key: kubernetes.io/hostname\roperator: in\rvalues:\r- dest_hostname\r YAML\n  当节点进行同步时，DaemonSetsController 会根据节点亲和的设置来验证节点和 Pod 的关系；\n  如果调度的谓词失败了，DaemonSet 持有的 Pod 就会保持在 Pending 的状态，所以可以通过修改 Pod 的优先级和抢占保证集群在高负载下也能正常运行 DaemonSet 的副本；\n  Pod 的优先级和抢占功能在 Kubernetes 1.8 版本引入，1.11 时转变成 beta 版本，在目前最新的 1.13 中依然是 beta 版本，感兴趣的读者可以阅读 Pod Priority and Preemption 文档了解相关的内容。\n滚动更新 DaemonSetsController 对滚动更新的实现其实比较简单，它其实就是根据 DaemonSet 规格中的配置，删除集群中的 Pod 并保证同时不可用的副本数不会超过 spec.updateStrategy.rollingUpdate.maxUnavailable，这个参数也是 DaemonSet 滚动更新可以配置的唯一参数：\nfunc (dsc *DaemonSetsController) rollingUpdate(ds *apps.DaemonSet, hash string) error {\rnodeToDaemonPods, err := dsc.getNodesToDaemonPods(ds)\r_, oldPods := dsc.getAllDaemonSetPods(ds, nodeToDaemonPods, hash)\rmaxUnavailable, numUnavailable, err := dsc.getUnavailableNumbers(ds, nodeToDaemonPods)\roldAvailablePods, oldUnavailablePods := util.SplitByAvailablePods(ds.Spec.MinReadySeconds, oldPods)\rvar oldPodsToDelete []string\rfor _, pod := range oldUnavailablePods {\rif pod.DeletionTimestamp != nil {\rcontinue\r}\roldPodsToDelete = append(oldPodsToDelete, pod.Name)\r}\rfor _, pod := range oldAvailablePods {\rif numUnavailable \u0026gt;= maxUnavailable {\rbreak\r}\roldPodsToDelete = append(oldPodsToDelete, pod.Name)\rnumUnavailable++\r}\rreturn dsc.syncNodes(ds, oldPodsToDelete, []string{}, hash)\r}\r Go\n删除 Pod 的顺序其实也非常简单并且符合直觉，上述代码会将不可用的 Pod 先加入到待删除的数组中，随后将历史版本的可用 Pod 加入待删除数组 oldPodsToDelete，最后调用 syncNodes 完成对副本的删除。\n删除 与 Deployment、ReplicaSet 和 StatefulSet 一样，DaemonSet 的删除也会导致它持有的 Pod 的删除，如果我们使用如下的命令删除该对象，我们能观察到如下的现象：\n$ kubectl delete daemonsets.apps fluentd-elasticsearch --namespace kube-system\rdaemonset.apps \u0026quot;fluentd-elasticsearch\u0026quot; deleted\r$ kubectl get pods --watch --namespace kube-system\rfluentd-elasticsearch-wvffx 1/1 Terminating 0 14s\r Bash\n这部分的工作就都是由 Kubernetes 中的垃圾收集器完成的，读者可以阅读 垃圾收集器 一文了解集群中的不同对象是如何进行关联的以及在删除单一对象时如何触发级联删除的原理。\n总结 DaemonSet 其实就是 Kubernetes 中的守护进程，它会在每一个节点上创建能够提供服务的副本，很多云服务商都会使用 DaemonSet 在所有的节点上内置一些用于提供日志收集、统计分析和安全策略的服务。\n在研究 DaemonSet 的调度策略的过程中，我们其实能够通过一些历史的 issue 和 PR 了解到 DaemonSet 调度策略改动的原因，也能让我们对于 Kubernetes 的演进过程和设计决策有一个比较清楚的认识。\nKubernetes Job \u0026amp; CronJob 之前介绍了 Kubernetes 中用于长期提供服务的 ReplicaSet、Deployment、StatefulSet 和 DaemonSet 等资源，但是作为一个容器编排引擎，任务和定时任务的支持是一个必须要支持的功能。\nKubernetes 中使用 Job 和 CronJob 两个资源分别提供了一次性任务和定时任务的特性，这两种对象也使用控制器模型来实现资源的管理，我们在这篇文章种就会介绍它们的实现原理。\n概述 Kubernetes 中的 Job 可以创建并且保证一定数量 Pod 的成功停止，当 Job 持有的一个 Pod 对象成功完成任务之后，Job 就会记录这一次 Pod 的成功运行；当一定数量的Pod 的任务执行结束之后，当前的 Job 就会将它自己的状态标记成结束。\n上述图片中展示了一个 spec.completions=3 的任务的状态随着 Pod 的成功执行而更新和迁移状态的过程，从图中我们能比较清楚的看到 Job 和 Pod 之间的关系，假设我们有一个用于计算圆周率的如下任务：\napiVersion: batch/v1\rkind: Job\rmetadata:\rname: pi\rspec:\rcompletions: 10\rparallelism: 5\rtemplate:\rspec:\rcontainers:\r- name: pi\rimage: perl\rcommand: [\u0026quot;perl\u0026quot;, \u0026quot;-Mbignum=bpi\u0026quot;, \u0026quot;-wle\u0026quot;, \u0026quot;print bpi(2000)\u0026quot;]\rrestartPolicy: Never\rbackoffLimit: 4\r YAML\n当我们在 Kubernetes 中创建上述任务时，使用 kubectl 能够观测到以下的信息：\n$ k apply -f job.yaml\rjob.batch/pi created\r$ kubectl get job --watch\rNAME COMPLETIONS DURATION AGE\rpi 0/10 1s 1s\rpi 1/10 36s 36s\rpi 2/10 46s 46s\rpi 3/10 54s 54s\rpi 4/10 60s 60s\rpi 5/10 65s 65s\rpi 6/10 90s 90s\rpi 7/10 99s 99s\rpi 8/10 104s 104s\rpi 9/10 107s 107s\rpi 10/10 109s 109s\r Bash\n由于任务 pi 在配置时指定了 spec.completions=10，所以当前的任务需要等待 10 个 Pod 的成功执行，另一个比较重要的 spec.parallelism=5 表示最多有多少个并发执行的任务，如果 spec.parallelism=1 那么所有的任务都会依次顺序执行，只有前一个任务执行成功时，后一个任务才会开始工作。\n每一个 Job 对象都会持有一个或者多个 Pod，而每一个 CronJob 就会持有多个 Job 对象，CronJob 能够按照时间对任务进行调度，它与 crontab 非常相似，我们可以使用 Cron 格式快速指定任务的调度时间：\napiVersion: batch/v1beta1\rkind: CronJob\rmetadata:\rname: pi\rspec:\rschedule: \u0026quot;*/1 * * * *\u0026quot;\rjobTemplate:\rspec:\rcompletions: 2\rparallelism: 1\rtemplate:\rspec:\rcontainers:\r- name: pi\rimage: perl\rcommand: [\u0026quot;perl\u0026quot;, \u0026quot;-Mbignum=bpi\u0026quot;, \u0026quot;-wle\u0026quot;, \u0026quot;print bpi(2000)\u0026quot;]\rrestartPolicy: OnFailure\r YAML\n上述的 CronJob 对象被创建之后，每分钟都会创建一个新的 Job 对象，所有的 CronJob 创建的任务都会带有调度时的时间戳，例如：pi-1551660600 和 pi-1551660660 两个任务：\n$ k get cronjob --watch\rNAME SCHEDULE SUSPEND ACTIVE LAST SCHEDULE AGE\rpi */1 * * * * False 0 \u0026lt;none\u0026gt; 3s\rpi */1 * * * * False 1 1s 7s\r$ k get job --watch\rNAME COMPLETIONS DURATION AGE\rpi-1551660600 0/3 0s 0s\rpi-1551660600 1/3 16s 16s\rpi-1551660600 2/3 31s 31s\rpi-1551660600 3/3 44s 44s\rpi-1551660660 0/3 1s\rpi-1551660660 0/3 1s 1s\rpi-1551660660 1/3 14s 14s\rpi-1551660660 2/3 28s 28s\rpi-1551660660 3/3 42s 43s\r Bash\nCronJob 中保存的任务其实是有上限的，spec.successfulJobsHistoryLimit 和 spec.failedJobsHistoryLimit 分别记录了能够保存的成功或者失败的任务上限，超过这个上限的任务都会被删除，默认情况下这两个属性分别为 spec.successfulJobsHistoryLimit=3 和 spec.failedJobsHistoryLimit=1。\n任务 Job 遵循 Kubernetes 的控制器模式进行设计，在发生需要监听的事件时，Informer 就会调用控制器中的回调将需要处理的资源 Job 加入队列，而控制器持有的工作协程就会处理这些任务。\n用于处理 Job 资源的 JobController 控制器会监听 Pod 和 Job 这两个资源的变更事件，而资源的同步还是需要运行 syncJob 方法。\n同步 syncJob 是 JobController 中主要用于同步 Job 资源的方法，这个方法的主要作用就是对资源进行同步，它的大体架构就是先获取一些当前同步的基本信息，然后调用 manageJob 方法管理 Job 对应的 Pod 对象，最后计算出处于 active、failed 和 succeed 三种状态的 Pod 数量并更新 Job 的状态：\nfunc (jm *JobController) syncJob(key string) (bool, error) {\rns, name, _ := cache.SplitMetaNamespaceKey(key)\rsharedJob, _ := jm.jobLister.Jobs(ns).Get(name)\rjob := *sharedJob\rjobNeedsSync := jm.expectations.SatisfiedExpectations(key)\rpods, _ := jm.getPodsForJob(\u0026amp;job)\ractivePods := controller.FilterActivePods(pods)\ractive := int32(len(activePods))\rsucceeded, failed := getStatus(pods)\rconditions := len(job.Status.Conditions)\r Go\n这一部分代码会从 apiserver 中该名字对应的 Job 对象，然后获取 Job 对应的 Pod 数组并根据计算不同状态 Pod 的数量，为之后状态的更新和比对做准备。\nvar manageJobErr error\rif jobNeedsSync \u0026amp;\u0026amp; job.DeletionTimestamp == nil {\ractive, manageJobErr = jm.manageJob(activePods, succeeded, \u0026amp;job)\r}\r Go\n准备工作完成之后，会调用 manageJob 方法，该方法主要负责管理 Job 持有的一系列 Pod 的运行，它最终会返回目前集群中当前 Job 对应的活跃任务的数量。\ncompletions := succeeded\rcomplete := false\rif job.Spec.Completions == nil {\rif succeeded \u0026gt; 0 \u0026amp;\u0026amp; active == 0 {\rcomplete = true\r}\r} else {\rif completions \u0026gt;= *job.Spec.Completions {\rcomplete = true\r}\r}\rif complete {\rjob.Status.Conditions = append(job.Status.Conditions, newCondition(batch.JobComplete, \u0026quot;\u0026quot;, \u0026quot;\u0026quot;))\rnow := metav1.Now()\rjob.Status.CompletionTime = \u0026amp;now\r}\rif job.Status.Active != active || job.Status.Succeeded != succeeded || job.Status.Failed != failed || len(job.Status.Conditions) != conditions {\rjob.Status.Active = active\rjob.Status.Succeeded = succeeded\rjob.Status.Failed = failed\rjm.updateHandler(\u0026amp;job)\r}\rreturn forget, manageJobErr\r}\r Go\n最后的这段代码会将 Job 规格中设置的 spec.completions 和已经完成的任务数量进行比对，确认当前的 Job 是否已经结束运行，如果任务已经结束运行就会更新当前 Job 的完成时间，同时当 JobController 发现有一些状态没有正确同步时，也会调用 updateHandler 更新资源的状态。\n并行执行 Pod 的创建和删除都是由 manageJob 这个方法负责的，这个方法根据 Job 的 spec.parallelism 配置对目前集群中的节点进行创建和删除。\n如果当前正在执行的活跃节点数量超过了 spec.parallelism，那么就会按照创建的升删除多余的任务，删除任务时会使用 DeletePod 方法：\nfunc (jm *JobController) manageJob(activePods []*v1.Pod, succeeded int32, job *batch.Job) (int32, error) {\ractive := int32(len(activePods))\rparallelism := *job.Spec.Parallelism\rif active \u0026gt; parallelism {\rdiff := active - parallelism\rsort.Sort(controller.ActivePods(activePods))\ractive -= diff\rwait := sync.WaitGroup{}\rwait.Add(int(diff))\rfor i := int32(0); i \u0026lt; diff; i++ {\rgo func(ix int32) {\rdefer wait.Done()\rjm.podControl.DeletePod(job.Namespace, activePods[ix].Name, job)\r}(i)\r}\rwait.Wait()\r Go\n当正在活跃的节点数量小于 spec.parallelism 时，我们就会根据当前未完成的任务数和并行度计算出最大可以处于活跃的 Pod 个数 wantActive 以及与当前的活跃 Pod 数相差的 diff：\n} else if active \u0026lt; parallelism {\rwantActive := int32(0)\rif job.Spec.Completions == nil {\rif succeeded \u0026gt; 0 {\rwantActive = active\r} else {\rwantActive = parallelism\r}\r} else {\rwantActive = *job.Spec.Completions - succeeded\rif wantActive \u0026gt; parallelism {\rwantActive = parallelism\r}\r}\rdiff := wantActive - active\rif diff \u0026lt; 0 {\rdiff = 0\r}\ractive += diff\r Go\n在方法的最后就会以批量的方式并行创建 Pod，所有 Pod 的创建都是通过 CreatePodsWithControllerRef 方法在 Goroutine 中执行的。\n这里会使用 WaitGroup 等待每个 Batch 中创建的结果返回才会执行下一个 Batch，Batch 的大小是从 1 开始指数增加的，以冷启动的方式避免首次创建的任务过多造成失败：\nwait := sync.WaitGroup{}\rfor batchSize := int32(integer.IntMin(int(diff), controller.SlowStartInitialBatchSize)); diff \u0026gt; 0; batchSize = integer.Int32Min(2*batchSize, diff) {\rwait.Add(int(batchSize))\rfor i := int32(0); i \u0026lt; batchSize; i++ {\rgo func() {\rdefer wait.Done()\rjm.podControl.CreatePodsWithControllerRef(job.Namespace, \u0026amp;job.Spec.Template, job, metav1.NewControllerRef(job, controllerKind))\r}()\r}\rwait.Wait()\rdiff -= batchSize\r}\r}\rreturn active, nil\r}\r Go\n通过对 manageJob 的分析，我们其实能够看出这个方法就是根据规格中的配置对 Pod 进行管理，它在较多并行时删除 Pod，较少并行时创建 Pod，也算是一个简单的资源利用和调度机制，代码非常直白并且容易理解，不需要花太大的篇幅。\n定时任务 用于管理 CronJob 资源的 CronJobController 虽然也使用了控制器模式，但是它的实现与其他的控制器不太一样，他没有从 Informer 中接受其他消息变动的通知，而是直接访问 apiserver 中的数据：\n从 apiserver 中获取了 Job 和 CronJob 的信息之后就会调用 JobControl 向 apiserver 发起请求创建新的 Job 资源，这一个过程都是由 CronJobController 的 syncAll 方法驱动的，我们接下来就来介绍这一方法的实现。\n同步 syncAll 方法会从 apiserver 中取出所有的 Job 和 CronJob 对象，然后通过 groupJobsByParent 将任务按照 spec.ownerReferences 进行分类并遍历去同步所有的 CronJob：\nfunc (jm *CronJobController) syncAll() {\rjl, _ := jm.kubeClient.BatchV1().Jobs(metav1.NamespaceAll).List(metav1.ListOptions{})\rjs := jl.Items\rsjl, _ := jm.kubeClient.BatchV1beta1().CronJobs(metav1.NamespaceAll).List(metav1.ListOptions{})\rsjs := sjl.Items\rjobsBySj := groupJobsByParent(js)\rfor _, sj := range sjs {\rsyncOne(\u0026amp;sj, jobsBySj[sj.UID], time.Now(), jm.jobControl, jm.sjControl, jm.recorder)\rcleanupFinishedJobs(\u0026amp;sj, jobsBySj[sj.UID], jm.jobControl, jm.sjControl, jm.recorder)\r}\r}\r Go\nsyncOne 就是用于同步单个 CronJob 对象的方法，这个方法会首先遍历全部的 Job 对象，只保留正在运行的活跃对象并更新 CronJob 的状态：\nfunc syncOne(sj *batchv1beta1.CronJob, js []batchv1.Job, now time.Time, jc jobControlInterface, sjc sjControlInterface, recorder record.EventRecorder) {\rchildrenJobs := make(map[types.UID]bool)\rfor _, j := range js {\rchildrenJobs[j.ObjectMeta.UID] = true\rfound := inActiveList(*sj, j.ObjectMeta.UID)\rif found \u0026amp;\u0026amp; IsJobFinished(\u0026amp;j) {\rdeleteFromActiveList(sj, j.ObjectMeta.UID)\r}\r}\rfor _, j := range sj.Status.Active {\rif found := childrenJobs[j.UID]; !found {\rdeleteFromActiveList(sj, j.UID)\r}\r}\rupdatedSJ, _ := sjc.UpdateStatus(sj)\r*sj = *updatedSJ\r Go\n随后的 getRecentUnmetScheduleTimes 方法会根据 CronJob 的调度配置 spec.schedule 和上一次执行任务的时间计算出我们缺失的任务次数。\ntimes, _ := getRecentUnmetScheduleTimes(*sj, now)\rif len(times) == 0 {\rreturn\r}\rscheduledTime := times[len(times)-1]\rif sj.Spec.ConcurrencyPolicy == batchv1beta1.ForbidConcurrent \u0026amp;\u0026amp; len(sj.Status.Active) \u0026gt; 0 {\rreturn\r}\rif sj.Spec.ConcurrencyPolicy == batchv1beta1.ReplaceConcurrent {\rfor _, j := range sj.Status.Active {\rjob, _ := jc.GetJob(j.Namespace, j.Name)\rif !deleteJob(sj, job, jc, recorder) {\rreturn\r}\r}\r}\r Go\n如果现在需要调度新的任务，但是当前已经存在活跃的任务，就会根据并发策略的配置执行不同的操作：\n 使用 ForbidConcurrent 策略跳过这一次任务的调度直接返回； 使用 ReplaceConcurrent 策略获取并删除全部活跃的任务，通过创建新的 Pod 替换这些正在执行的活跃 Pod；  jobReq, _ := getJobFromTemplate(sj, scheduledTime)\rjobResp, _ := jc.CreateJob(sj.Namespace, jobReq)\rref,_ := getRef(jobResp)\rsj.Status.Active = append(sj.Status.Active, *ref)\rsj.Status.LastScheduleTime = \u0026amp;metav1.Time{Time: scheduledTime}\rsjc.UpdateStatus(sj)\rreturn\r}\r Go\n在方法的最后，它会从 CronJob 的 spec.jobTemplate 中拿到创建 Job 使用的模板并调用 JobControl 的 CreateJob 向 apiserver 发起创建任务的 HTTP 请求，接下来的操作就都是由 JobController 负责了。\n总结 Job 作为 Kubernetes 中用于处理任务的资源，与其他的资源没有太多的区别，它也使用 Kubernetes 中常见的控制器模式，监听 Informer 中的事件并运行 syncHandler 同步任务\n而 CronJob 由于其功能的特殊性，每隔 10s 会从 apiserver 中取出资源并进行检查是否应该触发调度创建新的资源，需要注意的是 CronJob 并不能保证在准确的目标时间执行，执行会有一定程度的滞后。\n两个控制器的实现都比较清晰，只是边界条件比较多，分析其实现原理时一定要多注意。\n如何为 Kubernetes 定制特性 Kubernetes 是非常复杂的集群编排系统，然而哪怕包含丰富的功能和特性，因为容器的调度和管理本身就有较高的复杂性，所以它无法满足所有场景下的需求。虽然 Kubernetes 能够解决大多数场景中的常见问题，但是为了实现更加灵活的策略，我们需要使用 Kubernetes 提供的扩展能力实现特定目的。\n每个项目在不同的周期会着眼于不同的特性，我们可以将项目的演进过程简单分成三个不同的阶段：\n 最小可用：项目在早期更倾向于解决通用的、常见的问题，给出开箱即用的解决方案以吸引用户，这时代码库的规模还相对比较小，提供的功能较为有限，能够覆盖领域内 90% 的场景； 功能完善：随着项目得到更多的使用者和支持者，社区会不断实现相对重要的功能，社区治理和自动化工具也逐渐变得完善，能够解决覆盖内 95% 的场景； 扩展能力：因为项目的社区变得完善，代码库变得逐渐庞大，项目的每个变动都会影响下游的开发者，任何新功能的加入都需要社区成员的讨论和审批，这时社区会选择增强项目的扩展性，让使用者能够为自己的场景定制需求，能够解决覆盖内 99% 的场景；  图 1 - 开源项目的演进\n从 90%、95% 到 99%，每个步骤都需要社区成员花费很多精力，但是哪怕提供了较好的扩展性也无法解决领域内的全部问题，在一些极端场景下仍然需要维护自己的分支或者另起炉灶满足业务上的需求。\n然而无论是维护自己的分支，还是另起炉灶都会带来较高的开发和维护成本，这需要结合实际需求进行抉择。但是能够利用项目提供的配置能力和扩展能力就可以明显地降低定制化的开发成本，而我们今天要梳理的就是 Kubernetes 的可扩展性。\n扩展接口 API 服务器是 Kubernetes 中的核心组件，它承担着集群中资源读写的重任，虽然社区提供的资源和接口可以满足大多数的日常需求，但是我们仍然会有一些场景需要扩展 API 服务器的能力，这一节简单介绍几个扩展该服务的方法。\n自定义资源 自定义资源（Custom Resource Definition、CRD）应该是 Kubernetes 最常见的扩展方式1，它是扩展 Kubernetes API 的方式之一。Kubernetes 的 API 就是我们向集群提交的 YAML，系统中的各个组件会根据提交的 YAML 启动应用、创建网络路由规则以及运行工作负载。\napiVersion: v1\rkind: Pod\rmetadata:\rname: static-web\rlabels:\rrole: myrole\rspec:\rcontainers:\r- name: web\rimage: nginx\rports:\r- name: web\rcontainerPort: 80\rprotocol: TCP\r YAML\nPod、Service 以及 Ingress 都是 Kubernetes 对外暴露的接口，当我们在集群中提交上述 YAML 时，Kubernetes 中的控制器会根据配置创建满足条件的容器。\napiVersion: apiextensions.k8s.io/v1\rkind: CustomResourceDefinition\rmetadata:\rname: crontabs.stable.example.com\rspec:\rgroup: stable.example.com\rversions:\r- name: v1\rserved: true\rstorage: true\rschema:\ropenAPIV3Schema:\rtype: object\rproperties:\rspec:\rtype: object\rproperties:\rcronSpec:\rtype: string\rimage:\rtype: string\rreplicas:\rtype: integer\rscope: Namespaced\rnames:\rplural: crontabs\rsingular: crontab\rkind: CronTab\rshortNames:\r- ct\r YAML\n除了这些系统内置的 API 之外，想要实现定制的接口就需要使用 CRD，然而 CRD 仅仅是实现自定义资源的冰山一角，因为它只定义了资源中的字段，我们还需要遵循 Kubernetes 的控制器模式，实现消费 CRD 的 Operator，通过组合 Kubernetes 提供的资源实现更复杂、更高级的功能。\n图 2 - Kubernetes API 模块化设计\n如上图所示，Kubernetes 中的控制器等组件会消费 Deployment、StatefulSet 等资源，而用户自定义的 CRD 会由自己实现的控制器消费，这种设计极大地降低了系统之间各个模块的耦合，让不同模块可以无缝协作。\n当我们想要让 Kubernetes 集群提供更加复杂的功能时，选择 CRD 和控制器是首先需要考虑的方法，这种方式与现有的功能耦合性非常低，同时也具有较强的灵活性，但是在定义接口时应该遵循社区 API 的最佳实践设计出优雅的接口2。\n聚合层 Kubernetes API 聚合层是 v1.7 版本实现的功能，它的目的是将单体的 API 服务器拆分成多个聚合服务，每个开发者都能够实现聚合 API 服务暴露它们需要的接口，这个过程不需要重新编译 Kubernetes 的任何代码3。\n图 3 - Kubernetes API 聚合\n当我们需要在集群中加入新的 API 聚合服务时，需要提交一个 APIService 资源，这个资源描述了接口所属的组、版本号以及处理该接口的服务，下面是 Kubernetes 社区中 metrics-server 服务对应的 APIService：\napiVersion: apiregistration.k8s.io/v1\rkind: APIService\rmetadata:\rname: v1beta1.metrics.k8s.io\rspec:\rservice:\rname: metrics-server\rnamespace: kube-system\rgroup: metrics.k8s.io\rversion: v1beta1\rinsecureSkipTLSVerify: true\rgroupPriorityMinimum: 100\rversionPriority: 100\r YAML\n如果我们将上述资源提交到 Kubernetes 集群中后，用户在访问 API 服务器的 /apis/metrics.k8s.io/v1beta1 路径时，会被转发到集群中的 metrics-server.kube-system.svc 服务上。\n与应用范围很广的 CRD 相比，API 聚合机制在项目中比较少见，它的主要目的还是扩展 API 服务器，而大多数的集群都不会有类似的需求，在这里也就不过多介绍了。\n准入控制 Kubernetes 的准入控制机制可以修改和验证即将被 API 服务器持久化的资源，API 服务器收到的全部写请求都会经过如下所示的阶段持久化到 etcd 中4：\n图 4 - Kubernetes 准入控制\nKubernetes 的代码仓库中包含 20 多个准入控制插件5，我们以 TaintNodesByCondition 插件6为例简单介绍一下它们的实现原理：\nfunc (p *Plugin) Admit(ctx context.Context, a admission.Attributes, o admission.ObjectInterfaces) error {\rif a.GetResource().GroupResource() != nodeResource || a.GetSubresource() != \u0026quot;\u0026quot; {\rreturn nil\r}\rnode, ok := a.GetObject().(*api.Node)\rif !ok {\rreturn admission.NewForbidden(a, fmt.Errorf(\u0026quot;unexpected type %T\u0026quot;, a.GetObject()))\r}\raddNotReadyTaint(node)\rreturn nil\r}\r Go\n所有的准入控制插件都可以实现上述的 Admit 方法修改即将提交到存储中的资源，也就是上面提到的 Mutating 修改阶段，这段代码会为所有传入节点加上 NotReady 污点保证节点在更新期间不会有任务调度到该节点上；除了 Admit 方法之外，插件还可以实现 Validate 方法验证传入资源的合法性。\n在 Kubernetes 实现自定义的准入控制器相对比较复杂，我们需要构建一个实现准入控制接口的 API 服务并将该 API 服务通过 MutatingWebhookConfiguration 和 ValidatingWebhookConfiguration 两种资源将服务的地址和接口注册到集群中，而 Kubernetes 的 API 服务器会在修改资源时调用 WebhookConfiguration 中定义的服务修改和验证资源。Kubernetes 社区中的比较热门的服务网格 Istio 就利用该特性实现了一些功能7。\n容器接口 Kubernetes 作为容器编排系统，它的主要逻辑还是调度和管理集群中运行的容器，虽然它不需要从零开始实现新的容器运行时，但是因为网络和存储等模块是容器运行的必需品，所以它要与这些模块打交道。Kubernetes 选择的方式是设计网络、存储和运行时接口隔离实现细节，自己把精力放在容器编排上，让第三方社区实现这些复杂而且极具专业性的模块。\n网络插件 容器网络接口（Container Network Interface、CNI）包含一组用于开发插件去配置 Linux 容器中网卡的接口和框架。CNI 仅会关注容器的网络连通性并在容器删除时回收所有分配的网络资源8。\n图 5 - 容器网络接口\nCNI 插件虽然与 Kubernetes 有密切的关系，但是不同的容器管理系统都可以使用 CNI 插件来创建和管理网络，例如：mesos、Cloud Foundry 等。\n所有的 CNI 插件都应该实现包含 ADD、DEL 和 CHECK 操作的二进制可执行文件，容器管理系统会执行二进制文件来创建网络9。\n在 Kubernetes 中，无论使用哪种网络插件都需要遵循它的网络模型，除了每个 Pod 都需要有独立的 IP 地址之外，Kubernetes 还对网络模型做出了以下的需求：\n 任意节点上的 Pod 在不使用 NAT 的情况下都访问到所有节点上的所有 Pod； 节点上的 Kubelet 和守护进程等服务可以访问节点上的其他 Pod；  type CNI interface {\rAddNetworkList(ctx context.Context, net *NetworkConfigList, rt *RuntimeConf) (types.Result, error)\rCheckNetworkList(ctx context.Context, net *NetworkConfigList, rt *RuntimeConf) error\rDelNetworkList(ctx context.Context, net *NetworkConfigList, rt *RuntimeConf) error\rGetNetworkListCachedResult(net *NetworkConfigList, rt *RuntimeConf) (types.Result, error)\rGetNetworkListCachedConfig(net *NetworkConfigList, rt *RuntimeConf) ([]byte, *RuntimeConf, error)\rAddNetwork(ctx context.Context, net *NetworkConfig, rt *RuntimeConf) (types.Result, error)\rCheckNetwork(ctx context.Context, net *NetworkConfig, rt *RuntimeConf) error\rDelNetwork(ctx context.Context, net *NetworkConfig, rt *RuntimeConf) error\rGetNetworkCachedResult(net *NetworkConfig, rt *RuntimeConf) (types.Result, error)\rGetNetworkCachedConfig(net *NetworkConfig, rt *RuntimeConf) ([]byte, *RuntimeConf, error)\rValidateNetworkList(ctx context.Context, net *NetworkConfigList) ([]string, error)\rValidateNetwork(ctx context.Context, net *NetworkConfig) ([]string, error)\r}\r Go\n开发 CNI 插件对于多数工程师来说都非常遥远，在正常情况下，我们只需要在一些常见的开源框架中根据需求做出选择，例如：Flannel、Calico 和 Cilium 等，当集群的规模变得非常庞大时，也自然会有网络工程师与 Kubernetes 开发者配合开发相应的插件。\n存储插件 容器存储接口（Container Storage Interface、CSI）是 Kubernetes 在 v1.9 引入的新特性，该特性在 v1.13 中达到稳定，目前常见的容器编排系统 Kubernetes、Cloud Foundry、Mesos 和 Nomad 都选择使用该接口扩展集群中容器的存储能力。\n图 6 - 容器存储接口\nCSI 是在容器编排系统向容器化的工作负载暴露块存储和文件存储的标准，第三方的存储提供商可以通过实现 CSI 插件在 Kubernetes 集群中提供新的存储10。\nKubernetes 的开发团队在 CSI 的文档中给出了开发和部署 CSI 插件的最佳实践11，其中最主要的工作是创建实现 Identity、Node 和可选的 Controller 接口的容器化应用，并通过官方的 sanity 包测试 CSI 插件的合法性，需要实现的接口都定义在 CSI 的规格文档中12。\nservice Identity {\rrpc GetPluginInfo(GetPluginInfoRequest)\rreturns (GetPluginInfoResponse) {}\rrpc GetPluginCapabilities(GetPluginCapabilitiesRequest)\rreturns (GetPluginCapabilitiesResponse) {}\rrpc Probe (ProbeRequest)\rreturns (ProbeResponse) {}\r}\rservice Controller {\r...\r}\rservice Node {\r...\r}\r Protocol Buffers\nCSI 的规格文档非常复杂，除了详细地定义了不同接口的请求和响应参数。它还定义不同接口在出现相应错误时应该返回的 gRPC 错误码，开发者想要实现一个完全遵循 CSI 接口的插件还是很麻烦的。\nKubernetes 在较早的版本中分别接入了不同的云厂商的接口，其中包括 Google PD、AWS、Azure 以及 OpenStack，但是随着 CSI 接口的成熟，社区未来会在上游移除云厂商特定的实现，减少上游的维护成本，也能加快各个厂商自身存储的迭代和支持13。\n运行时接口 容器运行时接口（Container Runtime Interface、CRI）是一系列用于管理容器运行时和镜像的 gRPC 接口，它是 Kubernetes 在 v1.5 中引入的新接口，Kubelet 可以通过它使用不同的容器运行时。\n图 7 - CRI 和容器运行时\nCRI 主要定义的是一组 gRPC 方法，我们能在规格文档中找到 RuntimeService 和 ImageService 两个服务14，它们的名字很好地解释了各自的作用：\nservice RuntimeService {\rrpc Version(VersionRequest) returns (VersionResponse) {}\rrpc RunPodSandbox(RunPodSandboxRequest) returns (RunPodSandboxResponse) {}\rrpc StopPodSandbox(StopPodSandboxRequest) returns (StopPodSandboxResponse) {}\rrpc RemovePodSandbox(RemovePodSandboxRequest) returns (RemovePodSandboxResponse) {}\rrpc PodSandboxStatus(PodSandboxStatusRequest) returns (PodSandboxStatusResponse) {}\rrpc ListPodSandbox(ListPodSandboxRequest) returns (ListPodSandboxResponse) {}\rrpc CreateContainer(CreateContainerRequest) returns (CreateContainerResponse) {}\rrpc StartContainer(StartContainerRequest) returns (StartContainerResponse) {}\rrpc StopContainer(StopContainerRequest) returns (StopContainerResponse) {}\rrpc RemoveContainer(RemoveContainerRequest) returns (RemoveContainerResponse) {}\rrpc ListContainers(ListContainersRequest) returns (ListContainersResponse) {}\rrpc ContainerStatus(ContainerStatusRequest) returns (ContainerStatusResponse) {}\rrpc UpdateContainerResources(UpdateContainerResourcesRequest) returns (UpdateContainerResourcesResponse) {}\rrpc ReopenContainerLog(ReopenContainerLogRequest) returns (ReopenContainerLogResponse) {}\rrpc ExecSync(ExecSyncRequest) returns (ExecSyncResponse) {}\rrpc Exec(ExecRequest) returns (ExecResponse) {}\rrpc Attach(AttachRequest) returns (AttachResponse) {}\rrpc PortForward(PortForwardRequest) returns (PortForwardResponse) {}\r...\r}\rservice ImageService {\rrpc ListImages(ListImagesRequest) returns (ListImagesResponse) {}\rrpc ImageStatus(ImageStatusRequest) returns (ImageStatusResponse) {}\rrpc PullImage(PullImageRequest) returns (PullImageResponse) {}\rrpc RemoveImage(RemoveImageRequest) returns (RemoveImageResponse) {}\rrpc ImageFsInfo(ImageFsInfoRequest) returns (ImageFsInfoResponse) {}\r}\r Protocol Buffers\n容器运行时的接口相对比较简单，上面的这些接口不仅暴露了 Pod 沙箱管理、容器管理以及命令执行和端口转发等功能，还包含用于管理镜像的多个接口，容器运行时只要实现上面的二三十个方法可以为 Kubelet 提供服务。\n设备插件 CPU、内存、磁盘是主机上常见的资源，然而随着大数据、机器学习和硬件的发展，部分场景可能需要异构的计算资源，例如：GPU、FPGA 等设备。异构资源的出现不仅需要节点代理 Kubelet 的支持，还需要调度器的配合，为了良好的兼容后出现的不同计算设备，Kubernetes 社区在上游引入了设备插件（Device Plugin）用于支持多种类型资源的调度和分配15。\n图 8 - 设备插件概述\n设备插件是独立在 Kubelet 之外单独运行的服务，它通过 Kubelet 暴露的 Registration 服务注册自己的相关信息并实现 DevicePlugin 服务用于订阅和分配自定义的设备16。\nservice Registration {\rrpc Register(RegisterRequest) returns (Empty) {}\r}\rservice DevicePlugin {\rrpc GetDevicePluginOptions(Empty) returns (DevicePluginOptions) {}\rrpc ListAndWatch(Empty) returns (stream ListAndWatchResponse) {}\rrpc Allocate(AllocateRequest) returns (AllocateResponse) {}\rrpc GetPreferredAllocation(PreferredAllocationRequest) returns (PreferredAllocationResponse) {}\rrpc PreStartContainer(PreStartContainerRequest) returns (PreStartContainerResponse) {}\r}\r Protocol Buffers\n当设备插件刚刚启动时，它会调用 Kubelet 的注册接口传入自己的版本号、Unix 套接字和资源名，例如：nvidia.com/gpu；Kubelet 会通过 Unix 套接字与设备插件通信，它会通过 ListAndWatch 接口持续获得设备中资源的最新状态，并在 Pod 申请资源时通过 Allocate 接口分配资源。设备插件的实现逻辑相对比较简单，感兴趣的读者可以研究 Nvidia GPU 插件的实现原理17。\n调度框架 调度器是 Kubernetes 中的核心组件之一，它的主要作用是在 Kubernetes 集群中的一组节点中为工作负载做出最优的调度决策，不同场景下的调度需求往往都是很复杂的，然而调度器在 Kubernetes 项目早期并不支持易用的扩展能力，仅支持调度器扩展（Extender）这种比较难用的方法。\nKubernetes 从 v1.15 引入的调度框架才是今天比较主流的调度器扩展技术，通过在 Kubernetes 调度器的内部抽象出关键的扩展点（Extension Point）并通过插件的方式在扩展点上改变调度器做出的调度决策18。\n图 9 - 调度框架扩展点\n目前的调度框架总共支持 11 个不同的扩展点，每个扩展点都对应 Kubernetes 调度器中定义的接口，这里仅展示 FilterPlugin 和 ScorePlugin 两个常见接口中的方法19：\ntype FilterPlugin interface {\rPlugin\rFilter(ctx context.Context, state *CycleState, pod *v1.Pod, nodeInfo *NodeInfo) *Status\r}\rtype ScoreExtensions interface {\rNormalizeScore(ctx context.Context, state *CycleState, p *v1.Pod, scores NodeScoreList) *Status\r}\rtype ScorePlugin interface {\rPlugin\rScore(ctx context.Context, state *CycleState, p *v1.Pod, nodeName string) (int64, *Status)\rScoreExtensions() ScoreExtensions\r}\r Go\n调度框架的出现让实现复杂的调度策略和调度算法变得更加容易，社区通过调度框架替代更早的谓词和优先级并实现了协作式调度、基于容量调度等功能更强大的插件20。虽然今天的调度框架已经变得非常灵活，但是串行的调度器可能无法满足大集群的调度需求，而 Kubernetes 目前也很难实现多调度器，不知道未来是否会提供更灵活的接口。\n总结 Kubernetes 从 2014 年发布至今已经过去将近 7 年了，从一个最小可用的编排系统到今天的庞然大物，社区的每个代码贡献者和成员都有责任。从这篇文章中，我们可以看到随着 Kubernetes 项目的演进方向，社区越来越关注系统的可扩展性，通过设计接口、移除第三方代码降低社区成员的负担，让 Kubernetes 能够更专注于容器的编排和调度。\n其它 为什么 Kubernetes 要替换 Docker Kubernetes 是今天容器编排领域的事实标准，而 Docker 从诞生之日到今天都在容器中扮演着举足轻重的地位，也都是 Kubernetes 中的默认容器引擎。然而在 2020 年 12 月，Kubernetes 社区决定着手移除仓库中 Dockershim 相关代码1，这对于 Kubernetes 和 Docker 两个社区来说都意义重大。\n图 1 - Dockershim\n相信大多数的开发者都听说过 Kubernetes 和 Docker，也知道我们可以使用 Kubernetes 管理 Docker 容器，但是可能没有听说过 Dockershim，即 Docker 垫片。如上图所示，Kubernetes 中的节点代理 Kubelet 为了访问 Docker 提供的服务需要先经过社区维护的 Dockershim，Dockershim 会将请求转发给管理容器的 Docker 服务。\n其实从上面的架构图中，我们就能猜测出 Kubernetes 社区从代码仓库移除 Dockershim 的原因：\n Kubernetes 引入容器运行时接口（Container Runtime Interface、CRI）隔离不同容器运行时的实现机制，容器编排系统不应该依赖于某个具体的运行时实现； Docker 没有支持也不打算支持 Kubernetes 的 CRI 接口，需要 Kubernetes 社区在仓库中维护 Dockershim；  可扩展性 Kubernetes 通过引入新的容器运行时接口将容器管理与具体的运行时解耦，不再依赖于某个具体的运行时实现。很多开源项目在早期为了降低用户的使用成本，都会提供开箱即用的体验，而随着用户群体的扩大，为了满足更多定制化的需求、提供更强的可扩展性，会引入更多的接口。Kubernetes 通过下面的一系列接口为不同模块提供了扩展性：\n图 2 - Kubernetes 接口和可扩展性\nKubernetes 在较早期的版本中就引入了 CRD、CNI、CRI 和 CSI 等接口，只有用于扩展调度器的调度框架是 Kubernetes 中比较新的特性。我们在这里就不展开分析其他的接口和扩展了，简单介绍一下容器运行时接口。\nKubernetes 早在 1.3 就在代码仓库中同时支持了 rkt 和 Docker 两种运行时，但是这些代码为 Kubelet 组件的维护带来了很大的困难，不仅需要维护不同的运行时，接入新的运行时也很困难；容器运行时接口（Container Runtime Interface、CRI）是 Kubernetes 在 1.5 中引入的新接口，Kubelet 可以通过这个新接口使用各种各样的容器运行时。其实 CRI 的发布就意味着 Kubernetes 一定会将 Dockershim 的代码从仓库中移除。\nCRI 是一系列用于管理容器运行时和镜像的 gRPC 接口，我们能在它的定义中找到 RuntimeService 和 ImageService 两个服务2，它们的名字很好地解释了各自的作用：\nservice RuntimeService {\rrpc Version(VersionRequest) returns (VersionResponse) {}\rrpc RunPodSandbox(RunPodSandboxRequest) returns (RunPodSandboxResponse) {}\rrpc StopPodSandbox(StopPodSandboxRequest) returns (StopPodSandboxResponse) {}\rrpc RemovePodSandbox(RemovePodSandboxRequest) returns (RemovePodSandboxResponse) {}\rrpc PodSandboxStatus(PodSandboxStatusRequest) returns (PodSandboxStatusResponse) {}\rrpc ListPodSandbox(ListPodSandboxRequest) returns (ListPodSandboxResponse) {}\rrpc CreateContainer(CreateContainerRequest) returns (CreateContainerResponse) {}\rrpc StartContainer(StartContainerRequest) returns (StartContainerResponse) {}\rrpc StopContainer(StopContainerRequest) returns (StopContainerResponse) {}\rrpc RemoveContainer(RemoveContainerRequest) returns (RemoveContainerResponse) {}\rrpc ListContainers(ListContainersRequest) returns (ListContainersResponse) {}\rrpc ContainerStatus(ContainerStatusRequest) returns (ContainerStatusResponse) {}\rrpc UpdateContainerResources(UpdateContainerResourcesRequest) returns (UpdateContainerResourcesResponse) {}\rrpc ReopenContainerLog(ReopenContainerLogRequest) returns (ReopenContainerLogResponse) {}\r...\r}\rservice ImageService {\rrpc ListImages(ListImagesRequest) returns (ListImagesResponse) {}\rrpc ImageStatus(ImageStatusRequest) returns (ImageStatusResponse) {}\rrpc PullImage(PullImageRequest) returns (PullImageResponse) {}\rrpc RemoveImage(RemoveImageRequest) returns (RemoveImageResponse) {}\rrpc ImageFsInfo(ImageFsInfoRequest) returns (ImageFsInfoResponse) {}\r}\r Protocol Buffers\n对 Kubernetes 稍有了解的人都能从上面的定义中找到一些熟悉的方法，它们都是容器运行时需要暴露给 Kubelet 的接口。Kubernetes 将 CRI 垫片实现成 gRPC 服务器与 Kubelet 中的客户端通信，所有的请求都会被转发给容器运行时处理。\n图 3 - Kubernetes 和 CRI\nKubernetes 中的声明式接口非常常见，作为声明式接口的拥趸，CRI 没有使用声明式的接口是一件听起来『非常怪异』的事情3。不过 Kubernetes 社区考虑过让容器运行时重用 Pod 资源，这样容器运行时可以实现不同的控制逻辑来管理容器，能够极大地简化 Kubelet 和容器运行时之间的接口，但是社区出于以下两点考虑，最终没有选择声明式的接口：\n 所有的运行时都需要重新实现相同的逻辑支持很多 Pod 级别的功能和机制； Pod 的定义在 CRI 设计时演进地非常快，初始化容器等功能都需要运行时的配合；  虽然社区最终为 CRI 选择了命令式的接口，但是 Kubelet 仍然会保证 Pod 的状态会不断地向期望状态迁移。\n不兼容接口 与容器运行时相比，Docker 更像是一个复杂的开发者工具，它提供了从构建到运行的全套功能。开发者可以很快地上手 Docker 并在本地运行并管理一些 Docker 容器，然而在集群中运行的容器运行时往往不需要这么复杂的功能，Kubernetes 需要的只是 CRI 中定义的那些接口。\n图 4 - Docker \u0026amp; CRI\nDocker 的官方文档加起来可能有一本书的厚度，相信没有任何开发者可以熟练运用 Docker 提供的全部功能。而作为开发者工具，虽然 Docker 中包含 CRI 需要的所有功能，但是都需要实现一层包装以兼容 CRI。除此之外，社区提出的很多新功能都没有办法在 Dockershim 中实现，例如 cgroups v2 以及用户命名空间。\nKubernetes 作为比较松散的开源社区，每个成员尤其是各个 SIG 的成员都只会在开源社区上花费有限的时间，而维护 Kubelet 的 sig-node 又尤其繁忙，很多新的功能都因为维护者没有足够的精力而被搁置，所以既然 Docker 社区看起来没有打算支持 Kubernetes 的 CRI 接口，维护 Dockershim 又需要花费很多精力，那么我们就能理解为什么 Kubernetes 会移除 Dockershim 了。\n总结 今天的 Kubernetes 已经是非常成熟的项目，它的关注点也逐渐从提供更完善的功能转变到提供更好的扩展性，这样才能满足不同场景和不同公司定制化的业务需求。Kubernetes 在过去因为 Docker 的热门而选择 Docker，而在今天又因为高昂的维护成本而放弃 Docker，我们能够从这个过程中体会到容器领域的发展和进步。\n移除 Docker 的种子其实从 CRI 发布时就种下了，Dockershim 一直都是 Kubernetes 为了兼容 Docker 获得市场采取的临时决定，对于今天已经统治市场的 Kubernetes 来说，Docker 的支持显得非常鸡肋，移除代码也就顺理成章了。我们在这里重新回顾一下 Kubernetes 在仓库中移除 Docker 支持的两个原因：\n Kubernetes 在早期版本中引入 CRI 摆脱依赖某个具体的容器运行时依赖，屏蔽底层的诸多实现细节，让 Kubernetes 能够更关注容器的编排； Docker 本身不兼容 CRI 接口，而且官方并没有实现 CRI 的打算，同时也不支持容器的一些新需求，所以 Dockershim 的维护成为了社区的想要摆脱负担；  到最后，我们还是来看一些比较开放的相关问题，有兴趣的读者可以仔细思考一下下面的问题：\n Kubernetes 中还有哪些模块提供良好的扩展性？ 除了文中提到的 CRI-O、Containerd，还有哪些支持 CRI 的容器运行时？   如果对文章中的内容有疑问或者想要了解更多软件工程上一些设计决策背后的原因，可以在博客下面留言，作者会及时回复本文相关的疑问并选择其中合适的主题作为后续的内容。\n 待整理  特点:  轻量级:消耗资源小 开源 弹性伸缩 负载均衡 IPVS   梳理  Kubernetes：构建 K8S 集群 资源清单：资源 掌握资源清单的语法 编写Pod 掌握Pod的生命周期*** Pod控制器：掌握各种控制器的特点以及使用定义方式 服务发现：掌握SVC原理及其构建方式 存储：掌握多种存储类型的特点。并且能够在不同环境中选择合适的存储方案(有自己的简介) 调度器：掌握调度器原理能够 根据要求把Pod定义到想要的节点 安全：集群的认证 鉴权 访问控制 原理及其流程 HELM: Linux yum，掌握HELM原理，HELM模板自定义，HELM部署一些常用插件 运维：修改Kubeadm 达到证书可用期限为10年，能够构建高可用的Kubernetes集群   主要组件：  APISERVER: 所有服务访问统一入口 CrontrollerManager: 维持副本期望数目 Scheduler :. 负责介绍任务，选择合适的节点进行分配任务 ETCD: 键值对数据库 储存K8S集群所有重要信息(持久化) Kubelet:直接跟容器引擎交互实现容器的生命周期管理， Kube-proxy:负责写入规则至IPTABLES、 IPVS实现服务映射访问的   其他插件  COREDNS: 可以为集群中的svC创建一 个域名IP的对应关系解析 DASHBOARD: 给K8S 集群提供个B/S 结构访问体系 INGRESS CONTROLLER: 官方只能实现四层代理，INGRESS 可以实现七层代理 FEDERATION: 提供-个可以跨集群中心多K8S统一 管理功能 PROMETHEUS: 提供K8S集群的监控能力 ELK: 提供K8S集群日志统一分析介入平台    pod   pod：1个pod中可以有1或多个容器，pod中运行这一个特殊容器pause，其它的则被称为业务容器，业务容器之间通过pause共享网络栈、数据卷等，同一pod中的不同业务容器上的服务互相访问就无需代理地址、映射端口等等，等于都在一个localhost上了，注意自然也不能用相同的端口了，这样通信更加高效\n  pod类型\n 自主式Pod 控制器管理的Pod  ReplicationController\u0026amp;ReplicaSet\u0026amp;Deployment  ReplicationController用来确保容器应用的副本数始终保持在用户定义的副本数，即如果有容器异常退出，会自动创建新的Pod来替代;而如果异常多出来的容器也会自动回收。在新版本的Kubernetes中建议使用ReplicaSet来取代Repl icat ionControlle . Repl icaSet跟ReplicationController没有本质的不同，只是名字不一-样，但是ReplicaSet支持集合式的selector 虽然ReplicaSet 可以独立使用，但一般还是建议 使用Deployment 来自动管理ReplicaSet，这样就无需担心跟其他机制的不兼容问题(比如ReplicaSet 不支持rolling- update 但Deployment 支持)   HPA (Hori zontalPodAutoScale)  Hori zontal Pod Autoscaling仅适用于Deployment 和ReplicaSet ，在V1版本中仅支持根据Pod的CPU利用率扩所容，在v1alpha 版本中，支持根据内存和用户自定义的metric 扩缩容   StatefulSet：是为了解决有状态服务的问题(对应Deployments 和Repl icaSets是为无状态服务而设 计)，其应用场景包括:  稳定的持久化存储，即Pod重新调度后还是能访问到相同的持久化数据，基于PVC来实现 稳定的网络标志，即Pod重新调度后其PodName 和HostName 不变，基于Headless Service(即没有Cluster IP的Service )来实现， 有序部署，有序扩展，即Pod是有顺序的，在部署或者扩展的时候要依据定义的顺序依次依次进行(即从0到N-1,在下一个Pod运行之前所有之前的Pod必须都是Running 和Ready 状态)，基于init containers 来实现 有序收缩，有序删除(即从N-1到0)   DaemonSet确保全部(或者一些) Node. 上运行一个Pod 的副本。当有Node 加入集群时，也会为他们新增一个Pod。当有Node 从集群移除时，这些Pod也会被回收。删除DaemonSet将会删除它创建的所有Pod。使用DaemonSet 的一些典型用法:  运行集群存储daemon, 例如在每个Node. 上运行glusterd、 ceph。 在每个Node.上运行 日志收集daemon, 例如fluentd、logstash。 在每个Node.上运行监控daemon, 例如Prometheus Node Exporter   Job负责批处理任务，即仅执行一次的任务， 它保证批处理任务的一个或多 个Pod成功结束。Cron Job管理基于时间的Job, 即:  在给定时间点只运行一次 周期性地在给定时间点运行        网络通信方式：Kubernetes的网络模型假定了所有Pod都在一个何以直接连通的扁平的网络空间中，这在GCE (Google Compute Engine) 里面是现成的网络模型，Kubernetes 假定这个网络已经存在。而在私有云里搭建Kubernetes集群，就不能假定这个网络已经存在了。我们需要自己实现这个网络假设，将不同节点上的Docker 容器之间的互相访问先打通，然后运行Kubernetes\n 同一个Pod内的多个容器之间:localhost（pause的网络工作栈） 各Pod 之间的通讯: Overlay Network Pod与Service之间的通讯：各节点的Iptables规则    Flannel是CoreOS 团队针对Kubernetes 设计的一个网络规划服务，简单来说，它的功能是让集群中的不同节点主机创建的Docker容器都具有全集群唯一的虚拟IP地址。而且它还能在这些IP地址之间建立-一个覆盖网络(OverlayNetwork)，通过这个覆盖网络，将数据包原封不动地传递到目标容器内\n  ETCD之Flannel 提供说明:\n 存储管理 Flannel可分配的IP地址段资 源 监控ETCD中每个Pod的实际地址，并在内存中建立维护Pod 节点路由表    流程：\n  同-一个Pod内部通讯：同 一个Pod共享同一一个网络命名空间，共享同一一个Linux协议栈\n  Pod1至Pod2 ;\nPod1与Pod2不在同- -台主机，Pod的地址是与docker0在同- 一个网段的，但docker0网段与宿主机网卡是两个完 全不同的IP网段，并且不同Node之间的通信只能通过宿主机的物理网卡进行。将Pod的IP和所 在Node的IP关联起来，通过 这个关联让Pod可以互相访问 Pod1与Pod2在同一.台机器，由Docker0 网桥直接转发请求至Pod2， 不需要经过Flannel 演示\n  Pod至Service的网络:目前基于性能考虑，全部为iptables 维护和转发\n  Pod到外网: Pod 向外网发送请求，查找路由表，转发数据包到宿主机的网卡，宿主网卡完成路由选择后，iptables执 行Masquerade,把源IP更改为宿主网卡的IP， 然后向外网服务器发送请求\n  外网访问Pod: Service\n      Kubernetes  Kubernetes是Google Omega的开源版本 据说Google的数据中心里运行着20多亿个容器，而且Google十年前就开始使用容器技术 最初Google开发了一个叫Borg的系统（现在命名为Omega）来调度如此庞大数量的容器和工作负载 之后通过Golang重写Borg即成就Kubernetes，并将其贡献到开源社区让全世界都能受益 Kubernetes：编排容器，优化资源利用、高可用、滚动更新、网络插件、服务发现、监控、数据管理、日志管理等  Hello 官方提供了一个交互式教程，通过Web浏览器就能使用预先部署好的一个Kubernetes集群，快速体验Kubernetes的功能和应用场景：https://kubernetes.io/zh/docs/tutorials/kubernetes-basics/\n#集群命令。minikube是官方提供的一个迷你kubernetes集群\r$ minikube version\r$ minikube start #启动集群\r#客户端命令。kubectl是官方提供的客户端\r$ kubectl version\r$ kubectl cluster-info #集群信息。如：地址...\r$ kubectl get nodes\r$ kubectl run kubernetes-bootcamp --image=gcr.io/google-samples/kubernetes-bootcamp:v1 --port=8080 #部署应用，需要提供部署名称(kubernetes-bootcamp)和应用程序映像位置（--image指定，包括Docker Hub外部托管映像的完整存储库URL）。\r$ kubectl get deployments #列出部署。内容：名字、副本数...\r$ kubectl get pods #列出pods\r$ kubectl expose deployment/kubernetes-bootcamp --type=\u0026quot;NodePort\u0026quot; --port 8080 #默认情况下，所有Pod只能在集群内部访问，要访问上面这个应用只能直接访问容器的8080端口，为了能够从外部访问应用，需要将容器的8080端口映射到节点的端口\r$ kubectl get services #列出服务。service是对外提供的服务？可以查看到应用被映射到节点的哪个端口\r$ curl 地址:32253 #service的端口是随机分配的\r$ kubectl scale deployments/kubernetes-bootcamp --replicas=3 #默认情况下应用只会运行一个副本，这里将副本数增加到3个\r$ kubectl get pods\r$ curl 地址:32253 #将负载均衡轮询处理请求\r$ kubectl scale deployments/kubernetes-bootcamp --replicas=2 #同样的方式，将有一个副本被终止\r$ kubectl set image deployments/kubernetes-bootcamp kubernetes-bootcamp=jocatalin/kubernetes-bootcamp:v2 #升级image的版本\r$ kubectl get pods #s可以观察滚动更新的过程：v1的Pod被逐个 删除，同时启动了新的v2 Pod\r$ kubectl rollout undo deployments/kubernetes-bootcamp #回退版本\r 概念  Cluster：Cluster是计算、存储和网络资源的集合，Kubernetes利用这些资源运行各种基于容器的应用 Master：Master是Cluster的大脑，它的主要职责是调度，即决定将应用放在哪里运行。Master运行Linux操作系统，可以是物理机或者虚拟机。为了实现高可用，可以运行多个Master Node：Node的职责是运行容器应用。Node由Master管理，Node负责监控并汇报容器的状态，同时根据Master的要求管理容器的生命周期。 Node运行在Linux操作系统上，可以是物理机或者是虚拟机 Pod：Kubernetes的最小工作单元。每个Pod包含一个或多个容器。Pod中的容器会作为一个整体被Master调度到一个Node上运行  Pod被引入的两个主要目的  可管理性：有些容器天生就是需要紧密联系，一起工作。Pod提供了比容器 更高层次的抽象，将它们封装到一个部署单元中。Kubernetes以Pod为最小单位进行调度、扩展、共享资源、管理生命周期 通信和资源共享：Pod中的所有容器使用同一个网络namespace，即相同的IP地址和Port空间。它们可以直接用localhost通信。同样的，这些容器可以共 享存储，当Kubernetes挂载volume到Pod，本质上是将volume挂载到Pod中的每一个容器   Pods的两种使用方式  运行单一容器：one-container-per-Pod是Kubernetes最常见的模型，这种情况下，只是将单个容器简单封装成Pod。即便是只有一个容器，Kubernetes管理的也是Pod而不是直接管理容器 运行多个容器：问题在于哪些容器应该放到一个Pod中？这些容器联系必须非常紧密，而且需要直接共享资源。比如Pod包含两个容器，一个是File Puller，一个是Web Server，File Puller会定期从外部的Content Manager中拉取最新的文件，将其存放在共享的volume中。Web Server从volume读取文件，响应Consumer的请求，这两个容器是紧密协作的，它们一起为Consumer提供最新的数据，同时它们也通过volume共享数据，所以放到一个Pod是合适的；是否需要将Tomcat和MySQL放到一个Pod中？Tomcat从MySQL读取数据，它们之间需要协作，但还不至于需要放到一个Pod中一起部署、一起启动、一起停止。同时它们之间是通过JDBC交换数据，并不是直接共享存储，所以放到各自的Pod中更合适     Controller：Kubernetes通常不会直接创建Pod，而是通过Controller来管理Pod的。Controller中定义了Pod的部署特性，比如有几个副本、在什么样的Node上运行等。为了满足不同的业务场景，Kubernetes提供了多种Controller，包括Deployment、ReplicaSet、DaemonSet、StatefuleSet、Job等  Deployment：最常用的Controller，比如在线教程中就是通过创建Deployment来部署应用的。Deployment可以管理Pod的多个副本，并确保Pod按照期望的状态运行 ReplicaSet：实现了Pod的多副本管理。使用Deployment时会自动创建ReplicaSet，也就是说Deployment是通过ReplicaSet来管理Pod的多个副本的，我们通常不需要直接使用ReplicaSet DaemonSet：用于每个Node最多只运行一个Pod副本的场景。正如其名称所揭示的，DaemonSet通常用于运行daemon StatefuleSet：能够保证Pod的每个副本在整个生命周期中名称 是不变的，而其他Controller不提供这个功能。当某个Pod发生故障需要删除并重新启动时，Pod的名称会发生变化，同时StatefuleSet会保证副本按照固定的顺序启动、更新或者删除 Job：用于运行结束就删除的应用，而其他Controller中的Pod通常是长期持续运行   Service：Deployment可以部署多个副本，每个Pod都有自己的IP，外界如何访问这些副本呢？通过Pod的IP吗？要知道Pod很可能会被频繁地销毁和重启，它们的IP会发生变化，用IP来访问不太现实。Kubernetes Service定义了外界访问一组特定Pod的方式。Service有自己的IP和端口，Service为Pod提供了负载均衡。Kubernetes运行容器（Pod）与访问容器（Pod）这两项任务分别由Controller和Service执行 Namespace：如果有多个用户或项目组使用同一个Kubernetes Cluster，如何将 他们创建的Controller、Pod等资源分开呢？Namespace可以将一个物理的Cluster逻辑上划分成多个虚拟 Cluster，每个Cluster就是一个Namespace。不同Namespace里的资源是完全隔离的。Kubernetes默认创建了两个Namespace，通过kubectl get namespace列出  default：创建资源时如果不指定，将被放到这个Namespace中 kube-system：Kubernetes自己创建的系统资源将放到这个Namespace中    Cluster  部署三个节点的Kubernetes Cluster。k8s-master是Master，k8s-node1和k8s-node2是Node 所有节点都需要安装Docker、kubelet、kubeadm、kubectl：https://kubernetes.io/zh/docs/setup/production-environment/tools/kubeadm/install-kubeadm/  kubelet：。运行在Cluster所有节点上，负责启动Pod和容器 kubeadm：用来初始化集群的指令 kubectl是Kubernetes命令行工具。通过kubectl可以部署和管理应用，查看各种资源，创建、删除和更新各种组件    #配置kubernetes源\r$ cat \u0026lt;\u0026lt;EOF \u0026gt; /etc/yum.repos.d/kubernetes.repo\r[kubernetes]\rname=Kubernetes\rbaseurl=https://mirrors.aliyun.com/kubernetes/yum/repos/kubernetes-el7-x86_64\renabled=1\rgpgcheck=1\rrepo_gpgcheck=1\rgpgkey=https://mirrors.aliyun.com/kubernetes/yum/doc/yum-key.gpg https://mirrors.aliyun.com/kubernetes/yum/doc/rpm-package-key.gpg\rEOF\r$ setenforce 0 #关闭SELinux（防火墙），临时生效\r$ cp /etc/selinux/config /etc/selinux/config.bk\r$ sed -i 's/^SELINUX=enforcing$/SELINUX=permissive/' /etc/selinux/config #关闭SELinux，永久生效：将SELinux设置为permissive模式，相当于将其禁用\r$ getenforce #查看selinux状态\r$ yum install kubelet kubeadm kubectl -y --disableexcludes=kubernetes\r$ systemctl enable --now kubelet #设置开机启动，并现在立即启动\r kubeadm init --apiserver-advertise-address 192.168.56.105 --pod-network-cidr=10.244.0.0/16\r ","date":"0001-01-01","permalink":"https://yuanyatianchi.github.io/post/it.cloudnative.kubernetes/","tags":["it","cloudnative"],"title":"Kubernetes"},{"content":"linux  Linux  Linus Benedict Torvalds（Linux之父）编写的开源操作系统的内核 广义上的基于 Linux内核 的 Linux操作系统   内核版本：https://www.kernel.org/  内核版本分为三个部分：主版本号、次版本号、末版本号 次版本号是奇数为开发版，偶数为稳定版。但是实际上在2.6以后不这么区分了   发行版本：RedHat Enterprise Linux、Fedora、CentOS、Debian、Ubuntu\u0026hellip; 终端：图形终端，命令行终端，远程终端（SSH、VNC）  概论 hello  镜像  CentOS：http://mirrors.aliyun.com/centos/ ，7.8.2003/isos/x86_64/CentOS-7-x86_64-DVD-2003.iso Ubuntu：http://mirrors.aliyun.com/ubuntu-releases/ ，20.04/ubuntu-20.04.1-live-server-amd64.iso    环境 VirtualBox   新建\n  名称：centos7 → 文件夹：D:\\it\\virtual_machine\\VirtualBoxVM → 类型：Linux → 版本：Red Hat (64-bit)\n  内存大小：2048mb\n  现在创建虚拟硬盘 → VDI (VirtualBox 磁盘映像) → 动态分配 → 硬盘大小：20G\n  设置 → 存储 → 控制器: IDE → 右侧光盘图标选择虚拟盘 → OK\n  启动 → Install CentOS Linux 7 → English\n  DATE \u0026amp; TIME → Asia Shanghai\n  SOFTWARE SELECTION → Minimal Install （按需选择）\n  INSTALL ATION DESTINATION → 直接点done即可\n  NETWORK \u0026amp; HOST NAME → Ethernet (enp0s3)：ON\n  开始安装 → 设置 ROOT PASSWORD → 安装成功后 reboot\n  虚拟机与宿主机 共享粘贴板、拖拽文件\n  设置 - 常规 - 高级 - 共享粘贴板 双向 \u0026amp; 拖放 双向\n  设置 - 存储 - 控制器：SATA - √ 使用主机输入输出（I/O）缓存\n  设置 - 存储 - 控制器：SATA - .vdi - √ 固态驱动器(s)\n  kernel-devel\n# 安装 kernel-devel 和 gcc\ryum install -y kernel-devel gcc\r# 更新 kernel 和 kernel-devel 到最新版本\ryum upgrade kernel kernel-devel -y\r# 重启\rreboot\r   虚拟机窗口上方菜单栏 - 设备 - 安装增强功能（会挂载一个光盘，重新安装要先弹出iso，否则报\u0026quot;未能加载虚拟光盘\u0026rdquo;）\n    VMware  新建虚拟机，自定义，稍后安装操作系统，2cpu2核心以上，内存2048以上，NAT网络，LSI Logic，SCSI 创建新虚拟磁盘，磁盘大小20G以上，将虚拟磁盘存储为单个文件   CentOS 修改/etc/sysconfig/network-scripts/ifcfg-ens33中NOBOOT=yes，表示系统启动时激活网卡  目录  目录  /：根目录 /root：root用户的家目录 /home/username：普通用户的家目录 /etc：配置文件目录 /bin：命令目录 /sbin：管理命令目录 /usr/bin、/usr/sbin：系统预装的其他命令    命令行  命令（command）：通过使用命令调用对应的命令程序文件，如使用ls命令将调用/bin/ls程序文件 参数（option选项）：  语法  命令 \u0026lt;必选参数1 | 必选参数2\u0026gt; [-option {必选参数1 | 必选参数2 | 必选参数3}] [可选参数\u0026hellip;] {(默认参数) | 参数 | 参数} 命令行语法符号  方括号****：可选参数，在命令中根据需要加以取舍 尖括号\u0026lt; \u0026gt;：必选参数，实际使用时应将其替换为所需要的参数 大括号**{ }**：必选参数, 内部使用, 包含此处允许使用的参数 小括号**()**：指明参数的默认值, 只用于{ }中 管道符（竖线）|：分隔多个互斥参数, 含义为\u0026quot;或\u0026rdquo;, 使用时只能选择一个 省略号**\u0026hellip;**：多个参数 分号**;**：分割多个命令，命令将按顺序执行   注意：命令行语法（包括在 UNIX 和 Linux 平台中使用的用户名、密码和文件名）是区分大小写的，如commandline、CommandLine、COMMANDLINE 是不一样的  简写  多命令：命令之间;分隔，顺序执行  cd / ; ls\r  多参数简写  ls -l -r -t -R\rls -lrtR #简写\r  当前目录简写  ls ./\rls . #简写。具体到文件（或目录）时无法使用\u0026quot;.\u0026quot;作简写\rls #简写\rcd ../\rcd .. #简写\rcat ./config.yml\rcat config.yml #简写\r 用户 clear #清屏。或者快捷键CTRL+L\rsu - root #切换到root用户\rexit #退出当前系统用户\rinit 0 #关机\r 帮助命令  为什么要学习帮助命令：Linux的基本操作方式是命令行，海量的命令不适合“死记硬背”，你要升级你的大脑 使用网络资源（搜索引擎和官方文档)  help help cd #查看内部命令帮助\rtype cd #查看命令类型。shell（命令解释器）自带的命令称为内部命令，其他的是外部命令\rls --help #查看外部命令帮助\r info info ls #查看ls命令的信息。info比help更详细，作为help的补充\r man  man [(1)|2|3|4|5|6|7|8|9] [文件名]：查看指定文件（程序）对应的手册（如果有的话）。这里1-9指定要查看手册的类型，因为系统中很多重名文件，所以需要分类来区分 按Q退出手册  man man #查看man命令的手册。可以看到默认的选项\u0026quot;1\u0026quot;，表示查看 可执行程序或shell命令 这一类型的文件的手册\rman ls #查看ls命令的手册。选项\u0026quot;1\u0026quot;是默认选项，可以省略\rman 1 ls\rman -a ls #查看所有名为ls的文件的手册\r 文件操作  一切皆文件 TAB快捷键补全文件（目录）名 路径：  / ./ ../    pwd pwd #显示当前的目录名称\r cd  更改当前的操作目录  cd /path/to/...：绝对路径 cd ./path/to/...：相对路径 cd ../path/to/...：相对路径    man cd #将查看/bin/bash（一个命令解释器shell的实现）的手册，因为cd是bash的内置命令\rcd ../ #回到上一级目录\rcd .. #简写\rcd - #回到上一次使用的目录。即可以实现两个目录来回切换的效果\r ls  ls [option]... [path]...：查看指定目录下的文件：ls / /root，可以同时显示/和/root下的目录 参数（常用）  -l：  长格式显示文件：文件类型和权限，文件个数，创建用户，创建用户所属用户组，文件大小，最后修改时间，文件名 文件类型：文件为-，目录为d   -a：显示隐藏文件。文件名以\u0026rdquo;.\u0026ldquo;开头即隐藏文件 -r：逆序显示（根据文件名） -h：文件大小将使用单位m、g、t等（根据文件大小自判定的），一般配合-l使用 -t：按照时间顺序显示 -R：递归显示所有文件    ls -lrt / /root #列出 / 和 /root 下的所有文件\rls -lh\r touch  touch \u0026lt;路径文件名\u0026gt;：新建文件  mkdir  mkdir [option]... [dirName]...：新建文件夹  mkdir ./a/1 ./a/2 #指定路径新建文件夹\rmkdir a #在当前目录新建文件夹可以简写\rmkdir -p /a/b/c/d/e #-p忽略报错，已存在的不会提示报错，路径上不存在的目录也都将被新建\r rm rmdir /a #只能删除空的目录\rrm -r /a #可以删除非空目录，但是会进行递归提示询问\rrm -r -f /a #参数f使删除时不进行递归提示询问\rrm -rf / a #参数f风险很大，一定要检查好参数，如果像这样/和a之间多了个空格，即删除整个根目录和当前目录下的a目录中的所有文件，gg\r cp  cp \u0026lt;源文件名\u0026gt; \u0026lt;目标文件名\u0026gt;：复制文件 -r：可以复制目录 -v：显示复制进度 -p：保留文件原来的时间属性 -a：保留文件原来的所有属性（ls -l能看到）  cp /a/\r mv  mv \u0026lt;源\u0026gt; \u0026lt;目标\u0026gt;：移动。在同一个目录内移动即实现改名  mv ./a1/b ./a2 #移动b到a2目录下\rmv ./a1/b ./a2/bre #移动并重命名\rmv file* ./a #*匹配1或多个字符，所有对应的文件都可以被移动\r  通配符  定义: shell 内建的符号 用途:操作多个相似（有简单规律）的文件 常用通配符  *：匹配任何字符串 ?：匹配1个字符串 [xyz]：匹配xyz任意一个字符   [a-z]：匹配一个范围 [!xyz]、[^xyz]：不匹配     *：匹配一或多个字符 ?：匹配一个字符    dd  文件空洞：文件从开头到结尾的扇区所占用的磁盘空间中，未存储任何数据的部分被称为空洞。比如给linux创建一个虚拟机的磁盘空间1T，但是实际只存放1M的数据，其它都是空洞，用du查看将是1M，ls -lh查看将是1T\n  dd：复制文件，可以制造文件空洞。cp则不能制造文件空洞 参数  if：输入文件 of：输出文件 bs：块大小，即指定多少空间作为一个块进行读写 count：读写次数（以块为单位的） seek：跳过块数    # 从/dev/zero可以读取无穷多个0，用以测试dd。跳过5个块，即有10M不写入数据，将成为文件的空洞\rdd if=/dev/zero of=testfile bs=2M count=10 seek=5\r du  du：查看文件实际占用磁盘空间，不计算文件空洞，默认单位byte。ls -lh中显示的文件大小有些不同，是记录文件开头到结尾的磁盘空间，包括范围内的文件空洞  # 查看：ls -lh和du分别查看afile都是20m\rdd if=/dev/zero bs=2M count=10 of=afile\rls -lh afile\rdu afile\r# 查看：ls -lh和du分别查看bfile分别为30、20M\rdd if=/dev/zero bs=2M count=10 seek=5 of=bfile\rls -lh bfile\rdu bfile\r 文本 cat  cat：文本内容显示到终端  head  head：查看文件开头。默认显示10行，-5参数显示5行  tail  tail：查看文件结尾。默认显示10行，-5参数显示5行  常用参数-f 文件内容更新后，显示信息同步更新，ctrl+c停止    wc  wc：统计文件内容信息。-l参数显示文件行数  more  more：将分页显示 less more  打包和压缩  Linux的备份压缩  最早的Linux备份介质是磁带，使用的命令是tar，即打包 可以对打包后的磁带文件进行压缩储存，压缩的命令是gzip和bzip2 经常使用的扩展名是.tar.gz.tar.bz2.tgz   /etc一般是保存配置文件的目录，属于重点备份的文件，以etc为例进行打包和压缩 tar [option]... \u0026lt;目标.tar.压缩方式\u0026gt; \u0026lt;源\u0026gt;：可以采用双扩展名以方便知道是那种方式打包的  #打包\rtar cf /tmp/etc-bk.tar /etc #c表示打包，f指定文件，tar的选项没有-或者--作选项引导符；\r#解包\rtar xf /tmp/etc-bk.tar -C /tmp #x表示解包，f指定文件，-C指定要存放的位置，不指定则存放在同目录下\r#打包并压缩。可以单独使用gzip或bzip2命令进行压缩，但是tar已经集成了它们的功能。gzip压缩速度更快，bzip2压缩比例更小\rtar zcf /tmp/etc-bk.tar.gz /etc #z即使用gzip进行压缩，也可以单后缀.tgz\rtar jcf /tmp/etc-bk.tar.bz2 /etc #j即使用bzip2进行压缩，也可以但后缀.tbz2\r#解压缩并解包，即zxf或jxf\rtar x\r 文本编辑 vi  vi是多模式文本编辑器 多模式产生的原因 四种模式，通过模式的切换，就可以无需鼠标仅使用键盘进行各种各样的文本操作  正常模式(Normal-mode)：进入编辑器界面时的初始模式，显示文本内容，有光标，光标可以移动。该模式下所有键盘输入的按键都是对编辑器所下的命令。 插入模式(Insert-mode)：  Normal模式下，使用快捷键 i 进入insert模式，可以输入文本内容    命令模式(Command-mode) 可视模式(Visual-mode)   vi：进入编辑器，默认进入的是vim的版本，是原始vi编辑器的扩展，是一个同vi向上兼容的文本编辑器 vim：进入vim编辑器，或者vim \u0026lt;filename\u0026gt;以vim编辑器打开某个文件  esc回到Normal模式，有光标 正常模式  h、j、k、l：左、下、上、右，移动光标。在图形界面或远程终端上，如果有 左、下、上、右（箭头）方向键，效果是一样的，但是如果是字符终端，可能会出现乱码 复制  一行：Normal模式下，按yy可以复制一行内容 多行：Normal模式下，按3yy即可复制当前光标所在行开始的3行 光标到行尾：y$   剪切：dd、d$，与y类似 粘贴：按p键，可以在光标所在行的下一行粘贴，继续按p键可以粘贴多次 撤销：u键 重做：把撤销的内容恢复，ctrl+R 删除单个字符：x 替换单个字符：按r键，在输入新的字符 显示行数：:set nu 移动到指定行：5G移动到第5行，g移动到第一行，G移动到最后一行 ^，或者说shift+6：移动光标到一行开头 $，或者或shift+4：移动光标到一行末尾   insert  Normal模式下，使用快捷键 i 进入insert模式，光标位置不变 大写的I，进入插入模式，光标将从，光标将移动到光标所在行的开头 小写a：光标右移一位， 大写A：光标行最右 o：光标到下一行，并且是新开空行 O：光标到上一行，并且是新开空行   命令模式：  :：进入命令模式，窗口末尾显示:时即表示处于命令模式了，可以键入命令  按esc退出命令模式 w \u0026lt;filename\u0026gt;：保存文件到目标，如w /tmp/a.txt，如果是通过文件名打开的vim编辑器，只需要w命令即可保存修改 q：退出vim编辑器 wq、wq \u0026lt;filename\u0026gt;：连用，即保存并退出 q!：不保存并退出 !\u0026lt;命令\u0026gt;：如果想要临时执行一些系统命令，查看系统命令的结果，可以通过该命令来进行，如!ip addr查看ip地址，按回车即可重新回到编辑器界面   /字符串：查找指定字符串，按n跳到下一个，shift+n跳到上一个 :s/旧串/新串：替换光标所在的行的匹配的字符 :%s/旧串/新串：替换全文匹配的字符第一个字符 :%s/旧串/新串/g：替换全文匹配的字符所有 :set nu：显示行数。只单次生效，需要修改vim配置以长期生效  vim /etc/vimrc：移动到最后一行插入内容set nu，保存退出即可   :set nonu：不显示行数   Visual：可视模式  v：字符可视模式 V：行可视模式 ctrl+v：块可视模式（类似于列模式）  d：删除选中块 ctrl+i：进入插入模式，输入要插入的内容，按2次esc，可以批量给选中的每一行前面添加相同内容        环境变量  PATH：指定命令的搜索路径  PATH=$PAHT:\u0026lt;PATH 1\u0026gt;:\u0026lt;PATH 2\u0026gt;:\u0026mdash;\u0026mdash;\u0026ndash;:\u0026lt; PATH n \u0026gt; export PATH     临时：仅对当前 shell(bash) 或其子 shell(bash) 生效\nexport PATH=$PATH:/usr/local/go/bin\r 用户\n# 编辑文件 ~/.bash_profile\rvim ~/.bash_profile\r# 写入环境变量\rexport PATH=$PATH:/usr/local/go/bin\r# 刷新环境变量使生效\rsource ~/.bash_profile\r 全局\n# 编辑文件 /etc/profile\rvim /etc/profile\r# 写入环境变量\rexport PATH=$PATH:/usr/local/go/bin\r# 刷新环境变量使生效\rsource /etc/profile\r 用户 useradd user1 #添加新用户\r#创建用户后会在/home中创建用户对应的目录，目录名与用户名一致\rls -a /home/yuanya #会有一些隐藏文件\rtail -10 /etc/passwd #会有用户相关内容\rtail -10 /etc/shaow #会有用户相关内容，密码相关\rid #查看当前用户\rid root #查看指定用户。可以借此验证是否存在指定用户\r#用户会有唯一id，系统是通过id识别不同用户的。root用户uid=0，如果把普通用户的uid改为0，则也会被系统当作root用户对待\r#用户有用户组，创建时不指定用户组则属于其同名组，即用户组名与用户名一致，只有该用户一人\rgroupadd group1 #新建用户组\rgroupdel group1 #删除用户组\ruseradd user2 group1 #添加新用户并指定用户组\rpasswd #为当前用户自己设置密码\rpasswd yuanya #为指定用户设置密码\ruserdel yuanya#删除用户，用户的家目录/home/yuanya会被保留，文件所属用户变为数字，只有root用户可以使用，加-r则不会保留家目录。/etc/passwd、/etc/shaow中相关用户信息都将被删除\rman usermod #usermod用于修改用户属性\rusermod /home/yuanya yuanyatianchi #修改家目录/home/yuanya的名字为yuanyatianchi，相当于搬家\rusernod -g group1 yuanya #将指定用户分配到指定用户组\rman chage #更改用户密码过期信息\r#root用户切换普通用户无需密码，普通用户切换其它用户需要密码\rsu - user1 #切换用户，\u0026quot;-\u0026quot;表示用户及用户运行环境的完全切换，这里运行环境切换指自动进入到user1的家目录/home/user1，root的家目录是/root\rsu user1 #不完全切换，仍在之前用户的家目录中，但是已经没有ls读取权限了，需要手动切换user1自己的家目录\r 网络配置 网卡 查看\nip addr #查看网卡及相关信息。简写，等价于ip addr ls、ip addr show\rip addr ls ens33 #查看指定网卡。不能省略ls或show\rip addr add 10.0.0.1/24 dev ens33 #设置ip\rip link #查看网卡物理连接情况。ip link ls、ip link show等都是一样的\rip link ens33 #查看指定网卡\rip link set dev ens33 up #网卡启动\rip link set dev ens33 down #网卡停止\r 修改\n 网卡接口命名修改：默认为ens33或enp0s3之类的。没有特殊需求默认即可  vim /etc/default/grub编辑grub，在GRUB_CMDLINE_LINUX参数内容的末尾rhgb quiet的后面添加 biosdevname=0 net.ifnames=0两项，网卡命名受这两个参数影响（如下表） grub2-mkconfig -o /boot/grub2/grub.cfg更新到真正的启动配置文件 reboot重启        biosdevname net.ifnames 网卡名     默认 0 1 ens33或enp0s3\u0026hellip;   组合1 1 0 em1   组合2 0 0 eth0    网关 ip route #查看网关信息。ip route ls、ip route show等都是一样的\rip route | column -t #格式化一下\r#设置路由\rip route add default gw \u0026lt;网关ip\u0026gt; #设置默认路由\rip route add -host\u0026lt;指定ip\u0026gt; gw\u0026lt;网关ip\u0026gt; #为目的地址设置指定网关\rip route add -net\u0026lt;指定网段\u0026gt; netmask \u0026lt;子网掩码\u0026gt; gw\u0026lt;网关ip\u0026gt; #为目的网段设置指定网关\rip route add 192.168.56.0/24 via 192.168.56.2 dev ens33 #设置静态路由\r#删除路由\rip route del 192.168.56.0/24 #删除静态路由\r 网络故障排除命令  从上到下逐步分析，基本能够解决大部分网络问题了 ping www.baidu.com：到目标主机是否畅通 traceroute www.baidu.com：追踪路由。centos7没有原装 mtr www.baidu.com：检查到目标主机之间是否有数据丢失。centos7没有原装 nslookup www.baidu.com：搜索域名的ip。centos7没有原装 telnet www.baidu.com：检查端口连接状态，键入^]然后quit退出telnet。centos7没有原装 tcpdump -i any -n host 10.0.0.1 and port 80 -w /tmp/filename：更细致的分析数据包，抓包工具，后面详细讲。centos7没有原装。centos7没有原装 netstat -ntpl：查看本机服务监听地址。n显示ip地址而不是域名，t以tcp截取显示的内容（udp等就不显示了），p显示端口对应的进程，l即listen表示处于监听状态的服务。centos7没有原装，通过ss -ntpl代替 ss -ntpl：查看本机服务监听地址。n显示ip地址而不是域名，t以tcp截取显示的内容（udp等就不显示了），p显示端口对应的进程，l即listen表示处于监听状态的服务  网络服务管理   前面的很多配置都是临时的，网卡重启后就重置了，通过配置文件修改才能实现持久配置\n  网络服务管理程序分为两种，分别为SysV和systemd（centos7中新的），最好不要同时使用两套网络管理系统，可以选择关闭其一\n  SysV\n service network start|stop|restart|status：service的操作 chkconfig -list network：查看SysV服务在不同运行级别中的启用状况 chkconfig --level 2345 network \u0026lt;on|off\u0026gt;：打开或关闭运行级别为2、3、4、5的SysV服务。这样就将网络管理都交给systemd的NetworkManager了    systemd的NetworkManager额外功能在于：比如插入网线（或者连接无线）可以识别网卡激活状态自动进行一些网络激活，对个人电脑来说颇有用处，对服务器来说稍显鸡肋。如果都是新写一些脚本，可以使用systemd；如果有一些SysV的脚本需要延用，为了方便可以继续使用SysV\n systemctl list-unit-files NetworkManager.service：查看是否开启 systemctl start|stop|restart NetworkManger.service：启动、停止、重启NetworkManger systemctl enable|disable NetworkManger：启用、禁用NetworkManger      网络配置相关文件   /etc/sysconfig/network-scripts/ifcfg-ens33（根据网卡名有变）\n  BOOTPROTO=\u0026quot;dhcp\u0026rdquo;：遵循dhcp协议的动态ip，可以改为\u0026quot;none\u0026quot;表示静态ip\n  ONBOOT=\u0026quot;yes\u0026rdquo;：是否开机启动，\u0026ldquo;no\u0026quot;时则需要手动启动网卡\n  静态内容\nTYPE=Ethernet\rUUID=045d35e8-49bc-4865-b0\rNAME=eth0\rDEVICE=eth0\rONBOOT=yes\rB0OTPROTO=none #静态\rIPADDR=10.211.55.3 #地址\rNETMASK=255.255.255.0 #子网掩码\rGATEWAY=10.211.55.1 #网关\rDNS1=114.114.114.114 #nds，可以有3个，NDS1、NDS2、NDS3\r   service network restart或systemctl restart NetworkManger.service重启网卡即可生效\n    主机\n  hostname查看\n  hostname 临时主机名\n  hostname set-hostname 永久主机名\n  hosts配置文件：/etc/hosts。因为有些服务绑定的可能是主机名，所以记得在hosts文件的最后面中配置映射。否则可能启动时会卡住某些服务直到超时，会很慢\n127.0.0.1 主机名如yuanya.tianchi\r   reboot重启主机以验证\n    软件管理  包管理器是方便软件安装、卸载，解决软件依赖关系的重要工具\n  Centos、RedHat使用yum包管理器，软件安装包格式为rpm（RedhatPackageManager） Debian、Ubuntu使用apt包管理器，软件安装包格式为deb（）  软件包管理器 rpm包和rpm命令 yum仓库 源代码编译安装 内核升级 grub配置文件\nrpm  rpm包格式  vim-common-7.4.10-5.el7.x86_64.rpm 软件名称-软件版本.系统版本.平台.rpm   参数  -q：查询软件包 -i：安装软件包 -e：卸载软件包   ls -l查看/dev，这里面都是设备文件，发现c和b的文件类型，c表示字符设备，b表示块设备，把光盘加载到虚拟机即加载到/dev/sr0中的，dd if=/dev/sr0 of=/xxx/xxx.iso就可以把真的光盘做成光盘镜像，块设备不能通过cp等命令直接进行操作，需要挂载（类似于windows中插入光盘后会自动挂载弹出新盘符，linux需要自行手动操作），mount /dev/sr0 /mnt ，linux下推荐挂载到/mnt目录，-t可以指定挂载类型，默认则会是自动识别。之后可以发现文件是只读的，但是可以拷贝 rmp -qa | more：a是查询所有的意思，可以查询所有系统安装的软件包，软件包很多，通过管道符 | more分屏显示，按空格下一页，按q退出 rmp -q vim-common：根据软件名查询软件包名 rmp -e vim-enhanced：卸载软件 rmp -i vim-enhanced-7.4.160-5.el7.x86_64.rpm：安装软件包。vim-enhanced依赖vim-common，如果vim-common没有被安装，将安装失败，所以需要先安装vim-common，如果把两个软件包都放在同一个目录，也可以自动安装依赖  yum   rpm问题很明显了，如果是一个庞大的依赖树，将会非常恐怖，难以人工安装，所以就有了yum仓库（包管理器），用于实现rmp安装自动依赖；还有如果版本不符合要求，还需要通过源代码编译安装软件包\n  rpm包的问题：需要自己解决依赖关系，软件包来源不可靠\n  Centos yum源：http://mirror.centos.org/centos/7/\n  国内镜像：https://developer.aliyun.com/mirror/\n  配置yum源\n  #备份\rmv /etc/yum.repos.d/CentOS-Base.repo /etc/yum.repos.d/CentOS-Base.repo.bk\r#下载ailiyun的yum源配置文件，无wget用curl亦可\rwget -O /etc/yum.repos.d/CentOS-Base.repo http://mirrors.aliyun.com/repo/Centos-7.repo\r#安装epel，用于扩展yum仓库可以安装的软件包，比如最新的linux内核等\rinstall epel-release -y\r#epel的aliyun镜像配置\rwget http://mirrors.aliyun.com/repo/epel-7.repo -O /etc/yum.repos.d/epel.repo\r#清空并刷新缓存\ryum clean all \u0026amp;\u0026amp; yum makecache\r# 查看当前源上可下载的指定软件所有版本\ryum list docker-ce --showduplicates|sort -r\r apt 替换为阿里源：https://blog.csdn.net/wangyijieonline/article/details/105360138\nlsb_release -a #查看代号codename\r 到阿里源看下对应代号的源是否存在：http://archive.ubuntu.com/ubuntu/dists/ ，存在则可以根据模板进行替换\n#把所有的TODO替换成系统的codename\rdeb http://mirrors.aliyun.com/ubuntu/ TODO main restricted universe multiverse\rdeb-src http://mirrors.aliyun.com/ubuntu/ TODO main restricted universe multiverse\rdeb http://mirrors.aliyun.com/ubuntu/ TODO-security main restricted universe multiverse\rdeb-src http://mirrors.aliyun.com/ubuntu/ TODO-security main restricted universe multiverse\rdeb http://mirrors.aliyun.com/ubuntu/ TODO-updates main restricted universe multiverse\rdeb-src http://mirrors.aliyun.com/ubuntu/ TODO-updates main restricted universe multiverse\rdeb http://mirrors.aliyun.com/ubuntu/ TODO-proposed main restricted universe multiverse\rdeb-src http://mirrors.aliyun.com/ubuntu/ TODO-proposed main restricted universe multiverse\rdeb http://mirrors.aliyun.com/ubuntu/ TODO-backports main restricted universe multiverse\rdeb-src http://mirrors.aliyun.com/ubuntu/ TODO-backports main restricted universe multiverse\r #以codename=focal为例\rdeb http://mirrors.aliyun.com/ubuntu/ focal main restricted universe multiverse\rdeb-src http://mirrors.aliyun.com/ubuntu/ focal main restricted universe multiverse\rdeb http://mirrors.aliyun.com/ubuntu/ focal-security main restricted universe multiverse\rdeb-src http://mirrors.aliyun.com/ubuntu/ focal-security main restricted universe multiverse\rdeb http://mirrors.aliyun.com/ubuntu/ focal-updates main restricted universe multiverse\rdeb-src http://mirrors.aliyun.com/ubuntu/ focal-updates main restricted universe multiverse\rdeb http://mirrors.aliyun.com/ubuntu/ focal-proposed main restricted universe multiverse\rdeb-src http://mirrors.aliyun.com/ubuntu/ focal-proposed main restricted universe multiverse\rdeb http://mirrors.aliyun.com/ubuntu/ focal-backports main restricted universe multiverse\rdeb-src http://mirrors.aliyun.com/ubuntu/ focal-backports main restricted universe multiverse\r wget yum install wget -y\r  yum常用选项：-y：安装、更新过程中选yes  yum install [软件名]：安装软件包 yum remove [软件名]：卸载软件包 yum list查看软件列表，listl grouplist查看软件包 yum update检查yum仓库并更新所有可更新软件包，yum update [软件名]更新指定软件包   如果要使用yum中没有的或者还未更新的包，可以使用使用源代码编译安装的方式安装软件包  二进制安装 即类似与windows上的大部分软件安装方式，安装过程也需要授权各种协议，也非常麻烦\n源代码编译安装   将源代码编译成可以执行程序，copy到指定目录使用即可\n  源代码编译安装\n 获取压缩包：wget https://openresty.org/download/openresty-1.15.8.1.tar.gz 解压：tar -zxf openresty-VERSION.tar.gz 进入目录：cd openresty-VERSION/ ./configure \u0026ndash;prefix=/usr/local/openresty：当前的系统环境已经预设在源代码中了，但是没有与真正的系统环境匹配，运行可执行文件configure使其配置（没有的话看看有没有README等指导），\u0026ndash;prefix指定程序目录 make -j2：编译。j2表示用2个逻辑上的cpu进行编译，如果代码之间没有上下文依赖关系则可以提高编译速度 make install：把编译好的程序安装到\u0026ndash;prefix指定的目录    源代码编译安装可能会遇到各种错误，比如依赖问题，需要根据提示逐一解决，非常麻烦，不到迫不得已还是yum吧\n  linux内核升级\n rpm格式内核  uname -r：查看内核版本 yum install kernel-3.10.0：升级指定内核版本 yum update：升级已安装的软件包（包括linux内核）和补丁   源代码编译安装内核  yum install gcc gcc-C++ make ncurses-devel openssl-develeurutls-Deu-uevet：安装依赖包，可以看到有这么多依赖包，如果提前不知道，直接安装的话就需要逐个查看报错并安装解决 下载并解压缩内核并解压：https://www.kernel.org ，生成环境一定要用稳定stable或者长期支持langterm版本，tar xvf linux-5.1..10.tar.xz -C /usr/src/kernels解压（tar已经直接支持xz格式了） 配置内核编译参数  cd /usr/src/kernels/linux-5.1.10/ make menuconfig | allyesconfig | allnoconfig。选择menuconfig弹出会出来界面菜单自行配置，allyesconfig则全部配置yes，allnoconfig则全部配置no（甚至可能无法启动）  menuconfig配置支持NTFS文件系统：找到Filesytems，找到NTFS file system support，空格进行选择，默认为空表示no，变为[M]表示作为内核的模块，将编译进内核使用模块的方式加载，模块意味着可以移除，以减小内核体积，[*]表示固化到内核中，不能被移除。选中NTFS后还出现了子选项，比如NTFS write support表示写入支持，子选项的[*]表示固化到父选项，而不是内核。 其实这个未安装的内核，其所有配置都在内核目录下的.config文件中，启用的项等于m或者y，未启用的则被#注释掉了，非常熟悉的话甚至可以直接修改配置文件 当前使用中的内核的配置文件，在/boot目录下，如果想要沿用当前系统内核配置，拷贝配置文件即可：cp /boot/config-kernelversion.platform /usr/src/kernels/linux-5.1.10/.config 当然仍然可以再make menuconfig进入菜单进行更多修改     编译：make -j2 all，通过lscpu查看CPU信息 安装内核：注意保证磁盘空间充足，df -h可以查看磁盘分区信息  先安装内核所支持的模块：make modules_install 再安装内核：make install   reboot重启进入系统引导界面，可以发现新安装的内核可以选择了      grub配置   grub：centos7使用的是grub2，此前是grub1，grub1中所有的配置文件需要手动去编辑，要像网卡配置文件一样记住每个项是什么功能，grub2则可以用命令即可进行修改\n  grub配置文件：/boot/grub2/grub.cfg，一般不要直接编辑该文件，而是修改/etc/default/grub文件（一些基本配置），如果还有更详细的配置，可以修改/etc/grub.d/下的文件\n  grub2-mkconfig -o /boot/grub2/grub.cfg：产生新的grub配置文件\n  /etc/default/grub\n  GRUB_DEFAULT=saved。表示系统默认引导的版本内核。通过命令grub2-editenv list查看默认引导的版本内核\n grep ^menu /boot/grub2/grub.cfg：找到文本文件当中包含关键字的一行，^表示以什么开头，这里即在grub配置文件去找到以menu开头的，能够看到内核的列表，按索引以0开始 grub2-set-default 0：选择第一个内核作为linux启动时的默认引导 grub2-editenv list：查看当前默认引导内核已经改变了    GRUB_CMDLINE_LINUX：确认引导时对linux内核增加什么参数\n  rhgb：引导时为图形界面\n  quiet：静默模式，引导时只打印必要的消息，启动出现异常时可以去除quiet和rhgb以显示更多信息\n  readhat7重置root密码\n  引导界面时，选择要引导的内核，按E进入设置信息，找到linux16开头的一段，可以发现刚刚quiet、rhgb等信息，可以直接键盘输入添加更多项，在该行末尾添加rd.break，ctrl+x启动\n  进入后是内存中的虚拟的一个文件系统，而真正的根目录是/sysroot（输入命令mount可以发现根为/sysroot），在这里所做的操作是不会进行保存的，并且是只读方式的挂载，不能写，防止修复时损坏原有文件\n  mount -o remount,rw /sysroot，重新挂载到根目录并且是要可读写的，之后mount，发现有了r,w权限\n  chroot /sysroot 选择根，即设置根为/sysroot目录；\n  echo 123456|passwd \u0026ndash;stdin root 修改root密码为redhat，echo正常情况是打印到终端，这里通过管道符发送给password命令，\u0026ndash;stdin是password命令的参数，正常情况是通过终端输入，这里即表示通过标准输入进行输入，并传递给root用户。或者password 123456；或者输入passwd，交互修改；\n  SELinux安全组件，叫做强制访问控制，会对etc/password和/etc/shadow进行校验，如果这两个文件不是在系统进行标准修改的，会导致无法进入系统。vim /etc/selinux/config中可以通过设置SELINUX=disabled关闭SELinux，即使生产环境也多半会关掉它\n  或者修改/etc/shadow文件，touch /.autorelabel，这句是为了selinux生效\n  注意备份\n    exit退出根回到虚拟的root中，然后reboot\n          进程管理  进程：进行中的程序。运行中的程序，从程序开始运行到终止的整个生命周期是可管理的。C程序的启动是从main函数int main(int agrc, char*argv[])开始的，终止的方式分为正常终止、异常终止，正常终止有从main返回、调用exit等方式，异常终止有调用abort、接收信号等方式。计算机资源不足时等情况进行，进程和权限有着密不可分的关系。  进程查看 ps  ps：process status，进程状态 参数  -e：可以查看更多进程，类似于win中的系统进程 -f：额外的信息UID、PPID -L：Light，表示轻量级，轻量级进程，实际上即线程    ps -efL\r   内容\n PID：唯一标识进程（不同用户使用同样的程序也是不同的进程） TTY：终端的次要装置号码（minor device number of tty）。即表示当前执行程序的终端 UID：效用户id。表示进程是由哪个用户启动的信息（可修改），默认显示为启动进程的用户 PPID：进程的父进程id。（linux的 0号进程 和 1 号进程：https://www.cnblogs.com/alantu2018/p/8526970.html ，注意centos7的systemd即以前的init）    pstree：查看进程的树形结构，父子进程关系清晰，但是是静态查看的，ps是动态刷新的。非默认安装的程序，需要自行下载\n  top  top：系统状态，包括进程状态，是动态更新的 参数  -p 1：指定pid查看进程    top\r   内容\n  系统状态\n up：运行时长 users：当前登录用户数量 load average：平均负载，系统进行采用对不同时间内的系统负载进行的计算，3个数值分别是1、5、15分钟内的负载，1即满负载 Tasks：任务状态  total：当前运行的总进程数 running：运行进程数 sleeping：睡眠进程数 stopped：停止进程数 zombie：僵尸进程数   %Cpu(s)：cpu使用情况（平均值）  us：用户空间使用cpu占比 sy：内核空间使用cpu占比 ni：用户进程空间内改变过优先级的进程使用cpu占比 id：空闲cpu占比 wa：等待输入输出的CPU时间百分比 hi：硬件CPU中断占用百分比 si：软中断占用百分比 st：虚拟机占用百分比   KiB Mem：内存状态  total：物理内存总量 used：使用内存量 free：空闲内存量 buffers：用作内核缓存的内存量   KiB Swap：交换区状态  total：交换区总量 used：使用交换区量 free：空闲交换区量 buffers：缓冲的交换区量。内存中的内容被换出到交换区，而后又被换入到内存，但使用过的交换区尚未被覆盖，该数值即为这些内容已存在于内存中的交换区的大小，相应的内存再次被换出时可不必再对交换区写入      进程状态：\n PID：进程id USER：进程所有者的用户名 PR：优先级 NI：nice值。负值表示高优先级，正值表示低优先级 VIRT：进程使用的虚拟内存总量，单位kb。VIRT=SWAP+RES RES：进程使用的、未被换出的物理内存大小，单位kb。RES=CODE+DATA SHR：共享内存大小，单位kb S：进程状态(D=不可中断的睡眠状态,R=运行,S=睡眠,T=跟踪/停止,Z=僵尸进程 %CPU：上次更新到现在的CPU时间占用百分比 %MEM：进程使用的物理内存百分比 TIME+：进程使用的CPU时间总计，单位1/100秒 COMMAND：命令名/命令行    操作\n s：按s可以输入数字更改状态刷新间隔（默认3秒/次），回车确认      控制命令 优先级 nice  nice：优先级调整。优先级值从-20到19，值越小优先级越高，抢占资源就越多。  -n 10 ./demo.sh：设置demo.sh的优先级为10。写一个无限循环的脚本demo.sh并运行   renice：重新设置优先级  -n 15：重新设置优先级值为15    作业控制 jobs  jobs：查看后台作业的工作程序   内容  bg 1指定程序序号使其到后台运行 fg 1指定程序序号使其到前台运行，通过CTRL+Z可以再入后台并暂停挂起，可以通过bg或fg再运行    \u0026amp;  \u0026amp;：使程序后台运行  ./demo.sh \u0026amp;\r 进程的通信方式—信号  信号是进程间通信方式之一，典型用法是：终端用户输入中断命令，通过信号机制停止一个程序的运行。\n kill   kill\n  参数\n -l：查看所有信号 SIGINT：2号信号，通知前台进程组终止进程，可以被程序处理，快捷键CTRL+C SIGKILL：9号信号，立即强制结束程序，不能被阻塞和处理，kill -9 [pid]    nohup：一般与\u0026amp;符号配合运行一个命令\n nohup命令使进程忽略hangup（挂起）信号：比如一个进程正在前台运行，关闭该终端则将发起hangup信号，会使该进程被关掉，如果使用nohup则忽略该信号，即使关闭终端也不会被关闭，但是会变成一个孤儿进程，因为终端关闭了，其父进程没了，但是会被新终端的1号进程作为父进程，nohup启动的进程仍然是用户有关的，会随着用户终端而改变    守护（Daemon）进程：随着开机启动，是用户无关的、在用户之前启动的进程，不需要用户终端的，因为没有终端打印日志等信息，所以一般是以文件的形式记录日志信息\n 守护进程会将其使用的目录切换为根目录，这是什么意思呢，比如windows下，如果使用一个软件的时候，你要删除这个软件所在的目录，会提示目录被使用无法删除，实际上进程运行是基于所在目录的，而根目录只有在关机或重启时才会被卸载    系统日志：/proc，这个目录下所有的内容在硬盘中默认是不存在的，它是操作系统去内存中读取信息以文件的形式进行呈现。比如启动了一个进程，会有与进程号同名的目录，类似于：/proc/27451，进入即可看到关于进程属性的文件\n ps -ef | grep sshd：sshd为例 ls -l cwd：可以看到该进程使用的目录 ls -l fd：可以看到标准输入输出及其输入输出的目录，输入一般是/dev/null表示没有，因为因为终端都没有；输出如果是nohup则一般是输出到，如果是Daemon程序一般是通过socket输出给系统日志程序，系统日志程序将打印到默认的/var/log/下进程对应的目录文件下    cd /var/log：存放系统日志，由进程通过socket通信给日志系统，写入该目录对应文件。下面是一些常见日志\n tail -f /var/log/message：该文件被写入一些系统常规日志 tail -f /var/log/dmesg：内核启动日志 tail -f /var/log/secure：安全日志 tail -f /var/log/cron：cron周期任务日志信息    使用screen命令：因为守护进程就是为了使进程脱离终端，防止进程因终端关闭而关闭，screen工具也可以实现，进入终端操作时先进入screen的环境中，即使终端因为某些原因比如网络中断而断开，screen还是可以继续运行程序，下次连接时也能过screen恢复，\n yum install screen screen进入screen环境 先按ctrl+a，然后按d，即可退出(detached) screen环境 screen -ls查看screen的会话，会话有sessionid唯一标识 screen -r sessionid恢复会话    服务管理工具systemctl\n service：执行简单，但是启动停止重启的脚本完全由你自己编写，服务控制的好坏全凭编写脚本的人决定  cd /etc/init.d：该目录存放service启动的各种服务脚本，比如network 级别：chkconfig \u0026ndash;list，可以发现已经被systemd取代了，https://blog.csdn.net/ctthuangcheng/article/details/51219848  运行级别0：系统停机状态，系统默认运行级别不能设为0，否则不能正常启动 运行级别1：单用户工作状态，root权限，用于系统维护，禁止远程登陆 运行级别2：多用户状态(没有NFS) 运行级别3：完全的多用户状态(有NFS)，登陆后进入控制台命令行模式 运行级别4：系统未使用，保留 运行级别5：X11控制台，登陆后进入图形GUI模式 运行级别6：系统正常关闭并重启，默认运行级别不能设为6，否则不能正常启动     systemctl：  vi /usr/lib/systemd/system：该目录存放systemctl启动的各种服务脚本，比如sshd.service enable：随着开机启动 级别文件：cd /lib/systemd/system，有很多.service，其中级别相关的是：ls -l runlevel*.target 查看当前级别：systemctl get-default 设置默认开机级别：systemctl set-default multi-user.target        SELinux  安全增强linux。一般是利用用户、文件的权限进行安全控制，即自主访问控制；强制访问控制：给用户、进程、文件打上标签，只要用户、进程、文件的标签能对应一致即可以允许访问和控制，比如之前的通过grub进入救援模式修改etc中的password和shadow，selinux就会拒绝访问linux的密码，就需要在根目录下/.autorelabel 会降低服务器性能 MAC（强制访问控制）与DAC（自主访问控制) 查看SELinux的命令  getenforce：查看selinux的状态。默认有3种状态，在/etc/selinux/config文件中，持久修改的话需要重启，临时修改可以通过如setenfortce 0设置 查看标签（label）：ps -Z查看进程标签，id -Z查看用户标签，ls -Z查看文件标签 /usr/sbin/sestatus ps -Z and ls -z and id -z   关闭SELinux  setenforce 0 letc/selinux/sysconfig    内存 状态查看 top  top：可以查看内存和交换  free  free：查看内存和交换，默认单位bit，-m、-g等参数指定单位  free\r 磁盘 lsblk  lsblk [-dfimpt] [device]：list block device，列出存储设备，即磁盘  -d：仅列出磁盘本身，并不会列出该磁盘的分区数据 -f：同时列出该磁盘内的文件系统名称 -i：使用 ASCII 的线段输出，不要使用复杂的编码 （再某些环境下很有用） -m：同时输出该设备在 /dev 下面的权限数据 （rwx 的数据） -p：列出该设备的完整文件名！而不是仅列出最后的名字而已。 -t：列出该磁盘设备的详细数据，包括磁盘伫列机制、预读写的数据量大小等    lsblk\rlsblk /dev/sdb\r  内容  NAME：设备文件名，会省略 /dev 等前导目录 MAJ:MIN：主要：次要设备代码。核心认识的设备都是通过这两个代码来熟悉的 RM：是否为可卸载设备 （removable device），如光盘、USB 磁盘等等 SIZE：当然就是容量啰！ RO：是否为只读设备的意思 TYPE：是磁盘 （disk）、分区 （partition） 还是只读存储器 （rom） 等输出 MOUTPOINT：就是前一章谈到的挂载点！    blkid  blkid：可以列出设备的uuid（全域单一识别码），与lsblk -f类似。Linux 会将系统内所有的设备都给予一个独一无二的识别码， 这个识别码就可以拿来作为挂载或者是使用这个设备/文件系统之用了  blkid\rblkid /dev/sda*\r parted  parted  # 与`fdisk -l`展示的信息基本一致\rparted -l\r# 列出 /dev/vda 磁盘的相关数据\rparted /dev/sda print\r  内容  Model：磁盘的模块名称（厂商） Disk：磁盘的总容量 Sector size（logical/physical）：每个逻辑/物理扇区容量 Partition Table：分区表的格式 （MBR/GPT）    df  df：可以理解为fdisk的补充，可以看到分区挂载的目录等  df -h\r 分区 gdisk/fdisk  gdisk/fdisk操作基本一致\n   gdisk：\n -l：查看磁盘列表    内容：\n Start：起点扇区。磁盘在linux中也是作为文件，如/dev/sd?，以扇区划分 End：止点扇区 Size：是分区的容量 Code：Linux 为 8300，swap 为 8200。不过这个项目只是一个提示而已，不见得真的代表此分区内的文件系统，Linux 大概都是 8200/8300/8e00 等三种格式， Windows 几乎都用 0700 这样，gdisk下按L查看。文件系统类型一般为linux（或其它）类型，ntfs就不能被linux文件系统读取，除非格式化为linux文件系统，或者编译内核时设置开启ntfs    注意\n  切忌操作使用中的分区\n  GPT分区表使用gdisk分区，MBR分区表使用fdisk分区，否则将分区失败，甚至干掉分区记录\n  fdisk 有时会使用柱面 （cylinder） 作为分区的最小单位，与 gdisk 默认使用 sector 不太一\n样，另外， MBR 分区是有限制的 （Primary, Extended, Logical…）\n    # 操作指定磁盘/dev/sda，将进入交互模式\rgdisk /dev/sda\r  交互：m查看操作，p查看当前分区信息，更多操作根据m查看的信息进行即可  Command (? for help): n\rPartition number (1-128, default 1): First sector (34-4140724, default = 2048) or {+-}size{KMGTP}: Last sector (2048-4140724, default = 4140724) or {+-}size{KMGTP}: +1g\rCurrent type is 'Linux filesystem'\rHex code or GUID (L to show codes, Enter = 8300): Changed type of partition to 'Linux filesystem'\r # 查看分区表\rcat /proc/partitions\r# 如果操作分区更新的是linux正在使用的磁盘，分区表不会更新，可以通过重启linux或者partprobe更新\rpartprobe -s\r partprobe  partprobe：更新 Linux 的分区表信息（重启linux亦可），  -s：将打印信息    partprobe\rpartprobe -s\r 文件系统  Linux支持多种文件系统，常见的有  ext4（centos6默认） xfs（centos7默认） NTFS （需安装额外软件ntfs-3g，有版权的，windows用）    ext4   结构\n 超级块：会记录文件数，有副本 超级块副本：恢复数据用 inode：i 节点，记录每一个文件的信息，权限、编号等。文件名与编号不在同一inode，而是记录在其父目录的inode中  ls查看的是inode中的文件信息，ls -i可以查到每一个文件的inode vim写文件会改变inode，vim编辑时，会在家目录下复制一份临时文件进行修改，然后保存复制到源目录，并删除原来的文件（应该是） rm testfile是从父inode删除文件名，所以文件再大都是秒删 ln afile bfile将bfile指向afile并也加入到父inode，且不会占用额外空间   datablock：数据块，存放文件的数据内容，挂在inode上的，如果一个数据块不够就接着往后挂，链式。默认创建的数据块为4k，即使只写了1个字符也是4k，所以存储大量小文件会很费磁盘，所以网络上有一些专门用来存储小文件的文件系统  du统计的是数据块的个数用来计算大小 echo \u0026gt;写入文件只会改变数据块   软连接：亦称符号链接，类似快捷方式。ln -s afile cfile链接两个文件，ls -li afile cfile查看两个文件链接信息，其实cfile就记录了目标文件afile的路径，链接文件的权限修改对其自身是无意义的，对其权限修改将在目标文件上得到反馈。可以跨分区（跨文件系统） facl：文件访问控制，getfacl afile查看文件权限，setfacl -m u:user1:r afile：u表示为用户分配权限，g表示用户组，r表示读权限，m改成x即可收回对应用户（组）权限    磁盘配额的使用：给多个用户之间磁盘使用做限制\n  mkfs   mkfs：make filesystem，创建文件系统。实际上是一个综合的指令，它会去调用正确的文件系统格式化工具软件。常说的“格式化”其实就是“make filesystem”，创建的其实是 xfs 文件系统， 因此使用的是 mkfs.xfs 这个指令\n  mkfs.xfs [-b bsize] [-d parms] [-i parms] [-l parms] [-L label] [-f] \\ [-r parms]：加单位则为是Bytes值，可以用 k,m,g,t,p （小写）等来解释，s指的是 sector 个数\n -b：block 容量，可由 512 到 64k，不过最大容量限制为 Linux 的 4k -d：data section 的相关参数值  agcount=数值：设置需要几个储存群组的意思（AG），通常与 CPU 有关 agsize=数值：每个 AG 设置为多少容量的意思，通常 agcount/agsize 只选一个设置即可 file：指的是“格式化的设备是个文件而不是个设备”的意思！（例如虚拟磁盘） size=数值：data section 的容量，亦即你可以不将全部的设备容量用完的意思 su=数值：当有 RAID 时，那个 stripe 数值的意思，与下面的 sw 搭配使用 sw=数值：当有 RAID 时，用于储存数据的磁盘数量（须扣除备份碟与备用碟） sunit=数值：与 su 相当，不过单位使用的是“几个 sector（512Bytes大小）”的意思 swidth=数值：就是 su*sw 的数值，但是以“几个 sector（512Bytes大小）”来设置   -f：如果设备内已经有文件系统，则需要使用这个 -f 来强制格式化才行 -i：与 inode 有较相关的设置，主要的设置值有  size=数值：最小256Bytes，最大2k，一般保留 256 就足够使用了 internal=[0\u0026amp;#124;1]：log 设备是否为内置？默认为 1 内置，如果要用外部设备，使用下面设置 logdev=device：log 设备为后面接的那个设备上头的意思，需设置 internal=0 才可 size=数值：指定这块登录区的容量，通常最小得要有 512 个 block，大约 2M 以上才行   -L：后面接这个文件系统的标头名称 Label name 的意思    -r：指定 realtime section 的相关设置值，常见的有：\n extsize=数值：就是那个重要的 extent 数值，一般不须设置，但有 RAID 时，最好设置与 swidth的数值相同较佳！最小为 4K 最大为 1G 。    # 不带参数将使用默认值\rmkfs.xfs /dev/sdb3\rblkid /dev/sdb*\r 因为 xfs 可以使用多个数据流来读写系统，以增加速度，因此那个 agcount 可以跟 CPU 的核心数来做搭配！举例来说，如果我的服务器仅有一颗 4 核心，但是有启动 Intel 超线程功能，则系统会仿真 出 8 颗 CPU 时，那个 agcount 就可以设置为 8 喔\n# 找出系统的 CPU 数，并据以设置 agcount 数值\rgrep 'processor' /proc/cpuinfo\rmkfs.xfs -f -d agcount=2 /dev/sdb4\r 分区 如果是虚拟机，可以直接在virtualbox上给其添加一块硬盘进行练习（比如叫sdc），可能需要关机才能添加\n挂载 如果是虚拟机，可以直接在virtualbox上给其添加一块硬盘进行练习（比如叫sdc），可能需要关机才能添加\n  磁盘的分区与挂载：\n  常用命令\n mkfs：使用分区，输入mkfs.可以看到有很多不同后缀，都是指不同的文件系统，比如mkfd.ext4 /dev/sdc2即可格式化为ext4的文件系统。但是文件操作是文件系统之上的操作，无法直接操作，需要将其挂载到某个目录，对目录进行操作， mount：mount -t auto自动检测文件系统，或者直接mount /dev/sdc2 /mnt/sdc2也会自动检测，将/dev/sd2挂载到/mnt/sd2，但是是临时挂载，vim /etc/fstab进行修改，dev/sdc1 /mnt/sdc1 ext4 defaults 0 0，即磁盘目录 挂载目录 文件系统指定 权限(defauls表示可读写) 磁盘配额相关参数1 磁盘配额相关参数2  -t：指定档案系统的型态，通常不必指定，mount 会自动选择正确的型态，或者 mount -t auto 也会自动检测文件系统类型。   mount -t proc proc /proc：把proc这个虚拟文件系统挂载到/proc目录，mount的标准用法是 mount -t type device dir ，但是内核代码中，proc filesystem根本没有处理 dev_name这个参数，所以传什么都没有影响，只是影响mount命令的输出内容，好的实践应该将设备名定义为 nodev、none 等，或者就叫 proc 亦可。 parted：如果磁盘大于2T，不要用fdisk进行分区，而是parted，parted /dev/sdd    用户磁盘配额：限制用户对磁盘的使用，比如创建文件数量（即限制i节点数）、数据块数量\n  xfs文件系统的用户磁盘配额quota，修改步骤如下\n  mkfs.xfs /dev/sdb1：创建分区，如果分区已经存在，为了防止这是一个误操作，会提示使用-f参数强制覆盖，mkfs.xfs -f /dev/sdb1\n  mkdir /mnt/disk1\n  mount -o uquota,gquota /dev/sdb1 /mnt/disk1，-o开启磁盘配额，uquota表示支持用户磁盘配额，gquota表示支持用户组磁盘配额\n  chmod 1777 /mnt/disk1：赋予1777权限\n  xfs_quota -x -c \u0026lsquo;report -ugibh\u0026rsquo;/mnt/disk1：有参数时直接非交互配置，xfs_quota可以直接进入交互模式，但是一般非交互即可，\n -c表示命令，report -ugibh，report表示报告（查看）磁盘配额，-u表示用户磁盘配额，g表示组磁盘配额，i表示节点，b表示块，h可以更人性化显示    xfs_quota -x -c \u0026lsquo;limit -u isoft=5 ihard=10 user1\u0026rsquo; /mnt/disk1：root是无限制的，不要对root进行磁盘配额，没意义。这里对user1进行磁盘配额，limit表示限制磁盘配额，限制用户磁盘配额加-u，限制组磁盘配额加-g；isoft软限制i节点，ihard将硬限制，软限制比硬限制的配置的值更小，达到软限制之后，会提示用户在某一个宽限的时间条件内可以用超过软限制的值，硬限制则绝对不能超过限制的值；数据块限制即bsoft、bhard\n      常见配置文件\n letc/fstab      交换分区（虚拟内存）的查看与创建\n free查看mem和swap，前面提到过 增加交换分区的大小，使用硬盘分区扩充swap  mkswap：如mkswap /dev/sdd1将标记上swap swapon：swapon /dev/sdd1打开swap，通过free可以看到swap被扩充了，swapoff /dev/sdd1关闭swap   使用文件制作交换分区：可以直接创建一个比如10G的文件，或者创建带有空洞的文件，使其在swap的使用过程中逐渐扩大也可以  dd if=/dev/zero bs=4M count=1024 of=/swapfile：创建文件 mkswap /swapfile：即可为文件打上swap标记使其成为swap的空间 chmod 600 /swapfile：为了安全起见，一般修改为600权限 swapon、swapoff   同样swap设置也是临时的，vi /etc/fstab。/swapfile swap swap defaults 0 0，即 swap文件或分区 挂载到swap目录（这是一个虚拟目录，因为swap不需要用文件目录来进行操作，挂载到这个虚拟目录即可） 文件系统格式（也是swap），第一个0表示做dump备份时要不要备份该硬盘（分区），但是现在一般都是tar进行备份，所以设置为0即可，第二个0表示开机的时候进行磁盘的自检的顺序问题，是针对之前的ext2、ext3的文件系统的设置，但是现在已经不需要了，如果发现写入是不完整的自动会对那个分区进行检查，所以也是0即可  如果写错了东西，发现重启启动不起来了，通过grap进入到单一用户模式，来去修改/etc/fstab      raid  软件RAID的使用：RAID（磁盘阵列）  RAID的常见级别及含义  RAID 0 striping条带方式，提高单盘吞吐率 RAID 1 mirroring镜像方式，提高可靠性，需要两块磁盘组成，其中有给做镜像备份 RAID5有奇偶校验，至少需要3块硬盘，2块硬盘写数据，还有1块做奇偶校验（存储2块数据盘的校验数据，该盘损坏，校验数据还是可以通过2块数据盘重新生成），当某一块数据硬盘损坏了，奇偶校验的硬盘就通过奇偶校验来通过未损坏的数据盘来还原数据，但是如果2块数据盘都损坏了，还是没办法了 RAID 10是RAID 1与RAIDO的结合，共4块，两块硬盘做raid1，两块做raid1和raid0   raid控制器（raid卡）：硬件设备，通过数据读写自动计算校验值，自动计算把数据放在哪块硬盘上的，甚至可以带有缓存功能加速硬盘访问 软件RAID的使用：对cpu性能消耗较大，一般不实际使用，而是使用raid卡  需要安装软件包yum install mdadm，练习raid建议划分3个同样大小的空白分区，做软件raid如果分区有大有小默认采用最小的空间，fdisk -l /dev/sd?? mdadm -C /dev/md0 -a yes -l1 -n2 /dev/sdb1 /dev/sdc1：-C /dev/md0创建raid，-a yes表示全部提示都选择yes，比如有数据、格式化等提示，所以要注意分区会被格式化，-l1指定raid级别为raid1，-n2表示2块硬盘是活动的，/dev/sdb1 /dev/sdc1即指定这两块硬盘，实际上可以简写为通配符形式/dev/sd[b,c]1。 可能会提示may not suitable as a boot device什么的，因为是软件不支持      物理卷 逻辑卷  逻辑卷管理：在物理卷之上的虚拟卷，linux根目录就是逻辑卷的，可以将一块硬盘拆分为多个逻辑卷，也可以将多个硬盘合并为一个逻辑卷，根据场景对逻辑卷进行缩放容  pvcreate  新建逻辑卷  先添加几个磁盘/dev/sdb1、/dev/sdc1、/dev/sdd1：pvcreate /dev/sdb1 /dev/sdc1 /dev/sdd1或者简写pvcreate /dev/sd[b,c,d]1，注意前面做了软件raid的磁盘如果没有停掉raid将失败  停止raid：mdadm --stop /dev/md0，破坏超级块：dd if=/dev/zero of=/dev/sdb1 bs=1M count=1、dd if=/dev/zero of=/dev/sdc1 bs=1M count=1 重新创建一下：pvcreate /dev/sd[b,c,d]1，pvs查看。信息：lvm即逻辑卷管理器、PSIZE即物理大小、PREE即物理卷的物理空间剩余量 vgcreate vg1 /deb/sdb1 dev/sdc1给物理卷分组，pvs查看可以发现已经被分到vg1这个卷组了 vgs查看卷组 lvcreate -L 100M -n lv1 vg1：从卷组vg1创建名字为lv1大小为100M的逻辑卷，lvs查看逻辑卷        使用逻辑卷：  格式化：mkdir /mnt/test、mfs.xfs /dev/vg1/lv1 进行挂载：fdisk /dev/sd?? pv vg1 lv1 xfs mount。命令解释：fdisk命令用来分区，用/dev/sd??磁盘建立一个pv，通过pv创建一个vg1，通过vg1创建lv1，lv1上通过xfs命令创建文件系统，mount将文件系统进行挂载。这个复杂过程相当实现了：pv vg1 lv1实现可动态扩展的功能，xfs使可以以文件形式操作的功能，mount实现内存和管理的映射。所以如果不需要扩展，可以直接fdisk /dev/sd?? xfs mount在sd磁盘设备上使用文件系统即可，如果还需要实现更复杂的功能还可以在其上进行raid功能（但就不能直接在磁盘上搭建逻辑卷了，需要在raid的基础上比如搭建一个/dev/md0，然后再在其上进行逻辑卷实现）   用途：可以用来扩展现有的如root、user、src 等分区 扩充逻辑卷  vgextend centos /dev/sdd1，将/dev/sdd1这个pv划分到centos这个vg下 lvextend -L +50G /dev/centos/root df -h查看发现文件系统的容量并没有变大，也需要告知文件系统卷已经扩大了，xfs_growfs /dev/centos/root   缩容 可以发现，卷管理实际上就是一层一层的，从物理磁盘到逻辑卷到文件系统  系统综合状态 sar   使用sar命令查看系统综合状态，可以对系统进行全面的体检\n  参数\n sar -u 1 10：-u是cpu信息。1是采样时间间隔，每1秒采样10次 sar -r 1 10：-r显示内存情况 sar -b 1 10：-b显示IO信息 sar -d 1 10：-d磁盘信息 -q 1 10：进程信息    使用第三方命令iftop查看网络流量情况\n yum install epel-release yum install iftop iftop -P：默认只监听eth0的网络接口    更多好用的工具自行到网络上找寻\n  shell  shell：在计算机科学中，Shell俗称壳（用来区别于核），是指\u0026quot;为使用者提供操作界面\u0026quot;的软件-命令解析（解释）器，即用于解释用户对操作系统的操作，bash即shell实现之一，其它还有cat/etc/shells等\n shell会把用户所执行的命令翻译给内核，内核将命令执行的结果反馈给用户。ls为例，当输入ls时，首先由shell接收到用户执行的命令，对命令选项和参数进行分析，ls是查看文件的，将交给文件系统（属于内核层面了），然后内核把ls要查看的文件和目录翻译成硬盘对应的扇区（ssd硬盘是另外结构），硬件会把查询的结果交给内核，内核在返回给shell，最后返回给用户\nlinux启动过程   BIOS：基本输入输出系统。是主板上的功能，通过bios选择要引导的介质 - 硬盘、或光盘，如果选择了硬盘，就会有一个引导的部分-MBR\n  MBR：硬盘的主引导记录部分，就进入到linux的过程了（以下）\n  BootLoader(grub)：BootLoader指启动和引导内核的工具，现在用的grub2.0，可以引导linux内核（选择哪个内核启动），甚至是windows系统\n  kernel：内核启动\n  systemd（centos7，6中是init）：systemd启动（1号进程）。centos6时，init启动后的所有系统初始化内容都是由shell脚本完成（/etc/rc.d目录下会有大量脚本，比如用于激活软件raid、lvm等系统初始化工作），centos7中则有些内容改变为systemd的配置文件方式（到/etc/systemd/system目录下，根据启动级别来到/user/lib/systemd/system目录下，在这个目录下读取各种各样的配置文件），由应用程序引导。\n  系统初始化（）：比如通过驱动程序加载各种硬件，是由shell脚本完成的\n  shell：shell开始工作\n  bios：系统启动的时候按F2进入界面来选择不同的引导介质，如果选择了硬盘，就会有一个引导的部分-mbr，通过dd命令可以查看MBR（linux中一切皆文件，磁盘也可以当作文件被读取）\n dd if=/dev/sda of=mbr.bin bs=446 count=1，if是磁盘，of指定输出文件，bs指定块大小，count指定1个块。因为mbr.bin是没有文件系统的，所以不能通过cat直接查看其中内容，这里使用hexdump -C mbr.bin用16进制的方式去查，-C将能够显示为字符的内容显示为字符 dd if=/dev/sda of=mbr2.bin bs=512 count=1，扩大到512字节，mbr将包括自盘分区表，最后的55 aa即证明引导扇区是正确的    BootLoader：cd /boot/grub2\n grub2-editenv list：显示默认引导的内核 uname -r：查看当前所使用的内核    base   命令中也大量使用了shell脚本，以/sbin/grub2-mkconfig为例，file /sbin/grub2-mkconfig可以看到文件的描述shell script，vim打开可以发现也就是一个shell脚本\n  shell脚本区别于py、php等脚本，无需掌握那些语言函数，完全由命令构成\n  UNIX的哲学：一条命令只做一件事\n  为了组合命令和多次执行，使用脚本文件来保存需要执行的命令\n  如果是二进制可执行文件，赋予可执行（-x）权限即可，但是如果是shell脚本文件想要执行，还需要额外赋予该文件可读（-r）权限(chmod u+rx filename)\n  #比如某个目录经常反复cd进入并ls查看\rcd /var ; ls ; pwd ; du -sh ; du -sh *\r 可以保存为sh文件，比如叫tmp.sh\n#一般shell脚本头部上会加这样一个申明，这个申明叫做\u0026quot;Sha-Bang\u0026quot;\r#!/bin/bash\rcd /var ; ls ; pwd ; du -sh ; du -sh *\r #赋予权限\rchmod u+rx tmp.sh\r#可以通过bash执行，在bash中\u0026quot;#!/bin/bash\u0026quot;将被当作注释\rbash ./tmp.sh\r#直接运行，当前系统是什么shell就将用什么shell解释，\u0026quot;#!/bin/bash\u0026quot;将是非注释的，它将告诉系统使用/bin/bash来解释执行\r./tmp.sh\r  执行命令的方式  bash ./filename.sh：会在当前终端下fork一个叫做bash的子进程，通过子进程去运行脚本，所以执行脚本后当前终端线程并没有真正进入到/var目录，因为是子进程进到目录的。注意：通过bash执行脚本是不用赋予执行权限的，因为是通过bash的 ./filename.sh：也是产生子进程然后运行，不同的是通过Sha-Bang去解释运行脚本的。直接运行脚本文件，必须有可执行权限， source ./filename.sh：在当前进程运行，所以当前终端进入到了/var目录 . ./filename.sh：开头的.即source的缩写   内建命令和外部命令的区别  内建命令：内建命令不需要创建子进程，将对当前Shell 生效，如cd、pwd、source等 外部命令：需要创建子进程，不会对当前shell生效，如bash    管道 待整理  快捷键：  TAB快捷键补全文件（目录）名 CTRL+L清屏 CTRL+C终止（命令）程序   添加执行权限：chmod a+x 文件名，a表示所有，x表示执行   重定向符号 https://blog.csdn.net/hellozpc/article/details/46721811\n 标准输入输出重定向：即指定命令产生的 stdout（标准输出信息）、stdin（标准输入信息） 写入哪个文件  \u0026gt;：输出重定向到一个文件或设备，覆盖原来的文件。  goland.sh \u0026gt;/dev/null 表示将 goland 运行产生的 stdout 的输出到 /dev/null 中   \u0026gt;!：输出重定向到一个文件或设备 强制覆盖原来的文件 \u0026gt;\u0026gt;：输出重定向到一个文件或设备 追加原来的文件 \u0026lt;：输入重定向到一个程序   标准错误重定向：即指定命令产生的 stderr（标准错误信息） 写入哪个文件  2\u0026gt;：将一个标准错误输出重定向到一个文件或设备 覆盖原来的文件 b-shell  goland.sh \u0026gt;/dev/null 表示将 goland 运行产生的 stdout 的输出到 /dev/null 中   2\u0026gt;\u0026gt;：将一个标准错误输出重定向到一个文件或设备 追加到原来的文件 2\u0026gt;\u0026amp;1：将一个标准错误输出重定向到标准输出 注释:1 可能就是代表 标准输出 \u0026gt;\u0026amp;：将一个标准错误输出重定向到一个文件或设备 覆盖原来的文件 c-shell |\u0026amp;：将一个标准错误 管道 输送 到另一个命令作为输入    别名 sysctl 可以查看和修改系统参数 grubby 可以修改内核参数\n图形界面黑屏 centos7上，使用root用户登录，会出现图形界面黑屏问题，在输入用户名密码之前是有图形界面的，但是输入用户名密码之后桌面出现一秒钟之后转为黑屏\n解决问题：通过 ctrl+alt+F2 切换到命令行界面，或者通过其它终端连接该机器，通过 startx 命令重启图形界面，可以发现缺少（损失）了某些文件，因为系统启动时会检查用户家目录下所有文件，因缺少相应文件导致图形界面不可用，通过 startx 重启图形界面过程中系统会自动重新创建 /root/.Xauthority 文件，图形界面就可以正常使用了。但是重启发现还是会黑屏，仍然需要通过 startx 重开图形界面，但是仅提示 /root/.serverauth.xxx 文件不存在，\n$ startx\rxauth: file /root/.serverauth.3218 does not exist\rxauth: file /root/.Xauthority does not exist\rxauth: file /root/.Xauthority does not exist\rX.Org X Server 1.20.4\rX Protocol Version 11, Revision 0\rBuild Operating System: 3.10.0-957.1.3.el7.x86_64 Current Operating System: Linux MiWiFi-R3A-srv 3.10.0-1160.24.1.el7.x86_64 #1 SMP Thu Apr 8 19:51:47 UTC 2021 x86_64\rKernel command line: BOOT_IMAGE=/vmlinuz-3.10.0-1160.24.1.el7.x86_64 root=/dev/mapper/centos-root ro crashkernel=auto rd.lvm.lv=centos/root rd.lvm.lv=centos/swap rhgb quiet LANG=zh_CN.UTF-8 user_namespace.enable=1\rBuild Date: 24 February 2021 09:09:20PM\rBuild ID: xorg-x11-server 1.20.4-15.el7_9 Current version of pixman: 0.34.0\rBefore reporting problems, check http://wiki.x.org\rto make sure that you have the latest version.\rMarkers: (--) probed, (**) from config file, (==) default setting,\r(++) from command line, (!!) notice, (II) informational,\r(WW) warning, (EE) error, (NI) not implemented, (??) unknown.\r(==) Log file: \u0026quot;/var/log/Xorg.1.log\u0026quot;, Time: Wed Apr 21 00:16:03 2021\r(==) Using config directory: \u0026quot;/etc/X11/xorg.conf.d\u0026quot;\r(==) Using system config directory \u0026quot;/usr/share/X11/xorg.conf.d\u0026quot;\rVMware: No 3D enabled (0, Success).\r sh -c 这个命令将权限不够，因为重定向符号 “\u0026gt;” 和 \u0026ldquo;\u0026raquo;\u0026rdquo; 也是 bash 的命令。我们使用 sudo 只是让 echo 命令具有了 root 权限\nsudo echo \u0026quot;hahah\u0026quot; \u0026gt;\u0026gt; test.csv`\r sh -c 命令，它可以让 bash 将一个字串作为完整的命令来执行，这样就可以将 sudo 的影响范围扩展到整条命令\nsudo sh -c echo \u0026quot;hahah\u0026quot; \u0026gt;\u0026gt; test.csv`\r ","date":"0001-01-01","permalink":"https://yuanyatianchi.github.io/post/it.sys.linux/","tags":["it","sys"],"title":"Linux"},{"content":"MongoDB  文档数据库(schema free)，基于二进制JSON存储文档（BSON）。无需定义类型，field:value形式 高性能、高可用、直接加机器即可以解决扩展性问题 支持丰富的CRUD操作，例如:聚合统计、全文检索、坐标检索  client go https://github.com/mongodb/mongo-go-driver\n$ go get go.mongodb.org/mongo-driver/mongo\r .go func main() {\rvar (\rerr error\rclient *mongo.Client\rdatabase *mongo.Database\rcollection *mongo.Collection\rinsertOneResult *mongo.InsertOneResult\rrecord *LogRecord\rdocId primitive.ObjectID\rfindCond *FindByJobName\rcursor *mongo.Cursor\rdelCond *DeleteCond\rdelResult *mongo.DeleteResult\r)\r/*建立连接*/\rif client, err = mongo.Connect(context.TODO(),\roptions.Client().ApplyURI(\u0026quot;mongodb://127.0.0.1:27017\u0026quot;),\roptions.Client().SetConnectTimeout(5*time.Second),\r); err != nil {\rfmt.Println(err)\rreturn\r}\rdatabase = client.Database(\u0026quot;cron\u0026quot;) //数据库\rcollection = database.Collection(\u0026quot;log\u0026quot;) //表\r/*insert：如果没有写入_id字段，则将自动生成全局唯一ID，为ObjectID类型，是一个12字节的二进制*/\rrecord = \u0026amp;LogRecord{ //数据\rJobName: \u0026quot;job10\u0026quot;,\rCommand: \u0026quot;echo hello\u0026quot;,\rErr: \u0026quot;\u0026quot;,\rContent: \u0026quot;hello\u0026quot;,\rTimePoint: TimePoint{StartTime: time.Now().Unix(), EndTime: time.Now().Unix() + 10},\r}\rif insertOneResult, err = collection.InsertOne(context.TODO(), record); err != nil { //执行\rfmt.Println(err)\rreturn\r}\rdocId = insertOneResult.InsertedID.(primitive.ObjectID) //转ObjectID\rfmt.Println(\u0026quot;自增id：\u0026quot;, docId.Hex()) //转string\r/*find：通过options添加操作*/\rfindCond = \u0026amp;FindByJobName{JobName: \u0026quot;job10\u0026quot;} //过滤条件：{\u0026quot;jobName\u0026quot;: \u0026quot;job10\u0026quot;}\rif cursor, err = collection.Find( //执行查询，返回游标，游标通过Next函数遍历\rcontext.TODO(),\rfindCond, //过滤条件\roptions.Find().SetSkip(0), //偏移量0，等于limit 0 2\roptions.Find().SetLimit(2), //2个\r); err != nil {\rfmt.Println(err)\rreturn\r}\rdefer cursor.Close(context.TODO()) //defer释放资源\rfor cursor.Next(context.TODO()) { //可以用context控制超时时间等\rrecord = \u0026amp;LogRecord{}\rif err = cursor.Decode(record); err != nil { //解码，反序列化\rfmt.Println(err)\rreturn\r}\rfmt.Println(*record)\r}\r/*delete\r删除开始时间早于当前时间的所有日志：delete({\u0026quot;timePoint.startTime\u0026quot;: {\u0026quot;$lt\u0026quot;: 当前时间}})。$lt表示小于，less than\r这里为了简便通过结构体来组织bson，也可以通过bson包的方法来设置，但是比较麻烦*/\rdelCond = \u0026amp;DeleteCond{\rbeforeCond: TimeBeforeCond{ //beforeCond被映射成timePoint.startTime\rBefore: time.Now().Unix(), //Before被映射成$lt\r},\r}\r/*执行删除*/\rif delResult, err = collection.DeleteMany(context.TODO(), delCond); err != nil {\rfmt.Println(err)\rreturn\r}\rfmt.Println(\u0026quot;删除的行数:\u0026quot;, delResult.DeletedCount)\r}\r/*日志结构体*/\rtype LogRecord struct {\rJobName string `bson:\u0026quot;jobName\u0026quot;` // 任务名\rCommand string `bson:\u0026quot;command\u0026quot;` // shell命令\rErr string `bson:\u0026quot;err\u0026quot;` // 脚本错误\rContent string `bson:\u0026quot;content\u0026quot;` // 脚本输出\rTimePoint TimePoint `bson:\u0026quot;timePoint\u0026quot;` // 执行时间点\r}\rtype TimePoint struct {\rStartTime int64 `bson:\u0026quot;startTime\u0026quot;`\rEndTime int64 `bson:\u0026quot;endTime\u0026quot;`\r}\r// jobName过滤条件\rtype FindByJobName struct {\rJobName string `bson:\u0026quot;jobName\u0026quot;` // JobName赋值为job10\r}\r/*{\u0026quot;timePoint.startTime\u0026quot;: {\u0026quot;$lt\u0026quot;: timestamp} }*/\rtype DeleteCond struct {\rbeforeCond TimeBeforeCond `bson:\u0026quot;timePoint.startTime\u0026quot;`\r}\r// startTime小于某时间：{\u0026quot;$lt\u0026quot;: timestamp}\rtype TimeBeforeCond struct {\rBefore int64 `bson:\u0026quot;$lt\u0026quot;`\r}\r 应用场景\n传统的关系型数据库(如MySQL)， 在数据操作的\u0026quot;三高 ’需求以及应对Web2.0的网站需求面前，显得力不从心。 解释: \u0026ldquo;三高”需求: ●High performance -对数据库高并发读写的需求。 ●Huge Storage -对海量数据的高效率存储和访问的需求。 ●High Scalability \u0026amp;\u0026amp; High Availability-对数据库的高可扩展性和高可用性的需求。 而MongoDB可应对“三高\u0026quot;需求。\n具体的应用场景如:\n 社交场景，使用MongoDB存储存储用户信息,以及用户发表的朋友圈信息，通过地理位置索引实现附近的 人、地点等功能。 游戏场景，高效率存储和访问。使用MongoDB存储游戏用户信息，用户的装备、积分等直接以内嵌文档的形式存储，方便查询 物流场景，使用MongoDB存储订单信息，订单状态在运送过程中会不断更新，以MongoDB内嵌数组的形式 来存储，一次查询就能将订单所有的变更读取出来。 物联网场景，使用MongoDB存储所有接入的智能设备信息，以及设备汇报的日志信息，并对这些信息进行多 维度的分析。 视频直播,使用MongoDB存储用户信息、点赞互动信息等。  这些应用场景中，数据操作方面的共同特点是: (1) 数据量大 (2)写入操作频繁(读写都很频繁) (3)价值较低的数据，对事务性要求不高 对于这样的数据，我们更适合使用MongoDB来实现数据的存储。\n什么时候选择MongoDB ? 在架构选型上，除了上述的三个特点外，如果你还犹豫是否要选择它?可以考虑以下的一些问题: 应用不需要事务及复杂join支持 新应用，需求会变,数据模型无法确定,想快速迭代开发 应用需要2000-3000以上的读写QPS (更高也可以) 应用需要TB甚至PB级别数据存储 应用发展迅速，需要能快速水平扩展. 应用要求存储的数据不丢失 应用需要99.999%高可用 应用需要大量的地理位置查询、文本查询 如果上述有1个符合，可以考虑MongoDB, 2个及以上的符合，选择MongoDB绝不会后悔。\nMongoDB简介 MongoDB是一个开源、 高性能、无模式的文档型数据库，当初的设计就是用于简化开发和方便扩展,是NoSQL数 据库产品中的一种。是最像关系型数据库I(MySQL)的非关系型数据库。 它支持的数据结构非常松散,是一种类似于JSON的格式叫BSON(二进制存储的json),所以它既可以存储比较复杂的数据类型，又相 当的灵活。 MongoDB中的记录是一个文档, 它是一个由字段和值对 (field:value) 组成的数据结构。MongoDB文档类似于 JSON对象，即一个文档认为就是一个对象。 字段的数据类型是字符型，它的值除了使用基本的一些类型外,还可 以包括其他文档、普通数组和文档数组。\n   MongoDB术语/概念 SQL术语/概念 解释/说明     database database 数据库   collection table 数据库表/集合   document(bson格式) row 数据记录行/文档   field column 数据字段/域   index index 索引   嵌入文档 $lookup table joins 跨表。MongoDB通过嵌入式文档来实现。mysql通过表连接实现   _id primary key 主键。主键MongoDB自动将id字段设置为主键   aggregation pipeline group by 聚合    BSON数据类型参考列表，基本和JSON一致\n   数据类型 描述 举例     字符串 UTF-8字符串都可表示为字符串类型的数据 {\u0026ldquo;x\u0026rdquo; : \u0026ldquo;foobar\u0026rdquo;}   对象id 对象id是文档的12字节的唯一ID {\u0026ldquo;X\u0026rdquo; :objectld() }   布尔值 真或者假: true或者false {\u0026ldquo;x\u0026rdquo;:true}+   数组 值的集合或者列表可以表示成数组 {\u0026ldquo;x”: [\u0026ldquo;a\u0026rdquo;, \u0026ldquo;b\u0026rdquo;,\u0026ldquo;c\u0026rdquo;]}   32位整数 类型不可用。JavaScript仅支持64位浮点数， 所以32位整数会被自动转换。 shell是不支持该类型的，shell中默认会转换成64位浮点数   64位整数 不支持这个类型。shell会使用一 个特殊的内嵌文档来显示64位整数 shell是不支持该类型的，shell中默认会转换成64位浮点数   64位浮点 shell中的数字就是这一种类型 {\u0026ldquo;x\u0026rdquo;: 3.14159, \u0026ldquo;y\u0026rdquo;: 3}   null 表示空值或者未定义的对象 {\u0026ldquo;x\u0026quot;null}   undefined 文档中也可以使用未定义类型 {\u0026ldquo;x\u0026rdquo;:undefined}   符号 shell不支持，shell会将数据库中的符号类型的数据自动转换成字符串    正则表达 文档中可以包含正则表达式，采用JavaScript的正则表达式语法 {\u0026ldquo;x\u0026rdquo; : /foobar/}   代码 文档中还可以包含JavaScript代码 {\u0026ldquo;x\u0026rdquo; : function0{/* \u0026hellip; */}}   二进制数据 二进制数据可以由任意字节的串组成,不过shell中无法使用    最大值/最小值 BSON包括一个特殊类型，表示可能的最大值。shell中没有这个类型。     提示: shell默认使用64位浮点型数值。{\u0026ldquo;x\u0026rdquo;: 3.14}或{\u0026ldquo;x\u0026rdquo;: 3}。对于整型值，可以使用NumberInt (4字节符号整数)或 NumberLong (8字节符号整数) , {\u0026ldquo;X\u0026rdquo;:Number1It*\u0026ldquo;3\u0026rdquo;}{\u0026ldquo;x\u0026rdquo;:NumberLngl\u0026rdquo;}\nMongoDB的特点 MongoDB主要有如下特点: (1)高性能: MongoDB提供高性能的数据持久性。特别是，对嵌入式数据模型的支持减少了数据库系统上的I/O活动。 索引支持更快的查询，并粗可以包含来自嵌入式文档和数组的键。(文本索弓 |解决搜索的需求、TTL索引解决历史 数据自动过期的需求、地理位置索引可用于构建各种020应用) mmapv1、wiredtiger. mongorocks (rocksdb) 、in-memory 等多引擎支持满足各种场景需求。 Gridfs解决文件存储的需求。 (2)高可用性: MongoDB的复制工具称为副本集(replicaset) ，它可提供自动故障转移和数据冗余。 (3)高扩展性: MongoDB提供了水平可扩展性作为其核心功能的一-部分。 分片将数据分布在一组集群的机器上。 (海 量数据存储，服务能力水平扩展) 从3.4开始，MongoDB支持基于片键创建数据区域。在-个平衡的集群中，MongoDB将一个区域所覆 盖的读写只 定向到该区域内的那些片。 (4)丰富的查询支持: MongoDB支持丰高的查询语言，支持读和写操作(CRUD),比如数据聚合、文本搜索和地理空间查询等。 (5)其他特点:如无模式(动态模式)、灵活的文档模型、\n安装 windows   下载并解压：https://www.mongodb.com/download-center/community\n  数据存储目录：手动创建目录，如data/db，\n  启动MongoDB：mongod命令。启动过后该cmd窗口不能关，否则服务就关闭了\n  指定参数启动\n \u0026ndash;dbpath：指定数据存储目录 \u0026ndash;port：指定端口，缺省27017  mongod --dbpath=../data\r   指定配置文件启动：启动时可以指定配置文件，config目录下新建.conf文件如mongod.conf\nmongod -f ../config/mongod.conf\r#或者\rmongod --config ../config/mongod.conf\r 配置文件内容参考\n详细配置项内容可以参考官方文档: https://ocs.mongodb.com/manual/reference/configuration-options/ [注意] 1)配置文件中如果使用双引号，比如路径地址，自动会将双引号的内容转义。如果不转义,则会报错: error -parsing-yam1-config-file-yaml-cpp-error -at-1ine-3-column-15-unknown-escape-character-d 解决: a.把 \\ 换成 / 或 \\\nb.如果路径中没有空格,则无需加引号。\n#配置文件，遵循yaml文件的格式\rstorage:\rdbPath: E:\\it\\database\\mongodb\\data\r   链接数据库：\n mongo命令  mongo #连接\rmongo --host=127.0.0.1 --port=27017 #连接with参数。这里均为缺省值\r\u0026gt;show databases #显示数据库，databases可简写为dbs\r\u0026gt;exit #退出\r  MongoDBCompass：图形化界面  下载并解压：https://www.mongodb.com/download-center/compass 找到MongoDBCompass.exe即可打开        Linux (2) - 上传压缩包到Linux中,解压到当前目录: tar -xvf mongodb-1inux-x86_ 64-4.0.10.tgz (3)移动解压后的文件夹到指定的目录中: mv mongodb-1inux-x86_ 64-4.0.10 /usr /loca1/mongodb (4)新建几个目录,分别用来存储数据和日志: 擞数据存储目录 mkdir -p /mongodb/sing1e/data/db #日志存储目录 mkdir -p /mongodb/sing1e/log (5)新建并修改配置文件 vi /mongodb/s ing1e/mongod. conf\n启动\nbin/mogond --dbpath=./data #监听到所有网络上，一般当然不能这么做\r /usr/yuanya/it/database/mongodb/bin/mongod -f /mongodb/single/mongod.conf #启动with配置文件\rps -ef | grep mongod #检测是否启动\rkill -2 27017 #关闭服务，数据可能出错\r#标准关闭方法，数据不容易出错\rmongo --port 27017\ruse admin\rdb.shutdownServer()\r 配置 systemLog:\rdestination: file #MongoDB发送所有日志输出的目标指定为文件\rpath: \u0026quot;/mongodb/sing1e/log/mongod.log\u0026quot; #mongod或imongos应向其发送所有诊断日志记录信息的日志文件的路径\rlogAppend: true #当mongos或mongod实例重新启动时，mongos或mongod会将新条目附加到现有日志文件的末尾。\rstorage:\rdbPath: \u0026quot;/mongodb/sing1e/data/db' #mongod实例存储其数据的目录。storage.dbPath设置仅适用于mongod\rjournal:\renabled: true #启用或禁用持久性日志以确保数据文件保持有效和可恢复。缺省为true\rprocessManagement:\rfork: true #启用在后台运行mongos或imongod进程的守护进程模式。启用后即可关闭控制台\rnet:\rbindIp: 192.168.0.2 #服务实例绑定的IP，默认只监听localhost，默认其它机器无法访问\rport: 27017 #绑定端口，默认是27017\r 命令   db.help()：帮助\n  基本库\n admin：类似于mysql的root库，有关mongodb的用户和权限等 local：部署集群时库之间数据会相互复制，但是local中的数据不会复制，可以用来存储限于本地单台服务器的任意集合 config：当mongo用于分片设置时，cofig数据库在内部使用，用于保存分片的相关信息    database：库操作\n  show databases：显示库\n  use 数据库名称：有则进入库，无则创建并进入库。在内存中创建库，当库非空时才会持久化到磁盘\n  db：显示当前数据库\n  db.dropDatabase()：删除当前库。其实是js代码\n    collection：集合操作\n show collections：显示当前库的集合 db.createCollection(\u0026ldquo;集合名\u0026rdquo;)：显式创建集合。隐式创建只创建文档时如果没有指定的集合则将顺便创建集合 db.集合名.drop()：删    document：文档操作\n  db.collection.insert(文档, {写关心, 排序})：插入文档。参数1为文档数据，参数2为可选\n如db.collection.insert({\u0026ldquo;username\u0026rdquo;: \u0026ldquo;username1\u0026rdquo;, \u0026ldquo;password\u0026rdquo;: \u0026ldquo;password1\u0026rdquo;})插入单个文档\n如db.collection.insert( [{\u0026ldquo;username\u0026rdquo;: \u0026ldquo;username1\u0026rdquo;}, {\u0026ldquo;username\u0026rdquo;:\u0026ldquo;username2\u0026rdquo;}] )插入多个文档\n 参数列表    Parameter Type Description     document document or array 要插入到集合中的文档或文档数组。数据格式为bson(基本与json一致)   writeConcern document Optional. A document expressing the write concern. Omit to use the default write concern. See Write Concern.Do not explicitly set the write concern for the operation if run in a transaction. To use write concern with transactions, see Transactions and Write Concern. 翻译：可选。表示写关心的文档。忽略使用默认的写关注点。看到写问题。如果在事务中运行，不要显式地设置操作的写关注点。要在事务中使用写关注，请参阅事务并写关注。\n简单理解为：插入时选择的性能和可靠性的级别。   ordered boolean 可选。如果为真,则按顺序插入数组中的文档，如果其中一个文档出现错误，MongoDB将返回而不处理数组中的其余文档。如果为假，则执行无序插入，如果其中一个文档出现错误，则继续处理数组中的主文档。在版本2.6+中默认为true        db.集合名.find()：查询指定集合所有文档\ndb.集合名.find({\u0026ldquo;username\u0026rdquo;: \u0026ldquo;username1\u0026rdquo;})：条件查询\ndb.集合名.findOne({\u0026ldquo;username\u0026rdquo;: \u0026ldquo;username1\u0026rdquo;})：返回查询到的数据中的第一个，类似于limit 0,1\ndb.集合名.findOne({\u0026ldquo;username\u0026rdquo;: \u0026ldquo;username1\u0026rdquo;}, {username:1})：投影查询，只显示指定字段，这里将只显示username字段和_id字段，_id字段是mongodb中自动添加的主键，在不指定排除的情况下都会显示\ndb.集合名.findOne({username,password:1,_id:0})：显示username和password字段(可以分开写)，排除_id字段\n      MongoDB 功能   增：db.my_collection.insertOne({uid: 10000, name: \u0026ldquo;xiaoming \u0026ldquo;, likes: [\u0026ldquo;football\u0026rdquo;, \u0026ldquo;game\u0026rdquo;]})：插入一条json\n 任意嵌套层级的BSON (二 进制的JSON) 文档ID是自动生成的，通常无需自己指定    查：db,my_collection.find( { likes: \u0026lsquo;football\u0026rsquo;; name: {$in: [ \u0026lsquo;xiaoming\u0026rsquo;, \u0026lsquo;libai\u0026rsquo; ] } } ).sort({uid: 1})：$in表示只要在满足是一组名字 [ \u0026lsquo;xiaoming\u0026rsquo;, \u0026lsquo;libai\u0026rsquo; ] 中的一个即可，.sort({uid: 1}) 表示按uid升序排列，正数即表示升序。\n 可以基于任意BSON层级过滤 支持的功能与MYSQL相当    改：db.my_collection.updateMany( { likes: \u0026lsquo;football\u0026rsquo; }, { $set: { name: \u0026lsquo;libai\u0026rsquo; } } )：更新喜欢足球的所有文档，设置其name为libai\n 第一个参数是过滤条件 第二个参数是更新操作    删：db.my_ collection.deleteMany({name: \u0026lsquo;xiaoming\u0026rsquo;})：删除name为xiaoming的文档\n  参数是过滤条件\n  创建索引：db.my collection.createlndex({uid: 1, name: -1})：uid和name都创建索引，联合索引，\n 可以指定建立索引时的正反顺序，正数表示正序，负数表示反序，非常灵活，这将影响到排序的性能，mysql指定索引不能指定正反顺序，    聚合操作：db.my_collection.aggregate( [ { $unwind: \u0026lsquo;$likes\u0026rsquo; }, { $group: { id: { likes: \u0026lsquo;$likes\u0026rsquo; } }, { $project: { id: 0, like: \u0026ldquo;$_ id.likes\u0026rdquo;, total: { $sum:1 } } } )。pipeline流水线模式，将前一个阶段的输出，作为后一个阶段的输入，内即pipeline\n { $unwind: \u0026lsquo;$likes\u0026rsquo; }：unwind，解开，会把 likes: [\u0026ldquo;football\u0026rdquo;, \u0026ldquo;game\u0026rdquo;] 的内容拆分开，拆成 喜欢football 是一行，喜欢game 是另一行，将两行输入给下一阶段{ $group: { id: { likes: \u0026lsquo;$likes\u0026rsquo; } } { $group: { id: { likes: \u0026lsquo;$likes\u0026rsquo; } }：聚合（即分组），将把 喜欢 football 的聚合在一起，喜欢game 的聚合在一起，然后进入下一阶段 { $project: { id: 0, like: \u0026ldquo;$_id.likes\u0026rdquo;, total: { $sum:1 } } }：project操作符可以取出每一个桶类，按group去分桶，每个桶类是相同喜好的文档，like: \u0026ldquo;$_id.likes\u0026quot;将把对应的likes提取出来 输出：{\u0026ldquo;Iike\u0026rdquo;: \u0026ldquo;game\u0026rdquo;, \u0026ldquo;total\u0026rdquo;: 1} {\u0026ldquo;Ilike\u0026rdquo;: \u0026ldquo;football\u0026rdquo;, \u0026ldquo;total\u0026rdquo;: 1}。pipeline流式计算，功能复杂，使用时多查手册     MongoDB MYSQL 说明     $match where    $group group by    $match having    $project select    $sort order by    $limit limit    $sum sum    $count count       原理  整体存储架构：类似于单机-\u0026gt;主从-\u0026gt;多主从的分布式  Mongod：单机版数据库。没错就是叫\u0026quot;Mongod\u0026quot;不是\u0026quot;MongoDB\u0026rdquo; Replica Set：复制集，由多个Mongod组成的高可用存储单元。  有主从关系，数据写入主节点，异步复制数据到从节点。 客户端默认读取主节点，可以要求客户端取读从节点，但从节点读取的数据可能存在延迟 主节点宕机，复制集内会重新选举 至少3个节点组成，其中1个可以只充当arbiter 主从基于oplog复制同步(类比mysql binlog)   Sharding：分布式集群，由多 个Replica Set组成的可扩展集群  每个Replica Set称为一个shard（分片），对外提供服务时通过一个或多个router（程序叫mongos）来代理转发客户端请求到正确的分片上 存在一个ConfigServers，它也是一个Replica Set，专门用来存储数据元信息，即存放分片与数据的对应信息，记录哪些key存放在那个分片上的 Collection分片：Collection自动分裂成多个chunk，每个chunk被自动负载均衡到不同的shard，每个shard可以保证其上的chunk高可用。通俗点说就是一个表的数据是被分为多份（比如8份），均匀的分摊在多个分片（比如2个分片，每个分片上有4份该Collection的数据）上的  按range拆分chunk：与mysql分库分表是类似的，假如表有一个x字段，按x拆分，该字段为shard key。假设x的取键是 负无穷~正无穷，可能就按 负无穷~-25、-75~25、25~175、175~正无穷 区分为4个chunk。具体按什么拆分，要根据数据特征来定，假如x是时间，可能过了175这个时间以后，所有数据都写到一个chunk里面，造成写入热点。  Shard key可以是单索引|字段或者联合索引字段：即必须要上索引 超过16MB的chunk被一分为二 一个collection的所有chunk首尾相连，构成整个表   按hash拆分chunk  Shard key必须是哈希索引字段：mongodb支持一种hash索引，会先把这个字段hash成一个整型，用这个整型来建立索引。因为可能出现hash碰撞，无法保证key的唯一性，hash索引的字段是不能建立唯一键的 Shard key经过hash函数打散，避免写热点 支持预分配chunk，提前分配较多chunk，可以避免运行时分裂影响写入     shard用法示例  为DB激活特性：sh.enableSharding( \u0026lsquo;my_db\u0026rsquo; ) 配置hash分片: sh.shardCollection(\u0026rdquo; my_db.my_collection\u0026rdquo;, { _id: \u0026ldquo;hashed\u0026rdquo; }, false, { numInitialChunks: 10240} )。让my_db的表my_collection根据_id做hash分片，预分配10240个chunk   注意：如果按非shard key查询，比如如果我们按x字段分片，即x是shard key，但是查询的时候我们通过uid查询，请求将被扇出给所有shard，性能会差很多     Mongod架构：默认采用WiredTiger高性能存储引擎，  架构  Client：客户端。客户端发送请求到查询层 MongoDB Query Language：查询解释层，解析请求 MongoDB Data Model：存储层的抽象，支持多种底层的存储引擎  WiredTiger：默认的，非常高性能 MMAPv1：早期使用的引擎 In Memory Encrypted 3rd Party Engine     基于journaling log宕机恢复(类比mysql的redo log)：写入东西的时候先去写log，将 \u0026ldquo;操作内容\u0026rdquo; 顺序追加成log文件，而数据会暂时修改到内存里面，它将定期把内存中的数据写入到磁盘，如果写入磁盘之前宕机了，就会通过log把数据恢复出来    ","date":"0001-01-01","permalink":"https://yuanyatianchi.github.io/post/it.db.mongodb/","tags":["it","db"],"title":"MongoDB"},{"content":"消息队列  MQ解决的问题：解决了上下游工序之间的“通信”问题；解决了上下游生产速度不一致的问题，起到了“通信”过程中“缓存”的作用 哪些问题适合使用消息队列解决：异步处理、流量控制、服务解耦  秒杀系统与MQ  如何设计一个秒杀系统？秒杀（高并发）系统需要解决的核心问题是，如何利用有限的服务器资源，尽可能多地处理短时间内的海量请求。  异步处理  秒杀步骤：处理一个秒杀请求包含很多步骤，风险控制、库存锁定、生成订单、短信通知、更新统计数据等  决定了能否秒杀成功的实际上只有风险控制、库存锁定，完成这 2 个步骤，就可以马上给用户返回响应，然后把请求的数据放入消息队列中（生产者），后续的其它操作则在其它服务上异步执行（消费者） 处理一个秒杀请求，从 5 个步骤减少为 2 个步骤，这样不仅响应速度更快，并且在秒杀期间，我们可以把大量的服务器资源用来处理秒杀请求。秒杀结束后再把资源用于处理后面的步骤，充分利用有限的服务器资源处理更多的秒杀请求。 **在这个场景中，消息队列被用于实现服务的异步处理。**这样做的好处是：可以更快地返回结果；减少等待，自然实现了步骤之间的并发，提升系统总体的性能      网关处理：注意实际生产环境中的秒杀系统实现更加复杂和多样，这里只是提供一个比较简单的方案。\n 网关如何处理 APP 的秒杀请求：网关在收到 APP 的秒杀请求后，直接给消息队列发消息（消息的内容，并不一定是 APP 请求的 Request，只要包含足够的字段即可，比如用户 ID、设备 ID、请求时间……请求ID、网关ID……）。如果发送消息失败，直接给 APP 返回秒杀失败结果；如果发送消息成功，线程就阻塞等待秒杀结果（有超时时间）。等待结束之后，去存放秒杀结果的 Map 中查询是否有返回的秒杀结果，如果有就构建 Response并给 APP 返回秒杀结果，如果没有则按秒杀失败处理。这个解决方案不是一个性能最优的方案，处理 APP 请求的线程需要同步等待秒杀结果，后面会讲如何使用异步方式来提升程序的性能。 网关如何接收从后端秒杀服务返回的秒杀结果：秒杀系统从MQ取到消息，秒杀结果中需要包含请求 ID，通过RPC请求或者其它方式将秒杀结果发送给对应网关节点，可以通过这个网关 ID 来找到对应的网关实例 网关如何给 APP 返回响应：网关收到后端服务的秒杀结果之后，用请求 ID 为 Key，把这个结果保存到秒杀结果的 Map 中，然后通知对应的处理 APP 请求的线程，结束等待。我刚刚说过，处理 APP 请求的线程，在结束等待之后，会去秒杀的结果 Map 中查询这个结果，然后再给 APP 返回响应    // Java 语言来举例\rpublic class RequestHandler {\r// ID 生成器\r@Inject\rprivate IdGenerator idGenerator;\r// 消息队列生产者\r@Inject\rprivate Producer producer;\r// 保存秒杀结果的 Map\r@Inject\rprivate Map\u0026lt;Long, Result\u0026gt; results;\r// 保存 mutex 的 Map\rprivate Map\u0026lt;Long, Object\u0026gt; mutexes = new ConcurrentHashMap\u0026lt;\u0026gt;();\r// 这个网关实例的 ID\r@Inject\rprivate long myId;\r@Inject\rprivate long timeout;\r// 在这里处理 APP 的秒杀请求\rpublic Response onRequest(Request request) {\r// 获取一个进程内唯一的 UUID 作为请求 id\rLong uuid = idGenerator.next();\rtry {\rMessage msg = composeMsg(request, uuid, myId);\r// 生成一个 mutex，用于等待和通知\rObject mutex = new Object();\rmutexes.put(uuid, mutex)\r// 发消息\rproducer.send(msg);\r// 等待后端处理\rsynchronized(mutex) {\rmutex.wait(timeout);\r}\r// 查询秒杀结果\rResult result = results.remove(uuid);\r// 检查秒杀结果并返回响应\rif(null != result \u0026amp;\u0026amp; result.success()){\rreturn Response.success();\r}\r} catch (Throwable ignored) {\r} finally {\rmutexes.remove(uuid);\r}\r// 返回秒杀失败\rreturn Response.fail();\r}\r// 在这里处理后端服务返回的秒杀结果\rpublic void onResult(Result result) {\rObject mutex = mutexes.get(result.uuid());\rif(null != mutex) { // 如果查询不到，说明已经超时了，丢弃 result 即可。\r// 登记秒杀结果\rresults.put(result.uuid(), result);\r// 唤醒处理 APP 请求的线程\rsynchronized(mutex) {\rmutex.notify();\r}\r}\r}\r}\r 流量控制   即使实现了异步处理，但还面临另一个问题，如何避免过多的请求压垮我们的秒杀系统？一个设计健壮的程序有自我保护的能力，即在海量请求下还能在自身能力范围内尽可能多地处理请求、拒绝处理不了的请求、并保证自身运行正常。但现实中很多程序并没有那么“健壮”，而直接拒绝请求返回错误对于用户来说也不是好的体验。因此，我们需要设计一套足够健壮的架构来将后端的服务保护起来。我们的设计思路是，使用消息队列隔离网关和后端服务，以达到流量控制和保护后端服务的目的。\n 加入消息队列后，整个秒杀流程变为：网关在收到请求后，将请求放入请求消息队列；后端服务从请求消息队列中获取 APP 请求，完成后续秒杀处理过程，然后返回结果。秒杀开始后，当短时间内大量的秒杀请求到达网关时，不会直接冲击到后端的秒杀服务，而是先堆积在消息队列中，后端服务按照自己的最大处理能力，从消息队列中消费请求进行处理。对于超时的请求可以直接丢弃，APP 将超时无响应的请求处理为秒杀失败即可。运维人员还可以随时增加秒杀服务的实例数量进行水平扩容，而不用对系统的其他部分做任何更改。 这种设计的优点是：能根据下游的处理能力自动调节流量，达到“削峰填谷”的作用。 但这样做同样是有代价的：增加了系统调用链环节，导致总体的响应时延变长；上下游系统都要将同步调用改为异步消息，增加了系统的复杂度。    那还有没有更简单一点儿的流量控制方法呢？如果能预估出秒杀服务的处理能力，就可以用消息队列实现一个令牌桶，更简单地进行流量控制。\n  **令牌桶（池）**控制流量的原理是：单位时间内只发放固定数量的令牌到令牌桶中，规定服务在处理请求之前必须先从令牌桶中拿出一个令牌，如果令牌桶中没有令牌，则拒绝请求。这样就保证单位时间内，能处理的请求不超过发放令牌的数量，起到了流量控制的作用。\n  实现的方式也很简单，不需要破坏原有的调用链，只要网关在处理 APP 请求时增加一个获取令牌的逻辑。令牌桶可以简单地用一个有固定容量的消息队列加一个“令牌发生器”来实现：令牌发生器按照预估的处理能力，匀速生产令牌并放入令牌队列（如果队列满了则丢弃令牌），网关在收到请求时去令牌队列消费一个令牌，获取到令牌则继续调用后端秒杀服务，如果获取不到令牌则直接返回秒杀失败。\n  以上是常用的使用消息队列两种进行流量控制的设计方法，你可以根据各自的优缺点和不同的适用场景进行合理选择。\n    服务解耦  消息队列的另外一个作用，就是实现系统应用之间的解耦。再举一个电商的例子来说明解耦的作用和必要性  订单是电商系统中比较核心的数据，当一个新订单创建时：支付系统需要发起支付流程；风控系统需要审核订单的合法性；客服系统需要给用户发短信告知用户；经营分析系统需要更新统计数据；……。 这些订单下游的系统都需要实时获得订单数据。随着业务不断发展，这些订单下游系统不断的增加，不断变化，并且每个系统可能只需要订单数据的一个子集，负责订单服务的开发团队不得不花费很大的精力，应对不断增加变化的下游系统，不停地修改调试订单系统与这些下游系统的接口。任何一个下游系统接口变更，都需要订单模块重新进行一次上线，对于一个电商的核心服务来说，这几乎是不可接受的。 所有的电商都选择用消息队列来解决类似的系统耦合过于紧密的问题。引入消息队列后，订单服务在订单变化时发送一条消息到消息队列的一个主题 Order 中，所有下游系统都订阅主题 Order，这样每个下游系统都可以获得一份实时完整的订单数据。无论增加、减少下游系统或是下游系统需求如何变化，订单服务都无需做任何更改，实现了订单服务与下游服务的解耦。    小结   消息队列的适用范围不仅仅局限于这些场景，还有包括：\n 作为发布 / 订阅系统实现一个微服务级系统间的观察者模式 连接流计算任务和数据 用于将消息广播给大量接收者  简单的说，我们在单体应用里面需要用队列解决的问题，在分布式系统中大多都可以用消息队列来解决。\n同时我们也要认识到，消息队列也有它自身的一些问题和局限性，包括：\n 引入消息队列带来的延迟问题； 增加了系统的复杂度； 可能产生数据不一致的问题。    消息 消息模型  每种MQ产品都有自己的一套消息模型，像队列（Queue）、主题（Topic）或是分区（Partition），在每个消息队列模型中都会涉及一些，含义还不太一样。国际组织尝试制定过消息相关的标准，比如早期的 JMS 和 AMQP。但让人无奈的是，标准的进化跟不上消息队列的演进速度，这些标准实际上已经被废弃了。好的架构不是设计出来的，而是演进出来的  队列  最初的消息队列，就是一个严格意义上的队列  队列是先进先出（FIFO, First-In-First-Out）的线性表（Linear List）。在具体应用中通常用链表或者数组来实现。队列只允许在后端（称为 rear）进行插入操作，在前端（称为 front）进行删除操作。在消息入队出队过程中，需要保证这些消息严格有序，早期的消息队列，就是按照“队列”的数据结构来设计的，生产者（Producer）发消息就是入队操作，消费者（Consumer）收消息就是出队也就是删除操作，服务端存放消息的容器自然就称为“队列” 一个队列的多个消费者之间为竞争关系。如果需要将一份消息数据分发给多个消费者，要求每个消费者都能收到全量的消息，例如，对于一份订单数据，风控系统、分析系统、支付系统等都需要接收消息。这个时候，单个队列就满足不了需求，一个可行的解决方式是，为每个消费者创建一个单独的队列，让生产者发送多份。同样的一份消息数据被复制到多个队列中会浪费资源，更重要的是，生产者必须知道有多少个消费者。为每个消费者单独发送一份消息，这实际上违背了消息队列“解耦”这个设计初衷。所以演化出了另外一种消息模型：“发布 - 订阅模型（Publish-Subscribe Pattern）”    发布订阅  在发布 - 订阅模型中，消息的发送方称为发布者（Publisher），消息的接收方称为订阅者（Subscriber），服务端存放消息的容器称为主题（Topic）。发布者将消息发送到主题中，订阅者在接收消息之前需要先“订阅主题”。“订阅”在这里既是一个动作，同时还可以认为是主题在消费时的一个逻辑副本，每份订阅中，订阅者都可以接收到主题的所有消息。 消息的发送方称为发布者（Publisher），消息的接收方称为订阅者（Subscriber），服务端存放消息的容器称为主题（Topic）。发布者将消息发送到主题中，订阅者在接收消息之前需要先“订阅主题”。“订阅”在这里既是一个动作，同时还可以认为是主题在消费时的一个逻辑副本，每份订阅中，订阅者都可以接收到主题的所有消息 仔细对比一下这两种模型，生产者就是发布者，消费者就是订阅者，队列就是主题，并没有本质的区别。**它们最大的区别其实就是，一份消息数据能不能被消费多次的问题。**如果只有一个订阅者，那它和队列模型就基本是一样的了。也就是说，发布 - 订阅模型在功能层面上是可以兼容队列模型的  RabbitMQ的消息模型  少数依然坚持使用队列模型的产品之一。那它是怎么解决多个消费者的问题呢？在 RabbitMQ 中，Exchange 位于生产者和队列之间，生产者并不关心将消息发送给哪个队列，而是将消息发送给 Exchange，由 Exchange 上配置的策略来决定将消息投递到哪些队列中。 同一份消息如果需要被多个消费者来消费，需要配置 Exchange 将消息发送到多个队列，每个队列中都存放一份完整的消息数据，可以为一个消费者提供消费服务。这也可以变相地实现新发布 - 订阅模型中，“一份消息数据可以被多个订阅者来多次消费”这样的功能。  RocketMQ 的消息模型  RocketMQ 使用的消息模型是标准的发布 - 订阅模型 RocketMQ 也有队列（Queue）这个概念 几乎所有的消息队列产品都使用一种非常朴素的“请求 - 确认”机制，确保消息不会在传递过程中由于网络或服务器故障丢失。具体的做法也非常简单。  在生产端，生产者发送消息给服务端（Broker），服务端在收到消息并将消息写入主题或者队列中后，会给生产者发送确认的响应。如果生产者没有收到服务端的确认或者收到失败的响应，则会重新发送消息。在消费端，消费者在收到消息并完成自己的消费业务逻辑（比如，将数据保存到数据库中）后，也会给服务端发送消费成功的确认，服务端只有收到消费确认后，才认为一条消息被成功消费，否则它会给消费者重新发送这条消息，直到收到对应的消费成功确认。 确认机制很好地保证了消息传递的可靠性，但引入这个机制在消费端带来了一个不小的问题：为了确保消息的有序性，在某一条消息被成功消费之前，下一条消息是不能被消费的，否则就会出现消息空洞，违背有序性原则。即每个主题在任意时刻，至多只能有一个消费者实例在进行消费，那就没法通过水平扩展消费者的数量来提升消费端总体的消费性能。为了解决这个问题，RocketMQ 在主题下面增加了队列的概念。   **每个主题包含多个队列，通过多个队列来实现多实例并行生产和消费。**需要注意的是，RocketMQ 只在队列上保证消息的有序性，主题层面是无法保证消息的严格顺序的  RocketMQ 中，订阅者的概念是通过消费组（Consumer Group）来体现的。每个消费组都消费主题中一份完整的消息，不同消费组之间消费进度彼此不受影响，也就是说，一条消息被 Consumer Group1 消费过，也会再给 Consumer Group2 消费。消费组中包含多个消费者，同一个组内的消费者是竞争消费的关系，每个消费者负责消费组内的一部分消息。如果一条消息被消费者 Consumer1 消费了，那同组的其他消费者就不会再收到这条消息。 在 Topic 的消费过程中，由于消息需要被不同的组进行多次消费，所以消费完的消息并不会立即被删除，这就需要 RocketMQ 为每个消费组在每个队列上维护一个消费位置（Consumer Offset），这个位置之前的消息都被消费过，之后的消息都没有被消费过，每成功消费一条消息，消费位置就加一。这个消费位置是非常重要的概念，我们在使用消息队列的时候，丢消息的原因大多是由于消费位置处理不当导致的。    ![](./img/img-RocketMQ 的消息模型.jpg)\nKafka 的消息模型  Kafka 的消息模型与RocketMQ是完全一致，包括确认机制。唯一的区别是，在 Kafka 中，队列这个概念的名称不一样，Kafka 中对应的名称是“分区（Partition）”，含义和功能是没有任何区别的。 消息模型总结：假设有一个主题 MyTopic，为主题创建 5 个队列，分布到 2 个 Broker 中。 生产者、Queue：无对应。假设有3个生产者实例Produer0、Produer1、Producer2。每个生产者可以在 5 个队列中轮询发送，或者随机选一个队列发送，或者只往某个队列发送，皆可。比如 Producer0 要发 5 条消息，可以都发到队列 Q0 里面，也可以 5 个队列每个队列发一条。 Queue、消费组、消费者：每个消费组就是一份订阅，它要消费主题 MyTopic 下所有队列的全部消息。每个消费组都会去消费所配置的每个Queue中的消息，消费过的消息仍然被维护在Queue中，只是还需要各自维护各消费组在各Queue上的消费位置（offsite），每个消费组内部维护自己的一组消费位置，每个队列对应一个消费位置。消费位置在服务端保存，并且，消费位置和消费者是没有关系的。每个消费位置一般就是一个整数，记录这个消费组中，这个队列消费到哪个位置了，这个位置之前的消息都成功消费了，之后的消息都没有消费或者正在消费。。即多个消费组在消费同一个主题时，消费组之间是互不影响的。在消费组内部，一个消费组中可以包含多个消费者的实例，由于消费确认机制的限制，这里面有一个原则是，每个队列只能被一个消费组中的一个消费者实例占用，至于如何分配则是由很多策略的，总之保证每个队列分配该消费组的一个消费者即可，比如说消费组 G1，包含了 2 个消费者 C0 和 C1，可以让消费者 C0 消费 Q0，Q1 和 Q2，C1 消费 Q3 和 Q4，如果 C0 宕机了，会触发重新分配，这时候 C1 同时消费全部 5 个队列。    小结  队列模型和发布订阅模型其实并没有本质上的区别，都可以通过一些扩展或者变化来互相替代。 需要注意的是，消息模型和相关的概念是业务层面的模型，不等于就是实现层面的模型。比如说 MySQL 和 Hbase 同样是支持 SQL 的数据库，它们的业务模型中，存放数据的单元都是“表”，但是在实现层面，没有哪个数据库是以二维表的方式去存储数据的，MySQL 使用 B+ 树来存储数据，而 HBase 使用的是 KV 的结构来存储。际上这Kafka 和 RocketMQ的实现是完全不同的  思考  如果不要求严格顺序，如何实现单个队列的并行消费？  关于这个问题，有很多的实现方式，在 JMQ（京东自研的消息队列产品）中，它实现的思路是这样的。比如说，队列中当前有 10 条消息，对应的编号是 0-9，当前的消费位置是 5。同时来了三个消费者来拉消息，把编号为 5、6、7 的消息分别给三个消费者，每人一条。过了一段时间，三个消费成功的响应都回来了，这时候就可以把消费位置更新为 8 了，这样就实现并行消费。这是理想的情况。还有可能编号为 6、7 的消息响应回来了，编号 5 的消息响应一直回不来，怎么办？这个位置 5 就是一个消息空洞。为了避免位置 5 把这个队列卡住，可以先把消费位置 5 这条消息，复制到一个特殊重试队列中，然后依然把消费位置更新为 8，继续消费。再有消费者来拉消息的时候，优先把重试队列中的那条消息给消费者就可以了。这是并行消费的一种实现方式。需要注意的是，并行消费开销还是很大的，不应该作为一个常规的，提升消费并发的手段，如果消费慢需要增加消费者的并发数，还是需要扩容队列数   如何保证消息的严格顺序  主题层面是无法保证严格顺序的，只有在队列上才能保证消息的严格顺序。如果说，你的业务必须要求全局严格顺序，就只能把消息队列数配置成 1，生产者和消费者也只能是一个实例，这样才能保证全局严格顺序。大部分情况下，我们并不需要全局严格顺序，只要保证局部有序就可以满足要求了。比如，在传递账户流水记录的时候，只要保证每个账户的流水有序就可以了，不同账户之间的流水记录是不需要保证顺序的。如果需要保证局部严格顺序，可以这样来实现。在发送端，我们使用账户 ID 作为 Key，采用一致性哈希算法计算出队列编号，指定队列来发送消息。一致性哈希算法可以保证，相同 Key 的消息总是发送到同一个队列上，这样可以保证相同 Key 的消息是严格有序的。如果不考虑队列扩容，也可以用队列数量取模的简单方法来计算队列编号。    消息和事务  MQ也需要事务，很多场景下，我们“发消息”这个过程，目的往往是通知另外一个系统或者模块去更新数据，消息队列中的“事务”，主要解决的是消息生产者和消息消费者的数据一致性问题。 电商举例，购物时，先把商品加到购物车里，然后几件商品一起下单，最后支付，完成购物流程。这个过程中有一个需要用到消息队列的步骤，订单系统创建订单后，发消息给购物车系统，将已下单的商品从购物车中删除。因为从购物车删除已下单商品这个步骤，并不是用户下单支付这个主要流程中必需的步骤，使用消息队列来异步清理购物车是更加合理的设计。 订单系统来说，它创建订单的过程中实际上执行了 2 个步骤的操作：  在订单库中插入一条订单数据，创建订单； 发消息给消息队列，消息的内容就是刚刚创建的订单。   购物车系统订阅相应的主题，接收订单创建的消息，然后清理购物车，在购物车中删除订单中的商品。在分布式系统中，任何一个步骤都有可能失败，如果不做任何处理，那就有可能出现订单数据与购物车数据不一致的情况，比如说：  创建了订单，没有清理购物车； 订单没创建成功，购物车里面的商品却被清掉了。   需要解决的问题可以总结为：在上述任意步骤都有可能失败的情况下，还要保证订单库和购物车库这两个库的数据一致性。对于购物车系统收到订单创建成功消息清理购物车这个操作来说，失败的处理比较简单，只要成功执行购物车清理后再提交消费确认即可，如果失败，由于没有提交消费确认，消息队列会自动重试。问题的关键点集中在订单系统，创建订单和发送消息这两个步骤要么都操作成功，要么都操作失败，不允许一个成功而另一个失败的情况出现。这就是事务需要解决的问题。  事务  什么是事务  如果我们需要对若干数据进行更新操作，为了保证这些数据的完整性和一致性，我们希望这些更新操作**要么都成功，要么都失败。**至于更新的数据，不只局限于数据库中的数据，可以是磁盘上的一个文件，也可以是远端的一个服务，或者以其他形式存储的数据。 一个严格意义的事务实现，应该具有 4 个属性：原子性、一致性、隔离性、持久性。这四个属性通常称为 ACID 特性  原子性，是指一个事务操作不可分割，要么成功，要么失败，不能有一半成功一半失败的情况。 一致性，是指这些数据在事务执行完成这个时间点之前，读到的一定是更新前的数据，之后读到的一定是更新后的数据，不应该存在一个时刻，让用户读到更新过程中的数据。 隔离性，是指一个事务的执行不能被其他事务干扰。即一个事务内部的操作及使用的数据对正在进行的其他事务是隔离的，并发执行的各个事务之间不能互相干扰，这个有点儿像我们打网游中的副本，我们在副本中打的怪和掉的装备，与其他副本没有任何关联也不会互相影响。 持久性，是指一个事务一旦完成提交，后续的其他操作和故障都不会对事务的结果产生任何影响。   大部分传统的单体关系型数据库都完整的实现了 ACID，但是，对于分布式系统来说，严格的实现 ACID 这四个特性几乎是不可能的，或者说实现的代价太大，大到我们无法接受。分布式事务就是要在分布式系统中的实现事务。在分布式系统中，在保证可用性和不严重牺牲性能的前提下，光是要实现数据的一致性就已经非常困难了，所以出现了很多“残血版”的一致性，比如顺序一致性、最终一致性等等。 显然实现严格的分布式事务是更加不可能完成的任务。所以，目前大家所说的分布式事务，更多情况下，是在分布式系统中事务的不完整实现。在不同的应用场景中，有不同的实现，目的都是通过一些妥协来解决实际问题。 在实际应用中，比较常见的分布式事务实现有 2PC、TCC和事务消息。每一种实现都有其特定的使用场景，也有各自的问题，都不是完美的解决方案。  2PC（Two-phase Commit，二阶段提交）： TCC（Try-Confirm-Cancel）：   事务消息适用的场景主要是那些需要异步更新数据，并且对数据实时性要求不太高的场景。比如我们在开始时提到的那个例子，在创建订单后，如果出现短暂的几秒，购物车里的商品没有被及时清空，也不是完全不可接受的，只要最终购物车的数据和订单数据保持一致就可以了。   消息队列是如何实现分布式事务  事务消息需要消息队列提供相应的功能才能实现，Kafka 和 RocketMQ 都提供了事务相关功能。 首先，订单系统在消息队列上开启一个事务。然后订单系统给消息服务器发送一个“半消息”，这个半消息不是说消息内容不完整，它包含的内容就是完整的消息内容，半消息和普通消息的唯一区别是，在事务提交之前，对于消费者来说，这个消息是不可见的。 半消息发送成功后，订单系统就可以执行本地事务了，在订单库中创建一条订单记录，并提交订单库的数据库事务。然后根据本地事务的执行结果决定提交或者回滚事务消息。如果订单创建成功，那就提交事务消息，购物车系统就可以消费到这条消息继续后续的流程。如果订单创建失败，那就回滚事务消息，购物车系统就不会收到这条消息。这样就基本实现了“要么都成功，要么都失败”的一致性要求。 有一个问题是没有解决的。如果在第四步提交事务消息时失败了怎么办  Kafka 的解决方案比较简单粗暴，直接抛出异常，让用户自行处理。我们可以在业务代码中反复重试提交，直到提交成功，或者删除之前创建的订单进行补偿。 在 RocketMQ 中的事务实现中，增加了事务反查的机制来解决事务消息提交失败的问题。  如果 Producer 也就是订单系统，在提交或者回滚事务消息时发生网络异常，RocketMQ 的 Broker 没有收到提交或者回滚的请求，Broker 会定期去 Producer 上反查这个事务对应的本地事务的状态，然后根据反查结果决定提交或者回滚这个事务。为了支撑这个事务反查机制，我们的业务代码需要实现一个反查本地事务状态的接口，告知 RocketMQ 本地事务是成功还是失败。 在我们这个例子中，反查本地事务的逻辑也很简单，我们只要根据消息中的订单 ID，在订单库中查询这个订单是否存在即可，如果订单存在则返回成功，否则返回失败。RocketMQ 会自动根据事务反查的结果提交或者回滚事务消息。 这个反查本地事务的实现，并不依赖消息的发送方，也就是订单服务的某个实例节点上的任何数据。这种情况下，即使是发送事务消息的那个订单服务节点宕机了，RocketMQ 依然可以通过其他订单服务的节点来执行反查，确保事务的完整性。        ![](./img/img-RocketMQ 的事务反查机制jpg.jpg)\n小结  RocketMQ 的事务反查机制，这种机制通过定期反查事务状态，来补偿提交事务消息可能出现的通信失败。在 Kafka 的事务功能中，并没有类似的反查机制，需要用户自行去解决这个问题。但是，这不代表 RocketMQ 的事务功能比 Kafka 更好，只能说在我们这个例子的场景下，更适合使用 RocketMQ。Kafka 对于事务的定义、实现和适用场景，和 RocketMQ 有比较大的差异，后面的课程中，我们会专门讲到 Kafka 的事务的实现原理。  思考  建议你最好能通过写代码的方式，把我们这节课中的订单购物车的例子，用 RocketMQ 完整地实现出来。然后再思考一下，RocketMQ 的这种事务消息是不是完整地实现了事务的 ACID 四个特性？如果不是，哪些特性没有实现？  消息丢失  对于大部分业务系统来说，丢消息意味着数据丢失，是完全无法接受的。 现在主流的消息队列产品都提供了非常完善的消息可靠性保证机制，完全可以做到在消息传递过程中，即使发生网络中断或者硬件故障，也能确保消息的可靠传递，不丢消息。 绝大部分丢消息的原因都是由于开发者不熟悉消息队列，没有正确使用和配置消息队列导致的。虽然不同的消息队列提供的 API 不一样，相关的配置项也不同，但是在保证消息可靠传递这块儿，它们的实现原理是一样的。熟知原理以后，无论你使用任何一种消息队列，再简单看一下它的 API 和相关配置项，就能很快知道该如何配置消息队列，写出可靠的代码，避免消息丢失  检测消息丢失的方法  用消息队列最尴尬的情况不是丢消息，而是消息丢了还不知道 分布式链路追踪系统可以很方便地追踪每一条消息 如果没有这样的追踪系统，**可以利用消息队列的有序性来验证是否有消息丢失。**原理非常简单，在 Producer 端，我们给每个发出的消息附加一个连续递增的序号，然后在 Consumer 端来检查这个序号的连续性。  如果没有消息丢失，Consumer 收到消息的序号必然是连续递增的，或者说收到的消息，其中的序号必然是上一条消息的序号 +1。如果检测到序号不连续，那就是丢消息了。还可以通过缺失的序号来确定丢失的是哪条消息，方便进一步排查原因。 大多数消息队列的客户端都支持拦截器机制，你可以利用这个拦截器机制，在 Producer 发送消息之前的拦截器中将序号注入到消息中，在 Consumer 收到消息的拦截器中检测序号的连续性，这样实现的好处是消息检测的代码不会侵入到你的业务代码中，待你的系统稳定后，也方便将这部分检测的逻辑关闭或者删除。   如果是在一个分布式系统中实现这个检测方法，有几个问题需要注意。  首先，像 Kafka 和 RocketMQ 这样的消息队列，它是不保证在 Topic 上的严格顺序的，只能保证分区上的消息是有序的，所以我们在发消息的时候必须要指定分区，并且，在每个分区单独检测消息序号的连续性。 如果你的系统中 Producer 是多实例的，由于并不好协调多个 Producer 之间的发送顺序，所以也需要每个 Producer 分别生成各自的消息序号，并且需要附加上 Producer 的标识，在 Consumer 端按照每个 Producer 分别来检测序号的连续性。 Consumer 实例的数量最好和分区数量一致，做到 Consumer 和分区一一对应，这样会比较方便地在 Consumer 内检测消息序号的连续性。    确保消息可靠传递   生产阶段: 在这个阶段，从消息在 Producer 创建出来，经过网络传输发送到 Broker 端。\n  消息队列通过最常用的请求确认机制，来保证消息的可靠传递：当你的代码调用发消息方法时，消息队列的客户端会把消息发送到 Broker，Broker 收到消息后，会给客户端返回一个确认响应，表明消息已经收到了。客户端收到响应后，完成了一次正常消息的发送。\n  只要 Producer 收到了 Broker 的确认响应，就可以保证消息在生产阶段不会丢失。有些消息队列在长时间没收到发送确认响应后，会自动重试，如果重试再失败，就会以返回值或者异常的方式告知用户。\n  **你在编写发送消息代码时，需要注意，正确处理返回值或者捕获异常，就可以保证这个阶段的消息不会丢失。**以 Kafka 为例，我们看一下如何可靠地发送消息：\n  同步发送时，只要注意捕获异常即可。\ntry {\rRecordMetadata metadata = producer.send(record).get();\rSystem.out.println(\u0026quot; 消息发送成功。\u0026quot;);\r} catch (Throwable e) {\rSystem.out.println(\u0026quot; 消息发送失败！\u0026quot;);\rSystem.out.println(e);\r}\r   异步发送时，则需要在回调方法里进行检查。这个地方是需要特别注意的，很多丢消息的原因就是，我们使用了异步发送，却没有在回调中检查发送结果。\nproducer.send(record, (metadata, exception) -\u0026gt; {\rif (metadata != null) {\rSystem.out.println(\u0026quot; 消息发送成功。\u0026quot;);\r} else {\rSystem.out.println(\u0026quot; 消息发送失败！\u0026quot;);\rSystem.out.println(exception);\r}\r});\r     存储阶段: 在这个阶段，消息在 Broker 端存储，如果是集群，消息会在这个阶段被复制到其他的副本上。\n 在存储阶段正常情况下，只要 Broker 在正常运行，就不会出现丢失消息的问题，但是如果 Broker 出现了故障，比如进程死掉了或者服务器宕机了，还是可能会丢失消息的。 如果对消息的可靠性要求非常高，可以通过配置 Broker 参数来避免因为宕机丢消息。 对于单个节点的 Broker，需要配置 Broker 参数，在收到消息后，将消息写入磁盘后再给 Producer 返回确认响应，这样即使发生宕机，由于消息已经被写入磁盘，就不会丢失消息，恢复后还可以继续消费。例如，在 RocketMQ 中，需要将刷盘方式 flushDiskType 配置为 SYNC_FLUSH 同步刷盘。 如果是 Broker 是由多个节点组成的集群，需要将 Broker 集群配置成：至少将消息发送到 2 个以上的节点，再给客户端回复发送确认响应。这样当某个 Broker 宕机时，其他的 Broker 可以替代宕机的 Broker，也不会发生消息丢失。后面我会专门安排一节课，来讲解在集群模式下，消息队列是如何通过消息复制来确保消息的可靠性的。    消费阶段: 在这个阶段，Consumer 从 Broker 上拉取消息，经过网络传输发送到 Consumer 上。\n  消费阶段采用和生产阶段类似的确认机制来保证消息的可靠传递，客户端从 Broker 拉取消息后，执行用户的消费业务逻辑，成功后，才会给 Broker 发送消费确认响应。如果 Broker 没有收到消费确认响应，下次拉消息的时候还会返回同一条消息，确保消息不会在网络传输过程中丢失，也不会因为客户端在执行消费逻辑中出错导致丢失。\n  你在编写消费代码时需要注意的是，不要在收到消息后就立即发送消费确认，而是应该在执行完所有消费业务逻辑之后，再发送消费确认。\n  同样，我们以用 Python 语言消费 RabbitMQ 消息为例，来看一下如何实现一段可靠的消费代码：\ndef callback(ch, method, properties, body):\rprint(\u0026quot; [x] 收到消息 %r\u0026quot; % body)\r# 在这儿处理收到的消息\rdatabase.save(body)\rprint(\u0026quot; [x] 消费完成 \u0026quot;)\r# 完成消费业务逻辑后发送消费确认响应\rch.basic_ack(delivery_tag = method.delivery_tag)\rchannel.basic_consume(queue='hello', on_message_callback=callback)\r   你可以看到，在消费的回调方法 callback 中，正确的顺序是，先是把消息保存到数据库中，然后再发送消费确认响应。这样如果保存消息到数据库失败了，就不会执行消费确认的代码，下次拉到的还是这条消息，直到消费成功。\n    小结  在理解了这几个阶段的原理后，如果再出现丢消息的情况，应该可以通过在代码中加一些日志的方式，很快定位到是哪个阶段出了问题，然后再进一步深入分析，快速找到问题原因。  思考  如果消息在网络传输过程中发送错误，由于发送方收不到确认，会通过重发来保证消息不丢失。但是，如果确认响应在网络传输时丢失，也会导致重发消息。也就是说，**无论是 Broker 还是 Consumer 都是有可能收到重复消息的，**那我们在编写消费代码时，就需要考虑这种情况，你可以想一下，在消费消息的代码中，该如何处理这种重复消息，才不会影响业务逻辑的正确性？  消息重复  消息传递过程中，如果出现传递失败的情况，发送方会执行重试，重试的过程中就有可能会产生重复的消息。对使用消息队列的业务系统来说，如果没有对重复消息进行处理，就有可能会导致系统的数据出现错误。比如说，一个消费订单消息，统计下单金额的微服务，如果没有正确处理重复消息，那就会出现重复统计，导致统计结果错误。  消息重复的情况必然存在  在 MQTT 协议中，给出了三种传递消息时能够提供的服务质量标准，这三种服务质量从低到高依次是：  At most once: 至多一次。消息在传递时，最多会被送达一次。换一个说法就是，没什么消息可靠性保证，允许丢消息。一般都是一些对消息可靠性要求不太高的监控场景使用，比如每分钟上报一次机房温度数据，可以接受数据少量丢失。 At least once: 至少一次。消息在传递时，至少会被送达一次。也就是说，不允许丢消息，但是允许有少量重复消息出现。 Exactly once：恰好一次。消息在传递时，只会被送达一次，不允许丢失也不允许重复，这个是最高的等级。   这个服务质量标准不仅适用于 MQTT，对所有的消息队列都是适用的。我们现在常用的绝大部分消息队列提供的服务质量都是 At least once，包括 RocketMQ、RabbitMQ 和 Kafka 都是这样。也就是说，消息队列很难保证消息不重复。 说到这儿我知道肯定有的同学会反驳我：“你说的不对，我看过 Kafka 的文档，Kafka 是支持 Exactly once 的。”我在这里跟这些同学解释一下，你说的没错，Kafka 的确是支持 Exactly once，但是我讲的也没有问题，为什么呢？ Kafka 支持的“Exactly once”和我们刚刚提到的消息传递的服务质量标准“Exactly once”是不一样的，它是 Kafka 提供的另外一个特性，Kafka 中支持的事务也和我们通常意义理解的事务有一定的差异。在 Kafka 中，事务和 Excactly once 主要是为了配合流计算使用的特性，我们在专栏“进阶篇”这个模块中，会有专门的一节课来讲 Kafka 的事务和它支持的 Exactly once 特性。 稍微说一些题外话，Kafka 的团队是一个非常善于包装和营销的团队，你看他们很巧妙地用了两个所有人都非常熟悉的概念“事务”和“Exactly once”来包装它的新的特性，实际上它实现的这个事务和 Exactly once 并不是我们通常理解的那两个特性，但是你深入了解 Kafka 的事务和 Exactly once 后，会发现其实它这个特性虽然和我们通常的理解不一样，但确实和事务、Exactly once 有一定关系。这一点上，我们都要学习 Kafka 团队。一个优秀的开发团队，不仅要能写代码，更要能写文档，能写 Slide（PPT），还要能讲，会分享。对于每个程序员来说，也是一样的。 我们把话题收回来，继续来说重复消息的问题。既然消息队列无法保证消息不重复，就需要我们的消费代码能够接受“消息是可能会重复的”这一现状，然后，通过一些方法来消除重复消息对业务的影响。  幂等性解决消息重复问题   幂等（Idempotence） 本来是一个数学上的概念，它是这样定义的：\n 如果一个函数 f(x) 满足：f(f(x)) = f(x)，则函数 f(x) 满足幂等性。\n 这个概念被拓展到计算机领域，被用来描述一个操作、方法或者服务。一个幂等操作的特点是，其任意多次执行所产生的影响均与一次执行的影响相同。\n  一个幂等的方法，使用同样的参数，对它进行多次调用和一次调用，对系统产生的影响是一样的。所以，对于幂等的方法，不用担心重复执行会对系统造成任何改变。比如，在不考虑并发的情况下，“将账户 X 的余额设置为 100 元”，执行一次后对系统的影响是，账户 X 的余额变成了 100 元，只要提供的参数 100 元不变，那即使再执行多少次，账户 X 的余额始终都是 100 元，不会变化，这个操作就是一个幂等的操作；而“将账户 X 的余额加 100 元”，这个操作它就不是幂等的，每执行一次，账户余额就会增加 100 元，执行多次和执行一次对系统的影响（也就是账户的余额）是不一样的。\n  一般解决重复消息的办法是，在消费端，让我们消费消息的操作具备幂等性。如果我们系统消费消息的业务逻辑具备幂等性，那就不用担心消息重复的问题了，因为同一条消息，消费一次和消费多次对系统的影响是完全一样的。也就可以认为，消费多次等于消费一次。从对系统的影响结果来说：At least once + 幂等消费 = Exactly once。\n  那么如何实现幂等操作呢？最好的方式就是，**从业务逻辑设计上入手，将消费的业务逻辑设计成具备幂等性的操作。**但是，不是所有的业务都能设计成天然幂等的，这里就需要一些方法和技巧来实现幂等。\n  利用数据库的唯一约束实现幂等  对于每个转账单每个账户只可以执行一次变更操作，在分布式系统中，这个限制实现的方法非常多，最简单的是我们在数据库中建一张转账流水表，这个表有三个字段：转账单 ID、账户 ID 和变更金额，然后给转账单 ID 和账户 ID 这两个字段联合起来创建一个唯一约束，这样对于相同的转账单 ID 和账户 ID，表里至多只能存在一条记录。 这样，我们消费消息的逻辑可以变为：“在转账流水表中增加一条转账记录，然后再根据转账记录，异步操作更新用户余额即可。”在转账流水表增加一条转账记录这个操作中，由于我们在这个表中预先定义了“账户 ID 转账单 ID”的唯一约束，对于同一个转账单同一个账户只能插入一条记录，后续重复的插入操作都会失败，这样就实现了一个幂等的操作。我们只要写一个 SQL，正确地实现它就可以了。 基于这个思路，不光是可以使用关系型数据库，只要是支持类似“INSERT IF NOT EXIST”语义的存储类系统都可以用于实现幂等，比如，你可以用 Redis 的 SETNX 命令来替代数据库中的唯一约束，来实现幂等消费。  为更新的数据设置前置条件  另外一种实现幂等的思路是，给数据变更设置一个前置条件，如果满足条件就更新数据，否则拒绝更新数据，在更新数据的时候，同时变更前置条件中需要判断的数据。这样，重复执行这个操作时，由于第一次更新数据的时候已经变更了前置条件中需要判断的数据，不满足前置条件，则不会重复执行更新数据操作。 比如，刚刚我们说过，“将账户 X 的余额增加 100 元”这个操作并不满足幂等性，我们可以把这个操作加上一个前置条件，变为：“如果账户 X 当前的余额为 500 元，将余额加 100 元”，这个操作就具备了幂等性。对应到消息队列中的使用时，可以在发消息时在消息体中带上当前的余额，在消费的时候进行判断数据库中，当前余额是否与消息中的余额相等，只有相等才执行变更操作。 但是，如果我们要更新的数据不是数值，或者我们要做一个比较复杂的更新操作怎么办？用什么作为前置判断条件呢？更加通用的方法是，给你的数据增加一个版本号属性，每次更数据前，比较当前数据的版本号是否和消息中的版本号一致，如果不一致就拒绝更新数据，更新数据的同时将版本号 +1，一样可以实现幂等更新。  记录并检查操作  如果上面提到的两种实现幂等方法都不能适用于你的场景，我们还有一种通用性最强，适用范围最广的实现幂等性方法：记录并检查操作，也称为“Token 机制或者 GUID（全局唯一 ID）机制”，实现的思路特别简单：在执行数据更新操作之前，先检查一下是否执行过这个更新操作。 具体的实现方法是，在发送消息时，给每条消息指定一个全局唯一的 ID，消费时，先根据这个 ID 检查这条消息是否有被消费过，如果没有消费过，才更新数据，然后将消费状态置为已消费。 原理和实现是不是很简单？其实一点儿都不简单，在分布式系统中，这个方法其实是非常难实现的。首先，给每个消息指定一个全局唯一的 ID 就是一件不那么简单的事儿，方法有很多，但都不太好同时满足简单、高可用和高性能，或多或少都要有些牺牲。更加麻烦的是，在“检查消费状态，然后更新数据并且设置消费状态”中，三个操作必须作为一组操作保证原子性，才能真正实现幂等，否则就会出现 Bug。 比如说，对于同一条消息：“全局 ID 为 8，操作为：给 ID 为 666 账户增加 100 元”，有可能出现这样的情况：  t0 时刻：Consumer A 收到条消息，检查消息执行状态，发现消息未处理过，开始执行“账户增加 100 元”； t1 时刻：Consumer B 收到条消息，检查消息执行状态，发现消息未处理过，因为这个时刻，Consumer A 还未来得及更新消息执行状态。   这样就会导致账户被错误地增加了两次 100 元，这是一个在分布式系统中非常容易犯的错误，一定要引以为戒。 对于这个问题，当然我们可以用事务来实现，也可以用锁来实现，但是在分布式系统中，无论是分布式事务还是分布式锁都是比较难解决问题。  小结  主要介绍了通过幂等消费来解决消息重复的问题，然后我重点讲了几种实现幂等操作的方法，你可以利用数据库的约束来防止重复更新数据，也可以为数据更新设置一次性的前置条件，来防止重复消息，如果这两种方法都不适用于你的场景，还可以用“记录并检查操作”的方式来保证幂等，这种方法适用范围最广，但是实现难度和复杂度也比较高，一般不推荐使用。 这些实现幂等的方法，不仅可以用于解决重复消息的问题，也同样适用于，在其他场景中来解决重复请求或者重复调用的问题。比如，我们可以将 HTTP 服务设计成幂等的，解决前端或者 APP 重复提交表单数据的问题；也可以将一个微服务设计成幂等的，解决 RPC 框架自动重试导致的重复调用问题。这些方法都是通用的，希望你能做到触类旁通，举一反三。  思考  为什么大部分消息队列都选择只提供 At least once 的服务质量，而不是级别更高的 Exactly once 呢？  消息积压  据我了解，在使用消息队列遇到的问题中，消息积压这个问题，应该是最常遇到的问题了，并且，这个问题还不太好解决。 我们都知道，消息积压的直接原因，一定是系统中的某个部分出现了性能问题，来不及处理上游发送的消息，才会导致消息积压。 所以，我们先来分析下，在使用消息队列时，如何来优化代码的性能，避免出现消息积压。然后再来看看，如果你的线上系统出现了消息积压，该如何进行紧急处理，最大程度地避免消息积压对业务的影响。  优化性能来避免消息积压  在使用消息队列的系统中，对于性能的优化，主要体现在生产者和消费者这一收一发两部分的业务逻辑中 对于消息队列本身的性能，你作为使用者，不需要太关注。对于绝大多数使用消息队列的业务来说，消息队列本身的处理能力要远大于业务系统的处理能力。主流消息队列的单个节点，消息收发的性能可以达到每秒钟处理几万至几十万条消息的水平，还可以通过水平扩展 Broker 的实例数成倍地提升处理能力。 而一般的业务系统需要处理的业务逻辑远比消息队列要复杂，单个节点每秒钟可以处理几百到几千次请求，已经可以算是性能非常好的了。所以，对于消息队列的性能优化，我们更关注的是，在消息的收发两端，我们的业务代码怎么和消息队列配合，达到一个最佳的性能。  发送端性能优化  发送端业务代码的处理性能，实际上和消息队列的关系不大，因为一般发送端都是先执行自己的业务逻辑，最后再发送消息。如果说，你的代码发送消息的性能上不去，你需要优先检查一下，是不是发消息之前的业务逻辑耗时太多导致的。 对于发送消息的业务逻辑，只需要注意设置合适的并发和批量大小，就可以达到很好的发送性能。为什么这么说呢？ Producer 发送消息的过程，Producer 发消息给 Broker，Broker 收到消息后返回确认响应，这是一次完整的交互。假设这一次交互的平均时延是 1ms，我们把这 1ms 的时间分解开，它包括了下面这些步骤的耗时：  发送端准备数据、序列化消息、构造请求等逻辑的时间，也就是发送端在发送网络请求之前的耗时； 发送消息和返回响应在网络传输中的耗时； Broker 处理消息的时延。   如果是单线程发送，每次只发送 1 条消息，那么每秒只能发送 1000ms / 1ms * 1 条 /ms = 1000 条 消息，这种情况下并不能发挥出消息队列的全部实力。 无论是增加每次发送消息的批量大小，还是增加并发，都能成倍地提升发送性能。至于到底是选择批量发送还是增加并发，主要取决于发送端程序的业务性质。简单来说，只要能够满足你的性能要求，怎么实现方便就怎么实现。 比如说，你的消息发送端是一个微服务，主要接受 RPC 请求处理在线业务。很自然的，微服务在处理每次请求的时候，就在当前线程直接发送消息就可以了，因为所有 RPC 框架都是多线程支持多并发的，自然也就实现了并行发送消息。并且在线业务比较在意的是请求响应时延，选择批量发送必然会影响 RPC 服务的时延。这种情况，比较明智的方式就是通过并发来提升发送性能。 如果你的系统是一个离线分析系统，离线系统在性能上的需求是什么呢？它不关心时延，更注重整个系统的吞吐量。发送端的数据都是来自于数据库，这种情况就更适合批量发送，你可以批量从数据库读取数据，然后批量来发送消息，同样用少量的并发就可以获得非常高的吞吐量。  消费端性能优化  使用消息队列的时候，大部分的性能问题都出现在消费端，如果消费的速度跟不上发送端生产消息的速度，就会造成消息积压。如果这种性能倒挂的问题只是暂时的，那问题不大，只要消费端的性能恢复之后，超过发送端的性能，那积压的消息是可以逐渐被消化掉的。 要是消费速度一直比生产速度慢，时间长了，整个系统就会出现问题，要么，消息队列的存储被填满无法提供服务，要么消息丢失，这对于整个系统来说都是严重故障。 所以，我们在设计系统的时候，一定要保证消费端的消费性能要高于生产端的发送性能，这样的系统才能健康的持续运行。 消费端的性能优化除了优化消费业务逻辑以外，也可以通过水平扩容，增加消费端的并发数来提升总体的消费性能。特别需要注意的一点是，**在扩容 Consumer 的实例数量的同时，必须同步扩容主题中的分区（也叫队列）数量，确保 Consumer 的实例数和分区数量是相等的。**如果 Consumer 的实例数量超过分区数量，这样的扩容实际上是没有效果的。原因我们之前讲过，因为对于消费者来说，在每个分区上实际上只能支持单线程消费。 见到过很多消费程序，他们是这样来解决消费慢的问题的：它收消息处理的业务逻辑可能比较慢，也很难再优化了，为了避免消息积压，在收到消息的 OnMessage 方法中，不处理任何业务逻辑，把这个消息放到一个内存队列里面就返回了。然后它可以启动很多的业务线程，这些业务线程里面是真正处理消息的业务逻辑，这些线程从内存队列里取消息处理，这样它就解决了单个 Consumer 不能并行消费的问题。 这个方法是不是很完美地实现了并发消费？请注意，这是一个非常常见的错误方法！ 为什么错误？因为会丢消息。如果收消息的节点发生宕机，在内存队列中还没来及处理的这些消息就会丢失。关于“消息丢失”问题，你可以回顾一下我们的专栏文章《05 | 如何确保消息不会丢失？》。  消息积压处理  还有一种消息积压的情况是，日常系统正常运转的时候，没有积压或者只有少量积压很快就消费掉了，但是某一个时刻，突然就开始积压消息并且积压持续上涨。这种情况下需要你在短时间内找到消息积压的原因，迅速解决问题才不至于影响业务。 导致突然积压的原因肯定是多种多样的，不同的系统、不同的情况有不同的原因，不能一概而论。但是，我们排查消息积压原因，是有一些相对固定而且比较有效的方法的。 能导致积压突然增加，最粗粒度的原因，只有两种：要么是发送变快了，要么是消费变慢了。 大部分消息队列都内置了监控的功能，只要通过监控数据，很容易确定是哪种原因。如果是单位时间发送的消息增多，比如说是赶上大促或者抢购，短时间内不太可能优化消费端的代码来提升消费性能，唯一的方法是通过扩容消费端的实例数来提升总体的消费能力。 如果短时间内没有足够的服务器资源进行扩容，没办法的办法是，将系统降级，通过关闭一些不重要的业务，减少发送方发送的数据量，最低限度让系统还能正常运转，服务一些重要业务。 还有一种不太常见的情况，你通过监控发现，无论是发送消息的速度还是消费消息的速度和原来都没什么变化，这时候你需要检查一下你的消费端，是不是消费失败导致的一条消息反复消费这种情况比较多，这种情况也会拖慢整个系统的消费速度。 如果监控到消费变慢了，你需要检查你的消费实例，分析一下是什么原因导致消费变慢。优先检查一下日志是否有大量的消费错误，如果没有错误的话，可以通过打印堆栈信息，看一下你的消费线程是不是卡在什么地方不动了，比如触发了死锁或者卡在等待某些资源上了。  小结  主要讨论了 2 个问题，一个是如何在消息队列的收发两端优化系统性能，提前预防消息积压。另外一个问题是，当系统发生消息积压了之后，该如何处理。 优化消息收发性能，预防消息积压的方法有两种，增加批量或者是增加并发，在发送端这两种方法都可以使用，在消费端需要注意的是，增加并发需要同步扩容分区数量，否则是起不到效果的。 对于系统发生消息积压的情况，需要先解决积压，再分析原因，毕竟保证系统的可用性是首先要解决的问题。快速解决积压的方法就是通过水平扩容增加 Consumer 的实例数量。  思考 在消费端是否可以通过批量消费的方式来提升消费性能？在什么样场景下，适合使用这种方法？或者说，这种方法有什么局限性？\n提问 网关如何接收服务端的秒杀结果？ 详解 RocketMQ 和 Kafka 的消息模型 通信 高性能异步系统  对于开发者来说，异步是一种程序设计的思想，使用异步模式设计的程序可以显著减少线程等待，从而在高吞吐量的场景中，极大提升系统的整体性能，显著降低时延。因此，像消息队列这种需要超高吞吐量和超低时延的中间件系统，在其核心流程中，一定会大量采用异步的设计思想。 使用异步编程模型，虽然并不能加快程序本身的速度，但可以减少或者避免线程等待，只用很少的线程就可以达到超高的吞吐能力。 同时我们也需要注意到异步模型的问题：相比于同步实现，异步实现的复杂度要大很多，代码的可读性和可维护性都会显著的下降。虽然使用一些异步编程框架会在一定程度上简化异步开发，但是并不能解决异步模型高复杂度的问题。 异步性能虽好，但一定不要滥用，只有类似在像消息队列这种业务逻辑简单并且需要超高吞吐量的场景下，或者必须长时间等待资源的地方，才考虑使用异步模型。如果系统的业务逻辑比较复杂，在性能足够满足业务需求的情况下，采用符合人类自然的思路且易于开发和维护的同步模型是更加明智的选择。  同步实现的线程等待问题  假设我们要实现一个转账的服务，我们要从账户 A 中转账 100 元到账户 B 中：先从 A 的账户中减去 100 元；再给 B 的账户加上 100 元，转账完成。设计实现共2个微服务  TransferService：提供转账入口，即Transfer( accountFrom, accountTo, amount)，三个参数：分别是转出账户、转入账户和转账金额。 AccountService：提供账户金额更变得功能，即Add(account, amount)，它的功能是给账户 account 增加金额 amount，当 amount 为负值的时候，就是扣减响应的金额   性能分析：  假设调用AccountService.Add的平均响应有延时为50ms，TransferService.Transfer中执行了2次Add，即每处理一个转账请求耗时100ms，并且这 100ms 过程中是需要独占一个线程的，那么每个线程每秒钟最多可以处理 10 个请求，假设TransferService所在服务器同时打开的线程数量上限是 10,000，可以计算出这台服务器每秒钟可以处理的请求上限是100,000 次/秒 如果请求速度超过这个值，那么请求就不能被马上处理，只能阻塞或者排队，这时候 Transfer 服务的响应时延由 100ms 延长到了：排队的等待时延 + 处理时延 (100ms)。也就是说，在大量请求的情况下，我们的微服务的平均响应时延变长了。 这是不是已经到了这台服务器所能承受的极限了呢？其实远远没有，如果我们监测一下服务器的各项指标，会发现无论是 CPU、内存，还是网卡流量或者是磁盘的 IO 都空闲的很，那我们 Transfer 服务中的那 10,000 个线程在干什么呢？对，绝大部分线程都在等待 Add 服务执行更变账户的业务并返回结果。**采用同步实现的方式，整个服务器的所有线程大部分时间都没有在工作，而是都在等待。**如果能减少或者避免这种无意义的等待，就可以大幅提升服务的吞吐能力，从而提升服务的总体性能。    // 伪代码。为了使问题简化以便我们能专注于异步和性能优化，还省略了错误处理和事务相关的代码\rTransfer(accountFrom, accountTo, amount) {\r// 先从 accountFrom 的账户中减去相应的钱数\rAdd(accountFrom, -1 * amount)\r// 再把减去的钱数加到 accountTo 的账户中\rAdd(accountTo, amount)\rreturn OK\r}\r 异步实现解决线程等待问题  TransferAsync 服务比 Transfer 多了一个参数，并且这个参数传入的是一个回调方法 OnComplete()（虽然 Java 语言并不支持将方法作为方法参数传递，但像 JavaScript 等很多语言都具有这样的特性，在 Java 语言中，也可以通过传入一个回调类的实例来变相实现类似的功能）。TransferAsync() 方法的语义是：请帮我执行转账操作，当转账完成后，请调用 OnComplete() 方法。调用 TransferAsync 的线程不必等待转账完成就可以立即返回了，待转账结束后，TransferService 自然会调用 OnComplete() 方法来执行转账后续的工作  //TODO回调方法 OnComplete() 是在什么线程中运行的？我们是否能控制回调方法的执行线程数？该如何做？   如果调用账户失败，可以在异步callBack里执行通知客户端的逻辑；因为是两次调用add，如果是第一次失败，那后面的那一步就不用执行了，直接转账失败；如果是第一次成功但是第二次失败，首先考虑重试，如果转账服务是幂等的，可以考虑一定次数的重试，如果不能重试，可以考虑采用补偿机制，undo第一次的转账操作，需要注意的是采取补偿机制之前也要先检查，即先检查后补偿，避免发生重复补偿 性能分析：流程的时序和同步实现是一样，在低请求数量的场景下，平均响应时延一样是 100ms。在超高请求数量场景下，异步的实现不再需要线程等待执行结果，只需要个位数量的线程，即可实现同步场景大量线程一样的吞吐量。由于没有了线程的数量的限制，总体吞吐量上限会大大超过同步实现，并且在服务器 CPU、网络带宽资源达到极限之前，响应时延不会随着请求数量增加而显著升高，几乎可以一直保持约 100ms 的平均响应时延。  // 伪代码\rTransferAsync(accountFrom, accountTo, amount, OnComplete()) {\r// 异步调用 AccountService.AddAsync 从 accountFrom 的账户中减去相应的钱数，然后回调 TransferService.OnDebit 方法。\rAddAsync(accountFrom, -1 * amount, OnDebit(accountTo, amount, OnAllDone(OnComplete())))\r}\r// AccountService 扣减账户 accountFrom 完成后的回调\rOnDebit(accountTo, amount, OnAllDone(OnComplete())) {\r// 再异步 AccountService.AddAsync 把减去的钱数加到 accountTo 的账户中，然后再回调执行 TransferService.OnAllDone 方法\rAddAsync(accountTo, amount, OnAllDone(OnComplete()))\r}\r// AccountService 转入账户 accountTo 完成后的回调\rOnAllDone(OnComplete()) {\r// 转账后续业务\rOnComplete()\r}\r 简单实用的 Java8 内置异步框架  在实际开发时可以使用异步框架和响应式框架，来解决一些通用的异步编程问题，简化开发。CompletableFuture 是 Java8 中新增的一个非常强大的用于异步编程的类，几乎囊获了我们在开发异步程序的大部分功能，使用 CompletableFuture 很容易编写出优雅且易于维护的异步代码。其它异步框架还有如 ReactiveX 的 RxJava，功能更加强大但相对复杂。 使用 CompletableFuture 实现的转账异步设计  /**\r* CompletableFuture 实现的转账服务\r*/\rpublic interface TransferService {\r/**\r* 异步转账服务\r* @param fromAccount 转出账户\r* @param toAccount 转入账户\r* @param amount 转账金额，单位分\r*/\rCompletableFuture\u0026lt;Void\u0026gt; transfer(int fromAccount, int toAccount, int amount);\r}\r/**\r* 转账服务的实现\r*/\rpublic class TransferServiceImpl implements TransferService {\r// 注入账户服务的实例\r@Inject\rprivate AccountService accountService;\r@Override\rpublic CompletableFuture\u0026lt;Void\u0026gt; transfer(int fromAccount, int toAccount, int amount) {\r// 异步调用 add 方法从 fromAccount 扣减相应金额；然后调用 add 方法给 toAccount 增加相应金额\rreturn accountService.add(fromAccount, -1 * amount).thenCompose(v -\u0026gt; accountService.add(toAccount, amount));\r}\r}\r /**\r* CompletableFuture 实现的账户服务\r*/\rpublic interface AccountService {\r/**\r* 变更账户金额\r* @param account 账户 ID\r* @param amount 增加的金额，负值为减少\r*/\rCompletableFuture\u0026lt;Void\u0026gt; add(int account, int amount);\r}\r  CompletableFuture 的同步和异步调用：客户端使用 CompletableFuture 非常灵活，既可以同步调用，也可以异步调用  public class Client {\r@Inject\rprivate TransferService transferService; // 使用依赖注入获取转账服务的实例\rprivate final static int A = 1000;\rprivate final static int B = 1001;\rpublic void syncInvoke() throws ExecutionException, InterruptedException {\r// get：同步调用，将等待调用的方法执行结束并获得返回值\rtransferService.transfer(A, B, 100).get();\rSystem.out.println(\u0026quot; 转账完成！\u0026quot;);\r}\rpublic void asyncInvoke() {\r// thenXXX：异步调用，为 CompletableFuture 定义异步方法结束之后的后续操作\rtransferService.transfer(A, B, 100)\r.thenRun(() -\u0026gt; System.out.println(\u0026quot; 转账完成！\u0026quot;));\r}\r}\r 高性能异步网络传输  异步与同步模型最大的区别是，同步模型会阻塞线程等待资源，而异步模型不会阻塞线程，它是等资源准备好后，再通知业务代码来完成后续的资源处理逻辑。这种异步设计的方法，可以很好地解决 IO 等待的问题。 IO 密集型系统：IO 密集型系统大部分时间都在执行 IO 操作，IO 操作主要包括网络 IO 和磁盘 IO，以及与计算机连接的一些外围设备的访问。绝大多数业务系统都是 IO 密集型系统，很少有非常耗时的计算，更多的是网络收发数据，读写磁盘和数据库这些 IO 操作，特别适合使用异步的设计来提升系统性能，且由于现在的 SSD 的速度越来越快，对于本地磁盘IO，异步的意义越来越小。所以，使用异步设计的方法来提升 IO 性能，我们更加需要关注的问题是，如何来实现高性能的异步网络传输。 计算密集型系统：大部分时间都是在使用 CPU 执行计算操作。我们开发的业务系统，  同步网络IO模型  在我们开发的程序中，如果要实现通过网络来传输数据，需要用到开发语言提供的网络通信类库。大部分语言提供的网络通信基础类库都是同步的，一个 TCP 连接建立后，用户代码会获得一个用于收发数据的通道，每个通道会在内存中开辟两片区域用于收发数据的缓存。传统的同步网络 IO，一般采用的都是一个线程对应一个Channel接收数据，很难支持高并发和高吞吐量。  发送数据：同步发送，无需异步。发送数据的过程比较简单，直接往通信通道里面来写入数据就可以了，用户代码在发送时写入的数据会暂存在缓存中，然后操作系统会通过网卡，把发送缓存中的数据传输到对端的服务器上，只要通信的缓存不满，或者说，我们发送数据的速度没有超过网卡传输速度的上限，那这个发送数据的操作耗时，只不过是一次内存写入的时间，这个时间是非常快的 接收数据：数据接收方不知道何时会收到数据，则用一个线程阻塞等待接收，存在大量连接数时亦需要等量线程数，将造成频繁的CPU上下文切换，影响性能。当有数据到来的时候，操作系统会先把数据写入接收缓存，然后给接收数据的线程发一个通知，线程收到通知后结束等待，开始读取数据。处理完这一批数据后，继续阻塞等待下一批数据到来，这样周而复始地处理收到的数据。同步网络 IO 模型在处理少量连接的时候，是没有问题的。但是如果要同时处理非常多的连接，同步的网络 IO 模型就有点儿力不从心了。每个连接都需要阻塞一个线程来等待数据，大量的连接数就会需要相同数量的数据接收线程。当这些 TCP 连接都在进行数据收发的时候，会导致什么情况呢？对，会有大量的线程来抢占 CPU 时间，造成频繁的 CPU 上下文切换，导致 CPU 的负载升高，整个系统的性能就会比较慢。    异步网络IO模型  一个好的异步网络框架，无非就是只用少量的线程就能处理大量的连接，有数据到来的时候能第一时间处理就可以了。对于开发者来说，最简单的方式就是，事先定义好收到数据后的处理逻辑，把这个处理逻辑作为一个回调方法，在连接建立前就通过框架提供的 API 设置好。当收到数据的时候，由框架自动来执行这个回调方法就好了。实际上，有没有这么简单的框架呢？Netty设计就是这样的  Netty  Netty 基于 Java NIO 实现了全异步的设计，像线程控制、缓存管理、连接管理这些异步网络 IO 中通用的、复杂的问题，完全由 Netty 处理好了，并提供了一组非常友好 API，用户只需要通过Netty 提供的 ServerBootstrap 对象创建、初始化、启动服务，并实现对应的 Handler 来处理收到的数据即可。所以非常多的开源项目使用 Netty 作为其底层的网络 IO 框架 使用Netty实现的异步网络通信  服务启动后，如果有客户端来请求连接，Netty 会自动接受并创建一个 Socket 连接。可以看到代码中并没有像一些同步网络框架中那样，需要用户调用 Accept() 方法来接受创建连接的情况，在 Netty 中，这个过程是自动的。 当收到来自客户端的数据后，Netty 就会从 EventLoopGroup 中，获取一个 IO 线程，在这个 IO 线程中调用接收数据的回调方法，来执行接收数据的业务逻辑，这里即MyHandler 中的方法。 //TODO针对接收数据这个流程，Netty 是如何用 NIO 来实现的呢？    // 使用Netty实现的异步网络通信。代码非常简单，使用 Netty 提供的 ServerBootstrap 来创建并初始化一个 Socket Server 用以接收数据\r// 创建NioEventLoopGroup。简单将之理解为一组线程，这组线程的作用就是来执行收发数据的业务逻辑。\rEventLoopGroup group = new NioEventLoopGroup();\rtry{\rServerBootstrap serverBootstrap = new ServerBootstrap();\rserverBootstrap.group(group);\rserverBootstrap.channel(NioServerSocketChannel.class);\rserverBootstrap.localAddress(new InetSocketAddress(\u0026quot;localhost\u0026quot;, 9999));\r// 设置收到数据后的 Handler。MyHandler 即我们自己实现的处理类，它需要继承 Netty 提供的抽象类 ChannelInboundHandlerAdapter，其中定义收到数据后的处理逻辑\rserverBootstrap.childHandler(new ChannelInitializer\u0026lt;SocketChannel\u0026gt;() {\rprotected void initChannel(SocketChannel socketChannel) throws Exception {\rsocketChannel.pipeline().addLast(new MyHandler());\r}\r});\r// 绑定端口，启动 Socket 服务\rChannelFuture channelFuture = serverBootstrap.bind().sync();\rchannelFuture.channel().closeFuture().sync();\r} catch(Exception e){\re.printStackTrace();\r} finally {\rgroup.shutdownGracefully().sync();\r}\r NIO  NIO 提供了 Selector 机制，用单个线程同时管理多个连接，解决了异步网络通信的核心问题-多路复用（一个信道同时传输多路信号）。如果业务需要更灵活的实现，自己来维护收发数据的线程，可以选择于 Netty 更底层的 Java NIO。 使用 NIO 实现的异步网络通信  每个已经建立好的连接用一个 Channel 对象来表示，希望实现的是，在一个线程里，接收来自多个 Channel 的数据并处理，则有可能会出现这两种情况  线程正在处理收到的数据，这时候又有任意 Channel 收到了新的数据； 线程闲着没事儿干，所有的 Channel 中都没收到数据，也不能确定哪个 Channel 会在什么时候收到数据。   在 NIO 中，Selecor 通过一种类似于事件的机制来解决这个问题。  首先你需要把你的连接，也就是 Channel 绑定到 Selector 上 然后你可以在接收数据的线程来调用 Selector.select() 方法来等待数据到来。但这是一个阻塞方法，这个线程会一直卡在这儿，直到这些 Channel 中的任意一个有数据到来，就会结束等待返回数据，它的返回值是一个迭代器 可以从这个迭代器里面获取所有 Channel 收到的数据，然后来执行你的数据接收的业务逻辑。你可以选择直接在这个线程里面来执行接收数据的业务逻辑，也可以将任务分发给其他的线程来执行，如何选择完全可以由你的代码来控制。      高性能数据序列化   进程之间要通过网络传输结构化的数据，需要通过序列化和反序列化来实现结构化数据和二进制数据的双向转换。在选择序列化实现的时候，需要综合考虑数据可读性、实现复杂度、性能、信息密度四个因素。\n 大多数情况下，选择一个高性能的通用序列化框架都可以满足要求，在性能可以满足需求的前提下，推荐优先选择 JSON 这种可读性好的序列化方法。如果说我们需要超高的性能，或者是带宽有限的情况下，可以使用专用的序列化方法，来提升序列化性能，节省传输流量。不过实现起来很复杂，大部分情况下并不划算。 //TODO在内存里存放的任何数据，它最基础的存储单元也是二进制比特，也就是说，我们应用程序操作的对象，它在内存中也是使用二进制存储的，既然都是二进制，为什么不能直接把内存中，对象对应的二进制数据直接通过网络发送出去，或者保存在文件中呢？为什么还需要序列化和反序列化呢？因为内存中的对象数据应该具有语言独特性    数据的形式\n  二进制流：在 TCP 的连接上，传输数据的基本形式就是二进制流，也就是一段一段的 1 和 0，以 bit 为数量单位\n 字节流：在一般编程语言或者网络框架提供的 API 中，传输数据的基本形式是字节 Byte，一个字节就是 8 个二进制位，所以二进制流和字节流本质上是一样的。    结构化数据：那对于编写的程序来说，它需要通过网络传输的数据是什么形式的呢？是结构化的数据，比如，一条命令、一段文本或者是一条消息。对应到我们写的代码中，这些结构化的数据是什么？这些都可以用一个类（Class）或者一个结构体（Struct）来表示。\n  序列化与反序列化：将结构化数据转换成字节流的过程，称为序列化，反之则是反序列化\n 显然，要想使用网络框架的 API 来传输结构化的数据，必须得先实现结构化的数据与字节流之间的双向转换，即序列化和反序列化 因为在文件内保存数据的形式也是二进制序列，所以将结构化数据保存到文件，同样也需要序列化和反序列化。很多处理海量数据的场景中，都需要将对象序列化后，把它们暂时从内存转移到磁盘中，等需要用的时候，再把数据从磁盘中读取出来，反序列化成对象来使用，这样不仅可以长期保存不丢失数据，而且可以节省有限的内存空间。    序列化与反序列化实现：只是实现序列化和反序列的功能，并不难，方法也有很多，比如我们最常使用的，把一个对象转换成字符串并打印出来，这其实就是一种序列化的实现，这个字符串只要转成字节序列，就可以在网络上传输或者保存在文件中了，但千万不要在程序中这么用。\n  开源序列化实现选择  有很多通用的序列化实现可以直接拿来使用。Java 和 Go 语言都内置了序列化实现，也有一些流行的开源序列化实现，比如，Google 的 Protobuf、Kryo、Hessian 等；此外，像 JSON、XML、TOML 这些标准的数据格式，也可以作为一种序列化实现来使用。 序列化实现选择因素  可读性：序列化后的数据最好是易于人类阅读的； 实现复杂度：实现的复杂度是否足够低； 性能：序列化和反序列化的速度越快越好； 信息密度：序列化后的信息密度越大越好，也就是说，同样的一个结构化数据，序列化之后占用的存储空间越小越好；   当然，不存在一种序列化实现都是最优的，因为大多数情况下，易于阅读和信息密度是矛盾的，实现的复杂度和性能也是互相矛盾的。所以需要根据所实现的业务，来选择合适的序列化实现  像 JSON、XML 这些序列化方法，可读性最好，但信息密度也最低 像 Kryo、Hessian 这些通用的二进制序列化实现，适用范围广，使用简单，性能比 JSON、XML 要好一些，但是肯定不如专用的序列化实现。 对于一些强业务类系统，比如电商类、社交类的应用系统，这些系统的特点是业务复杂、需求变化快、对性能的要求没有那么苛刻，这种情况下，推荐使用 JSON 这种实现简单，数据可读性好的序列化实现，使用简单，可读性高，无论是接口调试还是排查问题都非常方便，付出的代价就是多一点 CPU 时间和存储空间罢了   使用序列化实现：这里使用 JSON 和 Kryo 序列化一个 User 对象。如果 JSON 序列化的性能达不到系统的要求，可以采用性能更好的二进制序列化实现，实现的复杂度和 JSON 序列化是差不多的，都很简单，但是序列化性能更好，信息密度也更高，代价是失去了可读性  User:\rname: \u0026quot;zhangsan\u0026quot;\rage: 23\rmarried: true\r {\u0026quot;name\u0026quot;:\u0026quot;zhangsan\u0026quot;,\u0026quot;age\u0026quot;:\u0026quot;23\u0026quot;,\u0026quot;married\u0026quot;:\u0026quot;true\u0026quot;}\r // JSON 序列化\rbyte [] serializedUser = JsonConvert.SerializeObject(user).getBytes(\u0026quot;UTF-8\u0026quot;);\r // Kryo 序列化。先要向 Kryo 注册一下 User 类，然后创建一个流，最后调用 writeObject 方法，将 user 对象序列化后直接写到流中\rkryo.register(User.class);\rOutput output = new Output(new FileOutputStream(\u0026quot;file.bin\u0026quot;));\rkryo.writeObject(output, user);\r 实现高性能的序列化  绝大部分系统，使用前面这两类通用的开源序列化实现，都可以满足需求，而像消息队列这种用于解决通信问题的中间件，对性能要求非常高，一般的序列化实现达不到性能要求，所以，很多消息队列都选择自己实现高性能的专用序列化和反序列化。使用专用的序列化方法，可以提高序列化性能，并有效减小序列化后的字节长度，并且不必考虑通用性 比如，我们可以固定字段的顺序，这样在序列化后的字节里面就不必包含字段名，只要字段值就可以了，不同类型的数据也可以做针对性的优化。对于同样的 User 对象，可以把它序列化成这样：  首先我们需要标识一下这个对象的类型，这里面我们用一个字节来表示类型，比如用 03 表示这是一个 User 类型的对象。我们约定，按照 name、age、married 这个固定顺序来序列化这三个属性。按照顺序，第一个字段是 name，我们不存字段名，直接存字段值“zhangsan”就可以了，由于名字的长度不固定，我们用第一个字节 08 表示这个名字的长度是 8 个字节，后面的 8 个字节就是 zhangsan。第二个字段是年龄，我们直接用一个字节表示就可以了，23 的 16 进制是 17 。最后一个字段是婚姻状态，我们用一个字节来表示，01 表示已婚，00 表示未婚，这里面保存一个 01。 可以看到，同样的一个 User 对象，JSON 序列化后需要 47 个字节，这里只要 12 个字节就够了。专用的序列化方法显然更高效，序列化出来的字节更少，在网络传输过程中的速度也更快。但缺点是，需要为每种对象类型定义专门的序列化和反序列化方法，实现起来太复杂了，大部分情况下是不划算的。    03 | 08 7a 68 61 6e 67 73 61 6e | 17 | 01\rUser | z h a n g s a n | 23 | true\r 高性能传输协议  传输协议就是应用程序之间对话的语言。应用程序之间要想互相通信，一起配合来实现业务功能，还需要有一套传输协议来支持。设计传输协议，并没有太多规范和要求，只要是通信双方的应用程序都能正确处理这个协议，并且没有歧义即可。 解决断句问题，实现了双工通信，配合专用的序列化方法，就可以实现一套高性能的网络通信协议，实现高性能的进程间通信。很多的消息队列、RPC 框架都是用这种方式来实现它们自己的私有应用层传输协议。  前置长度解决断句问  传输协议也是一种语言，在传输数据的的时候，首先要解决的就是断句问题 对于传输层来说，收到的数据是一段一段的字节，但因为网络的不确定性，你收到的分段并不一定是我发出去的分段。比如我发送的数据是\u0026quot;下雨天 留客天 天留 我不留\u0026rdquo;，这样断句意思就是\u0026quot;作为主人我不想让你在我这儿住\u0026rdquo;。经过网络传输，可能就变成\u0026quot;下雨天 留客天 天留我不 留\u0026rdquo;，意思完全变了，\u0026ldquo;客人想赖在这儿不走了\u0026rdquo;。  分隔符解决断句问题：在协议中也加上“标点符号”，并且并不需要像自然语言中那么多，只需要定义一个分隔符就可以了 这是可行的，有很多传输协议采用这种方法，比如 HTTP1 协议的分隔符是换行（\\r\\n）  但是，这个办法有一个问题比较难处理，在自然语言中，标点符号是专用的，它没有别的含义，和文字是有天然区分的。在数据传输的过程中，无论你定义什么字符作为分隔符，理论上，它都有可能会在传输的数据中出现。为了区分“数据内的分隔符”和真正的分隔符，你必须得在发送数据阶段，加上分隔符之前，把数据内的分隔符做转义，收到数据之后再转义回来。这是个比较麻烦的过程，还要损失一些性能。     预置长度解决断句问题：更加实用的方法是，我们给每句话前面加一个表示这句话长度的数字，收到数据的时候，我们按照长度来读取就可以了  比如：\u0026ldquo;03 下雨天 03 留客天 02 天留 03 我不留\u0026rdquo;。这里面固定使用 2 位数字来存放长度，每句话最长可以支持到 99 个字。接收后的处理就比较简单了，我们先读取 2 位数字 03，知道接下来的 3 个字是第一句话，那我们接下来就等着这 3 个字都收到了，就可以作为第一句话来处理了，接下来再按照这个方法来读第二句话、第三句话 这种预置长度的方法就很好解决了断句的问题，并且它实现起来要比分隔符的方法简单很多，性能也更好，是目前普遍采用的一种分隔数据的方法。    全双工收发协议提升吞吐量  单工通信：消息只能单方向传输的通信方式。单工通信信道上，发送端和接收端的身份是固定的，发送端只能发送信息，不能接收信息；接收端只能接收信息，不能发送信息，数据信号仅从一端传送到另一端，即信息流是单方向的  HTTP1 协议就是一个单工协议，客户端与服务端建立一个连接后，客户端发送一个请求，直到服务端返回响应或者请求超时，这段时间内，这个连接通道上是不能再发送其他请求的。单工通信的效率是比较低的，很多浏览器和 App 为了解决这个问题，只能同时在服务端和客户端之间创建多个连接。   半双工通信：双向交替通信。半双工通信信道上，通信双方都可以发送信息，但不能双方同时发送或接收 全双工通信：双向同时通信。全双工通信信道上，双方同时进行数据的双向收发，互相是不会受到任何影响的。要提高吞吐量，应用层的协议也必须支持双工通信。TCP 连接就是一个全双工通信信道。  全双工通信时存在问题：请求和响应无法对应；请求因网络等因素无法保证顺序。而在实际上设计协议时，一般不关心顺序，只需要确保请求和响应能够正确对应即可，要解决这个问题，只需要发送请求时，给每个请求加一个序号，这个序号在本次会话内保证唯一，然后在响应中带上请求的序号，即可使请求和响应相对应    高性能通信程序（实践）  //TODO实现一个简单的高性能通信程序。功能是两个大爷，服务端张大爷，客户端李大爷，让俩人在胡同口碰见一百万次并对话，记录下总耗时  server  client  磁盘与内存 内存管理：如何避免内存溢出和频繁的垃圾回收？  内存管理问题：简单的事情在高并发、高吞吐量的极限情况下，就会变得复杂。一个业务逻辑非常简单的微服务，日常情况下都能稳定运行，一到大促就卡死甚至进程挂掉；一个做数据汇总的应用，按照小时、天这样的粒度进行数据汇总都没问题，到年底需要汇总全年数据的时候，没等数据汇总出来，程序就死掉了。之所以出现这些情况，大部分的原因是，程序在设计的时候，没有针对高并发高吞吐量的情况做好内存管理 现代的编程语言，像 Java、Go 语言等，采用的都是自动内存管理机制。我们在编写代码的时候，不需要显式去申请和释放内存。当我们创建一个新对象的时候，系统会自动分配一块内存用于存放新创建的对象，对象使用完毕后，系统会自动择机收回这块内存，完全不需要开发者干预。对于开发者来说，这种自动内存管理的机制，显然是非常方便的，不仅极大降低了开发难度，提升了开发效率，更重要的是，它完美地解决了内存泄漏的问题。是不是很厉害？当年，Java 语言能够迅速普及和流行，超越 C 和 C++，自动内存管理机制是非常重要的一个因素。但是它也会带来一些问题，什么问题呢？这就要从它的实现原理中来分析。  自动内存管理机制  内存管理，主要需要考虑内存申请和内存回收这两个部分。 有效地解决了内存泄漏问题，带来的代价是执行垃圾回收时会暂停进程，如果暂停的时间过长，程序看起来就像“卡死了”一样。 内存申请：  计算要创建对象所需要占用的内存大小； 在内存中找一块儿连续并且是空闲的内存空间，标记为已占用； 把申请的内存地址绑定到对象的引用上，这时候对象就可以使用了。   内存回收：完成垃圾回收，并进行内存碎片整理，将不连续的空闲内存移动到一起，以便空出足够的连续内存空间供后续使用。  垃圾回收：现代的 GC 算法大多采用的是“标记 - 清除”算法或是它的变种算法，这种算法分为标记和清除两个阶段，  标记阶段：从 GC Root 开始，你可以简单地把 GC Root 理解为程序入口的那个对象，标记所有可达的对象，因为程序中所有在用的对象一定都会被这个 GC Root 对象直接或者间接引用。 清除阶段：遍历所有对象，找出所有没有标记的对象。这些没有标记的对象都是可以被回收的，清除这些对象，释放对应的内存即可。 这个算法有一个最大问题就是，在执行标记和清除过程中，必须把进程暂停，否则计算的结果就是不准确的。这也就是为什么发生垃圾回收的时候，我们的程序会卡死的原因。后续产生了许多变种的算法，这些算法更加复杂，可以减少一些进程暂停的时间，但都不能完全避免暂停进程。   内存碎片整理：也有很多非常复杂的实现方法，由于整理过程中需要移动内存中的数据，也都不可避免地需要暂停进程  完成对象回收后，还需要整理内存碎片。什么是内存碎片呢？我举个例子你就明白了。 假设，我们的内存只有 10 个字节，一开始这 10 个字节都是空闲的。我们初始化了 5 个 Short 类型的对象，每个 Short 占 2 个字节，正好占满 10 个字节的内存空间。程序运行一段时间后，其中的 2 个 Short 对象用完并被回收了。这时候，如果我需要创建一个占 4 个字节的 Int 对象，是否可以创建成功呢？ 答案是，不一定。我们刚刚回收了 2 个 Short，正好是 4 个字节，但是，创建一个 Int 对象需要连续 4 个字节的内存空间，2 段 2 个字节的内存，并不一定就等于一段连续的 4 字节内存。如果这两段 2 字节的空闲内存不连续，我们就无法创建 Int 对象，这就是内存碎片问题。      高并发下频繁内存回收甚至内存溢出  微服务在收到一个请求后，执行一段业务逻辑，然后返回响应。这个过程中，会创建一些对象，请求响应的处理流程结束后，这些对象都是垃圾对象，并且直到下一次垃圾回收之前将一直占用内存。  低并发场景下，单位时间内请求不多，创建的对象数量不多，自动垃圾回收机制可以很好地发挥作用，它可以选择在系统不太忙的时候来执行垃圾回收，每次垃圾回收的对象数量也不多，相应的，程序暂停的时间非常短，短到我们都无法感知到这个暂停。这是一个良性的循环。 高并发场景下，自动内存管理的机制会更容易触发进程暂停。虚拟机决定何时执行垃圾回收，策略非常复杂，也有很多不同的实现，但无论是什么策略，只要内存不够用了，就一定要执行一次垃圾回收，否则程序将无法继续运行。短时间内海量请求导致海量对象创建，迅速占满内存，GC被迫执行，大量的对象也将导致GC执行时间较长，并带来较长时间的程序暂停，进一步导致大量的请求积压，垃圾回收刚结束，更多的请求立刻涌进来，再次迅速占满内存，再次被迫GC，形成恶性循环。如果垃圾回收的速度跟不上创建对象的速度，还可能会产生内存溢出的现象。最终服务卡死    内存管理技巧  对于开发者来说，垃圾回收是不可控的，而且是无法避免的。但还是可以通过一些方法来降低垃圾回收的频率，减少进程暂停的时长。 我们知道，只有使用过被丢弃的对象才是垃圾回收的目标，所以，我们需要想办法在处理大量请求的同时，尽量少的产生这种一次性对象。  重用：最有效的方法就是，优化你的代码中处理请求的业务逻辑，尽量少的创建一次性对象，特别是占用内存较大的对象  比如说，我们可以把收到请求的 Request 对象在业务流程中一直传递下去，而不是每执行一个步骤，就创建一个内容和 Request 对象差不多的新对象。这里面没有多少通用的优化方法，你需要根据我告诉你的这个原则，针对你的业务逻辑来想办法进行优化。 对于需要频繁使用，占用内存较大的一次性对象，我们可以考虑自行回收并重用这些对象。实现的方法是这样的：我们可以为这些对象建立一个对象池。收到请求后，在对象池内申请一个对象，使用完后再放回到对象池中，这样就可以反复地重用这些对象，非常有效地避免频繁触发垃圾回收。   硬件升级：如果可能的话，使用更大内存的服务器，也可以非常有效地缓解这个问题。 自行回收：当然，要从根本上来解决这个问题，办法只有一个，那就是绕开自动垃圾回收机制，自己来实现内存管理。但是，自行管理内存将会带来非常多的问题，比如说极大增加了程序的复杂度，可能会引起内存泄漏等等。流计算平台 Flink，就是自行实现了一套内存管理机制，一定程度上缓解了处理大量数据时垃圾回收的问题，但是也带来了一些问题和 Bug，总体看来，效果并不是特别好。因此，一般情况下并不推荐这样做，具体还是要根据你的应用情况，综合权衡做出一个相对最优的选择。   //TODO如果我们的微服务的需求是处理大量的文本，比如说，每次请求会传入一个 10KB 左右的文本，在高并发的情况下，你会如何来优化这个程序，来尽量避免由于垃圾回收导致的进程卡死问题？  Kafka的高性能IO  Kafka 是如何做到这么高的性能的？开发一个高性能的网络应用程序，像全异步化线程模型、高性能异步网络传输、定制私有传输协议、定制序列化反序列化等，这些方法和优化技巧，都可以在 Kafka 的源代码中找到对应的实现。除了这些通用的性能优化手段之外，Kafka 还有其它“独门绝技”  批量消息提升服务端处理能力  批量处理是一种非常有效的提升系统吞吐量的方法。 在 Kafka 内部，消息都是以“批”为单位处理的，采用了异步批量发送的机制  发送端：在 Kafka 的客户端 SDK（软件开发工具包）中，Producer 只提供了发送单条消息的 send() 方法，没有提供任何批量发送的接口，因为根本不提供单条发送的功能，当调用 send() 方法发送一条消息之后，无论是同步发送还是异步发送，Kafka 都不会立即就把这条消息发送出去，而是先把这条消息，存放在内存中缓存起来，然后选择合适的时机把缓存中的所有消息组成一批，一次性发给 Broker 服务端：Kafka 不会把一批消息还原成多条消息，再一条一条地处理，这样太慢了。而是每批消息都会被当做一个“批消息”处理。即在 Broker 整个处理流程中，无论是读写磁盘、复制到其它副本这些流程中，批消息都不会被解开，一直是作为一条“批消息”来处理的。构建批消息和解开批消息分别在发送端和消费端的客户端完成，不仅减轻了 Broker 的压力，最重要的是减少了 Broker 处理请求的次数，提升了总体的处理能力。 接收端：在消费时，消息同样是以批为单位进行传递的，Consumer 从 Broker 拉到一批消息后，在客户端把批消息解开，再一条一条交给用户代码处理    顺序读写提升磁盘 IO 性能  磁盘的顺序读写性远大于随机读写  在 SSD（固态硬盘）上，顺序读写的性能要比随机读写快几倍，如果是机械硬盘，差距会达到几十倍 因为操作系统每次从磁盘读写数据的时候，需要先寻址，即找到数据在磁盘上的物理位置，再进行数据读写。且机械硬盘寻址时间比较长，因为是要移动磁头的机械运动。顺序读写相比随机读写省去了大部分的寻址时间，只需要寻址一次，就可以连续地读写下去   Kafka 就充分利用了磁盘的这个特性：其存储设计非常简单，充分利用了顺序读写这个特性，极大提升了 Kafka 在使用磁盘时的 IO 性能  生产时：对于每个分区，把从 Producer 收到的消息，顺序地写入对应的 log 文件中，一个文件写满了，就开启一个新的文件这样顺序写下去 消费时：也是从某个全局的位置开始，也就是某一个 log 文件中的某个位置开始，顺序地把消息读出来    PageCache加速消息读写  Kafka 会利用 PageCache 加速消息读写 PageCache ：是现代操作系统都具有的一项基本特性，是操作系统在内存中给磁盘上的文件建立的缓存。无论使用什么语言编写的程序，在调用系统的 API 读写文件时，都不会直接去磁盘上读写文件，应用程序实际操作的都是 PageCache，即文件在内存中缓存的副本  读：读文件时，PageCache 中有数据就直接读取；无数据则操作系统会引发一个缺页中断，应用程序的读取线程会被阻塞，操作系统会真正读一次磁盘上的文件，把数据从磁盘中复制到 PageCache ，然后应用程序再从 PageCache 中继续把数据读出来，这个读的过程就会比较慢 写：应用程序在写入文件的时候，操作系统会先把数据写入到内存中的 PageCache，然后再异步的一批一批地写到磁盘上，需要注意的是这之间存在延迟，虽然操作系统可以保证即使应用程序意外退出了，也会把这部分数据同步到磁盘上，但如果服务器突然掉电了，这部分数据就丢失了。   用户的应用程序在使用完某块 PageCache 后，操作系统并不会立刻就清除这个 PageCache，而是尽可能地利用空闲的物理内存保存这些 PageCache，除非系统内存不够用，操作系统才会清理掉一部分 PageCache。清理的策略一般是 LRU 或它的变种算法，这个算法不展开讲，它保留 PageCache 的逻辑是：优先保留最近一段时间最常使用的那些 PageCache。 Kafka 在读写消息文件的时候，充分利用了 PageCache 的特性。一般来说，消息刚刚写入到服务端就会被消费，按照 LRU 的“优先清除最近最少使用的页”这种策略，读取的时候，对于这种刚刚写入的 PageCache，命中的几率会非常高。也就是说，大部分情况下，消费读消息都会命中 PageCache，带来的好处有两个：一个是读取的速度会非常快，另外一个是，给写入消息让出磁盘的 IO 资源，间接也提升了写入的性能。  ZeroCopy零拷贝技术  Kafka 的服务端在消费过程中，还使用了一种“零拷贝”的操作系统特性来进一步提升消费的性能。  在服务端，处理消费的大致逻辑是这样的：首先，从文件中找到消息数据，读到内存中；然后，把消息通过网络发给客户端。这个过程中，数据实际上做了 2 次或者 3 次复制：从文件复制数据到 PageCache 中，如果命中 PageCache，这一步可以省掉；从 PageCache 复制到w应用程序的内存空间中，也就是我们可以操作的对象所在的内存；从应用程序的内存空间复制到 Socket 的缓冲区，这个过程就是我们调用网络应用框架的 API 发送数据的过程。 Kafka 使用零拷贝技术可以把这个复制次数减少一次，上面的 2、3 步骤两次复制合并成一次复制。直接从 PageCache 中把数据复制到 Socket 缓冲区中，这样不仅减少一次数据复制，更重要的是，由于不用把数据复制到用户内存空间，DMA 控制器可以直接完成数据复制，不需要 CPU 参与，速度更快。 如果遇到这种从文件读出数据后再通过网络发送出去的场景，并且这个过程中你不需要对这些数据进行处理，那一定要使用这个零拷贝的方法，可以有效地提升性能。   下面是这个零拷贝对应的系统调用：它的前两个参数分别是目的端和源端的文件描述符，后面两个参数是源端的偏移量和复制数据的长度，返回值是实际复制数据的长度。  #include \u0026lt;sys/socket.h\u0026gt;\rssize_t sendfile(int out_fd, int in_fd, off_t *offset, size_t count);\r  //TODO读一读 Kafka 的源代码，从上面找一两个技术点，找到对应的代码部分，真正去看一下，这些优化技术，是如何落地到代码上的  缓存策略减少磁盘IO  消息队列，都会使用磁盘文件来持久化存储消息，但内存的随机读写速度是磁盘的 10 万倍，所以使用内存作为缓存来加速应用程序的访问速度，是几乎所有高性能系统都会采用的方法。缓存的思想很简单，就是把低速存储的数据，复制一份副本放到高速的存储中，用来加速数据的访问 缓存的命中率如何？或者说怎样才能提高缓存的命中率？缓存是否总能返回最新的数据？如果缓存返回了过期的数据该怎么办？  只读缓存与读写缓存  只读缓存：写不经过缓存。适用于读远大于写的场景，我们开发的大部分业务类应用程序读频次都远高于写频次  只读缓存实现：主要考虑的是缓存更新策略。主要有三种方法，这三种方法在更新及时性和实现复杂度上，都是依次递减的，需按需选择。对于缓存的置换策略，最优的策略一定是你根据业务来设计的定制化的置换策略，当然你也可以考虑 LRU 这样通用的缓存置换算法  第一种是在更新数据的同时去更新缓存 第二种是定期来更新全部缓存 第三种是给缓存中的每个数据设置一个有效期，让它自然过期以达到更新的目的     读写缓存：写会经过缓存，所以存在数据丢失可能。适用于写比例较大的场景以换取更高写性能  读写缓存实现：应用程序不停地更新 PageCache 中的数据，操作系统需要记录哪些数据有变化，同时还要在另外一个线程中，把缓存中变化的数据更新到磁盘文件中。在提供并发读写的同时来异步更新数据，这个过程中要保证数据的一致性，并且有非常好的性能，实现起来并不容易。所以一般不推荐使用读写缓存。 Kafka 使用读写缓存 PageCache 来提升性能  MQ 读写比例大致为1:1的特性，写占比大自然就需要写缓存 Kafka 不是只靠磁盘来保证数据的可靠性，更依赖的是不同节点上的多副本来解决数据可靠性问题 和 Kafka 一样，大部分其他的消息队列，同样也会采用读写缓存来加速消息写入的过程，只是使用的缓存实现各不相同   应用程序可以调用 sync 等系统调用，强制操作系统立即把缓存数据同步到磁盘文件中去，但是这个同步的过程是很慢的，也就失去了缓存的意义   读缓存和读写缓存的唯一区别，就是在更新数据时是否经过缓存。  缓存与磁盘的数据一致性问题  缓存数据与磁盘数据的一致性问题：只读缓存的缓存数据仅来源于磁盘，当有数据写入磁盘时，如何向缓存中同步数据以保证缓存与磁盘的数据一致性？这在并发的分布式的环境中实现并不太容易  同步更新：更新磁盘成功了，但是更新缓存失败了，是不是要反复重试来保证更新成功？如果多次重试都失败，那这次更新是算成功还是失败呢？ 异步更新：怎么保证更新的时序？比如，我先把一个文件中的某个数据设置成 0，然后又设为 1，这个时候文件中的数据肯定是 1，但是缓存中的数据可不一定就是 1 了 这些问题都会导致缓存的数据和磁盘中的数据不一致，而且，在下次更新这条数据之前，这个不一致的问题它是一直存在的。当然，这些问题也不是不能解决的，比如，你可以使用分布式事务来解决，只是付出的性能、实现复杂度等代价比较大。    缓存更新策略  缓存更新策略：  同时更新：数据写入磁盘的同时写入到缓存 定时更新：定时将磁盘上的数据全量更新到缓存中，在异步的线程中执行速度稍慢也不算大问题；数据量过大速度无法接受也可以选择增量更新，只更新时间内变化的数据，但实现会复杂一些。某次同步过程中发生了错误，等到下一个同步周期也会自动把数据纠正过来。定时更新的缺点是缓存更新不那么及时，优点是实现简单，鲁棒性好 过期更新：不更新缓存中的数据，而是给缓存中的每条数据设置一个比较短的过期时间，数据过期以后即使它还存在缓存中，我们也认为它不再有效，需要从磁盘上重新加载，变相地实现数据更新。   策略选择  很多情况下，缓存数据更新即使不太及时，系统也是可以接受的。比如发了一封邮件，收件人过了一会儿才收到，或者更改微信头像，在一段时间内，好友看到还是你的旧头像，这些都是可以接受的。一般来说对数据一致性要求不强的场景下，一定要选择后面两种方法 而像交易类的系统，需要数据的强一致性。比如你给别人转一笔钱，你的余额减少了而他余额没有增加，这必然不可接受。对于这样的系统，要么不使用缓存，要么即时更新缓存    缓存置换策略  缓存置换问题：在内存有限的情况下，要优先缓存哪些数据，让缓存的命中率最高，避免更多的缓存穿透，避免挖坟  缓存穿透：没有命中缓存，穿过缓存读取磁盘 挖坟：某个客户端正在从很旧的位置开始向后读取一批历史数据，内存中的缓存很快都会被替换成这些历史数据，相当于大部分缓存资源都被消耗掉了，这样会导致其他客户端的访问命中率下降。   如果系统数据的访问是可预测的，比如有的系统会定期做数据同步，每次同步的数据范围都是一样的，像这样的系统，要访问什么数据就缓存什么数据即可，甚至可以做到百分之百的命中。但大部分系统，它并没有办法准确地预测未来会有哪些数据会被访问到，所以只能使用一些策略来尽可能地提高缓存命中率。 缓存置换：一般数据在首次被访问时，被更新到缓存中。随着访问的数据增多，直到缓存占满时，就需要删除一些缓存数据，以存放新的数据，这个过程称为缓存置换 缓存置换策略：决定删除哪些缓存数据的策略，使将来保持高的缓存命中率。命中率最高的置换策略，一定是由业务决定的定制策略。如果知道某些数据已经从磁盘删除，永远不会再被访问到，那优先置换这些数据肯定是没问题的；如果系统是一个有会话的系统，可以知道哪些用户是在线的，哪些用户是离线的，则优先置换已经离线用户的数据，尽量保留在线用户的数  LRU：它的思想是，最近刚刚被访问的数据，它在将来被访问的可能性也很大，而很久都没被访问过的数据，未来再被访问的几率也不大，也叫最近最少使用算法，总是把最长时间未被访问的数据置换出去，效果极好，是最经典最实用的通用置换算法 LRU 变种定制：  PageCache 的置换算法就是 LRU 的变种算法 LRU 2Q JMQ 的缓存策略也是采用一种改进的 LRU 算法，LRU 淘汰最近最少使用的页，JMQ 根据消息这种流数据存储的特点，在淘汰时增加了一个考量维度：页面位置与尾部的距离。因为越是靠近尾部的数据，被访问的概率越大，加入位置权重后，比较旧的页面会很快被淘汰掉，减少“挖坟”对系统的影响。      LRU实现  //TODO实现一个采用 LRU 置换算法的缓存。你需要继承 LruCache 这个抽象类，实现你自己的 LRU 缓存。lowSpeedStorage 是提供给你可用的低速存储，你不需要实现它  // /**\r* KV 存储抽象\r*/\rpublic interface Storage\u0026lt;K,V\u0026gt; {\r/**\r* 根据提供的 key 来访问数据\r* @param key 数据 Key\r* @return 数据值\r*/\rV get(K key);\r}\r/**\r* LRU 缓存。你需要继承这个抽象类来实现 LRU 缓存。\r* @param \u0026lt;K\u0026gt; 数据 Key\r* @param \u0026lt;V\u0026gt; 数据值\r*/\rpublic abstract class LruCache\u0026lt;K, V\u0026gt; implements Storage\u0026lt;K,V\u0026gt;{\r// 缓存容量\rprotected final int capacity;\r// 低速存储，所有的数据都可以从这里读到\rprotected final Storage\u0026lt;K,V\u0026gt; lowSpeedStorage;\rpublic LruCache(int capacity, Storage\u0026lt;K,V\u0026gt; lowSpeedStorage) {\rthis.capacity = capacity;\rthis.lowSpeedStorage = lowSpeedStorage;\r}\r}\r 并发编程 用锁保护共享数据以及协调异步线程   JMQ 为了提升整个流程的处理性能，使用了一个“近乎无锁”的设计，这里面其实隐含着两个信息点。第一个是，在消息队列中，“锁”是一个必须要使用的技术。第二个是，使用锁其实会降低系统的性能\n  锁：任何时间都只能有一个线程持有锁，只有持有锁的线程才能访问被锁保护的资源。\n 异步和并发的设计可以大幅提升程序性能，代价是程序更加复杂，多线程并行执行时带来很多不确定性，尤其是多线程并发读写共享数据时，如果处理不好，很可能会产出不可预期的结果。使用锁可以非常有效地解决问题。 但用锁会减低系统性能，加锁和解锁过程都是需要 CPU 时间的，且用锁就有可能导致线程等待锁，等待锁过程中线程是阻塞的状态，过多的锁等待会显著降低程序的性能；用锁多线程程序的调试困难复杂，使用不当容易死锁，将导致整个程序阻塞甚至卡死    原则：能不用锁，就不用锁；如果不确定是否应该用锁，那就不用锁\n 用锁之前，一定要非常明确地知道，这个问题必须要用一把锁来解决，切忌看到一个共享数据，也搞不清它在并发环境中会不会出现争用问题，就\u0026quot;为了保险给它加锁吧\u0026rdquo;，千万不能有这种不负责任的想法，否则你将会付出惨痛的代价，只有在并发环境中，共享资源不支持并发访问，或者说并发访问共享资源会导致系统错误的情况下，才需要使用锁。    锁的一般用法：访问共享资源之前获取锁，成功获取锁则可以访问共享资源，最后释放锁以让其它线程继续访问共享资源\n  // Lock\rprivate Lock lock = new ReentrantLock();\rpublic void visitShareResWithLock() {\rlock.lock();\rtry {\r// 在这里安全的访问共享资源\r} finally {\rlock.unlock();\r}\r}\r // synchronized 关键字，效果与 Lock 一致\rprivate Object lock = new Object();\rpublic void visitShareResWithLock() {\rsynchronized (lock) {\r// 在这里安全的访问共享资源\r}\r}\r  需要注意的是：很多语言都有异常机制，当抛出异常的时候，不再执行后面的代码，如果在访问共享资源时抛出异常，那后面释放锁的代码就不会被执行，锁就无法被释放形成死锁。所以要考虑到代码可能走到的所有正常和异常的分支，确保所有情况下锁都能被释放。有些语言提供了 try-with 的机制，不需要显式地获取和释放锁，可以简化编程，有效避免这种问题，比如 Python。//TODO在 Java 中实现一个 try-with-lock 呢？  lock = threading.RLock()\rdef visitShareResWithLock():\rwith lock:\r# 注意缩进\r# 在这里安全的访问共享资源\r# 锁会在 with 代码段执行完成后自动释放\r 死锁  死锁：由于某种原因，锁一直没有释放，后续需要获取锁的线程都将处于等待锁的状态，这样程序就卡死了。 第一种原因就是我在刚刚讲的，获取了锁之后没有释放，有经验的程序员很少会犯这种错误，即使出现这种错误，也很容易通过查看代码找到 Bug。 锁的重入问题：如下代码，当前的线程获取到了锁 lock，然后在持有这把锁的情况下，再次去尝试获取这把锁，这样会导致死锁吗？答案是不一定，会不会死锁取决于，你获取的这把锁它是不是可重入锁，如果是可重入锁就不会死锁，否则会死锁。  大部分编程语言都提供了可重入锁，如果没有特别的要求，你要尽量使用可重入锁。既然已经获取到锁了，干嘛还要再次获取同一把锁呢？如果程序足够复杂，调用栈很深，很多情况下，当你需要获取一把锁的时候，是不太好判断在 n 层调用之外的某个地方，是不是已经获取过这把锁了，这个时候，获取可重入锁就有意义了。    public void visitShareResWithLock() {\rlock.lock(); // 获取锁\rtry {\rlock.lock(); // 再次获取锁，会导致死锁吗？\r} finally {\rlock.unlock();\r}\r  最后一种死锁的情况是最复杂的，也是最难解决的。如果程序中存在多把锁，就有可能出现这些锁互相锁住的情况  这里模拟了一个最简单最典型的死锁情况，在这个程序里面有两把锁：lockA 和 lockB，然后我们定义了两个线程，这两个线程反复地去获取这两把锁，然后释放。 这两个线程获取锁的顺序是不一样的。第一个线程，先获取 lockA，再获取 lockB，而第二个线程正好相反，先获取 lockB，再获取 lockA。死锁前的最后两行日志，线程 1 持有了 lockA，现在尝试获取 lockB，而线程 2 持有了 lockB，尝试获取 lockA。两个线程，各持有一把锁，都等着对方手里的另外一把锁，这样就僵持住了。这是最简单的两把锁两个线程死锁的情况，还可以分析清楚，但如果你的程序中有十几把锁，几十处加锁解锁，几百的线程，如果出现死锁你还能分析清楚是什么情况吗？    import threading\rdef func1(lockA, lockB):\rwhile True:\rprint(\u0026quot;Thread1: Try to accquire lockA...\u0026quot;)\rwith lockA:\rprint(\u0026quot;Thread1: lockA accquired. Try to accquire lockB...\u0026quot;)\rwith lockB:\rprint(\u0026quot;Thread1: Both lockA and LockB accrquired.\u0026quot;)\rdef func2(lockA, lockB):\rwhile True:\rprint(\u0026quot;Thread2: Try to accquire lockB...\u0026quot;)\rwith lockB:\rprint(\u0026quot;Thread2: lockB accquired. Try to accquire lockA...\u0026quot;)\rwith lockA:\rprint(\u0026quot;Thread2: Both lockA and LockB accrquired.\u0026quot;)\rif __name__ == '__main__':\rlockA = threading.RLock();\rlockB = threading.RLock()\rt1 = threading.Thread(target=func1, args=(lockA, lockB,))\rt2 = threading.Thread(target=func2, args=(lockA, lockB,))\rt1.start()\rt2.start()\r $ python3 DeadLock.py\rThread1: Try to accquire lockA...\rThread1: lockA accquired. Try to accquire lockB...\rThread1: Both lockA and LockB accrquired.\rThread1: Try to accquire lockA...\r... ...\rThread1: Try to accquire lockA...\rThread2: Try to accquire lockB...\rThread1: lockA accquired. Try to accquire lockB...\rThread2: lockB accquired. Try to accquire lockA...\r  避免死锁的几点建议。  避免滥用锁，程序里用的锁少，写出死锁 Bug 的几率自然就低。 对于同一把锁，加锁和解锁必须要放在同一个方法中，这样一次加锁对应一次解锁，代码清晰简单，便于分析问题。 尽量避免在持有一把锁的情况下，去获取另外一把锁，就是要尽量避免同时持有多把锁。 如果需要持有多把锁，一定要注意加解锁的顺序，解锁的顺序要和加锁顺序相反。比如，获取三把锁的顺序是 A、B、C，释放锁的顺序必须是 C、B、A。 给你程序中所有的锁排一个顺序，在所有需要加锁的地方，按照同样的顺序加解锁。比如我刚刚举的那个例子，如果两个线程都按照先获取 lockA 再获取 lockB 的顺序加锁，就不会产生死锁    读写锁兼顾性能与安全  对于共享数据来说，如果说某个方法在访问它的时候，只读，并不更新，也是需要加锁的，因为如果一个线程读数据的同时，另外一个线程同时在更新数据，那么你读到的数据有可能是更新到一半的数据，这肯定是不符合预期的。所以，无论是只读访问，还是读写访问，都是需要加锁的。如果给数据简单地加一把锁，虽然解决了安全性的问题，但是牺牲了性能，因为，那无论读还是写，都无法并发了，跟单线程的程序性能是一样。实际上，如果没有线程在更新数据，那即使多个线程都在并发读，也是没有问题的。大部分情况下，数据的读写比是不均衡的，读要远远多于写，所以，我们希望的是：读可以并发执行，写的同时不能并发读，也不能并发写。这样就兼顾了性能和安全性。 读写锁就是为这一需求设计的。我们来看一下 Java 中提供的读写锁：在这段代码中，需要读数据的时候，我们获取读锁，获取到的读锁不是一个互斥锁，也就是说 read() 方法是可以多个线程并行执行的，这样使得读数据的性能依然很好。写数据的时候，我们获取写锁，当一个线程持有写锁的时候，其他线程既无法获取读锁，也不能获取写锁，达到保护共享数据的目的。这样使用读写锁就兼顾了性能和安全。  ReadWriteLock rwlock = new ReentrantReadWriteLock();\rpublic void read() {\rrwlock.readLock().lock();\rtry {\r// 在这儿读取共享数据\r} finally {\rrwlock.readLock().unlock();\r}\r}\rpublic void write() {\rrwlock.writeLock().lock();\rtry {\r// 在这儿更新共享数据\r} finally {\rrwlock.writeLock().unlock();\r}\r}\r 硬件同步原语  硬件同步原语（Atomic Hardware Primitives）是由计算机硬件提供的一组原子操作，比较常用的原语主要是 CAS 和 FAA，他们在各种编程语言中，都有相应的实现 下面用代码表示的AHP，肯定是无法保证原子性的。而原语的特殊之处就是，它们都是由计算机硬件，具体说就是 CPU 提供的实现，可以保证操作的原子性。原子操作具有不可分割性，也就不存在并发的问题  CAS   CAS：Compare and Swap，比较并交换\n  这里输入参数一共有三个，分别是：\n p: 要修改的变量的指针。 old: 旧值。 new: 新值。    返回的是一个布尔值，标识是否赋值成功。逻辑，非常简单，就是先比较一下变量 p 当前的值是不是等于 old，如果等于，那就把变量 p 赋值为 new，并返回 true，否则就不改变变量 p，并返回 false。这是 CAS 这个原语的语义\n  // CAS 的实现伪代码\r\u0026lt;\u0026lt; atomic \u0026gt;\u0026gt;\rfunction cas(p : pointer to int, old : int, new : int) returns bool {\rif *p ≠ old {\rreturn false\r}\r*p ← new\rreturn true\r}\r FAA  FAA 原语（Fetch and Add）：FAA 原语的语义是，先获取变量 p 当前的值 value，然后给变量 p 增加 inc，最后返回变量 p 之前的值 value。  \u0026lt;\u0026lt; atomic \u0026gt;\u0026gt;\rfunction faa(p : pointer to int, inc : int) returns int {\rint value \u0026lt;- *location\r*p \u0026lt;- value + inc\rreturn value\r}\r CAS 替代锁  某些情况下，可以使用硬件同步原语来替代锁，保证和锁一样的数据安全性，同时具有更好的并发性能。 在今年的 NSDI（NSDI 是 USENIX 组织开办的关于网络系统设计的著名学术会议）上，伯克利大学发表了一篇论文《Confluo: Distributed Monitoring and Diagnosis Stack for High-speed Networks》，这个论文中提到的 Confluo，也是一个类似于消息队列的流数据存储，它的吞吐量号称是 Kafka 的 4～10 倍。对于这个实验结论我个人不是很认同，因为它设计的实验条件对 Kafka 来说不太公平。但不可否认的是，Confluo 它的这个设计思路是一个创新，并且实际上它的性能也非常好。Confluo 是如何做到这么高的吞吐量的呢？这里面非常重要的一个创新的设计就是，它使用硬件同步原语来代替锁，在一个日志上（你可以理解为消息队列中的一个队列或者分区），保证严格顺序的前提下，实现了多线程并发写入。  CAS 版账户服务  假设我们有一个共享变量 balance，它保存的是当前账户余额，然后我们模拟多个线程并发转账的情况，看一下如何使用 CAS 原语来保证数据的安全性 这个例子中，我们让账户的初始值为 0，然后启动多个协程来并发执行 10000 次转账，每次往账户中转入 1 元，全部转账执行完成后，账户中的余额应该正好是 10000 元。反复多次执行，每次 balance 的结果都应该正好是 10000  // 锁实现\rpackage main\rimport (\r\u0026quot;fmt\u0026quot;\r\u0026quot;sync\u0026quot;\r)\rfunc main() {\r// 账户初始值为 0 元\rvar balance int32\rbalance = int32(0)\rdone := make(chan bool)\r// 执行 10000 次转账，每次转入 1 元\rcount := 10000\rvar lock sync.Mutex\rfor i := 0; i \u0026lt; count; i++ {\r// 这里模拟异步并发转账\rgo transfer(\u0026amp;balance, 1, done, \u0026amp;lock)\r}\r// 等待所有转账都完成\rfor i := 0; i \u0026lt; count; i++ {\r\u0026lt;-done\r}\r// 打印账户余额\rfmt.Printf(\u0026quot;balance = %d \\n\u0026quot;, balance)\r}\r// 转账服务\rfunc transfer(balance *int32, amount int, done chan bool, lock *sync.Mutex) {\rlock.Lock()\r*balance = *balance + int32(amount)\rlock.Unlock()\rdone \u0026lt;- true\r}\r // CAS 实现\rfunc transferCas(balance *int32, amount int, done chan bool) {\rfor {\rold := atomic.LoadInt32(balance)\rnew := old + int32(amount)\rif atomic.CompareAndSwapInt32(balance, old, new) {\rbreak\r}\r}\rdone \u0026lt;- true\r}\r // FAA 实现\rfunc transferFaa(balance *int32, amount int, done chan bool) {\ratomic.AddInt32(balance, int32(amount))\rdone \u0026lt;- true\r}\r  CAS 和锁实现的程序，总体结构是一样的，主要的区别在于，“异步给账户余额 +1”这一小块儿代码的实现。  锁实现，需要先获取锁，然后变更账户的值，最后释放锁，完成一次转账 CAS 实现：CAS 原语 + 反复重试的方式来保证数据安全。用 for 来做了一个没有退出条件的循环。在这个循环的内部，反复地调用 CAS 原语，来尝试给账户的余额 +1。先取得账户当前的余额，暂时存放在变量 old 中，再计算转账之后的余额，保存在变量 new 中，然后调用 CAS 原语来尝试给变量 balance 赋值。我们刚刚讲过，CAS 原语它的赋值操作是有前置条件的，只有变量 balance 的值等于 old 时，才会将 balance 赋值为 new  在 for 循环中执行了 3 条语句，在并发的环境中执行，这里面会有两种可能情况：  一种情况是，执行到第 3 条 CAS 原语时，没有其他线程同时改变了账户余额，那我们是可以安全变更账户余额的，这个时候执行 CAS 的返回值一定是 true，转账成功，就可以退出循环了。并且，CAS 这一条语句，它是一个原子操作，赋值的安全性是可以保证的。 另外一种情况，那就是在这个过程中，有其他线程改变了账户余额，这个时候是无法保证数据安全的，不能再进行赋值。执行 CAS 原语时，由于无法通过比较的步骤，所以不会执行赋值操作。本次尝试转账失败，当前线程并没有对账户余额做任何变更。由于返回值为 false，不会退出循环，所以会继续重试，直到转账成功退出循环。   这样每一次转账操作，都可以通过若干次重试，在保证安全性的前提下，完成并发转账操作 上面使用 CAS 原语反复重试赋值的方法，它是比较耗费 CPU 资源的，因为在 for 循环中，如果赋值不成功，是会立即进入下一次循环没有等待的。如果线程之间的碰撞非常频繁，经常性的反复重试，这个重试的线程会占用大量的 CPU 时间，随之系统的整体性能就会下降。缓解这个问题的一个方法是使用 Yield()， 大部分编程语言都支持 Yield() 这个系统调用，Yield() 的作用是，告诉操作系统，让出当前线程占用的 CPU 给其他线程使用。每次循环结束前调用一下 Yield() 方法，可以在一定程度上减少 CPU 的使用率，缓解这个问题。你也可以在每次循环结束之后，Sleep() 一小段时间，但是这样做的代价是，性能会严重下降。所以，这种方法它只适合于线程之间碰撞不太频繁，也就是说绝大部分情况下，执行 CAS 原语不需要重试这样的场景。 相比FAA，CAS适用范围更加广泛一些。类似于这样的逻辑：先读取数据，做计算，然后更新数据，无论这个计算是什么样的，都可以使用 CAS 原语来保护数据安全，但是 FAA 原语，这个计算的逻辑只能局限于简单的加减法。   FAA：它的操作是，获取变量当前的值，然后把它做一个加法，并且保证这个操作的原子性，一行代码就可以搞定了。相比CAS，FAA更简单、性能更好，这个例子里面，肯定是使用 FAA 原语更合适   //TODO账户服务这个例子，用你熟悉的语言，用锁、CAS 和 FAA 这三种方法，都完整地实现一遍。每种实现方法都要求是完整的，可以执行的程序。  数据压缩时间换空间  数据压缩：节省存储空间，提升网络传输效率 网络传输数据时间：传输不压缩数据时间；压缩时间+传输已压缩数据时间+解压时间。有数据压缩率、网络带宽、收发两端服务器的繁忙程度等诸多影响因素，属于CPU资源换存储资源或网络资源的行为 使用场景：压缩和解压的操作都是计算密集型的操作，非常耗费 CPU 资源。如果你的系统的瓶颈是磁盘的 IO 性能，CPU 资源又很闲，这种情况就非常适合在把数据写入磁盘前先进行压缩；如果你的应用处理业务逻辑就需要耗费大量的 CPU 资源，就不太适合再进行压缩和解压；如果你的系统读写比严重不均衡，你还要考虑，每读一次数据就要解压一次是不是划算。  压缩算法  分类  有损压缩：一般用来压缩音视频，压缩之后会丢失信息 无损压缩：数据经过压缩和解压过程后，与压缩前相比是 100% 相同的。   是什么：简单举例00000000000000000000→20 个 0，20 个字符被压缩成 4 个字符，并可以无损还原，虽然这个压缩算法没什么实用性，但与其它压缩算法本质没什么区别 选择：目前常用的压缩算法有ZIP，GZIP，SNAPPY，LZ4等。有兴趣，可以选择学习了解最经典的压缩算，法哈夫曼编码（霍夫曼编码，Huffman Coding）。  主要需要考虑数据的压缩率和压缩耗时，一般压缩率越高压缩耗时也越高。如果是对性能要求高的系统，可以选择压缩速度快的算法，比如 LZ4；如果需要更高的压缩比，可以考虑 GZIP 或者压缩率更高的 XZ 等算法 压缩样本对压缩速度和压缩比的影响也是比较大的，同样大小的一段数字和一段新闻的文本，即使是使用相同的压缩算法，压缩率和压缩时间的差异也是比较大的。所以有时在选择压缩算法之前，用系统的样例业务数据做一个测试，可以帮助找到最合适的压缩算法    压缩分段  大部分的压缩算法的区别主要是，对数据进行编码的算法，压缩的流程和压缩包的结构大致一样的。而在压缩过程中，最需要了解的就是如何选择合适的压缩分段大小。 压缩分段：在压缩时，给定的被压缩数据它必须有确定的长度，或者说，是有头有尾的，不能是一个无限的数据流，如果要对流数据进行压缩，那必须把流数据划分成多个帧，一帧一帧的分段压缩  主要原因是，压缩算法在开始压缩之前，一般都需要对被压缩数据从头到尾进行一次扫描，扫描的目的是确定如何对数据进行划分和编码，一般的原则是重复次数多、占用空间大的内容，使用尽量短的编码，这样压缩率会更高 另外，被压缩的数据长度越大，重码率会更高，压缩比也就越高。这个很好理解，比如我们这篇文章，可能出现了几十次“压缩”这个词，如果将整篇文章压缩，这个词的重复率是几十次，但如果我们按照每个自然段来压缩，那每段中这个词的重复率只有二三次。显然全文压缩的压缩率肯定高于分段压缩。 分段也不是越大越好，实际上分段大小超过一定长度之后，再增加长度对压缩率的贡献就不太大了，这是一个原因。另外，过大的分段长度，在解压缩的时候，会有更多的解压浪费。比如，一个 1MB 大小的压缩文件，即使你只是需要读其中很短的几个字节，也不得不把整个文件全部解压缩，造成很大的解压浪费。   需要根据你的业务，选择合适的压缩分段，在压缩率、压缩速度和解压浪费之间找到一个合适的平衡。 确定了如何对数据进行划分和压缩算法之后，就可以进行压缩了，压缩的过程就是用编码来替换原始数据的过程。压缩之后的压缩包就是由这个编码字典和用编码替换之后的数据组成的。这就是数据压缩的过程。解压的时候，先读取编码字典，然后按照字典把压缩编码还原成原始的数据就可以了。  Kafka 的消息压缩处理  Kafka 是否开启压缩，这是可以配置，它也支持配置使用哪一种压缩算法。原因我们在上面说过，不同的业务场景是否需要开启压缩，选择哪种压缩算法是不能一概而论的。所以，Kafka 的设计者把这个选择权交给使用者。 开启压缩时，Kafka 选择一批消息一起压缩，每一个批消息就是一个压缩分段。使用者也可以通过参数来控制每批消息的大小。 在服务端也不会对这批消息进行解压，可以整批直接存储，然后整批发送给消费者。最后，批消息由消费者进行解压。在服务端不用解压，就不会耗费服务端宝贵的 CPU 资源，同时还能获得压缩后，占用传输带宽小，占用存储空间小的这些好处 使用 Kafka 时，如果生产者和消费者的 CPU 资源不是特别吃紧，开启压缩后，可以节省网络带宽和服务端的存储空间，提升总体的吞吐量，一般都是个不错的选择。 //TODO看一下 RocketMQ 的文档或者源代码，看一下，RocketMQ 是怎么处理消息压缩的。和 Kafka 的压缩方式对比一下，想一想哪种处理方式更适合你的系统？  实现 RocketMQ Producer源码分析：消息生产的实现过程 对于消息队列来说，它最核心的功能就是收发消息。也就是消息生产和消费这两个流程。我们在之前的课程中提到了消息队列一些常见问题，比如，“如何保证消息不会丢失？”“为什么会收到重复消息？”“消费时为什么要先执行消费业务逻辑再确认消费？”，针对这些问题，我讲过它们的实现原理，这些最终落地到代码上，都包含在这一收一发两个流程中。\n在接下来的两节课中，我会带你一起通过分析源码的方式，详细学习一下这两个流程到底是如何实现的。你在日常使用消息队列的时候，遇到的大部分问题，更多的是跟 Producer 和 Consumer，也就是消息队列的客户端，关联更紧密。搞清楚客户端的实现原理和代码中的细节，将对你日后使用消息队列时进行问题排查有非常大的帮助。所以，我们这两节课的重点，也将放在分析客户端的源代码上。\n秉着先易后难的原则，我们选择代码风格比较简明易懂的 RocketMQ 作为分析对象。一起分析 RocketMQ 的 Producer 的源代码，学习消息生产的实现过程。\n在分析源代码的过程中，我们的首要目的就是搞清楚功能的实现原理，另外，最好能有敏锐的嗅觉，善于发现代码中优秀的设计和巧妙构思，学习、总结并记住这些方法。在日常开发中，再遇到类似场景，你就可以直接拿来使用。\n我们使用当前最新的 release 版本 release-4.5.1 进行分析，使用 Git 在 GitHub 上直接下载源码到本地：\ngit clone git@github.com:apache/rocketmq.git\rcd rocketmq\rgit checkout release-4.5.1\r 客户端是一个单独的 Module，在 rocketmq/client 目录中。\n从单元测试看 Producer API 的使用 在专栏之前的课程《09 | 学习开源代码该如何入手？》中我和你讲过，不建议你从 main() 方法入手去分析源码，而是带着问题去分析。我们本节课的问题是非常清晰的，就是要搞清楚 Producer 是如何发消息的。带着这个问题，接下来我们该如何分析源码呢？\n我的建议是，先看一下单元测试用例。因为，一般单元测试中，每一个用例就是测试代码中的一个局部或者说是一个小流程。那对于一些比较完善的开源软件，它们的单元测试覆盖率都非常高，很容易找到我们关心的那个流程所对应的测试用例。我们的源码分析，就可以从这些测试用例入手，一步一步跟踪其方法调用链路，理清实现过程。\n首先我们先分析一下 RocketMQ 客户端的单元测试，看看 Producer 提供哪些 API，更重要的是了解这些 API 应该如何使用。\nProducer 的所有测试用例都在同一个测试类\u0026quot;org.apache.rocketmq.client.producer.DefaultMQProducerTest\u0026quot;中，看一下这个测试类中的所有单元测试方法，大致可以了解到 Producer 的主要功能。\n这个测试类的主要测试方法如下：\n init terminate testSendMessage_ZeroMessage testSendMessage_NoNameSrv testSendMessage_NoRoute testSendMessageSync_Success testSendMessageSync_WithBodyCompressed testSendMessageAsync_Success testSendMessageAsync testSendMessageAsync_BodyCompressed testSendMessageSync_SuccessWithHook\n 其中 init 和 terminate 是测试开始初始化和测试结束销毁时需要执行的代码，其他以 testSendMessage 开头的方法都是在各种情况和各种场景下发送消息的测试用例，通过这些用例的名字，你可以大致看出测试的功能。\n比如，testSendMessageSync 和 testSendMessageAsync 分别是测试同步发送和异步发送的用例，testSendMessageSync_WithBodyCompressed 是压缩消息发送的测试用例，等等。\n像 RocketMQ 这种开源项目，前期花费大量时间去编写测试用例，看似浪费时间，实际上会节省非常多后期联调测试、集成测试、以及上线后出现问题解决问题的时间，并且能够有效降低线上故障的概率，总体来说是非常划算的。强烈建议你在日常进行开发的过程中，也多写一些测试用例，尽量把单元测试的覆盖率做到 50% 以上。\nRockectMQ 的 Producer 入口类为“org.apache.rocketmq.client.producer.DefaultMQProducer”，大致浏览一下代码和类的继承关系，我整理出 Producer 相关的几个核心类和接口如下：\n这里面 RocketMQ 使用了一个设计模式：门面模式（Facade Pattern）。\n 门面模式主要的作用是给客户端提供了一个可以访问系统的接口，隐藏系统内部的复杂性。\n 接口 MQProducer 就是这个模式中的门面，客户端只要使用这个接口就可以访问 Producer 实现消息发送的相关功能，从使用层面上来说，不必再与其他复杂的实现类打交道了。\n类 DefaultMQProducer 实现了接口 MQProducer，它里面的方法实现大多没有任何的业务逻辑，只是封装了对其他实现类的方法调用，也可以理解为是门面的一部分。Producer 的大部分业务逻辑的实现都在类 DefaultMQProducerImpl 中，这个类我们会在后面重点分析其实现。\n有的时候，我们的实现分散在很多的内部类中，不方便用接口来对外提供服务，你就可以仿照 RocketMQ 的这种方式，使用门面模式来隐藏内部实现，对外提供服务。\n接口 MQAdmin 定义了一些元数据管理的方法，在消息发送过程中会用到。\n启动过程 通过单元测试中的代码可以看到，在 init() 和 terminate() 这两个测试方法中，分别执行了 Producer 的 start 和 shutdown 方法，说明在 RocketMQ 中，Producer 是一个有状态的服务，在发送消息之前需要先启动 Producer。这个启动过程，实际上就是为了发消息做的准备工作，所以，在分析发消息流程之前，我们需要先理清 Producer 中维护了哪些状态，在启动过程中，Producer 都做了哪些初始化的工作。有了这个基础才能分析其发消息的实现流程。\n首先从测试用例的方法 init() 入手：\n @Before\rpublic void init() throws Exception {\rString producerGroupTemp = producerGroupPrefix + System.currentTimeMillis();\rproducer = new DefaultMQProducer(producerGroupTemp);\rproducer.setNamesrvAddr(\u0026quot;127.0.0.1:9876\u0026quot;);\rproducer.setCompressMsgBodyOverHowmuch(16);\r// 省略构造测试消息的代码\rproducer.start();\r// 省略用于测试构造 mock 的代码\r}\r 这段初始化代码的逻辑非常简单，就是创建了一个 DefaultMQProducer 的实例，为它初始化一些参数，然后调用 start 方法启动它。接下来我们跟进 start 方法的实现，继续分析其初始化过程。\nDefaultMQProducer#start() 方法中直接调用了 DefaultMQProducerImpl#start() 方法，我们直接来看这个方法的代码：\npublic void start(final boolean startFactory) throws MQClientException {\rswitch (this.serviceState) {\rcase CREATE_JUST:\rthis.serviceState = ServiceState.START_FAILED;\r// 省略参数检查和异常情况处理的代码\r// 获取 MQClientInstance 的实例 mQClientFactory，没有则自动创建新的实例\rthis.mQClientFactory = MQClientManager.getInstance().getAndCreateMQClientInstance(this.defaultMQProducer, rpcHook);\r// 在 mQClientFactory 中注册自己\rboolean registerOK = mQClientFactory.registerProducer(this.defaultMQProducer.getProducerGroup(), this);\r// 省略异常处理代码\r// 启动 mQClientFactory\rif (startFactory) {\rmQClientFactory.start();\r}\rthis.serviceState = ServiceState.RUNNING;\rbreak;\rcase RUNNING:\rcase START_FAILED:\rcase SHUTDOWN_ALREADY:\r// 省略异常处理代码\rdefault:\rbreak;\r}\r// 给所有 Broker 发送心跳\rthis.mQClientFactory.sendHeartbeatToAllBrokerWithLock();\r}\r 这里面，RocketMQ 使用一个成员变量 serviceState 来记录和管理自身的服务状态，这实际上是状态模式 (State Pattern) 这种设计模式的变种实现。\n 状态模式允许一个对象在其内部状态改变时改变它的行为，对象看起来就像是改变了它的类。\n 与标准的状态模式不同的是，它没有使用状态子类，而是使用分支流程（switch-case）来实现不同状态下的不同行为，在管理比较简单的状态时，使用这种设计会让代码更加简洁。这种模式非常广泛地用于管理有状态的类，推荐你在日常开发中使用。\n在设计状态的时候，有两个要点是需要注意的，第一是，不仅要设计正常的状态，还要设计中间状态和异常状态，否则，一旦系统出现异常，你的状态就不准确了，你也就很难处理这种异常状态。比如在这段代码中，RUNNING 和 SHUTDOWN_ALREADY 是正常状态，CREATE_JUST 是一个中间状态，START_FAILED 是一个异常状态。\n第二个要点是，将这些状态之间的转换路径考虑清楚，并在进行状态转换的时候，检查上一个状态是否能转换到下一个状态。比如，在这里，只有处于 CREATE_JUST 状态才能转换为 RUNNING 状态，这样就可以确保这个服务是一次性的，只能启动一次。从而避免了多次启动服务而导致的各种问题。\n接下来看一下启动过程的实现：\n 通过一个单例模式（Singleton Pattern）的 MQClientManager 获取 MQClientInstance 的实例 mQClientFactory，没有则自动创建新的实例； 在 mQClientFactory 中注册自己； 启动 mQClientFactory； 给所有 Broker 发送心跳。  这里面又使用了一个最简单的设计模式：单例模式。我们在这儿给出单例模式的定义，不再详细说明了，不会的同学需要自我反省一下，然后赶紧去复习设计模式基础去。\n 单例模式涉及一个单一的类，该类负责创建自己的对象，同时确保只有单个对象被创建。这个类提供了一种访问其唯一的对象的方式，可以直接访问，不需要实例化该类的对象。\n 其中实例 mQClientFactory 对应的类 MQClientInstance 是 RocketMQ 客户端中的顶层类，大多数情况下，可以简单地理解为每个客户端对应类 MQClientInstance 的一个实例。这个实例维护着客户端的大部分状态信息，以及所有的 Producer、Consumer 和各种服务的实例，想要学习客户端整体结构的同学可以从分析这个类入手，逐步细化分析下去。\n我们进一步分析一下 MQClientInstance#start() 中的代码：\n// 启动请求响应通道\rthis.mQClientAPIImpl.start();\r// 启动各种定时任务\rthis.startScheduledTask();\r// 启动拉消息服务\rthis.pullMessageService.start();\r// 启动 Rebalance 服务\rthis.rebalanceService.start();\r// 启动 Producer 服务\rthis.defaultMQProducer.getDefaultMQProducerImpl().start(false);\r 这一部分代码的注释比较清楚，流程是这样的：\n 启动实例 mQClientAPIImpl，其中 mQClientAPIImpl 是类 MQClientAPIImpl 的实例，封装了客户端与 Broker 通信的方法； 启动各种定时任务，包括与 Broker 之间的定时心跳，定时与 NameServer 同步数据等任务； 启动拉取消息服务； 启动 Rebalance 服务； 启动默认的 Producer 服务。  以上是 Producer 的启动流程。这里面有几个重要的类，你需要清楚它们的各自的职责。后续你在使用 RocketMQ 时，如果遇到问题需要调试代码，了解这几个重要类的职责会对你有非常大的帮助。\n DefaultMQProducerImpl：Producer 的内部实现类，大部分 Producer 的业务逻辑，也就是发消息的逻辑，都在这个类中。 MQClientInstance：这个类中封装了客户端一些通用的业务逻辑，无论是 Producer 还是 Consumer，最终需要与服务端交互时，都需要调用这个类中的方法； MQClientAPIImpl：这个类中封装了客户端服务端的 RPC，对调用者隐藏了真正网络通信部分的具体实现； NettyRemotingClient：RocketMQ 各进程之间网络通信的底层实现类。  消息发送过程 接下来我们一起分析 Producer 发送消息的流程。\n在 Producer 的接口 MQProducer 中，定义了 19 个不同参数的发消息的方法，按照发送方式不同可以分成三类：\n 单向发送（Oneway）：发送消息后立即返回，不处理响应，不关心是否发送成功； 同步发送（Sync）：发送消息后等待响应； 异步发送（Async）：发送消息后立即返回，在提供的回调方法中处理响应。  这三类发送实现基本上是相同的，异步发送稍微有一点儿区别，我们看一下异步发送的实现方法\u0026quot;DefaultMQProducerImpl#send()\u0026quot;（对应源码中的 1132 行）：\n@Deprecated\rpublic void send(final Message msg, final MessageQueueSelector selector, final Object arg, final SendCallback sendCallback, final long timeout)\rthrows MQClientException, RemotingException, InterruptedException {\rfinal long beginStartTime = System.currentTimeMillis();\rExecutorService executor = this.getAsyncSenderExecutor();\rtry {\rexecutor.submit(new Runnable() {\r@Override\rpublic void run() {\rlong costTime = System.currentTimeMillis() - beginStartTime;\rif (timeout \u0026gt; costTime) {\rtry {\rtry {\rsendSelectImpl(msg, selector, arg, CommunicationMode.ASYNC, sendCallback,\rtimeout - costTime);\r} catch (MQBrokerException e) {\rthrow new MQClientException(\u0026quot;unknownn exception\u0026quot;, e);\r}\r} catch (Exception e) {\rsendCallback.onException(e);\r}\r} else {\rsendCallback.onException(new RemotingTooMuchRequestException(\u0026quot;call timeout\u0026quot;));\r}\r}\r});\r} catch (RejectedExecutionException e) {\rthrow new MQClientException(\u0026quot;exector rejected \u0026quot;, e);\r}\r}\r 我们可以看到，RocketMQ 使用了一个 ExecutorService 来实现异步发送：使用 asyncSenderExecutor 的线程池，异步调用方法 sendSelectImpl()，继续发送消息的后续工作，当前线程把发送任务提交给 asyncSenderExecutor 就可以返回了。单向发送和同步发送的实现则是直接在当前线程中调用方法 sendSelectImpl()。\n我们来继续看方法 sendSelectImpl() 的实现：\n// 省略部分代码\rMessageQueue mq = null;\r// 选择将消息发送到哪个队列（Queue）中\rtry {\rList\u0026lt;MessageQueue\u0026gt; messageQueueList =\rmQClientFactory.getMQAdminImpl().parsePublishMessageQueues(topicPublishInfo.getMessageQueueList());\rMessage userMessage = MessageAccessor.cloneMessage(msg);\rString userTopic = NamespaceUtil.withoutNamespace(userMessage.getTopic(), mQClientFactory.getClientConfig().getNamespace());\ruserMessage.setTopic(userTopic);\rmq = mQClientFactory.getClientConfig().queueWithNamespace(selector.select(messageQueueList, userMessage, arg));\r} catch (Throwable e) {\rthrow new MQClientException(\u0026quot;select message queue throwed exception.\u0026quot;, e);\r}\r// 省略部分代码\r// 发送消息\rif (mq != null) {\rreturn this.sendKernelImpl(msg, mq, communicationMode, sendCallback, null, timeout - costTime);\r} else {\rthrow new MQClientException(\u0026quot;select message queue return null.\u0026quot;, null);\r}\r// 省略部分代码\r 方法 sendSelectImpl() 中主要的功能就是选定要发送的队列，然后调用方法 sendKernelImpl() 发送消息。\n选择哪个队列发送由 MessageQueueSelector#select 方法决定。在这里 RocketMQ 使用了策略模式（Strategy Pattern），来解决不同场景下需要使用不同的队列选择算法问题。\n 策略模式：定义一系列算法，将每一个算法封装起来，并让它们可以相互替换。策略模式让算法独立于使用它的客户而变化。\n RocketMQ 提供了很多 MessageQueueSelector 的实现，例如随机选择策略，哈希选择策略和同机房选择策略等，如果需要，你也可以自己实现选择策略。之前我们的课程中提到过，如果要保证相同 key 消息的严格顺序，你需要使用哈希选择策略，或者提供一个自己实现的选择策略。\n接下来我们再看一下方法 sendKernelImpl()。这个方法的代码非常多，大约有 200 行，但逻辑比较简单，主要功能就是构建发送消息的头 RequestHeader 和上下文 SendMessageContext，然后调用方法 MQClientAPIImpl#sendMessage()，将消息发送给队列所在的 Broker。\n至此，消息被发送给远程调用的封装类 MQClientAPIImpl，完成后续序列化和网络传输等步骤。\n可以看到，RocketMQ 的 Producer 整个发消息的流程，无论是同步发送还是异步发送，都统一到了同一个流程中。包括异步发送消息的实现，实际上也是通过一个线程池，在异步线程执行的调用和同步发送相同的底层方法来实现的。\n在底层方法的代码中，依靠方法的一个参数来区分同步还是异步发送。这样实现的好处是，整个流程是统一的，很多同步异步共同的逻辑，代码可以复用，并且代码结构清晰简单，便于维护。\n使用同步发送的时候，当前线程会阻塞等待服务端的响应，直到收到响应或者超时方法才会返回，所以在业务代码调用同步发送的时候，只要返回成功，消息就一定发送成功了。异步发送的时候，发送的逻辑都是在 Executor 的异步线程中执行的，所以不会阻塞当前线程，当服务端返回响应或者超时之后，Producer 会调用 Callback 方法来给业务代码返回结果。业务代码需要在 Callback 中来判断发送结果。这和我们在之前的课程《05 | 如何确保消息不会丢失？》讲到的发送流程是完全一样的。\n小结 这节课我带你分析了 RocketMQ 客户端消息生产的实现过程，包括 Producer 初始化和发送消息的主流程。Producer 中包含的几个核心的服务都是有状态的，在 Producer 启动时，在 MQClientInstance 这个类中来统一来启动。在发送消息的流程中，RocketMQ 分了三种发送方式：单向、同步和异步，这三种发送方式对应的发送流程基本是相同的，同步和异步发送是由已经封装好的 MQClientAPIImpl 类来分别实现的。\n对于我们在分析代码中提到的几个重要的业务逻辑实现类，你最好能记住这几个类和它的功能，包括 ：DefaultMQProducerImpl 封装了大部分 Producer 的业务逻辑，MQClientInstance 封装了客户端一些通用的业务逻辑，MQClientAPIImpl 封装了客户端与服务端的 RPC，NettyRemotingClient 实现了底层网络通信。\n我在课程中，只能带你把主干流程分析清楚，但是很多细节并没有涉及，课后请你一定要按照流程把源代码仔细看一遍，仔细消化一下没有提及到的分支流程，将这两个流程绘制成详细的流程图或者时序图。\n分析过程中提到的几个设计模式，是非常实用且常用的设计模式，希望你能充分理解并熟练运用。\n思考 你有没有注意到，在源码中，异步发送消息方法 DefaultMQProducerImpl#send()(1132 行) 被开发者加了 @Deprecated（弃用）注解，显然开发者也意识到了这种异步的实现存在一些问题，需要改进。请你结合我们专栏文章《10 | 如何使用异步设计提升系统性能？》中讲到的异步设计方法想一想，应该如何改进这个异步发送的流程？\nKafka Consumer源码分析：消息消费的实现过程 我们在上节课中提到过，用于解决消息队列一些常见问题的知识和原理，最终落地到代码上，都包含在收、发消息这两个流程中。对于消息队列的生产和消费这两个核心流程，在大部分消息队列中，它实现的主要流程都是一样的，所以，通过这两节课的学习之后，掌握了这两个流程的实现过程。无论你使用的是哪种消息队列，遇到收发消息的问题，你都可以用同样的思路去分析和解决问题。\n上一节课我和你一起通过分析源代码学习了 RocketMQ 消息生产的实现过程，本节课我们来看一下 Kafka 消费者的源代码，理清 Kafka 消费的实现过程，并且能从中学习到一些 Kafka 的优秀设计思路和编码技巧。\n在开始分析源码之前，我们一起来回顾一下 Kafka 消费模型的几个要点：\n Kafka 的每个 Consumer（消费者）实例属于一个 ConsumerGroup（消费组）； 在消费时，ConsumerGroup 中的每个 Consumer 独占一个或多个 Partition（分区）； 对于每个 ConsumerGroup，在任意时刻，每个 Partition 至多有 1 个 Consumer 在消费； 每个 ConsumerGroup 都有一个 Coordinator(协调者）负责分配 Consumer 和 Partition 的对应关系，当 Partition 或是 Consumer 发生变更是，会触发 reblance（重新分配）过程，重新分配 Consumer 与 Partition 的对应关系； Consumer 维护与 Coordinator 之间的心跳，这样 Coordinator 就能感知到 Consumer 的状态，在 Consumer 故障的时候及时触发 rebalance。  掌握并理解 Kafka 的消费模型，对于接下来理解其消费的实现过程是至关重要的，如果你对上面的这些要点还有不清楚的地方，建议回顾一下之前的课程或者看一下 Kafka 相关的文档，然后再继续接下来的内容。\n我们使用当前最新的版本 2.2 进行分析，使用 Git 在 GitHub 上直接下载源码到本地：\ngit clone git@github.com:apache/kafka.git\rcd kafka\rgit checkout 2.2\r 在《09 | 学习开源代码该如何入手？》这节课中，我讲过，分析国外源码最好的方式就是从文档入手，接下来我们就找一下 Kafka 的文档，看看从哪儿来入手开启我们的分析流程。\nKafka 的 Consumer 入口类KafkaConsumer 的 JavaDoc，给出了关于如何使用 KafkaConsumer 非常详细的说明文档，并且给出了一个使用 Consumer 消费的最简代码示例：\n // 设置必要的配置信息\rProperties props = new Properties();\rprops.put(\u0026quot;bootstrap.servers\u0026quot;, \u0026quot;localhost:9092\u0026quot;);\rprops.put(\u0026quot;group.id\u0026quot;, \u0026quot;test\u0026quot;);\rprops.put(\u0026quot;enable.auto.commit\u0026quot;, \u0026quot;true\u0026quot;);\rprops.put(\u0026quot;auto.commit.interval.ms\u0026quot;, \u0026quot;1000\u0026quot;);\rprops.put(\u0026quot;key.deserializer\u0026quot;, \u0026quot;org.apache.kafka.common.serialization.StringDeserializer\u0026quot;);\rprops.put(\u0026quot;value.deserializer\u0026quot;, \u0026quot;org.apache.kafka.common.serialization.StringDeserializer\u0026quot;);\r// 创建 Consumer 实例\rKafkaConsumer\u0026lt;String, String\u0026gt; consumer = new KafkaConsumer\u0026lt;\u0026gt;(props);\r// 订阅 Topic\rconsumer.subscribe(Arrays.asList(\u0026quot;foo\u0026quot;, \u0026quot;bar\u0026quot;));\r// 循环拉消息\rwhile (true) {\rConsumerRecords\u0026lt;String, String\u0026gt; records = consumer.poll(100);\rfor (ConsumerRecord\u0026lt;String, String\u0026gt; record : records)\rSystem.out.printf(\u0026quot;offset = %d, key = %s, value = %s%n\u0026quot;, record.offset(), record.key(), record.value());\r}\r 这段代码主要的主要流程是：\n 设置必要的配置信息，包括：起始连接的 Broker 地址，Consumer Group 的 ID，自动提交消费位置的配置和序列化配置； 创建 Consumer 实例； 订阅了 2 个 Topic：foo 和 bar； 循环拉取消息并打印在控制台上。  通过上面的代码实例我们可以看到，消费这个大的流程，在 Kafka 中实际上是被分成了“订阅”和“拉取消息”这两个小的流程。另外，我在之前的课程中反复提到过，Kafka 在消费过程中，每个 Consumer 实例是绑定到一个分区上的，那 Consumer 是如何确定，绑定到哪一个分区上的呢？这个问题也是可以通过分析消费流程来找到答案的。所以，我们分析整个消费流程主要聚焦在三个问题上：\n 订阅过程是如何实现的？ Consumer 是如何与 Coordinator 协商，确定消费哪些 Partition 的？ 拉取消息的过程是如何实现的？  了解前两个问题，有助于你充分理解 Kafka 的元数据模型，以及 Kafka 是如何在客户端和服务端之间来交换元数据的。最后一个问题，拉取消息的实现过程，实际上就是消费的主要流程，我们上节课讲过，这是消息队列最核心的两个流程之一，也是必须重点掌握的。我们就带着这三个问题，来分析 Kafka 的订阅和拉取消息的过程如何实现。\n订阅过程如何实现？ 我们先来看看订阅的实现流程。从上面的例子跟踪到订阅的主流程方法：\n public void subscribe(Collection\u0026lt;String\u0026gt; topics, ConsumerRebalanceListener listener) {\racquireAndEnsureOpen();\rtry {\r// 省略部分代码\r// 重置订阅状态\rthis.subscriptions.subscribe(new HashSet\u0026lt;\u0026gt;(topics), listener);\r// 更新元数据\rmetadata.setTopics(subscriptions.groupSubscription());\r} finally {\rrelease();\r}\r}\r 在这个代码中，我们先忽略掉各种参数和状态检查的分支代码，订阅的主流程主要更新了两个属性：一个是订阅状态 subscriptions，另一个是更新元数据中的 topic 信息。订阅状态 subscriptions 主要维护了订阅的 topic 和 patition 的消费位置等状态信息。属性 metadata 中维护了 Kafka 集群元数据的一个子集，包括集群的 Broker 节点、Topic 和 Partition 在节点上分布，以及我们聚焦的第二个问题：Coordinator 给 Consumer 分配的 Partition 信息。\n请注意一下，这个 subscribe() 方法的实现有一个非常值得大家学习的地方：就是开始的 acquireAndEnsureOpen() 和 try-finally release()，作用就是保护这个方法只能单线程调用。\nKafka 在文档中明确地注明了 Consumer 不是线程安全的，意味着 Consumer 被并发调用时会出现不可预期的结果。为了避免这种情况发生，Kafka 做了主动的检测并抛出异常，而不是放任系统产生不可预期的情况。\nKafka“主动检测不支持的情况并抛出异常，避免系统产生不可预期的行为”这种模式，对于增强的系统的健壮性是一种非常有效的做法。如果你的系统不支持用户的某种操作，正确的做法是，检测不支持的操作，直接拒绝用户操作，并给出明确的错误提示，而不应该只是在文档中写上“不要这样做”，却放任用户错误的操作，产生一些不可预期的、奇怪的错误结果。\n具体 Kafka 是如何实现的并发检测，大家可以看一下方法 acquireAndEnsureOpen() 的实现，很简单也很经典，我们就不再展开讲解了。\n继续跟进到更新元数据的方法 metadata.setTopics() 里面，这个方法的实现除了更新元数据类 Metadata 中的 topic 相关的一些属性以外，还调用了 Metadata.requestUpdate() 方法请求更新元数据。\n public synchronized int requestUpdate() {\rthis.needUpdate = true;\rreturn this.updateVersion;\r}\r 跟进到 requestUpdate() 的方法里面我们会发现，这里面并没有真正发送更新元数据的请求，只是将需要更新元数据的标志位 needUpdate 设置为 true 就结束了。Kafka 必须确保在第一次拉消息之前元数据是可用的，也就是说在第一次拉消息之前必须更新一次元数据，否则 Consumer 就不知道它应该去哪个 Broker 上去拉哪个 Partition 的消息。\n分析完订阅相关的代码，我们来总结一下：在订阅的实现过程中，Kafka 更新了订阅状态 subscriptions 和元数据 metadata 中的相关 topic 的一些属性，将元数据状态置为“需要立即更新”，但是并没有真正发送更新元数据的请求，整个过程没有和集群有任何网络数据交换。\n那这个元数据会在什么时候真正做一次更新呢？我们可以先带着这个问题接着看代码。\n拉取消息的过程如何实现？ 接下来，我们分析拉取消息的流程。这个流程的时序图如下（点击图片可放大查看）：\n我们对着时序图来分析它的实现流程。在 KafkaConsumer.poll() 方法 (对应源码 1179 行) 的实现里面，可以看到主要是先后调用了 2 个私有方法：\n updateAssignmentMetadataIfNeeded(): 更新元数据。 pollForFetches()：拉取消息。  方法 updateAssignmentMetadataIfNeeded() 中，调用了 coordinator.poll() 方法，poll() 方法里面又调用了 client.ensureFreshMetadata() 方法，在 client.ensureFreshMetadata() 方法中又调用了 client.poll() 方法，实现了与 Cluster 通信，在 Coordinator 上注册 Consumer 并拉取和更新元数据。至此，“元数据会在什么时候真正做一次更新”这个问题也有了答案。\n类 ConsumerNetworkClient 封装了 Consumer 和 Cluster 之间所有的网络通信的实现，这个类是一个非常彻底的异步实现。它没有维护任何的线程，所有待发送的 Request 都存放在属性 unsent 中，返回的 Response 存放在属性 pendingCompletion 中。每次调用 poll() 方法的时候，在当前线程中发送所有待发送的 Request，处理所有收到的 Response。\n我们在之前的课程中讲到过，这种异步设计的优势就是用很少的线程实现高吞吐量，劣势也非常明显，极大增加了代码的复杂度。对比上节课我们分析的 RocketMQ 的代码，Producer 和 Consumer 在主要收发消息流程上功能的复杂度是差不多的，但是你可以很明显地感受到 Kafka 的代码实现要比 RocketMQ 的代码实现更加的复杂难于理解。\n我们继续分析方法 pollForFetches() 的实现。\n private Map\u0026lt;TopicPartition, List\u0026lt;ConsumerRecord\u0026lt;K, V\u0026gt;\u0026gt;\u0026gt; pollForFetches(Timer timer) {\r// 省略部分代码\r// 如果缓存里面有未读取的消息，直接返回这些消息\rfinal Map\u0026lt;TopicPartition, List\u0026lt;ConsumerRecord\u0026lt;K, V\u0026gt;\u0026gt;\u0026gt; records = fetcher.fetchedRecords();\rif (!records.isEmpty()) {\rreturn records;\r}\r// 构造拉取消息请求，并发送\rfetcher.sendFetches();\r// 省略部分代码\r// 发送网络请求拉取消息，等待直到有消息返回或者超时\rclient.poll(pollTimer, () -\u0026gt; {\rreturn !fetcher.hasCompletedFetches();\r});\r// 省略部分代码\r// 返回拉到的消息\rreturn fetcher.fetchedRecords();\r}\r 这段代码的主要实现逻辑是：\n 如果缓存里面有未读取的消息，直接返回这些消息； 构造拉取消息请求，并发送； 发送网络请求并拉取消息，等待直到有消息返回或者超时； 返回拉到的消息。  在方法 fetcher.sendFetches() 的实现里面，Kafka 根据元数据的信息，构造到所有需要的 Broker 的拉消息的 Request，然后调用 client.Send() 方法将这些请求异步发送出去。并且，注册了一个回调类来处理返回的 Response，所有返回的 Response 被暂时存放在 Fetcher.completedFetches 中。需要注意的是，这时的 Request 并没有被真正发给各个 Broker，而是被暂存在了 client.unsend 中等待被发送。\n然后，在调用 client.poll() 方法时，会真正将之前构造的所有 Request 发送出去，并处理收到的 Response。\n最后，fetcher.fetchedRecords() 方法中，将返回的 Response 反序列化后转换为消息列表，返回给调用者。\n综合上面的实现分析，我在这里给出整个拉取消息的流程涉及到的相关类的类图，在这个类图中，为了便于你理解，我并没有把所有类都绘制上去，只是把本节课两个流程相关的主要类和这些类里的关键属性画在了图中。你可以配合这个类图和上面的时序图进行代码阅读。\n类图（点击图片可放大查看）：\n小结 本节课我们一起分析了 Kafka Consumer 消费消息的实现过程。大家来分析代码过程中，不仅仅是要掌握 Kafka 整个消费的流程是是如何实现的，更重要的是理解它这种完全异步的设计思想。\n发送请求时，构建 Request 对象，暂存入发送队列，但不立即发送，而是等待合适的时机批量发送。并且，用回调或者 RequestFeuture 方式，预先定义好如何处理响应的逻辑。在收到 Broker 返回的响应之后，也不会立即处理，而是暂存在队列中，择机处理。那这个择机策略就比较复杂了，有可能是需要读取响应的时候，也有可能是缓冲区满了或是时间到了，都有可能触发一次真正的网络请求，也就是在 poll() 方法中发送所有待发送 Request 并处理所有 Response。\n这种设计的好处是，不需要维护用于异步发送的和处理响应的线程，并且能充分发挥批量处理的优势，这也是 Kafka 的性能非常好的原因之一。这种设计的缺点也非常的明显，就是实现的复杂度太大了，如果没有深厚的代码功力，很难驾驭这么复杂的设计，并且后续维护的成本也很高。\n总体来说，不推荐大家把代码设计得这么复杂。代码结构简单、清晰、易维护是是我们在设计过程中需要考虑的一个非常重要的因素。很多时候，为了获得较好的代码结构，在可接受的范围内，去牺牲一些性能，也是划算的。\n思考题 我们知道，Kafka Consumer 在消费过程中是需要维护消费位置的，Consumer 每次从当前消费位置拉取一批消息，这些消息都被正常消费后，Consumer 会给 Coordinator 发一个提交位置的请求，然后消费位置会向后移动，完成一批消费过程。那 kafka Consumer 是如何维护和提交这个消费位置的呢？请你带着这个问题再回顾一下 Consumer 的代码，尝试独立分析代码并找到答案。\nKafka和RocketMQ的消息复制实现 之前我在《05 | 如何确保消息不会丢失？》那节课中讲过，消息队列在收发两端，主要是依靠业务代码，配合请求确认的机制，来保证消息不会丢失的。而在服务端，一般采用持久化和复制的方式来保证不丢消息。\n把消息复制到多个节点上，不仅可以解决丢消息的问题，还可以保证消息服务的高可用。即使某一个节点宕机了，还可以继续使用其他节点来收发消息。所以大部分生产系统，都会把消息队列配置成集群模式，并开启消息复制，来保证系统的高可用和数据可靠性。\n这节课我们来讲一下，消息复制需要解决的一些问题，以及 RocketMQ 和 Kafka 都是如何应对这些问题来实现复制的。\n消息复制面临什么问题？ 我们希望消息队列最好能兼具高性能、高可用并且还能提供数据一致性的保证。虽然很多消息队列产品宣称三个特性全都支持，但你需要知道，这都是有前置条件的。\n首先来说性能。任何的复制实现方式，数据的写入性能一定是不如单节点的。这个很好理解，因为无论采用哪种复制实现方式，都需要数据被写入到多个节点之后再返回，性能一定是不如只写入一个节点的。\n**需要写入的节点数量越多，可用性和数据可靠性就越好，但是写入性能就越低，这是一个天然的矛盾。**不过，复制对消费的性能影响不大，不管采用哪种复制方式，消费消息的时候，都只是选择多副本中一个节点去读数据而已，这和单节点消费并没有差别。\n再来说一致性，消息队列对数据一致性的要求，既包括了“不丢消息”这个要求，也包括“严格顺序”的要求。如果要确保数据一致性，必须采用“主 - 从”的复制方式，这个结论是有严格的数学论证的，大家只要记住就可以了。\n在“主 - 从”模式下，数据先写入到主节点上，从节点只从主节点上复制数据，如果出现主从数据不一致的情况，必须以主节点上的数据为准。这里面需要注意一下，这里面的主节点它并不是不可变的，在很多的复制实现中，当主节点出现问题的时候，其他节点可以通过选举的方式，变成主节点。只要保证，在任何一个时刻，集群的主节点数不能超过 1 个，就可以确保数据一致性。\n最后说一下高可用。既然必须要采用主从的复制方式，高可用需要解决的就是，当某个主节点宕机的时候，尽快再选出一个主节点来接替宕机的主节点。\n比较快速的实现方式是，使用一个第三方的管理服务来管理这些节点，发现某个主节点宕机的时候，由管理服务来指定一个新的主节点。但引入管理服务会带来一系列问题，比如管理服务本身的高可用、数据一致性如何保证？\n有的消息队列选择自选举的方式，由还存活的这些节点通过投票，来选出一个新的主节点，这种投票的实现方式，它的优点是没有外部依赖，可以实现自我管理。缺点就是投票的实现都比较复杂，并且选举的过程是比较慢的，几秒至几十秒都有可能，在选出新的主节点前，服务一直是不可用的。\n大部分复制的实现，都不会选择把消息写入全部副本再返回确认，因为这样虽然可以保证数据一致性，但是，一旦这些副本中有任何一个副本宕机，写入就会卡死了。如果只把消息写入到一部分副本就认为写入成功并返回确认，就可以解决卡死的问题，并且性能也会比写全部副本好很多。\n到底写入多少个副本算写入成功呢？这又是一个非常难抉择的问题。\n假设我们的集群采用“一主二从三副本”的模式，如果只要消息写入到两个副本就算是写入成功了，那这三个节点最多允许宕机一个节点，否则就没法提供服务了。如果说我们把要求写入的副本数量降到 1，只要消息写入到主节点就算成功了，那三个节点中，可以允许宕机两个节点，系统依然可以提供服务，这个可用性就更好一些。但是，有可能出现一种情况：主节点有一部分消息还没来得复制到任何一个从节点上，主节点就宕机了，这时候就会丢消息，数据一致性又没有办法保证了。\n以上我讲的这些内容，还没有涉及到任何复制或者选举的方法和算法，都是最朴素，最基本的原理。你可以看出，这里面是有很多天然的矛盾，所以，目前并没有一种完美的实现方案能够兼顾高性能、高可用和一致性。\n不同的消息队列选择了不同的复制实现方式，这些实现方式都有各自的优缺点，在高性能、高可用和一致性方面提供的能力也是各有高低。接下来我们一起来看一下 RocketMQ 和 Kafka 分别是如何来实现复制的。\nRocketMQ 如何实现复制？ RocketMQ 在 2018 年底迎来了一次重大的更新，引入 Deldger，增加了一种全新的复制方式。我们先来说一下传统的复制方式。\n在 RocketMQ 中，复制的基本单位是 Broker，也就是服务端的进程。复制采用的也是主从方式，通常情况下配置成一主一从，也可以支持一主多从。\nRocketMQ 提供了两种复制方式，一种是异步复制，消息先发送到主节点上，就返回“写入成功”，然后消息再异步复制到从节点上。另外一种方式是同步双写，消息同步双写到主从节点上，主从都写成功，才返回“写入成功”。这两种方式本质上的区别是，写入多少个副本再返回“写入成功”的问题，异步复制需要的副本数是 1，同步双写需要的副本数是 2。\n我刚刚讲过，如果在返回“写入成功”前，需要写入的副本数不够多，那就会丢消息。对 RocketMQ 来说，如果采用异步复制的方式会不会丢消息呢？答案是，并不会丢消息。\n我来跟你说一下为什么不会丢消息。\n在 RocketMQ 中，Broker 的主从关系是通过配置固定的，不支持动态切换。如果主节点宕机，生产者就不能再生产消息了，消费者可以自动切换到从节点继续进行消费。这时候，即使有一些消息没有来得及复制到从节点上，这些消息依然躺在主节点的磁盘上，除非是主节点的磁盘坏了，否则等主节点重新恢复服务的时候，这些消息依然可以继续复制到从节点上，也可以继续消费，不会丢消息，消息的顺序也是没有问题的。\n从设计上来讲，RocketMQ 的这种主从复制方式，牺牲了可用性，换取了比较好的性能和数据一致性。\n那 RocketMQ 又是如何解决可用性的问题的呢？一对儿主从节点可用性不行，多来几对儿主从节点不就解决了？RocketMQ 支持把一个主题分布到多对主从节点上去，每对主从节点中承担主题中的一部分队列，如果某个主节点宕机了，会自动切换到其他主节点上继续发消息，这样既解决了可用性的问题，还可以通过水平扩容来提升 Topic 总体的性能。\n这种复制方式在大多数场景下都可以很好的工作，但也面临一些问题。\n比如，在需要保证消息严格顺序的场景下，由于在主题层面无法保证严格顺序，所以必须指定队列来发送消息，对于任何一个队列，它一定是落在一组特定的主从节点上，如果这个主节点宕机，其他的主节点是无法替代这个主节点的，否则就无法保证严格顺序。在这种复制模式下，严格顺序和高可用只能选择一个。\nRocketMQ 引入 Dledger，使用新的复制方式，可以很好地解决这个问题。我们来看一下 Dledger 是怎么来复制的。\nDledger 在写入消息的时候，要求至少消息复制到半数以上的节点之后，才给客户端返回写入成功，并且它是支持通过选举来动态切换主节点的。\n同样拿 3 个节点举例说明一下。当主节点宕机的时候，2 个从节点会通过投票选出一个新的主节点来继续提供服务，相比主从的复制模式，解决了可用性的问题。由于消息要至少复制到 2 个节点上才会返回写入成功，即使主节点宕机了，也至少有一个节点上的消息是和主节点一样的。Dledger 在选举时，总会把数据和主节点一样的从节点选为新的主节点，这样就保证了数据的一致性，既不会丢消息，还可以保证严格顺序。\n当然，Dledger 的复制方式也不是完美的，依然存在一些不足：比如，选举过程中不能提供服务。最少需要 3 个节点才能保证数据一致性，3 节点时，只能保证 1 个节点宕机时可用，如果 2 个节点同时宕机，即使还有 1 个节点存活也无法提供服务，资源的利用率比较低。另外，由于至少要复制到半数以上的节点才返回写入成功，性能上也不如主从异步复制的方式快。\n讲完了 RocketMQ，我们再来看看 Kafka 是怎么来实现复制的。\nKafka 是如何实现复制的？ Kafka 中，复制的基本单位是分区。每个分区的几个副本之间，构成一个小的复制集群，Broker 只是这些分区副本的容器，所以 Kafka 的 Broker 是不分主从的。\n分区的多个副本中也是采用一主多从的方式。Kafka 在写入消息的时候，采用的也是异步复制的方式。消息在写入到主节点之后，并不会马上返回写入成功，而是等待足够多的节点都复制成功后再返回。在 Kafka 中这个“足够多”是多少呢？Kafka 的设计哲学是，让用户自己来决定。\nKafka 为这个“足够多”创造了一个专有名词：ISR（In Sync Replicas)，翻译过来就是“保持数据同步的副本”。ISR 的数量是可配的，但需要注意的是，这个 ISR 中是包含主节点的。\nKafka 使用 ZooKeeper 来监控每个分区的多个节点，如果发现某个分区的主节点宕机了，Kafka 会利用 ZooKeeper 来选出一个新的主节点，这样解决了可用性的问题。ZooKeeper 是一个分布式协调服务，后面，我会专门用一节课来介绍 ZooKeeper。选举的时候，会从所有 ISR 节点中来选新的主节点，这样可以保证数据一致性。\n默认情况下，如果所有的 ISR 节点都宕机了，分区就无法提供服务了。你也可以选择配置成让分区继续提供服务，这样只要有一个节点还活着，就可以提供服务，代价是无法保证数据一致性，会丢消息。\nKafka 的这种高度可配置的复制方式，优点是非常灵活，你可以通过配置这些复制参数，在可用性、性能和一致性这几方面做灵活的取舍，缺点就是学习成本比较高。\n总结 这节课我们主要来讲了一下，消息复制需要面临的问题以及 RocketMQ 和 Kafka 都是如何应对这些问题来实现复制的。\nRocketMQ 提供新、老两种复制方式：传统的主从模式和新的基于 Dledger 的复制方式。传统的主从模式性能更好，但灵活性和可用性稍差，而基于 Dledger 的复制方式，在 Broker 故障的时候可以自动选举出新节点，可用性更好，性能稍差，并且资源利用率更低一些。Kafka 提供了基于 ISR 的更加灵活可配置的复制方式，用户可以自行配置，在可用性、性能和一致性这几方面根据系统的情况来做取舍。但是，这种灵活的配置方式学习成本较高。\n并没有一种完美的复制方案，可以同时能够兼顾高性能、高可用和一致性。你需要根据你实际的业务需求，先做出取舍，然后再去配置消息队列的复制方式。\n思考题 假设我们有一个 5 节点的 RocketMQ 集群，采用 Dledger5 副本的复制方式，集群中只有一个主题，50 个队列均匀地分布到 5 个 Broker 上。\n如果需要你来配置一套 Kafka 集群，要求达到和这个 RocketMQ 集群一样的性能（不考虑 Kafka 和 RocketMQ 本身的性能差异）、可用性和数据一致性，该如何配置？\nRocketMQ客户端如何在集群中找到正确的节点？ 我们在《21 | RocketMQ Producer 源码分析：消息生产的实现过程》这节课中，讲解 RocketMQ 的生产者启动流程时提到过，生产者只要配置一个接入地址，就可以访问整个集群，并不需要客户端配置每个 Broker 的地址。RocketMQ 会自动根据要访问的主题名称和队列序号，找到对应的 Broker 地址。如果 Broker 发生宕机，客户端还会自动切换到新的 Broker 节点上，这些对于用户代码来说都是透明的。\n这些功能都是由 NameServer 协调 Broker 和客户端共同实现的，其中 NameServer 的作用是最关键的。\n展开来讲，不仅仅是 RocketMQ，任何一个弹性分布式集群，都需要一个类似于 NameServer 服务，来帮助访问集群的客户端寻找集群中的节点，这个服务一般称为 NamingService。比如，像 Dubbo 这种 RPC 框架，它的注册中心就承担了 NamingService 的职责。在 Flink 中，则是 JobManager 承担了 NamingService 的职责。\n也就是说，这种使用 NamingService 服务来协调集群的设计，在分布式集群的架构设计中，是一种非常通用的方法。你在学习这节课之后，不仅要掌握 RocketMQ 的 NameServer 是如何实现的，还要能总结出通用的 NamingService 的设计思想，并能应用于其他分布式系统的设计中。\n这节课，我们一起来分析一下 NameServer 的源代码，看一下 NameServer 是如何协调集群中众多的 Broker 和客户端的。\nNameServer 是如何提供服务的？ 在 RocketMQ 中，NameServer 是一个独立的进程，为 Broker、生产者和消费者提供服务。NameServer 最主要的功能就是，为客户端提供寻址服务，协助客户端找到主题对应的 Broker 地址。此外，NameServer 还负责监控每个 Broker 的存活状态。\nNameServer 支持只部署一个节点，也支持部署多个节点组成一个集群，这样可以避免单点故障。在集群模式下，NameServer 各节点之间是不需要任何通信的，也不会通过任何方式互相感知，每个节点都可以独立提供全部服务。\n我们一起通过这个图来看一下，在 RocketMQ 集群中，NameServer 是如何配合 Broker、生产者和消费者一起工作的。这个图来自RocketMQ 的官方文档。\n每个 Broker 都需要和所有的 NameServer 节点进行通信。当 Broker 保存的 Topic 信息发生变化的时候，它会主动通知所有的 NameServer 更新路由信息，为了保证数据一致性，Broker 还会定时给所有的 NameServer 节点上报路由信息。这个上报路由信息的 RPC 请求，也同时起到 Broker 与 NameServer 之间的心跳作用，NameServer 依靠这个心跳来确定 Broker 的健康状态。\n因为每个 NameServer 节点都可以独立提供完整的服务，所以，对于客户端来说，包括生产者和消费者，只需要选择任意一个 NameServer 节点来查询路由信息就可以了。客户端在生产或消费某个主题的消息之前，会先从 NameServer 上查询这个主题的路由信息，然后根据路由信息获取到当前主题和队列对应的 Broker 物理地址，再连接到 Broker 节点上进行生产或消费。\n如果 NameServer 检测到与 Broker 的连接中断了，NameServer 会认为这个 Broker 不再能提供服务。NameServer 会立即把这个 Broker 从路由信息中移除掉，避免客户端连接到一个不可用的 Broker 上去。而客户端在与 Broker 通信失败之后，会重新去 NameServer 上拉取路由信息，然后连接到其他 Broker 上继续生产或消费消息，这样就实现了自动切换失效 Broker 的功能。\n此外，NameServer 还提供一个类似 Redis 的 KV 读写服务，这个不是主要的流程，我们不展开讲。\n接下来我带你一起分析 NameServer 的源代码，看一下这些服务都是如何实现的。\nNameServer 的总体结构 由于 NameServer 的结构非常简单，排除 KV 读写相关的类之后，一共只有 6 个类，这里面直接给出这 6 个类的说明：\n NamesrvStartup：程序入口。 NamesrvController：NameServer 的总控制器，负责所有服务的生命周期管理。 RouteInfoManager：NameServer 最核心的实现类，负责保存和管理集群路由信息。 BrokerHousekeepingService：监控 Broker 连接状态的代理类。 DefaultRequestProcessor：负责处理客户端和 Broker 发送过来的 RPC 请求的处理器。 ClusterTestRequestProcessor：用于测试的请求处理器。  RouteInfoManager 这个类中保存了所有的路由信息，这些路由信息都是保存在内存中，并且没有持久化的。在代码中，这些路由信息保存在 RouteInfoManager 的几个成员变量中：\npublic class BrokerData implements Comparable\u0026lt;BrokerData\u0026gt; {\r// ...\rprivate final HashMap\u0026lt;String/* topic */, List\u0026lt;QueueData\u0026gt;\u0026gt; topicQueueTable;\rprivate final HashMap\u0026lt;String/* brokerName */, BrokerData\u0026gt; brokerAddrTable;\rprivate final HashMap\u0026lt;String/* clusterName */, Set\u0026lt;String/* brokerName */\u0026gt;\u0026gt; clusterAddrTable;\rprivate final HashMap\u0026lt;String/* brokerAddr */, BrokerLiveInfo\u0026gt; brokerLiveTable;\rprivate final HashMap\u0026lt;String/* brokerAddr */, List\u0026lt;String\u0026gt;/* Filter Server */\u0026gt; filterServerTable;\r// ...\r}\r 以上代码中的这 5 个 Map 对象，保存了集群所有的 Broker 和主题的路由信息。\ntopicQueueTable 保存的是主题和队列信息，其中每个队列信息对应的类 QueueData 中，还保存了 brokerName。需要注意的是，这个 brokerName 并不真正是某个 Broker 的物理地址，它对应的一组 Broker 节点，包括一个主节点和若干个从节点。\nbrokerAddrTable 中保存了集群中每个 brokerName 对应 Broker 信息，每个 Broker 信息用一个 BrokerData 对象表示：\npublic class BrokerData implements Comparable\u0026lt;BrokerData\u0026gt; {\rprivate String cluster;\rprivate String brokerName;\rprivate HashMap\u0026lt;Long/* brokerId */, String/* broker address */\u0026gt; brokerAddrs;\r// ...\r}\r BrokerData 中保存了集群名称 cluster，brokerName 和一个保存 Broker 物理地址的 Map：brokerAddrs，它的 Key 是 BrokerID，Value 就是这个 BrokerID 对应的 Broker 的物理地址。\n下面这三个 map 相对没那么重要，简单说明如下：\n brokerLiveTable 中，保存了每个 Broker 当前的动态信息，包括心跳更新时间，路由数据版本等等。 clusterAddrTable 中，保存的是集群名称与 BrokerName 的对应关系。 filterServerTable 中，保存了每个 Broker 对应的消息过滤服务的地址，用于服务端消息过滤。  可以看到，在 NameServer 的 RouteInfoManager 中，主要的路由信息就是由 topicQueueTable 和 brokerAddrTable 这两个 Map 来保存的。\n在了解了总体结构和数据结构之后，我们再来看一下实现的流程。\nNameServer 如何处理 Broker 注册的路由信息？ 首先来看一下，NameServer 是如何处理 Broker 注册的路由信息的。\nNameServer 处理 Broker 和客户端所有 RPC 请求的入口方法是：“DefaultRequestProcessor#processRequest”，其中处理 Broker 注册请求的代码如下：\npublic class DefaultRequestProcessor implements NettyRequestProcessor {\r// ...\r@Override\rpublic RemotingCommand processRequest(ChannelHandlerContext ctx,\rRemotingCommand request) throws RemotingCommandException {\r// ...\rswitch (request.getCode()) {\r// ...\rcase RequestCode.REGISTER_BROKER:\rVersion brokerVersion = MQVersion.value2Version(request.getVersion());\rif (brokerVersion.ordinal() \u0026gt;= MQVersion.Version.V3_0_11.ordinal()) {\rreturn this.registerBrokerWithFilterServer(ctx, request);\r} else {\rreturn this.registerBroker(ctx, request);\r}\r// ...\rdefault:\rbreak;\r}\rreturn null;\r}\r// ...\r}\r 这是一个非常典型的处理 Request 的路由分发器，根据 request.getCode() 来分发请求到对应的处理器中。Broker 发给 NameServer 注册请求的 Code 为 REGISTER_BROKER，在代码中根据 Broker 的版本号不同，分别有两个不同的处理实现方法：“registerBrokerWithFilterServer”和\u0026quot;registerBroker\u0026rdquo;。这两个方法实现的流程是差不多的，实际上都是调用了\u0026quot;RouteInfoManager#registerBroker\u0026quot;方法，我们直接看这个方法的代码：\npublic RegisterBrokerResult registerBroker(\rfinal String clusterName,\rfinal String brokerAddr,\rfinal String brokerName,\rfinal long brokerId,\rfinal String haServerAddr,\rfinal TopicConfigSerializeWrapper topicConfigWrapper,\rfinal List\u0026lt;String\u0026gt; filterServerList,\rfinal Channel channel) {\rRegisterBrokerResult result = new RegisterBrokerResult();\rtry {\rtry {\r// 加写锁，防止并发修改数据\rthis.lock.writeLock().lockInterruptibly();\r// 更新 clusterAddrTable\rSet\u0026lt;String\u0026gt; brokerNames = this.clusterAddrTable.get(clusterName);\rif (null == brokerNames) {\rbrokerNames = new HashSet\u0026lt;String\u0026gt;();\rthis.clusterAddrTable.put(clusterName, brokerNames);\r}\rbrokerNames.add(brokerName);\r// 更新 brokerAddrTable\rboolean registerFirst = false;\rBrokerData brokerData = this.brokerAddrTable.get(brokerName);\rif (null == brokerData) {\rregisterFirst = true; // 标识需要先注册\rbrokerData = new BrokerData(clusterName, brokerName, new HashMap\u0026lt;Long, String\u0026gt;());\rthis.brokerAddrTable.put(brokerName, brokerData);\r}\rMap\u0026lt;Long, String\u0026gt; brokerAddrsMap = brokerData.getBrokerAddrs();\r// 更新 brokerAddrTable 中的 brokerData\rIterator\u0026lt;Entry\u0026lt;Long, String\u0026gt;\u0026gt; it = brokerAddrsMap.entrySet().iterator();\rwhile (it.hasNext()) {\rEntry\u0026lt;Long, String\u0026gt; item = it.next();\rif (null != brokerAddr \u0026amp;\u0026amp; brokerAddr.equals(item.getValue()) \u0026amp;\u0026amp; brokerId != item.getKey()) {\rit.remove();\r}\r}\r// 如果是新注册的 Master Broker，或者 Broker 中的路由信息变了，需要更新 topicQueueTable\rString oldAddr = brokerData.getBrokerAddrs().put(brokerId, brokerAddr);\rregisterFirst = registerFirst || (null == oldAddr);\rif (null != topicConfigWrapper\r\u0026amp;\u0026amp; MixAll.MASTER_ID == brokerId) {\rif (this.isBrokerTopicConfigChanged(brokerAddr, topicConfigWrapper.getDataVersion())\r|| registerFirst) {\rConcurrentMap\u0026lt;String, TopicConfig\u0026gt; tcTable =\rtopicConfigWrapper.getTopicConfigTable();\rif (tcTable != null) {\rfor (Map.Entry\u0026lt;String, TopicConfig\u0026gt; entry : tcTable.entrySet()) {\rthis.createAndUpdateQueueData(brokerName, entry.getValue());\r}\r}\r}\r}\r// 更新 brokerLiveTable\rBrokerLiveInfo prevBrokerLiveInfo = this.brokerLiveTable.put(brokerAddr,\rnew BrokerLiveInfo(\rSystem.currentTimeMillis(),\rtopicConfigWrapper.getDataVersion(),\rchannel,\rhaServerAddr));\rif (null == prevBrokerLiveInfo) {\rlog.info(\u0026quot;new broker registered, {} HAServer: {}\u0026quot;, brokerAddr, haServerAddr);\r}\r// 更新 filterServerTable\rif (filterServerList != null) {\rif (filterServerList.isEmpty()) {\rthis.filterServerTable.remove(brokerAddr);\r} else {\rthis.filterServerTable.put(brokerAddr, filterServerList);\r}\r}\r// 如果是 Slave Broker，需要在返回的信息中带上 master 的相关信息\rif (MixAll.MASTER_ID != brokerId) {\rString masterAddr = brokerData.getBrokerAddrs().get(MixAll.MASTER_ID);\rif (masterAddr != null) {\rBrokerLiveInfo brokerLiveInfo = this.brokerLiveTable.get(masterAddr);\rif (brokerLiveInfo != null) {\rresult.setHaServerAddr(brokerLiveInfo.getHaServerAddr());\rresult.setMasterAddr(masterAddr);\r}\r}\r}\r} finally {\r// 释放写锁\rthis.lock.writeLock().unlock();\r}\r} catch (Exception e) {\rlog.error(\u0026quot;registerBroker Exception\u0026quot;, e);\r}\rreturn result;\r}\r 上面这段代码比较长，但总体结构很简单，就是根据 Broker 请求过来的路由信息，依次对比并更新 clusterAddrTable、brokerAddrTable、topicQueueTable、brokerLiveTable 和 filterServerTable 这 5 个保存集群信息和路由信息的 Map 对象中的数据。\n另外，在 RouteInfoManager 中，这 5 个 Map 作为一个整体资源，使用了一个读写锁来做并发控制，避免并发更新和更新过程中读到不一致的数据问题。这个读写锁的使用方法，和我们在之前的课程《17 | 如何正确使用锁保护共享数据，协调异步线程？》中讲到的方法是一样的。\n客户端如何寻找 Broker？ 下面我们来看一下，NameServer 如何帮助客户端来找到对应的 Broker。对于客户端来说，无论是生产者还是消费者，通过主题来寻找 Broker 的流程是一样的，使用的也是同一份实现。客户端在启动后，会启动一个定时器，定期从 NameServer 上拉取相关主题的路由信息，然后缓存在本地内存中，在需要的时候使用。每个主题的路由信息用一个 TopicRouteData 对象来表示：\npublic class TopicRouteData extends RemotingSerializable {\r// ...\rprivate List\u0026lt;QueueData\u0026gt; queueDatas;\rprivate List\u0026lt;BrokerData\u0026gt; brokerDatas;\r// ...\r}\r 其中，queueDatas 保存了主题中的所有队列信息，brokerDatas 中保存了主题相关的所有 Broker 信息。客户端选定了队列后，可以在对应的 QueueData 中找到对应的 BrokerName，然后用这个 BrokerName 找到对应的 BrokerData 对象，最终找到对应的 Master Broker 的物理地址。这部分代码在 org.apache.rocketmq.client.impl.factory.MQClientInstance 这个类中，你可以自行查看。\n下面我们看一下在 NameServer 中，是如何实现根据主题来查询 TopicRouteData 的。\nNameServer 处理客户端请求和处理 Broker 请求的流程是一样的，都是通过路由分发器将请求分发的对应的处理方法中，我们直接看具体的实现方法 RouteInfoManager#pickupTopicRouteData：\npublic TopicRouteData pickupTopicRouteData(final String topic) {\r// 初始化返回数据 topicRouteData\rTopicRouteData topicRouteData = new TopicRouteData();\rboolean foundQueueData = false;\rboolean foundBrokerData = false;\rSet\u0026lt;String\u0026gt; brokerNameSet = new HashSet\u0026lt;String\u0026gt;();\rList\u0026lt;BrokerData\u0026gt; brokerDataList = new LinkedList\u0026lt;BrokerData\u0026gt;();\rtopicRouteData.setBrokerDatas(brokerDataList);\rHashMap\u0026lt;String, List\u0026lt;String\u0026gt;\u0026gt; filterServerMap = new HashMap\u0026lt;String, List\u0026lt;String\u0026gt;\u0026gt;();\rtopicRouteData.setFilterServerTable(filterServerMap);\rtry {\rtry {\r// 加读锁\rthis.lock.readLock().lockInterruptibly();\r// 先获取主题对应的队列信息\rList\u0026lt;QueueData\u0026gt; queueDataList = this.topicQueueTable.get(topic);\rif (queueDataList != null) {\r// 把队列信息返回值中\rtopicRouteData.setQueueDatas(queueDataList);\rfoundQueueData = true;\r// 遍历队列，找出相关的所有 BrokerName\rIterator\u0026lt;QueueData\u0026gt; it = queueDataList.iterator();\rwhile (it.hasNext()) {\rQueueData qd = it.next();\rbrokerNameSet.add(qd.getBrokerName());\r}\r// 遍历这些 BrokerName，找到对应的 BrokerData，并写入返回结果中\rfor (String brokerName : brokerNameSet) {\rBrokerData brokerData = this.brokerAddrTable.get(brokerName);\rif (null != brokerData) {\rBrokerData brokerDataClone = new BrokerData(brokerData.getCluster(), brokerData.getBrokerName(), (HashMap\u0026lt;Long, String\u0026gt;) brokerData\r.getBrokerAddrs().clone());\rbrokerDataList.add(brokerDataClone);\rfoundBrokerData = true;\rfor (final String brokerAddr : brokerDataClone.getBrokerAddrs().values()) {\rList\u0026lt;String\u0026gt; filterServerList = this.filterServerTable.get(brokerAddr);\rfilterServerMap.put(brokerAddr, filterServerList);\r}\r}\r}\r}\r} finally {\r// 释放读锁\rthis.lock.readLock().unlock();\r}\r} catch (Exception e) {\rlog.error(\u0026quot;pickupTopicRouteData Exception\u0026quot;, e);\r}\rlog.debug(\u0026quot;pickupTopicRouteData {} {}\u0026quot;, topic, topicRouteData);\rif (foundBrokerData \u0026amp;\u0026amp; foundQueueData) {\rreturn topicRouteData;\r}\rreturn null;\r}\r 这个方法的实现流程是这样的：\n 初始化返回的 topicRouteData 后，\u0008获取读锁。 在 topicQueueTable 中获取主题对应的队列信息，并写入返回结果中。 遍历队列，找出相关的所有 BrokerName。 遍历这些 BrokerName，从 brokerAddrTable 中找到对应的 BrokerData，并写入返回结果中。 释放读锁并返回结果。  小结 这节课我们一起分析了 RocketMQ NameServer 的源代码，NameServer 在集群中起到的一个核心作用就是，为客户端提供路由信息，帮助客户端找到对应的 Broker。\n每个 NameServer 节点上都保存了集群所有 Broker 的路由信息，可以独立提供服务。Broker 会与所有 NameServer 节点建立长连接，定期上报 Broker 的路由信息。客户端会选择连接某一个 NameServer 节点，定期获取订阅主题的路由信息，用于 Broker 寻址。\nNameServer 的所有核心功能都是在 RouteInfoManager 这个类中实现的，这类中使用了几个 Map 来在内存中保存集群中所有 Broker 的路由信息。\n我们还一起分析了 RouteInfoManager 中的两个比较关键的方法：注册 Broker 路由信息的方法 registerBroker，以及查询 Broker 路由信息的方法 pickupTopicRouteData。\n建议你仔细读一下这两个方法的代码，结合保存路由信息的几个 Map 的数据结构，体会一下 RocketMQ NameServer 这种简洁的设计。\n把以上的这些 NameServer 的设计和实现方法抽象一下，我们就可以总结出通用的 NamingService 的设计思想。\nNamingService 负责保存集群内所有节点的路由信息，NamingService 本身也是一个小集群，由多个 NamingService 节点组成。这里我们所说的“路由信息”也是一种通用的抽象，含义是：“客户端需要访问的某个特定服务在哪个节点上”。\n集群中的节点主动连接 NamingService 服务，注册自身的路由信息。给客户端提供路由寻址服务的方式可以有两种，一种是客户端直接连接 NamingService 服务查询路由信息，另一种是，客户端连接集群内任意节点查询路由信息，节点再从自身的缓存或者从 NamingService 上进行查询。\n掌握了以上这些 NamingService 的设计方法，将会非常有助于你理解其他分布式系统的架构，当然，你也可以把这些方法应用到分布式系统的设计中去。\n思考 今天的思考题是这样的，在 RocketMQ 的 NameServer 集群中，各节点之间不需要互相通信，每个节点都可以独立的提供服务。课后请你想一想，这种独特的集群架构有什么优势，又有什么不足？\nKafka的协调服务ZooKeeper：实现分布式系统的“瑞士军刀” 上节课我带你一起学习了 RocketMQ NameServer 的源代码，RocketMQ 的 NameServer 虽然设计非常简洁，但很好地解决了路由寻址的问题。\n而 Kafka 却采用了完全不同的设计思路，它选择使用 ZooKeeper 这样一个分布式协调服务来实现和 RocketMQ 的 NameServer 差不多的功能。\n这节课我先带大家简单了解一下 ZooKeeper，然后再来一起学习一下 Kafka 是如何借助 ZooKeeper 来构建集群，实现路由寻址的。\nZooKeeper 的作用是什么？ Apache ZooKeeper 它是一个非常特殊的中间件，为什么这么说呢？一般来说，像中间件类的开源产品，大多遵循“做一件事，并做好它。”这样的 UNIX 哲学，每个软件都专注于一种功能上。而 ZooKeeper 更像是一个“瑞士军刀”，它提供了很多基本的操作，能实现什么样的功能更多取决于使用者如何来使用它。\nZooKeeper 作为一个分布式的协调服务框架，主要用来解决分布式集群中，应用系统需要面对的各种通用的一致性问题。ZooKeeper 本身可以部署为一个集群，集群的各个节点之间可以通过选举来产生一个 Leader，选举遵循半数以上的原则，所以一般集群需要部署奇数个节点。\nZooKeeper 最核心的功能是，它提供了一个分布式的存储系统，数据的组织方式类似于 UNIX 文件系统的树形结构。由于这是一个可以保证一致性的存储系统，所以你可以放心地在你的应用集群中读写 ZooKeeper 的数据，而不用担心数据一致性的问题。分布式系统中一些需要整个集群所有节点都访问的元数据，比如集群节点信息、公共配置信息等，特别适合保存在 ZooKeeper 中。\n在这个树形的存储结构中，每个节点被称为一个“ZNode”。ZooKeeper 提供了一种特殊的 ZNode 类型：临时节点。这种临时节点有一个特性：如果创建临时节点的客户端与 ZooKeeper 集群失去连接，这个临时节点就会自动消失。在 ZooKeeper 内部，它维护了 ZooKeeper 集群与所有客户端的心跳，通过判断心跳的状态，来确定是否需要删除客户端创建的临时节点。\nZooKeeper 还提供了一种订阅 ZNode 状态变化的通知机制：Watcher，一旦 ZNode 或者它的子节点状态发生了变化，订阅的客户端会立即收到通知。\n利用 ZooKeeper 临时节点和 Watcher 机制，我们很容易随时来获取业务集群中每个节点的存活状态，并且可以监控业务集群的节点变化情况，当有节点上下线时，都可以收到来自 ZooKeeper 的通知。\n此外，我们还可以用 ZooKeeper 来实现业务集群的快速选举、节点间的简单通信、分布式锁等很多功能。\n下面我带你一起来看一下 Kafka 是如何来使用 ZooKeeper 的。\nKafka 在 ZooKeeper 中保存了哪些信息？ 首先我们来看一下 Kafka 在 ZooKeeper 都保存了哪些信息，我把这些 ZNode 整理了一张图方便你来学习。\n你可能在网上看到过和这个图类似的其他版本的图，这些图中绘制的 ZNode 比我们这张图要多一些，这些图大都是描述的 0.8.x 的旧版本的情况，最新版本的 Kafka 已经将消费位置管理等一些原本依赖 ZooKeeper 实现的功能，替换成了其他的实现方式。\n图中圆角的矩形是临时节点，直角矩形是持久化的节点。\n我们从左往右来看，左侧这棵树保存的是 Kafka 的 Broker 信息，/brokers/ids/[0…N]，每个临时节点对应着一个在线的 Broker，Broker 启动后会创建一个临时节点，代表 Broker 已经加入集群可以提供服务了，节点名称就是 BrokerID，节点内保存了包括 Broker 的地址、版本号、启动时间等等一些 Broker 的基本信息。如果 Broker 宕机或者与 ZooKeeper 集群失联了，这个临时节点也会随之消失。\n右侧部分的这棵树保存的就是主题和分区的信息。/brokers/topics/ 节点下面的每个子节点都是一个主题，节点的名称就是主题名称。每个主题节点下面都包含一个固定的 partitions 节点，pattitions 节点的子节点就是主题下的所有分区，节点名称就是分区编号。\n每个分区节点下面是一个名为 state 的临时节点，节点中保存着分区当前的 leader 和所有的 ISR 的 BrokerID。这个 state 临时节点是由这个分区当前的 Leader Broker 创建的。如果这个分区的 Leader Broker 宕机了，对应的这个 state 临时节点也会消失，直到新的 Leader 被选举出来，再次创建 state 临时节点。\nKafka 客户端如何找到对应的 Broker？ 那 Kafka 客户端如何找到主题、队列对应的 Broker 呢？其实，通过上面 ZooKeeper 中的数据结构，你应该已经可以猜的八九不离十了。是的，先根据主题和队列，在右边的树中找到分区对应的 state 临时节点，我们刚刚说过，state 节点中保存了这个分区 Leader 的 BrokerID。拿到这个 Leader 的 BrokerID 后，再去左侧的树中，找到 BrokerID 对应的临时节点，就可以获取到 Broker 真正的访问地址了。\n在《21 | Kafka Consumer 源码分析：消息消费的实现过程》这一节课中，我讲过，Kafka 的客户端并不会去直接连接 ZooKeeper，它只会和 Broker 进行远程通信，那我们可以合理推测一下，ZooKeeper 上的元数据应该是通过 Broker 中转给每个客户端的。\n下面我们一起看一下 Kafka 的源代码，来验证一下我们的猜测是不是正确的。\n在之前的课程中，我和大家讲过，客户端真正与服务端发生网络传输是在 org.apache.kafka.clients.NetworkClient#poll 方法中实现的，我们一直跟踪这个调用链：\nNetworkClient#poll() -\u0026gt; DefaultMetadataUpdater#maybeUpdate(long) -\u0026gt; DefaultMetadataUpdater#maybeUpdate(long, Node)\r复制代码\r 直到 maybeUpdate(long, Node) 这个方法，在这个方法里面，Kafka 构造了一个更新元数据的请求：\nprivate long maybeUpdate(long now, Node node) {\rString nodeConnectionId = node.idString();\rif (canSendRequest(nodeConnectionId, now)) {\r// 构建一个更新元数据的请求的构造器\rMetadata.MetadataRequestAndVersion metadataRequestAndVersion = metadata.newMetadataRequestAndVersion();\rinProgressRequestVersion = metadataRequestAndVersion.requestVersion;\rMetadataRequest.Builder metadataRequest = metadataRequestAndVersion.requestBuilder;\rlog.debug(\u0026quot;Sending metadata request {} to node {}\u0026quot;, metadataRequest, node);\r// 发送更新元数据的请求\rsendInternalMetadataRequest(metadataRequest, nodeConnectionId, now);\rreturn defaultRequestTimeoutMs;\r}\r//...\r}\r 这段代码先构造了更新元数据的请求的构造器，然后调用 sendInternalMetadataRequest() 把这个请求放到待发送的队列中。这里面有两个地方我需要特别说明一下。\n第一点是，在这个方法里面创建的并不是一个真正的更新元数据的 MetadataRequest，而是一个用于构造 MetadataRequest 的构造器 MetadataRequest.Builder，等到真正要发送请求之前，Kafka 才会调用 Builder.buid() 方法把这个 MetadataRequest 构建出来然后发送出去。而且，不仅是元数据的请求，所有的请求都是这样来处理的。\n第二点是，调用 sendInternalMetadataRequest() 方法时，这个请求也并没有被真正发出去，依然是保存在待发送的队列中，然后择机来异步批量发送。\n请求的具体内容封装在 org.apache.kafka.common.requests.MetadataRequest 这个对象中，它包含的信息很简单，只有一个主题的列表，来表明需要获取哪些主题的元数据，另外还有一个布尔类型的字段 allowAutoTopicCreation，表示是否允许自动创建主题。\n然后我们再来看下，在 Broker 中，Kafka 是怎么来处理这个更新元数据的请求的。\nBroker 处理所有 RPC 请求的入口类在 kafka.server.KafkaApis#handle 这个方法里面，我们找到对应处理更新元数据的方法 handleTopicMetadataRequest(RequestChannel.Request)，这段代码是用 Scala 语言编写的：\n def handleTopicMetadataRequest(request: RequestChannel.Request) {\rval metadataRequest = request.body[MetadataRequest]\rval requestVersion = request.header.apiVersion\r// 计算需要获取哪些主题的元数据\rval topics =\r// 在旧版本的协议中，每次都获取所有主题的元数据\rif (requestVersion == 0) {\rif (metadataRequest.topics() == null || metadataRequest.topics.isEmpty)\rmetadataCache.getAllTopics()\relse\rmetadataRequest.topics.asScala.toSet\r} else {\rif (metadataRequest.isAllTopics)\rmetadataCache.getAllTopics()\relse\rmetadataRequest.topics.asScala.toSet\r}\r// 省略掉鉴权相关代码\r// ...\rval topicMetadata =\rif (authorizedTopics.isEmpty)\rSeq.empty[MetadataResponse.TopicMetadata]\relse\r// 从元数据缓存过滤出相关主题的元数据\rgetTopicMetadata(metadataRequest.allowAutoTopicCreation, authorizedTopics, request.context.listenerName,\rerrorUnavailableEndpoints, errorUnavailableListeners)\r// ...\r// 获取所有 Broker 列表\rval brokers = metadataCache.getAliveBrokers\rtrace(\u0026quot;Sending topic metadata %s and brokers %s for correlation id %d to client %s\u0026quot;.format(completeTopicMetadata.mkString(\u0026quot;,\u0026quot;),\rbrokers.mkString(\u0026quot;,\u0026quot;), request.header.correlationId, request.header.clientId))\r// 构建 Response 并发送\rsendResponseMaybeThrottle(request, requestThrottleMs =\u0026gt;\rnew MetadataResponse(\rrequestThrottleMs,\rbrokers.flatMap(_.getNode(request.context.listenerName)).asJava,\rclusterId,\rmetadataCache.getControllerId.getOrElse(MetadataResponse.NO_CONTROLLER_ID),\rcompleteTopicMetadata.asJava\r))\r}\r 这段代码的主要逻辑是，先根据请求中的主题列表，去本地的元数据缓存 MetadataCache 中过滤出相应主题的元数据，也就是我们上面那张图中，右半部分的那棵树的子集，然后再去本地元数据缓存中获取所有 Broker 的集合，也就是上图中左半部分那棵树，最后把这两部分合在一起，作为响应返回给客户端。\nKafka 在每个 Broker 中都维护了一份和 ZooKeeper 中一样的元数据缓存，并不是每次客户端请求元数据就去读一次 ZooKeeper。由于 ZooKeeper 提供了 Watcher 这种监控机制，Kafka 可以感知到 ZooKeeper 中的元数据变化，从而及时更新 Broker 中的元数据缓存。\n这样就完成了一次完整的更新元数据的流程。通过分析代码，可以证实，我们开始的猜测都是没有问题的。\n小结 最后我们对这节课的内容做一个总结。\n首先，我们简单的介绍了 ZooKeeper，它是一个分布式的协调服务，它的核心服务是一个高可用、高可靠的一致性存储，在此基础上，提供了包括读写元数据、节点监控、选举、节点间通信和分布式锁等很多功能，这些功能可以极大方便我们快速开发一个分布式的集群系统。\n但是，ZooKeeper 也并不是完美的，在使用的时候你需要注意几个问题：\n 不要往 ZooKeeper 里面写入大量数据，它不是一个真正意义上的存储系统，只适合存放少量的数据。依据服务器配置的不同，ZooKeeper 在写入超过几百 MB 数据之后，性能和稳定性都会严重下降。 不要让业务集群的可用性依赖于 ZooKeeper 的可用性，什么意思呢？你的系统可以使用 Zookeeper，但你要留一手，要考虑如果 Zookeeper 集群宕机了，你的业务集群最好还能提供服务。因为 ZooKeeper 的选举过程是比较慢的，而它对网络的抖动又比较敏感，一旦触发选举，这段时间内的 ZooKeeper 是不能提供任何服务的。  Kafka 主要使用 ZooKeeper 来保存它的元数据、监控 Broker 和分区的存活状态，并利用 ZooKeeper 来进行选举。\nKafka 在 ZooKeeper 中保存的元数据，主要就是 Broker 的列表和主题分区信息两棵树。这份元数据同时也被缓存到每一个 Broker 中。客户端并不直接和 ZooKeeper 来通信，而是在需要的时候，通过 RPC 请求去 Broker 上拉取它关心的主题的元数据，然后保存到客户端的元数据缓存中，以便支撑客户端生产和消费。\n可以看到，目前 Kafka 的这种设计，集群的可用性是严重依赖 ZooKeeper 的，也就是说，如果 ZooKeeper 集群不能提供服务，那整个 Kafka 集群也就不能提供服务了，这其实是一个不太好的设计。\n如果你需要要部署大规模的 Kafka 集群，建议的方式是，拆分成多个互相独立的小集群部署，每个小集群都使用一组独立的 ZooKeeper 提供服务。这样，每个 ZooKeeper 中存储的数据相对比较少，并且如果某个 ZooKeeper 集群故障，只会影响到一个小的 Kafka 集群，故障的影响面相对小一些。\nKafka 的开发者也意识到了这个问题，目前正在讨论开发一个元数据服务来替代 ZooKeeper，感兴趣的同学可以看一下他们的Proposal。\n思考题 本节课的思考题是这样的，请你顺着我们这节课源码分析的思路继续深挖进去，看一下 Broker 中的元数据缓存，又是如何与 ZooKeeper 中的元数据保持同步的呢？\nRocketMQ与Kafka中如何实现事务？ 在之前《04 | 如何利用事务消息实现分布式事务？》这节课中，我通过一个小例子来和大家讲解了如何来使用事务消息。在这节课的评论区，很多同学都提出来，非常想了解一下事务消息到底是怎么实现的。不仅要会使用，还要掌握实现原理，这种学习态度，一直是我们非常提倡的，这节课，我们就一起来学习一下，在 RocketMQ 和 Kafka 中，事务消息分别是如何来实现的？\nRocketMQ 的事务是如何实现的？ 首先我们来看 RocketMQ 的事务。我在之前的课程中，已经给大家讲解过 RocketMQ 事务的大致流程，这里我们再一起通过代码，重温一下这个流程。\npublic class CreateOrderService {\r@Inject\rprivate OrderDao orderDao; // 注入订单表的 DAO\r@Inject\rprivate ExecutorService executorService; // 注入一个 ExecutorService\rprivate TransactionMQProducer producer;\r// 初始化 transactionListener 和 producer\r@Init\rpublic void init() throws MQClientException {\rTransactionListener transactionListener = createTransactionListener();\rproducer = new TransactionMQProducer(\u0026quot;myGroup\u0026quot;);\rproducer.setExecutorService(executorService);\rproducer.setTransactionListener(transactionListener);\rproducer.start();\r}\r// 创建订单服务的请求入口\r@PUT\r@RequestMapping(...)\rpublic boolean createOrder(@RequestBody CreateOrderRequest request) {\r// 根据创建订单请求创建一条消息\rMessage msg = createMessage(request);\r// 发送事务消息\rSendResult sendResult = producer.sendMessageInTransaction(msg, request);\r// 返回：事务是否成功\rreturn sendResult.getSendStatus() == SendStatus.SEND_OK;\r}\rprivate TransactionListener createTransactionListener() {\rreturn new TransactionListener() {\r// 执行本地事务的方法\r@Override\rpublic LocalTransactionState executeLocalTransaction(Message msg, Object arg) {\rCreateOrderRequest request = (CreateOrderRequest ) arg;\rtry {\r// 执行本地事务创建订单\rorderDao.createOrderInDB(request);\r// 如果没抛异常说明执行成功，提交事务消息\rreturn LocalTransactionState.COMMIT_MESSAGE;\r} catch (Throwable t) {\r// 失败则直接回滚事务消息\rreturn LocalTransactionState.ROLLBACK_MESSAGE;\r}\r}\r// 反查本地事务\r@Override\rpublic LocalTransactionState checkLocalTransaction(MessageExt msg) {、\r// 从消息中获得订单 ID\rString orderId = msg.getUserProperty(\u0026quot;orderId\u0026quot;);\r// 去数据库中查询订单号是否存在，如果存在则提交事务；\r// 如果不存在，可能是本地事务失败了，也可能是本地事务还在执行，所以返回 UNKNOW\r//（PS：这里 RocketMQ 有个拼写错误：UNKNOW）\rreturn orderDao.isOrderIdExistsInDB(orderId)?\rLocalTransactionState.COMMIT_MESSAGE: LocalTransactionState.UNKNOW;\r}\r};\r}\r//....\r}\r 在这个流程中，我们提供一个创建订单的服务，功能就是在数据库中插入一条订单记录，并发送一条创建订单的消息，要求写数据库和发消息这两个操作在一个事务内执行，要么都成功，要么都失败。在这段代码中，我们首先在 init() 方法中初始化了 transactionListener 和发生 RocketMQ 事务消息的变量 producer。真正提供创建订单服务的方法是 createOrder()，在这个方法里面，我们根据请求的参数创建一条消息，然后调用 RocketMQ producer 发送事务消息，并返回事务执行结果。\n之后的 createTransactionListener() 方法是在 init() 方法中调用的，这里面直接构造一个匿名类，来实现 RocketMQ 的 TransactionListener 接口，这个接口需要实现两个方法：\n executeLocalTransaction：执行本地事务，在这里我们直接把订单数据插入到数据库中，并返回本地事务的执行结果。 checkLocalTransaction：反查本地事务，在这里我们的处理是，在数据库中查询订单号是否存在，如果存在则提交事务，如果不存在，可能是本地事务失败了，也可能是本地事务还在执行，所以返回 UNKNOW。  这样，就使用 RocketMQ 的事务消息功能实现了一个创建订单的分布式事务。接下来我们一起通过 RocketMQ 的源代码来看一下，它的事务消息是如何实现的。\n首先看一下在 producer 中，是如何来发送事务消息的：\npublic TransactionSendResult sendMessageInTransaction(final Message msg, final LocalTransactionExecuter localTransactionExecuter, final Object arg) throws MQClientException {\rTransactionListener transactionListener = getCheckListener();\rif (null == localTransactionExecuter \u0026amp;\u0026amp; null == transactionListener) {\rthrow new MQClientException(\u0026quot;tranExecutor is null\u0026quot;, null);\r}\rValidators.checkMessage(msg, this.defaultMQProducer);\rSendResult sendResult = null;\r// 这里给消息添加了属性，标明这是一个事务消息，也就是半消息\rMessageAccessor.putProperty(msg, MessageConst.PROPERTY_TRANSACTION_PREPARED, \u0026quot;true\u0026quot;);\rMessageAccessor.putProperty(msg, MessageConst.PROPERTY_PRODUCER_GROUP, this.defaultMQProducer.getProducerGroup());\r// 调用发送普通消息的方法，发送这条半消息\rtry {\rsendResult = this.send(msg);\r} catch (Exception e) {\rthrow new MQClientException(\u0026quot;send message Exception\u0026quot;, e);\r}\rLocalTransactionState localTransactionState = LocalTransactionState.UNKNOW;\rThrowable localException = null;\rswitch (sendResult.getSendStatus()) {\rcase SEND_OK: {\rtry {\rif (sendResult.getTransactionId() != null) {\rmsg.putUserProperty(\u0026quot;__transactionId__\u0026quot;, sendResult.getTransactionId());\r}\rString transactionId = msg.getProperty(MessageConst.PROPERTY_UNIQ_CLIENT_MESSAGE_ID_KEYIDX);\rif (null != transactionId \u0026amp;\u0026amp; !\u0026quot;\u0026quot;.equals(transactionId)) {\rmsg.setTransactionId(transactionId);\r}\r// 执行本地事务\rif (null != localTransactionExecuter) {\rlocalTransactionState = localTransactionExecuter.executeLocalTransactionBranch(msg, arg);\r} else if (transactionListener != null) {\rlog.debug(\u0026quot;Used new transaction API\u0026quot;);\rlocalTransactionState = transactionListener.executeLocalTransaction(msg, arg);\r}\rif (null == localTransactionState) {\rlocalTransactionState = LocalTransactionState.UNKNOW;\r}\rif (localTransactionState != LocalTransactionState.COMMIT_MESSAGE) {\rlog.info(\u0026quot;executeLocalTransactionBranch return {}\u0026quot;, localTransactionState);\rlog.info(msg.toString());\r}\r} catch (Throwable e) {\rlog.info(\u0026quot;executeLocalTransactionBranch exception\u0026quot;, e);\rlog.info(msg.toString());\rlocalException = e;\r}\r}\rbreak;\rcase FLUSH_DISK_TIMEOUT:\rcase FLUSH_SLAVE_TIMEOUT:\rcase SLAVE_NOT_AVAILABLE:\rlocalTransactionState = LocalTransactionState.ROLLBACK_MESSAGE;\rbreak;\rdefault:\rbreak;\r}\r// 根据事务消息和本地事务的执行结果 localTransactionState，决定提交或回滚事务消息\r// 这里给 Broker 发送提交或回滚事务的 RPC 请求。\rtry {\rthis.endTransaction(sendResult, localTransactionState, localException);\r} catch (Exception e) {\rlog.warn(\u0026quot;local transaction execute \u0026quot; + localTransactionState + \u0026quot;, but end broker transaction failed\u0026quot;, e);\r}\rTransactionSendResult transactionSendResult = new TransactionSendResult();\rtransactionSendResult.setSendStatus(sendResult.getSendStatus());\rtransactionSendResult.setMessageQueue(sendResult.getMessageQueue());\rtransactionSendResult.setMsgId(sendResult.getMsgId());\rtransactionSendResult.setQueueOffset(sendResult.getQueueOffset());\rtransactionSendResult.setTransactionId(sendResult.getTransactionId());\rtransactionSendResult.setLocalTransactionState(localTransactionState);\rreturn transactionSendResult;\r}\r 这段代码的实现逻辑是这样的：首先给待发送消息添加了一个属性 PROPERTY_TRANSACTION_PREPARED，标明这是一个事务消息，也就是半消息，然后会像发送普通消息一样去把这条消息发送到 Broker 上。如果发送成功了，就开始调用我们之前提供的接口 TransactionListener 的实现类中，执行本地事务的方法 executeLocalTransaction() 来执行本地事务，在我们的例子中就是在数据库中插入一条订单记录。\n最后，根据半消息发送的结果和本地事务执行的结果，来决定提交或者回滚事务。在实现方法 endTransaction() 中，producer 就是给 Broker 发送了一个单向的 RPC 请求，告知 Broker 完成事务的提交或者回滚。由于有事务反查的机制来兜底，这个 RPC 请求即使失败或者丢失，也都不会影响事务最终的结果。最后构建事务消息的发送结果，并返回。\n以上，就是 RocketMQ 在 Producer 这一端事务消息的实现，然后我们再看一下 Broker 这一端，它是怎么来处理事务消息和进行事务反查的。\nBroker 在处理 Producer 发送消息的请求时，会根据消息中的属性判断一下，这条消息是普通消息还是半消息：\n// org.apache.rocketmq.broker.processor.SendMessageProcessor:sendMessage()\r// ...\rif (traFlag != null \u0026amp;\u0026amp; Boolean.parseBoolean(traFlag)) {\r// ...\rputMessageResult = this.brokerController.getTransactionalMessageService().prepareMessage(msgInner);\r} else {\rputMessageResult = this.brokerController.getMessageStore().putMessage(msgInner);\r}\r// ...\r 这段代码在 org.apache.rocketmq.broker.processor.SendMessageProcessor#sendMessage 方法中，然后我们跟进去看看真正处理半消息的业务逻辑，这段处理逻辑在类 org.apache.rocketmq.broker.transaction.queue.TransactionalMessageBridge 中：\n// org.apache.rocketmq.broker.transaction.queue.TransactionalMessageBridge\rpublic PutMessageResult putHalfMessage(MessageExtBrokerInner messageInner) {\rreturn store.putMessage(parseHalfMessageInner(messageInner));\r}\rprivate MessageExtBrokerInner parseHalfMessageInner(MessageExtBrokerInner msgInner) {\r// 记录消息的主题和队列，到新的属性中\rMessageAccessor.putProperty(msgInner, MessageConst.PROPERTY_REAL_TOPIC, msgInner.getTopic());\rMessageAccessor.putProperty(msgInner, MessageConst.PROPERTY_REAL_QUEUE_ID,\rString.valueOf(msgInner.getQueueId()));\rmsgInner.setSysFlag(\rMessageSysFlag.resetTransactionValue(msgInner.getSysFlag(), MessageSysFlag.TRANSACTION_NOT_TYPE));\r// 替换消息的主题和队列为：RMQ_SYS_TRANS_HALF_TOPIC，0\rmsgInner.setTopic(TransactionalMessageUtil.buildHalfTopic());\rmsgInner.setQueueId(0);\rmsgInner.setPropertiesString(MessageDecoder.messageProperties2String(msgInner.getProperties()));\rreturn msgInner;\r}\r 我们可以看到，在这段代码中，RocketMQ 并没有把半消息保存到消息中客户端指定的那个队列中，而是记录了原始的主题队列后，把这个半消息保存在了一个特殊的内部主题 RMQ_SYS_TRANS_HALF_TOPIC 中，使用的队列号固定为 0。这个主题和队列对消费者是不可见的，所以里面的消息永远不会被消费。这样，就保证了在事务提交成功之前，这个半消息对消费者来说是消费不到的。\n然后我们再看一下，RocketMQ 是如何进行事务反查的：在 Broker 的 TransactionalMessageCheckService 服务中启动了一个定时器，定时从半消息队列中读出所有待反查的半消息，针对每个需要反查的半消息，Broker 会给对应的 Producer 发一个要求执行事务状态反查的 RPC 请求，这部分的逻辑在方法 org.apache.rocketmq.broker.transaction.AbstractTransactionalMessageCheckListener#sendCheckMessage 中，根据 RPC 返回响应中的反查结果，来决定这个半消息是需要提交还是回滚，或者后续继续来反查。会一直定时轮询，直到有结果或者超时\n最后，提交或者回滚事务实现的逻辑是差不多的，首先把半消息标记为已处理，如果是提交事务，那就把半消息从半消息队列中复制到这个消息真正的主题和队列中去，如果要回滚事务，这一步什么都不需要做，最后结束这个事务。这部分逻辑的实现在 org.apache.rocketmq.broker.processor.EndTransactionProcessor 这个类中。\nKafka 的事务和 Exactly Once 可以解决什么问题？ 接下来我们再说一下 Kafka 的事务。之前我们讲事务的时候说过，Kafka 的事务解决的问题和 RocketMQ 是不太一样的。RocketMQ 中的事务，它解决的问题是，确保执行本地事务和发消息这两个操作，要么都成功，要么都失败。并且，RocketMQ 增加了一个事务反查的机制，来尽量提高事务执行的成功率和数据一致性。\n而 Kafka 中的事务，它解决的问题是，确保在一个事务中发送的多条消息，要么都成功，要么都失败。注意，这里面的多条消息不一定要在同一个主题和分区中，可以是发往多个主题和分区的消息。当然，你可以在 Kafka 的事务执行过程中，加入本地事务，来实现和 RocketMQ 中事务类似的效果，但是 Kafka 是没有事务反查机制的。\nKafka 的这种事务机制，单独来使用的场景不多。更多的情况下被用来配合 Kafka 的幂等机制来实现 Kafka 的 Exactly Once 语义。我在之前的课程中也强调过，这里面的 Exactly Once，和我们通常理解的消息队列的服务水平中的 Exactly Once 是不一样的。\n我们通常理解消息队列的服务水平中的 Exactly Once，它指的是，消息从生产者发送到 Broker，然后消费者再从 Broker 拉取消息，然后进行消费。这个过程中，确保每一条消息恰好传输一次，不重不丢。我们之前说过，包括 Kafka 在内的几个常见的开源消息队列，都只能做到 At Least Once，也就是至少一次，保证消息不丢，但有可能会重复。做不到 Exactly Once。\n那 Kafka 中的 Exactly Once 又是解决的什么问题呢？它解决的是，在流计算中，用 Kafka 作为数据源，并且将计算结果保存到 Kafka 这种场景下，数据从 Kafka 的某个主题中消费，在计算集群中计算，再把计算结果保存在 Kafka 的其他主题中。这样的过程中，保证每条消息都被恰好计算一次，确保计算结果正确。\n举个例子，比如，我们把所有订单消息保存在一个 Kafka 的主题 Order 中，在 Flink 集群中运行一个计算任务，统计每分钟的订单收入，然后把结果保存在另一个 Kafka 的主题 Income 里面。要保证计算结果准确，就要确保，无论是 Kafka 集群还是 Flink 集群中任何节点发生故障，每条消息都只能被计算一次，不能重复计算，否则计算结果就错了。这里面有一个很重要的限制条件，就是数据必须来自 Kafka 并且计算结果都必须保存到 Kafka 中，才可以享受到 Kafka 的 Excactly Once 机制。\n可以看到，Kafka 的 Exactly Once 机制，是为了解决在“读数据 - 计算 - 保存结果”这样的计算过程中数据不重不丢，而不是我们通常理解的使用消息队列进行消息生产消费过程中的 Exactly Once。\nKafka 的事务是如何实现的？ 那 Kafka 的事务又是怎么实现的呢？它的实现原理和 RocketMQ 的事务是差不多的，都是基于两阶段提交来实现的，但是实现的过程更加复杂。\n首先说一下，参与 Kafka 事务的几个角色，或者说是模块。为了解决分布式事务问题，Kafka 引入了事务协调者这个角色，负责在服务端协调整个事务。这个协调者并不是一个独立的进程，而是 Broker 进程的一部分，协调者和分区一样通过选举来保证自身的可用性。\n和 RocketMQ 类似，Kafka 集群中也有一个特殊的用于记录事务日志的主题，这个事务日志主题的实现和普通的主题是一样的，里面记录的数据就是类似于“开启事务”“提交事务”这样的事务日志。日志主题同样也包含了很多的分区。在 Kafka 集群中，可以存在多个协调者，每个协调者负责管理和使用事务日志中的几个分区。这样设计，其实就是为了能并行执行多个事务，提升性能。\n（图片来源：Kafka 官方）\n下面说一下 Kafka 事务的实现流程。\n首先，当我们开启事务的时候，生产者会给协调者发一个请求来开启事务，协调者在事务日志中记录下事务 ID。\n然后，生产者在发送消息之前，还要给协调者发送请求，告知发送的消息属于哪个主题和分区，这个信息也会被协调者记录在事务日志中。接下来，生产者就可以像发送普通消息一样来发送事务消息，这里和 RocketMQ 不同的是，RocketMQ 选择把未提交的事务消息保存在特殊的队列中，而 Kafka 在处理未提交的事务消息时，和普通消息是一样的，直接发给 Broker，保存在这些消息对应的分区中，Kafka 会在客户端的消费者中，暂时过滤未提交的事务消息。\n消息发送完成后，生产者给协调者发送提交或回滚事务的请求，由协调者来开始两阶段提交，完成事务。第一阶段，协调者把事务的状态设置为“预提交”，并写入事务日志。到这里，实际上事务已经成功了，无论接下来发生什么情况，事务最终都会被提交。\n之后便开始第二阶段，协调者在事务相关的所有分区中，都会写一条“事务结束”的特殊消息，当 Kafka 的消费者，也就是客户端，读到这个事务结束的特殊消息之后，它就可以把之前暂时过滤的那些未提交的事务消息，放行给业务代码进行消费了。最后，协调者记录最后一条事务日志，标识这个事务已经结束了。\n我把整个事务的实现流程，绘制成一个简单的时序图放在这里，便于你理解。\n总结一下 Kafka 这个两阶段的流程，准备阶段，生产者发消息给协调者开启事务，然后消息发送到每个分区上。提交阶段，生产者发消息给协调者提交事务，协调者给每个分区发一条“事务结束”的消息，完成分布式事务提交。\n小结 这节课我分别讲解了 Kafka 和 RocketMQ 是如何来实现事务的。你可以看到，它们在实现事务过程中的一些共同的地方，它们都是基于两阶段提交来实现的事务，都利用了特殊的主题中的队列和分区来记录事务日志。\n不同之处在于对处于事务中的消息的处理方式，RocketMQ 是把这些消息暂存在一个特殊的队列中，待事务提交后再移动到业务队列中；而 Kafka 直接把消息放到对应的业务分区中，配合客户端过滤来暂时屏蔽进行中的事务消息。\n同时你需要了解，RocketMQ 和 Kafka 的事务，它们的适用场景是不一样的，RocketMQ 的事务适用于解决本地事务和发消息的数据一致性问题，而 Kafka 的事务则是用于实现它的 Exactly Once 机制，应用于实时计算的场景中。\n思考 课后，请你根据我们课程中讲到的 Kafka 事务的实现流程，去 Kafka 的源代码中把这个事务的实现流程分析出来，将我们上面这个时序图进一步细化，绘制一个粒度到类和方法调用的时序图。然后请你想一下，如果事务进行过程中，协调者宕机了，那事务又是如何恢复的呢？\nMQTT协议：如何支持海量的在线IoT设备? IoT，也就是物联网，一直是最近几年技术圈非常火的一个概念，并且，随着 5G 大规模商用，IoT 还将持续火下去。\n那到底什么是物联网呢？物联网这个词儿，它的含义还不那么直观，但你看它的英文：IoT，也就是 Internet of Things 的缩写，Things 这个单词，我们知道，它在英语里面几乎可以指代一切。翻译成中文，我个人觉得，“东西”这个词儿比较贴切。那物联网，就可以理解为把所有东西都用互联网给连接起来。\n这里面不仅仅包括像电脑、手机这样的智能设备，还包括一些已经智能化的传统设备，比如汽车、冰箱、路边的摄像头等等，将来还将包括更多的、各种各样的物品：比如水杯、衣服、工业用的各种设备和工具等等，也就是所谓的万物互联。所以，IoT 它的未来绝对是大有可期的。\n那这些物联网设备，它要实现互相通信，也必须有一套标准的通信协议，MQTT 就是专门为物联网设备设计的一套标准的消息队列通信协议。使用 MQTT 协议的 IoT 设备，可以连接到任何支持 MQTT 协议的消息队列上，进行通信。\n这节课，我们就一起来聊一聊 MQTT 协议，以及如何把 MQTT 应用到生产系统中去。\nMQTT 和其他消息队列的传输协议有什么不同？ 从宏观上来说，MQTT 和其他消息队列采用的传输协议是差不多的。它采用的也是“发布 - 订阅”的消息模型。网络结构上，也是 C/S 架构，IoT 设备是客户端，Broker 是服务端，客户端与 Broker 通信进行收发消息。\n虽然 MQTT 和普通的消息队列相比，在消息模型、功能和网络结构上都是差不多的，但由于他们面对的使用场景是不一样的，所以，MQTT 和普通的消息队列相比，还是有很多区别的。我们看一下 MQTT 的使用场景有什么样的特点？\n首先，它的客户端都是运行在 IoT 设备上。IoT 设备它有什么特点？最大的特点就是便宜，一个水杯才卖几十块钱，它上面的智能模块的成本十块钱最多了，再贵就卖不出去了。十块钱的智能设备内存都是按照 KB 来计算的，可能都没有 CPU，也不一定有操作系统，整个设备就一个 SoC。这样的设备就需要通信协议不能太复杂，功能不能太多。另外，IoT 设备一般都采用无线连接，很多设备都是经常移动的，这就导致，IoT 设备的网络连接不稳定，并且这种不稳定的网络是一个常态。\nMQTT 协议在设计上，充分考虑了上面的这些特点。在协议的报文设计上极其的精简，可以说是惜字如金。协议的功能也非常简单，基本上就只有发布订阅主题和收发消息这两个最核心的功能。另外，为了应对网络连接不稳定的问题，MQTT 增加了心跳和会话的机制。加入心跳机制可以让客户端和服务端双方都能随时掌握当前连接状态，一旦发现连接中断，可以尽快地重连。MQTT 还加入了会话的机制，在服务端来保存会话状态，客户端重连之后就可以恢复之前的会话，继续来收发消息。这样，把复杂度转移到了服务端，客户端的实现就会更简单。\nMQTT 面临的使用场景中，另外一个很重要的特点就是，服务端需要支撑海量的 IoT 设备同时在线。对于普通的消息队列集群，服务的客户端都运行在性能强大的服务器上，所以客户端的数量不会特别多。比如京东的 JMQ 集群，日常在线的客户端数量大概是十万左右这样的规模，就足够支撑全国人民在京东上买买买。这个规模已经是这个地球上少有的，几个超大规模的消息队列集群之一了。\n而 MQTT 的使用场景中，需要支撑的客户端数量，远不止几万几十万。比如，北京交通委如果要把全市的车辆都接入进来，这是就一个几百万客户端的规模。路边随处可见的摄像头，每家每户都有的电视、冰箱，每个人随身携带的各种穿戴设备，这些设备的规模都是百万、千万级甚至是上亿的级别。\n另外，MQTT 它是不支持点对点通信的，一般的做法都是，每个客户端都创建一个以自己 ID 为名字的主题，然后客户端来订阅自己的专属主题，用于接收专门发给这个客户端的消息。这就意味着，在 MQTT 的集群中，主题的数量是和客户端的数量基本是同一个量级的。\n如何选择 MQTT 产品？ 如何能支持海量在线的 IoT 设备和海量的主题，是每一个支持 MQTT 协议的消息队列面临的最大挑战。也是你在做 MQTT 服务端技术选型时，需要重点考察的技术点。\n目前开源的 MQTT 产品中，有些是传统的消息队列，通过官方或者非官方的扩展，实现了 MQTT 协议的支持。也有一些专门的 MQTT Server 产品，这些 MQTT Server 在协议支持层面，大多数是没有问题的，性能和稳定性方面也都可以满足要求。但是，我还没有发现能很好支撑海量客户端和主题的开源产品。为什么呢？\n传统的消息队列，虽然它可以通过扩展来支持 MQTT 协议，但是它的整体架构在设计之初，并没有考虑能支撑海量客户端和主题。比如，之前我们讲过，RocketMQ 它的元数据是保存在 NameServer 的内存中，Kafka 是保存在 ZooKeeper 中，这些存储都不擅长保存大量数据，所以也支撑不了太多的客户端和主题。\n另外一些开源的 MQTT Server，很多根本就没有集群功能，或者集群功能做的不太完善。集群功能做的好的产品，它们的开发者大多都把集群功能放到企业版中拿去卖钱了。\n所以在做 MQTT Server 技术选型的时，如果你接入 IoT 设备数量在十万以内，是可以选择开源产品的，选型的原则和选择普通消息队列是一样的，我在《02 | 该如何选择消息队列？》这节课中讲过的选型原则都是适用的，优先选择一个流行的、你熟悉的开源产品就可以了。\n如果说客户端的规模超过十万的量级，需要支撑这么大规模的客户端数量，服务端只有单个节点肯定是不够的，必须用一个集群来支持，并且这个集群是要能支持水平扩容的，这些都是最基本的要求。这个时候就几乎没什么可供选择的开源产品了。这种情况建议选择一些云平台厂商提供的 MQTT 云服务，价格相对比较低，当然你可以选择价格更高的商业版 MQTT Server。\n另外一个选择就是，基于已有的开源 MQTT Server，通过一些集成和开发，来自行构建 MQTT 集群。接下来，我跟你说一下，构建一个支持海量客户端的 MQTT 集群，应该如何来设计。\nMQTT 集群如何支持海量在线的 IoT 设备？ 一般来说，一个 MQTT 集群它的架构应该是这样的：\n这个图从左向右看，首先接入的地址最好是一个域名，这样域名的后面可以配置多个 IP 地址做负载均衡，当然这个域名不是必需的。也可以直接连接负载均衡器。负载均衡可以选择像 F5 这种专用的负载均衡硬件，也可以使用 Nginx 这样的软件，只要是四层或者支持 MQTT 协议的七层负载均衡设备，都可以选择。\n负载均衡器的后面，需要部署一个 Proxy 集群，这个 Proxy 集群承担了三个重要的作用：第一个作用是来承接海量 IoT 设备的连接，第二个作用是来维护与客户端的会话，第三个作用是作为代理，在客户端和 Broker 之间进行消息转发。\n在 Proxy 集群的后面是 Broker 集群，负责保存和收发消息。\n有的 MQTT Server 它的集群架构是这样的：\n它的架构中没有 Proxy。实际上，它只是把 Proxy 和 Broker 的功能集成到了一个进程中，这两种架构它本质上没有太大的区别。所以这两种架构我们可以认为是同一种架构，一起来分析。\n前置 Proxy 的方式很容易解决海量连接的问题，由于 Proxy 是可以水平扩展的，只要用足够多数量的 Proxy 节点，就可以抗住海量客户端同时连接。每个 Proxy 和每个 Broker 只用一个连接通信就可以了，这样对于每个 Broker 来说，它的连接数量最多不会超过 Proxy 节点的数量。\nProxy 对于会话的处理方式，可以借鉴 Tomcat 处理会话的方式。一种方式是，将会话保存在 Proxy 本地，每个 Proxy 节点都只维护连接到自己的这些客户端的会话。但是，这种方式需要配合负载均衡来使用，负载均衡设备需要支持 sticky session，保证将相同会话的连接总是转发到同一个 Proxy 节点上。另一种方式是，将会话保存在一个外置的存储集群中，比如一个 Redis 集群或者 MySQL 集群。这样 Proxy 就可以设计成完全无状态的，对于负载均衡设备也没有特殊的要求。但这种方式要求外置存储集群具备存储千万级数据的能力，同时具有很好的性能。\n对于如何支持海量的主题，比较可行的解决方案是，在 Proxy 集群的后端，部署多组 Broker 小集群，比如说，可以是多组 Kafka 小集群，每个小集群只负责存储一部分主题。这样对于每个 Broker 小集群，主题的数量就可以控制在可接受的范围内。由于消息是通过 Proxy 来进行转发的，我们可以在 Proxy 中采用一些像一致性哈希等分片算法，根据主题名称找到对应的 Broker 小集群。这样就解决了支持海量主题的问题。\n总结 MQTT 是专门为物联网设备设计的一套标准的通信协议。这套协议在消息模型和功能上与普通的消息队列协议是差不多的，最大的区别在于应用场景不同。在物联网应用场景中，IoT 设备性能差，网络连接不稳定。服务端面临的挑战主要是，需要支撑海量的客户端和主题。\n已有的开源的 MQTT 产品，对于协议的支持都不错，在客户端数量小于十万级别的情况下，可以选择。对于海量客户端的场景，服务端必须使用集群来支撑，可以选择收费的云服务和企业版产品。也可以选择自行来构建 MQTT 集群。\n自行构建集群，最关键的技术点就是，通过前置的 Proxy 集群来解决海量连接、会话管理和海量主题这三个问题。前置 Proxy 负责在 Broker 和客户端之间转发消息，通过这种方式，将海量客户端连接收敛为少量的 Proxy 与 Broker 之间的连接，解决了海量客户端连接数的问题。维护会话的实现原理，和 Tomcat 维护 HTTP 会话是一样的。对于海量主题，可以在后端部署多组 Broker 小集群，每个小集群分担一部分主题这样的方式来解决。\n思考 课后请你针对我们这节课讲到的 Proxy，做一个详细设计，不用写文档，只要画出如下三个 UML 图即可：\nProxy 的类图 (UML Class Diagram)：粒度要包含到下面两个时序图用到的主要的类和方法； Proxy 收发消息的时序图 (UML Sequence Diagram)：分别绘制出 Proxy 生产消息和消费消息这两个流程的时序图。\nPulsar的存储计算分离设计：全新的消息队列设计思路 之前的课程，我们大部分时间都在以 RocketMQ、Kafka 和 RabbitMQ 为例，通过分析源码的方式，来讲解消息队列的实现原理。原因是，这三种消息队列在国内的受众群体非常庞大，大家在工作中会经常用到。这节课，我给你介绍一个不太一样的开源消息队列产品：Apache Pulsar。\nPulsar 也是一个开源的分布式消息队列产品，最早是由 Yahoo 开发，现在是 Apache 基金会旗下的开源项目。你可能会觉得好奇，我们的课程中为什么要花一节课来讲 Pulsar 这个产品呢？原因是，Pulsar 在架构设计上，和其他的消息队列产品有非常显著的区别。我个人的观点是，Pulsar 的这种全新的架构设计，很可能是消息队列这类中间件产品未来架构的发展方向。\n接下来我们一起看一下，Pulsar 到底有什么不同？\nPulsar 的架构和其他消息队列有什么不同？ 我们知道，无论是 RocketMQ、RabbitMQ 还是 Kafka，消息都是存储在 Broker 的磁盘或者内存中。客户端在访问某个主题分区之前，必须先找到这个分区所在 Broker，然后连接到这个 Broker 上进行生产和消费。\n在集群模式下，为了避免单点故障导致丢消息，Broker 在保存消息的时候，必须也把消息复制到其他的 Broker 上。当某个 Broker 节点故障的时候，并不是集群中任意一个节点都能替代这个故障的节点，只有那些“和这个故障节点拥有相同数据的节点”才能替代这个故障的节点。原因就是，每一个 Broker 存储的消息数据是不一样的，或者说，每个节点上都存储了状态（数据）。这种节点称为“有状态的节点（Stateful Node）”。\nPulsar 与其他消息队列在架构上，最大的不同在于，它的 Broker 是无状态的（Stateless）。也就是说，在 Pulsar 的 Broker 中既不保存元数据，也不存储消息。那 Pulsar 的消息存储在哪儿呢？我们来看一下 Pulsar 的架构是什么样的。\n这张 Pulsar 的架构图来自 Pulsar 的官方文档，如果你想了解这张架构图的细节，可以去看官方文档中的Architecture Overview。我来给你解读一下这张图中我们感兴趣的重点内容。\n先来看图中右侧的 Bookie 和 ZK 这两个方框，这两个方框分别代表了 BookKeeper 集群和 ZooKeeper 集群。ZooKeeper 集群的作用，我在《24 | Kafka 的协调服务 ZooKeeper：实现分布式系统的“瑞士军刀》这节课中专门讲过，在 Pulsar 中，ZooKeeper 集群的作用和在 Kafka 中是一样的，都是被用来存储元数据。BookKeeper 集群则被用来存储消息数据。\n那这个 BookKeeper 又是什么呢？BookKeeper 有点儿类似 HDFS，是一个分布式的存储集群，只不过它的存储单元和 HDFS 不一样，在 HDFS 中存储单元就是文件，这个很好理解。而 BookKeeper 的存储单元是 Ledger。这个 Ledger 又是什么呢？\n这里再次吐槽一下国外程序员喜欢发明概念、增加学习成本这个坏习惯。其实 Ledger 就是一段 WAL（Write Ahead Log），或者你可以简单地理解为某个主题队列（分区数据）的一段，它包含了连续的若干条消息，消息在 Ledger 中称为 Entry。为了保证 Ledger 中的 Entry 的严格顺序，Pulsar 为 Ledger 增加一次性的写入限制，Broker 创建一个 Ledger 后，只有这个 Broker 可以往 Ledger 中写入 Entry，一旦 Ledger 关闭后，无论是 Broker 主动关闭，还是因为 Broker 宕机异常关闭，这个 Ledger 就永远只能读取不能写入了。如果需要继续写入 Entry，只能新建另外一个 Ledger。\n请你注意一下，这种“一次性写入”的设计，它的主要目的是为了解决并发写入控制的问题，我在之前课程中讲过，对于共享资源数据的并发写一般都是需要加锁的，否则很难保证数据的一致性。对于分布式存储来说，就需要加“分布式锁”。\n但我们知道，分布式锁本身就很难实现，使用分布式锁对性能也会有比较大的损失。这种“一次性写入”的设计，只有创建 Ledger 的进程可以写入数据，Ledger 这个资源不共享，也就不需要加锁，是一种很巧妙的设计，你在遇到类似场景的时候可以借鉴。\n消息数据由 BookKeeper 集群负责存储，元数据由 ZooKeeper 集群负责存储，Pulsar 的 Broker 上就不需要存储任何数据了，这样 Broker 就成为了无状态的节点。\n虽然 Broker 是无状态的，不存储任何的数据，但是，在一个特定的时刻，每一个主题的分区，还是要落在某个具体的 Broker 上。不能说多个 Broker 同时读写同一个分区，因为这样是没有办法保证消息的顺序的，也没有办法来管理消费位置。\n再来看图中左侧最大的那个 Broker 方框，在 Broker 中包含了几个重要的模块。Load Balancer 负责动态的分配，哪些 Broker 管理哪些主题分区。Managed Ledger 这个模块负责管理本节点需要用到的那些 Ledger，当然这些 Ledger 都是保存在 BookKeeper 集群中的。为了提升性能，Pulsar 同样采用用了一个 Cache 模块，来缓存一部分 Ledger。\nPulsar 的客户端要读写某个主题分区上的数据之前，依然要在元数据中找到分区当前所在的那个 Broker，这一点是和其他消息队列的实现是一样的。不一样的地方是，其他的消息队列，分区与 Broker 的对应关系是相对稳定的，只要不发生故障，这个关系是不会变的。而在 Pulsar 中，这个对应关系是动态的，它可以根据 Broker 的负载情况进行动态调整，而且由于 Broker 是无状态的，分区可以调整到集群中任意一个 Broker 上，这个负载均衡策略就可以做得非常简单并且灵活。如果某一个 Broker 发生故障，可以立即用任何一个 Broker 来替代它。\n那在这种架构下，Pulsar 又是如何来完成消息收发的呢？客户端在收发消息之前，需要先连接 Service Discovery 模块，获取当前主题分区与 Broker 的对应关系，然后再连接到相应 Broker 上进行消息收发。客户端收发消息的整体流程，和其他的消息队列是差不多的。比较显著的一个区别就是，消息是保存在 BookKeeper 集群中的，而不是本机上。数据的可靠性保证也是 BookKeeper 集群提供的，所以 Broker 就不需要再往其他的 Broker 上复制消息了。\n图中的 Global replicators 模块虽然也会复制消息，但是复制的目的是为了在不同的集群之间共享数据，而不是为了保证数据的可靠性。集群间数据复制是 Pulsar 提供的一个特色功能，具体可以看一下 Pulsar 文档中的geo-replication这部分。\n存储计算分离的设计有哪些优点？ 在 Pulsar 这种架构下，消息数据保存在 BookKeeper 中，元数据保存在 ZooKeeper 中，Broker 的数据存储的职责被完全被剥离出去，只保留了处理收发消息等计算的职责，这就是一个非常典型的“存储计算分离”的设计。\n什么是存储计算分离呢？顾名思义，就是将系统的存储职责和计算职责分离开，存储节点只负责数据存储，而计算节点只负责计算，也就是执行业务逻辑。这样一种设计，称为存储计算分离。存储计算分离设计并不新鲜，它的应用其实是非常广泛的。\n比如说，所有的大数据系统，包括 Map Reduce 这种传统的批量计算，和现在比较流行的 Spark、Flink 这种流计算，它们都采用的存储计算分离设计。数据保存在 HDFS 中，也就是说 HDFS 负责存储，而负责计算的节点，无论是用 YARN 调度还是 Kubernetes 调度，都只负责“读取 - 计算 - 写入”这样一种通用的计算逻辑，不保存任何数据。\n更普遍的，我们每天都在开发的各种 Web 应用和微服务应用，绝大多数也采用的是存储计算分离的设计。数据保存在数据库中，微服务节点只负责响应请求，执行业务逻辑。也就是说，数据库负责存储，微服务节点负责计算。\n那存储计算分离有什么优点呢？我们分两方面来看。\n对于计算节点来说，它不需要存储数据，节点就变成了无状态的（Stateless）节点。一个由无状态节点组成的集群，管理、调度都变得非常简单了。集群中每个节点都是一样的，天然就支持水平扩展。任意一个请求都可以路由到集群中任意一个节点上，负载均衡策略可以做得非常灵活，可以随机分配，可以轮询，也可以根据节点负载动态分配等等。故障转移（Failover）也更加简单快速，如果某个节点故障了，直接把请求分配给其他节点就可以了。\n对比一下，像 ZooKeeper 这样存储计算不分离的系统，它们的故障转移就非常麻烦，一般需要用复杂的选举算法，选出新的 leader，提供服务之前，可能还需要进行数据同步，确保新的节点上的数据和故障节点是完全一致之后，才可以继续提供服务。这个过程是非常复杂而且漫长的。\n对于计算节点的开发者来说，可以专注于计算业务逻辑开发，而不需要关注像数据一致性、数据可靠性、故障恢复和数据读写性能等等这些比较麻烦的存储问题，极大地降低了开发难度，提升了开发效率。\n而对于存储系统来说，它需要实现的功能就很简单，系统的开发者只需要专注于解决一件事就可以了，那就是“如何安全高效地存储数据？”并且，存储系统的功能是非常稳定的，比如像 ZooKeeper、HDFS、MySQL 这些存储系统，从它们诞生到现在，功能几乎就没有变过。每次升级都是在优化存储引擎，提升性能、数据可靠性、可用性等等。\n接下来说存储计算分离这种设计的缺点。\n俗话说，背着抱着一样沉。对于一个系统来说，无论存储和计算是不是分离的，它需要完成的功能和解决的问题是一样的。就像我刚刚讲到的，Pulsar 的 Broker 相比于其他消息队列的 Broker，各方面都变的很简单。这并不是说，存储计算分离的设计能把系统面临的各种复杂的问题都解决了，其实一个问题都没解决，只是把这些问题转移到了 BookKeeper 这个存储集群上了而已。\n**BookKeeper 依然要解决数据一致性、节点故障转移、选举、数据复制等等这些问题。**并且，存储计算分离之后，原来一个集群变成了两个集群，整个系统其实变得更加复杂了。\n另外，存储计算分离之后，系统的性能也会有一些损失。比如，从 Pulsar 的 Broker 上消费一条消息，Broker 还需要去请求 BookKeeper 集群读取数据，然后返回给客户端，这个过程至少增加了一次网络传输和 n 次内存拷贝。相比于直接读本地磁盘，性能肯定是要差一些的。\n不过，对于业务系统来说，采用存储计算分离的设计，它并不需要自己开发一个数据库或者 HDFS，只要用现有的成熟的存储系统就可以了，所以相当于系统的复杂度还是降低了。相比于存储计算分离带来的各种优点，损失一些性能也是可以接受的。\n因此，对于大部分业务系统来说，采用存储计算分离设计，都是非常划算的。\n小结 这节课我们一起分析了 Apache Pulsar 的架构，然后一起学习了一下存储计算分离的这种设计思想。\nPulsar 和其他消息队列最大的区别是，它采用了存储计算分离的设计。存储消息的职责从 Broker 中分离出来，交给专门的 BookKeeper 存储集群。这样 Broker 就变成了无状态的节点，在集群调度和故障恢复方面更加简单灵活。\n存储计算分离是一种设计思想，它将系统的存储职责和计算职责分离开，存储节点只负责数据存储，而计算节点只负责计算，计算节点是无状态的。无状态的计算节点，具有易于开发、调度灵活的优点，故障转移和恢复也更加简单快速。这种设计的缺点是，系统总体的复杂度更高，性能也更差。不过对于大部分分布式的业务系统来说，由于它不需要自己开发存储系统，采用存储计算分离的设计，既可以充分利用这种设计的优点，整个系统也不会因此变得过于复杂，综合评估优缺点，利大于弊，更加划算。\n思考 课后请你想一下，既然存储计算分离这种设计有这么多的优点，那为什么除了 Pulsar 以外，大多数的消息队列都没有采用存储计算分离的设计呢？\n答疑解惑（二）：我的100元哪儿去了？ 今天这节课，是我们的“消息队列高手课第二阶段进阶篇的最后一节课，照例，我们在每一阶段的最后，安排一节课进行热点问题的答疑，针对同学们遇到的一些共同的问题，统一来进行详细的解答。\n1. 我的 100 元哪儿去了？聊聊并发调用情况下的幂等性 在期中测试中，有这样一道题。\n如果可以保证以下这些操作的原子性，哪些操作在并发调用的情况下具备幂等性？\n A. f(n, a)：给账户 n 转入 a 元 B. f(n, a)：将账户 n 的余额更新为 a 元 C. f(n, b, a)：如果账户 n 当前的余额为 b 元，那就将账户的余额更新为 n 元 D. f(n, v, a)：如果账户 n 当前的流水号等于 v，那么给账户的余额加 a 元，并将流水号加一  这道题的正确答案是 D。很多同学都留言提问，选项 B 中，将账户 n 的余额更新为 a 元，这个操作不具备幂等性吗？\n如果单单只是考虑这个操作，执行一次和执行多次，对系统的影响是一样的，账户 n 的余额都是 a 元。所以，这个操作确实是幂等的。但请你注意审题，我们的题目中说的是：“哪些操作在并发调用的情况下具备幂等性？”在并发调用的情况下，我们再来看一下 B 这个选项的操作是否还具备幂等性。\n假设，账户余额 100 元，依次执行 2 次转账：\n 将账户余额设为 200 元； 将账户余额设为 300 元；  经过两次转账后，账户余额应该是 300 元。\n再次注意，我们的题目中说的是在并发调用的情况下。\n按照时间顺序，就有可能会出现下面这种情况：\n t0 时刻客户端发送请求：将账户余额设为 200 元。 t1 时刻服务端收到请求，账户余额由 100 元变为 200 元，然后服务端发出给客户端操作成功的响应，但是这个响应在网络传输过程中丢失了。 t2 时刻客户端发送请求：将账户余额设为 300 元。 t3 时刻服务端收到请求，账户余额由 200 元变为 300 元，然后服务端发出给客户端操作成功的响应。 t4 时刻客户端：收到“将账户余额设为 300 元”这个请求的成功响应，本次调用成功。 t5 时刻客户端由于没收到“将账户余额设为 300 元”这个请求的成功响应，重新发送请求：将账户余额设为 200 元。 t6 时刻服务端收到请求，账户余额由 300 元变为 200 元，然后服务端给客户端发出操作成功的响应。 t7 时刻客户端收到响应，本次重试调用成功。  结果，账户余额错误地变成了 200 元。\n同学，请把我的 100 块钱还给我！通过这个题，我们可以总结出来，一个操作是否幂等，还跟调用顺序有关系，在线性调用情况下，具备幂等性的操作，在并发调用时，就不一定具备幂等性了。如果你在设计系统的时候，没有注意到这个细节，那系统就有可能出现我们上面这个例子中的错误，在生产系中，这是非常危险的。\n2. Kafka 和 RocketMQ 如何通过选举来产生新的 Leader？ 在《22 | Kafka 和 RocketMQ 的消息复制实现的差异点在哪？》这节课中，我给你讲了这两个消息队列是如何通过复制来保证数据一致性的。当 Broker 节点发生故障时，它们都是通过选举机制，来选出新的 Leader 来继续提供服务。当时限于篇幅，我们并没有深入进去来讲选举的实现原理。那 Kafka 和 RocketMQ（Dledger）都是怎么来实现的选举呢？\n先来说 Kafka 的选举，因为 Kafka 的选举实现比较简单。严格地说，Kafka 分区的 Leader 并不是选举出来的，而是 Controller 指定的。Kafka 使用 ZooKeeper 来监控每个分区的多个副本，如果发现某个分区的主节点宕机了，Controller 会收到 ZooKeeper 的通知，这个时候，Controller 会从 ISR 节点中选择一个节点，指定为新的 Leader。\n在 Kafka 中 Controller 本身也是通过 ZooKeeper 选举产生的。接下来我要讲的，Kafka Controller 利用 ZooKeeper 选举的方法，你一定要记住并学会，因为这种方法非常简单实用，并且适用性非常广泛，在设计很多分布式系统中都可以用到。\n这种选举方法严格来说也不是真正的“选举”，而是一种抢占模式。实现也很简单，每个 Broker 在启动后，都会尝试在 ZooKeeper 中创建同一个临时节点：/controller，并把自身的信息写入到这个节点中。由于 ZooKeeper 它是一个可以保证数据一致性的分布式存储，所以，集群中只会有一个 Broker 抢到这个临时节点，那它就是 Leader 节点。其他没抢到 Leader 的节点，会 Watch 这个临时节点，如果当前的 Leader 节点宕机，所有其他节点都会收到通知，它们会开始新一轮的抢 Leader 游戏。\n这就好比有个玉玺，也就是皇帝用的那个上面雕着龙纹的大印章，谁都可以抢这个玉玺，谁抢到谁做皇帝，其他没抢到的人也不甘心，时刻盯着这个玉玺，一旦现在这个皇帝驾崩了，所有人一哄而上，再“抢”出一个新皇帝。这个算法虽然不怎么优雅，但胜在简单直接，并且快速公平，是非常不错的选举方法。\n但是这个算法它依赖一个“玉玺”，也就是一个可以保证数据一致性的分布式存储，这个分布式存储不一定非得是 ZooKeeper，可以是 Redis，可以是 MySQL，也可以是 HDFS，只要是可以保证数据一致性的分布式存储，都可以充当这个“玉玺”，所以这个选举方法的适用场景也是非常广泛的。\n再来说 RocketMQ/Dledger 的选举，在 Dledger 中的 Leader 真的是通过投票选举出来的，所以它不需要借助于任何外部的系统，仅靠集群的节点间投票来达成一致，选举出 Leader。一般这种自我选举的算法，为了保证数据一致性、避免集群分裂，算法设计的都非常非常复杂，我们不太可能自己来实现这样一个选举算法，所以我在这里不展开讲。Dledger 采用的是Raft 一致性算法，感兴趣的同学可以读一下这篇经典的论文。\n像 Raft 这种自我选举的算法，相比于上面介绍的抢占式选举，优点是不需要借助外部系统，完全可以实现自我选举。缺点也非常明显，就是算法实在是太复杂了，非常难实现。并且，往往集群中的节点要通过多轮投票才能达成一致，这个选举过程是比较慢的，一次选举耗时几秒甚至几十秒都有可能。\n我们日常在设计一些分布式的业务系统时，如果需要选举 Leader，还是采用 Kafka 的这种“抢玉玺”的方法更加简单实用。\n3. 为什么说 Pulsar 存储计算分离的架构是未来消息队列的发展方向？ 在上节课《27 | Pulsar 的存储计算分离设计：全新的消息队列设计思路》中，我给你留的思考题是：为什么除了 Pulsar 以外，大多数的消息队列都没有采用存储计算分离的设计呢？这个问题其实是一个发散性的问题，并没有什么标准答案。因为，本来架构设计就是在权衡各种利弊，做出取舍和选择，并没有绝对的对错之分。\n很多同学在课后的留言中，都已经给出了自己的思路和想法，而且有些同学的想法和我个人的观点不谋而合。在这里我也和你分享一下我对这个问题的理解和看法。\n早期的消息队列，主要被用来在系统之间异步交换数据，大部分消息队列的存储能力都比较弱，不支持消息持久化，不提倡在消息队列中堆积大量的消息，这个时期的消息队列，本质上是一个数据的管道。\n现代的消息队列，功能上看似没有太多变化，依然是收发消息，但是用途更加广泛，数据被持久化到磁盘中，大多数消息队列具备了强大的消息堆积能力，只要磁盘空间足够，可以存储无限量的消息，而且不会影响生产和消费的性能。这些消息队列，本质上已经演变成为分布式的存储系统。\n理解了这一点，你就会明白，为什么大部分消息队列产品，都不使用存储计算分离的设计。为一个“分布式存储系统”做存储计算分离，计算节点就没什么业务逻辑需要计算的了。而且，消息队列又不像其他的业务系统，可以直接使用一些成熟的分布式存储系统来存储消息，因为性能达不到要求。分离后的存储节点承担了之前绝大部分功能，并且增加了系统的复杂度，还降低了性能，显然是不划算的。\n那为什么 Pulsar 还要采用这种存储和计算分离的设计呢？我们还是需要用发展的眼光看问题。我在上节课说过，Pulsar 的这种架构，很可能代表了未来消息队列的发展方向。为什么这么说呢？你可以看一下现在各大消息队列的 Roadmap（发展路线图），Kafka 在做 Kafka Streams，Pulsar 在做 Pulsar Functions，其实大家都在不约而同的做同一件事儿，就是流计算。\n原因是什么呢？现有的流计算平台，包括 Storm、Flink 和 Spark，它们的节点都是无状态的纯计算节点，是没有数据存储能力的。所以，现在的流计算平台，它很难做大量数据的聚合，并且在数据可靠性保证、数据一致性、故障恢复等方面，也做得不太好。\n而消息队列正好相反，它很好地保证了数据的可靠性、一致性，但是 Broker 只具备存储能力，没有计算的功能，数据流进去什么样，流出来还是什么样。同样是处理实时数据流的系统，一个只能计算不能存储，一个只能存储不能计算，那未来如果出现一个新的系统，既能计算也能存储，如果还能有不错的性能，是不是就会把现在的消息队列和流计算平台都给替代了？这是很有可能的。\n对于一个“带计算功能的消息队列”来说，采用存储计算分离的设计，计算节点负责流计算，存储节点负责存储消息，这个设计就非常和谐了。\n到这里，我们课程的第二个模块–进阶篇，也就全部结束了。进阶篇的中讲解知识有一定的难度，特别是后半部分的几节源码分析课，从评论区同学们的留言中，我也能感受到，有些同学学习起来会有些吃力。\n我给同学们的建议是，除了上课时听音频和读文稿之外，课后还要自己去把源代码下载下来，每一个流程从头到尾读一遍源码，最好是打开单步调试模式，一步一步地跟踪一下执行过程。读完源码之后，还要把类图、流程图或者时序图画出来，只有这样才能真正理解实现过程。\n从下节课开始，我们的课程就进入最后一个模块：案例篇。在这个模块中，我会带你一起动手来写代码，运用我们在课程中所学的知识，来做一些实践的案例。首先我会带你一起做一个消息队列和流计算的案例，你可以来体会一下现在的流计算平台它是什么样的。然后，我们还会用进阶篇中所学到的知识，来一起实现一个类似 Dubbo 的 RPC 框架\nJMQ的Broker是如何异步处理消息的？ 我们的课程更新到进阶篇之后，通过评论区的留言，我看到有一些同学不太理解，为什么在进阶篇中要讲这些“看起来和消息队列关系不大的”内容呢？\n在这里，我跟你分享一下这门课程的设计思路。我们这门课程的名称是“消息队列高手课”，我希望你在学习完这门课程之后，不仅仅只是成为一个使用消息队列的高手，而是设计和实现消息队列的高手。所以我们在设计课程的时候，分了基础篇、进阶篇和案例篇三部分。\n基础篇中我们给大家讲解消息队列的原理和一些使用方法，重点是让大家学会使用消息队列。\n你在进阶篇中，我们课程设计的重点是讲解实现消息队列必备的技术知识，通过分析源码讲解消息队列的实现原理。希望你通过进阶篇的学习能够掌握到设计、实现消息队列所必备的知识和技术，这些知识和技术也是设计所有高性能、高可靠的分布式系统都需要具备的。\n进阶篇的上半部分，我们每一节课一个专题，来讲解设计实现一个高性能消息队列，必备的技术和知识。这里面每节课中讲解的技术点，不仅可以用来设计消息队列，同学们在设计日常的应用系统中也一定会用得到。\n前几天我在极客时间直播的时候也跟大家透露过，由我所在的京东基础架构团队开发的消息队列 JMQ，它的综合性能要显著优于目前公认性能非常好的 Kafka。虽然在开发 JMQ 的过程中有很多的创新，但是对于性能的优化这块，并没有什么全新的划时代的新技术，JMQ 之所以能做到这样的极致性能，靠的就是合理地设计和正确地使用已有的这些通用的底层技术和优化技巧。我把这些技术和知识点加以提炼和总结，放在进阶篇的上半部分中。\n进阶篇的下半部分，我们主要通过分析源码的方式，来学习优秀开源消息队列产品中的一些实现原理和它们的设计思想。\n在最后的案例篇，我会和大家一起，利用进阶篇中学习的知识，一起来开发一个简单的 RPC 框架。为什么我们要开发一个 RPC 框架，而不是一个消息队列？这里面就是希望大家不只是机械的去学习，仅仅是我告诉这个问题怎么解决，你就只是学会了这个问题怎么解决，而是能做到真正理解原理，掌握知识和技术，并且能融会贯通，灵活地去使用。只有这样，你才是真的“学会了”。\n有的同学在看了进阶篇中已更新的这几节课程之后，觉得只讲技术原理不过瘾，希望能看到这些技术是如何在消息队列中应用并落地的，看到具体的实现和代码，所以我以京东 JMQ 为例，将这些基础技术点在消息队列实现中的应用讲解一下。\nJMQ 的 Broker 是如何异步处理消息的？ 对于消息队列的 Broker，它最核心的两个流程就是接收生产者发来的消息，以及给消费者发送消息。后者的业务逻辑相对比较简单，影响消息队列性能的关键，就是消息生产的这个业务流程。在 JMQ 中，经过优化后的消息生产流程，实测它每秒钟可以处理超过 100 万次请求。\n我们在之前的课程中首先讲了异步的设计思想，这里给你分享一下我在设计这个流程时，是如何来将异步的设计落地的。\n消息生产的流程需要完成的功能是这样的：\n 首先，生产者发送一批消息给 Broker 的主节点； Broker 收到消息之后，会对消息做一系列的解析、检查等处理； 然后，把消息复制给所有的 Broker 从节点，并且需要把消息写入到磁盘中； 主节点收到大多数从节点的复制成功确认后，给生产者回响应告知消息发送成功。  由于使用各种异步框架或多或少都会有一些性能损失，所以我在设计这个流程的时候，没有使用任何的异步框架，而是自行设计一组互相配合的处理线程来实现，但使用的异步设计思想和我们之前课程中所讲的是一样的。\n对于这个流程，我们设计的线程模型是这样的：\n图中白色的细箭头是数据流，蓝色的箭头是控制流，白色的粗箭头代表远程调用。蓝白相间的方框代表的是处理的步骤，我在蓝色方框中标注了这个步骤是在什么线程中执行的。圆角矩形代表的是流程中需要使用的一些关键的数据结构。\n这里我们设计了 6 组线程，将一个大的流程拆成了 6 个小流程。并且整个过程完全是异步化的。\n流程的入口在图中的左上角，Broker 在收到来自生产者的发消息请求后，会在一个 Handler 中处理这些请求，这和我们在普通的业务系统中，用 Handler 接收 HTTP 请求是一样的，执行 Handler 中业务逻辑使用的是 Netty 的 IO 线程。\n收到请求后，我们在 Handler 中不做过多的处理，执行必要的检查后，将请求放到一个内存队列中，也就是图中的 Requests Queue。请求被放入队列后，Handler 的方法就结束了。可以看到，在 Handler 中只是把请求放到了队列中，没有太多的业务逻辑，这个执行过程是非常快的，所以即使是处理海量的请求，也不会过多的占用 IO 线程。\n由于要保证消息的有序性，整个流程的大部分过程是不能并发的，只能单线程执行。所以，接下来我们使用一个线程 WriteThread 从请求队列中按照顺序来获取请求，依次进行解析请求等其他的处理逻辑，最后将消息序列化并写入存储。序列化后的消息会被写入到一个内存缓存中，就是图中的 JournalCache，等待后续的处理。\n执行到这里，一条一条的消息已经被转换成一个连续的字节流，每一条消息都在这个字节流中有一个全局唯一起止位置，也就是这条消息的 Offset。后续的处理就不用关心字节流中的内容了，只要确保这个字节流能快速正确的被保存和复制就可以了。\n这里面还有一个工作需要完成，就是给生产者回响应，但在这一步，消息既没有落盘，也没有完成复制，还不能给客户端返回响应，所以我们把待返回的响应按照顺序放到一个内存的链表 Pending Callbacks 中，并记录每个请求中的消息对应的 Offset。\n然后，我们有 2 个线程，FlushThread 和 ReplicationThread，这两个线程是并行执行的，分别负责批量异步进行刷盘和复制，刷盘和复制又分别是 2 个比较复杂的流程，我们暂时不展开讲。刷盘线程不停地将新写入 Journal Cache 的字节流写到磁盘上，完成一批数据的刷盘，它就会更新一个刷盘位置的内存变量，确保这个刷盘位置之前数据都已经安全的写入磁盘中。复制线程的逻辑也是类似的，同样维护了一个复制位置的内存变量。\n最后，我们设计了一组专门用于发送响应的线程 ReponseThreads，在刷盘位置或者复制位置更新后，去检查待返回的响应链表 Pending Callbacks，根据 QOS 级别的设置（因为不同 QOS 基本对发送成功的定义不一样，有的设置需要消息写入磁盘才算成功，有的需要复制完成才算成功），将刷盘位置或者复制位置之前所有响应，以及已经超时的响应，利用这组线程 ReponseThreads 异步并行的发送给各个客户端。\n这样就完成了消息生产这个流程。整个流程中，除了 JournalCache 的加载和卸载需要对文件加锁以外，没有用到其他的锁。每个小流程都不会等待其他流程的共享资源，也就不用互相等待资源（没有数据需要处理时等待上游流程提供数据的情况除外），并且只要有数据就能第一时间处理。\n这个流程中，最核心的部分在于 WriteThread 执行处理的这个步骤，对每条消息进行处理的这些业务逻辑，都只能在 WriteThread 中单线程执行，虽然这里面干了很多的事儿，但是我们确保这些逻辑中，没有缓慢的磁盘和网络 IO，也没有使用任何的锁来等待资源，全部都是内存操作，这样即使单线程可以非常快速地执行所有的业务逻辑。\n这个里面有很重要的几点优化：\n 一是我们使用异步设计，把刷盘和复制这两部分比较慢的操作从这个流程中分离出去异步执行； 第二是，我们使用了一个写缓存 Journal Cache 将一个写磁盘的操作，转换成了一个写内存的操作，来提升数据写入的性能，关于如何使用缓存，后面我会专门用一节课来讲； 第三是，这个处理的全流程是近乎无锁的设计，避免了线程因为等待锁导致的阻塞； 第四是，我们把回复响应这个需要等待资源的操作，也异步放到其他的线程中去执行。  你看，一个看起来很简单的接收请求写入数据并回响应的流程，需要涉及的技术包括：异步的设计、缓存设计、锁的正确使用、线程协调、序列化和内存管理，等等。你需要对这些技术都有深入的理解，并合理地使用，才能在确保逻辑正确、数据准确的前提下，做到极致的性能。这也是为什么我们在课程的进阶篇中，用这么多节课来逐一讲解这些“看起来和消息队列没什么关系”的知识点和技术。\n我也希望同学们在学习这些知识点的时候，不仅仅只是记住了，能说出来，用于回答面试问题，还要能真正理解这些知识点和技术背后深刻的思想，并使用在日常的设计和开发过程中。\n比如说，在面试的时候，很多同学都可以很轻松地讲 JVM 内存结构，也知道怎么用 jstat、jmap、jstack 这些工具来查看虚拟机的状态。但是，当我给出一个有内存溢出的问题程序和源代码，让他来分析原因并改正的时候，却很少有人能给出正确的答案。在我看来，对于 JVM 这些基础知识，这样的同学他以为自己已经掌握了，但是，无法领会技术背后的思想，做不到学以致用，那还只是别人知识，不是你的。\n再比如，我下面要说的这个俩大爷的作业，你是真的花时间把代码写出来了，还只是在脑子想了想怎么做，就算完成了呢？\n俩大爷的思考题 我们在进阶篇的开始，花了 4 节课的内容，来讲解如何实现高性能的异步网络通信，在《13 | 传输协议：应用程序之间对话的语言》中，我给大家留了一个思考题：写一个程序，让俩大爷在胡同口遇见 10 万次并记录下耗时。\n有几个同学在留言区分享了自己的代码，每一个同学分享的代码我都仔细读过，有的作业实现了异步的网络通信，有的作业序列化和协议设计实现得很好，但很遗憾的是，没有一份作业能在序列化、协议设计和异步网络传输这几方面都做到我期望的水平。\n在这个作业中，应用到了我们进阶篇中前四节课讲到的几个知识点：\n 使用异步设计的方法； 异步网络 IO； 专用序列化、反序列化方法； 设计良好的传输协议； 双工通信。  这里面特别是双工通信的方法，大家都没能正确的实现。所以，这些作业的实际执行性能也没能达到一个应有的水平。\n这里，我也给出一个作业的参考实现，我们用 Go 语言来实现这个作业：\npackage main\rimport (\r\u0026quot;encoding/binary\u0026quot;\r\u0026quot;fmt\u0026quot;\r\u0026quot;io\u0026quot;\r\u0026quot;net\u0026quot;\r\u0026quot;sync\u0026quot;\r\u0026quot;time\u0026quot;\r)\rvar count = uint32(0) // 俩大爷已经遇见了多少次\rvar total = uint32(100000) // 总共需要遇见多少次\rvar z0 = \u0026quot; 吃了没，您吶?\u0026quot;\rvar z3 = \u0026quot; 嗨！吃饱了溜溜弯儿。\u0026quot;\rvar z5 = \u0026quot; 回头去给老太太请安！\u0026quot;\rvar l1 = \u0026quot; 刚吃。\u0026quot;\rvar l2 = \u0026quot; 您这，嘛去？\u0026quot;\rvar l4 = \u0026quot; 有空家里坐坐啊。\u0026quot;\rvar liWriteLock sync.Mutex // 李大爷的写锁\rvar zhangWriteLock sync.Mutex // 张大爷的写锁\rtype RequestResponse struct {\rSerial uint32 // 序号\rPayload string // 内容\r}\r// 序列化 RequestResponse，并发送\r// 序列化后的结构如下：\r// 长度\t4 字节\r// Serial 4 字节\r// PayLoad 变长\rfunc writeTo(r *RequestResponse, conn *net.TCPConn, lock *sync.Mutex) {\rlock.Lock()\rdefer lock.Unlock()\rpayloadBytes := []byte(r.Payload)\rserialBytes := make([]byte, 4)\rbinary.BigEndian.PutUint32(serialBytes, r.Serial)\rlength := uint32(len(payloadBytes) + len(serialBytes))\rlengthByte := make([]byte, 4)\rbinary.BigEndian.PutUint32(lengthByte, length)\rconn.Write(lengthByte)\rconn.Write(serialBytes)\rconn.Write(payloadBytes)\r// fmt.Println(\u0026quot; 发送: \u0026quot; + r.Payload)\r}\r// 接收数据，反序列化成 RequestResponse\rfunc readFrom(conn *net.TCPConn) (*RequestResponse, error) {\rret := \u0026amp;RequestResponse{}\rbuf := make([]byte, 4)\rif _, err := io.ReadFull(conn, buf); err != nil {\rreturn nil, fmt.Errorf(\u0026quot; 读长度故障：%s\u0026quot;, err.Error())\r}\rlength := binary.BigEndian.Uint32(buf)\rif _, err := io.ReadFull(conn, buf); err != nil {\rreturn nil, fmt.Errorf(\u0026quot; 读 Serial 故障：%s\u0026quot;, err.Error())\r}\rret.Serial = binary.BigEndian.Uint32(buf)\rpayloadBytes := make([]byte, length-4)\rif _, err := io.ReadFull(conn, payloadBytes); err != nil {\rreturn nil, fmt.Errorf(\u0026quot; 读 Payload 故障：%s\u0026quot;, err.Error())\r}\rret.Payload = string(payloadBytes)\rreturn ret, nil\r}\r// 张大爷的耳朵\rfunc zhangDaYeListen(conn *net.TCPConn) {\rfor count \u0026lt; total {\rr, err := readFrom(conn)\rif err != nil {\rfmt.Println(err.Error())\rbreak\r}\r// fmt.Println(\u0026quot; 张大爷收到：\u0026quot; + r.Payload)\rif r.Payload == l2 { // 如果收到：您这，嘛去？\rgo writeTo(\u0026amp;RequestResponse{r.Serial, z3}, conn, \u0026amp;zhangWriteLock) // 回复：嗨！吃饱了溜溜弯儿。\r} else if r.Payload == l4 { // 如果收到：有空家里坐坐啊。\rgo writeTo(\u0026amp;RequestResponse{r.Serial, z5}, conn, \u0026amp;zhangWriteLock) // 回复：回头去给老太太请安！\r} else if r.Payload == l1 { // 如果收到：刚吃。\r// 不用回复\r} else {\rfmt.Println(\u0026quot; 张大爷听不懂：\u0026quot; + r.Payload)\rbreak\r}\r}\r}\r// 张大爷的嘴\rfunc zhangDaYeSay(conn *net.TCPConn) {\rnextSerial := uint32(0)\rfor i := uint32(0); i \u0026lt; total; i++ {\rwriteTo(\u0026amp;RequestResponse{nextSerial, z0}, conn, \u0026amp;zhangWriteLock)\rnextSerial++\r}\r}\r// 李大爷的耳朵，实现是和张大爷类似的\rfunc liDaYeListen(conn *net.TCPConn, wg *sync.WaitGroup) {\rdefer wg.Done()\rfor count \u0026lt; total {\rr, err := readFrom(conn)\rif err != nil {\rfmt.Println(err.Error())\rbreak\r}\r// fmt.Println(\u0026quot; 李大爷收到：\u0026quot; + r.Payload)\rif r.Payload == z0 { // 如果收到：吃了没，您吶?\rwriteTo(\u0026amp;RequestResponse{r.Serial, l1}, conn, \u0026amp;liWriteLock) // 回复：刚吃。\r} else if r.Payload == z3 {\r// do nothing\r} else if r.Payload == z5 {\r//fmt.Println(\u0026quot; 俩人说完走了 \u0026quot;)\rcount++\r} else {\rfmt.Println(\u0026quot; 李大爷听不懂：\u0026quot; + r.Payload)\rbreak\r}\r}\r}\r// 李大爷的嘴\rfunc liDaYeSay(conn *net.TCPConn) {\rnextSerial := uint32(0)\rfor i := uint32(0); i \u0026lt; total; i++ {\rwriteTo(\u0026amp;RequestResponse{nextSerial, l2}, conn, \u0026amp;liWriteLock)\rnextSerial++\rwriteTo(\u0026amp;RequestResponse{nextSerial, l4}, conn, \u0026amp;liWriteLock)\rnextSerial++\r}\r}\rfunc startServer() {\rtcpAddr, _ := net.ResolveTCPAddr(\u0026quot;tcp\u0026quot;, \u0026quot;127.0.0.1:9999\u0026quot;)\rtcpListener, _ := net.ListenTCP(\u0026quot;tcp\u0026quot;, tcpAddr)\rdefer tcpListener.Close()\rfmt.Println(\u0026quot; 张大爷在胡同口等着 ...\u0026quot;)\rfor {\rconn, err := tcpListener.AcceptTCP()\rif err != nil {\rfmt.Println(err)\rbreak\r}\rfmt.Println(\u0026quot; 碰见一个李大爷:\u0026quot; + conn.RemoteAddr().String())\rgo zhangDaYeListen(conn)\rgo zhangDaYeSay(conn)\r}\r}\rfunc startClient() {\rvar tcpAddr *net.TCPAddr\rtcpAddr, _ = net.ResolveTCPAddr(\u0026quot;tcp\u0026quot;, \u0026quot;127.0.0.1:9999\u0026quot;)\rconn, _ := net.DialTCP(\u0026quot;tcp\u0026quot;, nil, tcpAddr)\rdefer conn.Close()\rvar wg sync.WaitGroup\rwg.Add(1)\rgo liDaYeListen(conn, \u0026amp;wg)\rgo liDaYeSay(conn)\rwg.Wait()\r}\rfunc main() {\rgo startServer()\rtime.Sleep(time.Second)\rt1 := time.Now()\rstartClient()\relapsed := time.Since(t1)\rfmt.Println(\u0026quot; 耗时: \u0026quot;, elapsed)\r}\r 在我的 Mac 执行 10 万次大约需要不到 5 秒钟：\ngo run hutong.go\r张大爷在胡同口等着 ...\r碰见一个李大爷:127.0.0.1:50136\r耗时: 4.962786896s\r 在这段程序里面，我没有对程序做任何特殊的性能优化，只是使用了我们之前四节课中讲到的，上面列出来的那些知识点，完成了一个基本的实现。\n在这段程序中，我们首先定义了 RequestResponse 这个结构体，代表请求或响应，它包括序号和内容两个字段。readFrom 方法的功能是，接收数据，反序列化成 RequestResponse。\n协议的设计是这样的：首先用 4 个字节来标明这个请求的长度，然后用 4 个字节来保存序号，最后变长的部分就是大爷说的话。这里面用到了使用前置长度的方式来进行断句，这种断句的方式我在之前的课程中专门讲到过。\n这里面我们使用了专有的序列化方法，原因我在之前的课程中重点讲过，专有的序列化方法具备最好的性能，序列化出来的字节数也更少，而我们这个作业比拼的就是性能，所以在这个作业中采用这种序列化方式是最合适的选择。\nzhangDaYeListen 和 liDaYeListen 这两个方法，它们的实现是差不多的，就是接收对方发出的请求，然后给出正确的响应。zhangDaYeSay 和 liDaYeSay 这两个方法的实现也是差不多的，当俩大爷遇见后，就开始不停地说各自的请求，并不等待对方的响应，连续说 10 万次。\n这 4 个方法，分别在 4 个不同的协程中并行运行，两收两发，实现了全双工的通信。在这个地方，不少同学还是摆脱不了“一问一答，再问再答”这种人类交流的自然方式对思维的影响，写出来的依然是单工通信的程序，单工通信的性能是远远不如双工通信的，所以，要想做到比较好的网络传输性能，双工通信的方式才是最佳的选择。\n为了避免并发向同一个 socket 中写入造成数据混乱，我们给俩大爷分别定义了一个写锁，确保每个大爷同一时刻只能有一个协程在发送数据。后面的课程中，我们会专门来讲，如何正确地使用锁。\n最后，我们给张大爷定义为服务端，李大爷定义为客户端，连接建立后，分别开启两个大爷的耳朵和嘴，来完成这 10 万次遇见。\n思考 在我给出这个俩大爷作业的实现中，我们可以计算一下，10 万次遇见耗时约 5 秒，平均每秒可以遇见约 2 万次，考虑到数据量的大小，这里面仍然有非常大的优化空间。\n请你在充分理解这段代码之后，想一想，还有哪些地方是可以优化的，然后一定要动手把代码改出来，并运行，验证一下你的改进是否真的达到了效果。欢迎你在留言区提出你的改进意见。\n案例 做两个微型的项目，带你体验实际的代码开发。这两个微项目会用到我们在基础篇和进阶篇中学习的知识。\n 第一个微项目，一起用消息队列和流计算框架来实现一个流计算任务； 第二个微项目，一起来实现一个最简单的 RPC 框架，因为开发中间件用到的很多技术都是互通的，开发消息队列的技术同样可以用于开发 RPC 框架  流计算与消息（一）：通过Flink理解流计算的原理 在上节课中，我简单地介绍了消息队列和流计算的相关性。在生产中，消息队列和流计算往往是相互配合，一起来使用的。而流计算也是后端程序员技术栈中非常重要的一项技术。在接下来的两节课中，我们一起通过两个例子来实际演练一下，如何使用消息队列配合流计算框架实现一些常用的流计算任务。\n这节课，我们一起来基于 Flink 实现一个流计算任务，通过这个例子来感受一下流计算的好处，同时我还会给你讲解流计算框架的实现原理。下一节课中，我们会把本节课中的例子升级改造，使用 Kafka 配合 Flink 来实现 Exactly Once 语义，确保数据在计算过程中不重不丢。\n无论你之前是否接触过像 Storm、Flink 或是 Spark 这些流计算框架都没有关系，因为我们已经学习了消息队列的实现原理，以及实现消息队列必备的像异步网络传输、序列化这些知识。在掌握了这些知识和底层的原理之后，再来学习和理解流计算框架的实现原理，你会发现，事情就变得非常简单了。\n为什么这么说，一个原因是，对于很多中间件或者说基础框架这类软件来说，它们用到很多底层的技术都是一样；另外一个原因是，流计算和消息队列处理的都实时的、流动的数据，很多处理流数据的方法也是一样的。\n哪些问题适合用流计算解决？ 首先，我们来说一下，哪些问题适合用流计算来解决？或者说，流计算它的应用场景是什么样的呢？\n在这里，我用一句话来回答这个问题：对实时产生的数据进行实时统计分析，这类场景都适合使用流计算来实现。\n你在理解这句话的时候，需要特别注意的是，这里面有两个“实时”，一个是说，数据是“实时”产生的，另一个是说，统计分析这个过程是“实时”进行的，统计结果也是第一时间就计算出来了。对于这样的场景，你都可以考虑使用流计算框架。\n因为流计算框架可以自动地帮我们实现实时的并行计算，性能非常好，并且内置了很多常用的统计分析的算子，比如 TimeWindow、GroupBy、Sum 和 Count，所以非常适合用来做实时的统计和分析。举几个例子：\n 每分钟按照 IP 统计 Web 请求次数； 电商在大促时，实时统计当前下单量； 实时统计 App 中的埋点数据，分析营销推广效果。  以上这些场景，以及和这些场景类似的场景，都是比较适合用流计算框架来实现的。特别是基于时间维度的统计分析，使用流计算框架来实现是非常方便的。\n用代码定义 Job 并在 Flink 中执行 接下来，我们用 Flink 来实现一个实时统计任务：接收 NGINX 的 access.log，每 5 秒钟按照 IP 地址统计 Web 请求的次数。这个统计任务它一个非常典型的，按照 Key 来进行分类汇总的统计任务，并且汇总是按照一定周期来实时进行的，我们日常工作中遇到的很多统计分析类的需求，都可以套用这个例子的模式来实现，所以我们就以它为例来做一个实现。\n假设我们已经有一个实时发送 access.log 的日志服务，它运行在本地的 9999 端口上，只要有客户端连接上来，他就会通过 Socket 给客户端发送实时的访问日志，日志的内容只包含访问时间和 IP 地址，每条数据的结尾用一个换行符 (\\n) 作为分隔符。这个日志服务就是我们流计算任务的数据源。\n我们用 NetCat 连接到这个服务上，看一下数据格式：\n$nc localhost 9999\r14:37:11 192.168.1.3\r14:37:11 192.168.1.2\r14:37:12 192.168.1.4\r14:37:14 192.168.1.2\r14:37:14 192.168.1.4\r14:37:14 192.168.1.3\r...\r 接下来我们用 Scala 语言和 Flink 来实现这个流计算任务。你可以先不用关心如何部署启动 Flink，如何设置开发环境这些问题，一起来跟我看一下定义这个流计算任务的代码：\nobject SocketWindowIpCount {\rdef main(args: Array[String]) : Unit = {\r// 获取运行时环境\rval env: StreamExecutionEnvironment = StreamExecutionEnvironment.getExecutionEnvironment\r// 按照 EventTime 来统计\renv.setStreamTimeCharacteristic(TimeCharacteristic.EventTime)\r// 设置并行度\renv.setParallelism(4)\r// 定义输入：从 Socket 端口中获取数据输入\rval hostname: String = \u0026quot;localhost\u0026quot;\rval port: Int = 9999\r// Task 1\rval input: DataStream[String] = env.socketTextStream(hostname, port, '\\n')\r// 数据转换：将非结构化的以空格分隔的文本转成结构化数据 IpAndCount\r// Task 2\rinput\r.map { line =\u0026gt; line.split(\u0026quot;\\\\s\u0026quot;) }\r.map { wordArray =\u0026gt; IpAndCount(new SimpleDateFormat(\u0026quot;HH:mm:ss\u0026quot;).parse(wordArray(0)), wordArray(1), 1) }\r// 计算：每 5 秒钟按照 ip 对 count 求和\r.assignAscendingTimestamps(_.date.getTime) // 告诉 Flink 时间从哪个字段中获取\r.keyBy(\u0026quot;ip\u0026quot;) // 按照 ip 地址统计\r// Task 3\r.window(TumblingEventTimeWindows.of(Time.seconds(5))) // 每 5 秒钟统计一次\r.sum(\u0026quot;count\u0026quot;) // 对 count 字段求和\r// 输出：转换格式，打印到控制台上\r.map { aggData =\u0026gt; new SimpleDateFormat(\u0026quot;HH:mm:ss\u0026quot;).format(aggData.date) + \u0026quot; \u0026quot; + aggData.ip + \u0026quot; \u0026quot; + aggData.count }\r.print()\renv.execute(\u0026quot;Socket Window IpCount\u0026quot;)\r}\r/** 中间数据结构 */\rcase class IpAndCount(date: Date, ip: String, count: Long)\r}\r 我来给你解读一下这段代码。\n首先需要获取流计算的运行时环境，也就是这个 env 对象，对 env 做一些初始化的设置。然后，我们再定义输入的数据源，这里面就是我刚刚讲的，运行在 9999 端口上的日志服务。\n在代码中，env.socketTextStream(hostname, port, ‘\\n’) 这个语句中的三个参数分别是主机名、端口号和分隔符，返回值的数据类型是 DataStream[String]，代表一个数据流，其中的每条数据都是 String 类型的。它告诉 Flink，我们的数据源是一个 Socket 服务。这样，Flink 在执行这个计算任务的时候，就会去连接日志服务来接收数据。\n定义完数据源之后，需要做一些数据转换，把字符串转成结构化的数据 IpAndCount，便于后续做计算。在定义计算的部分，依次告诉 Flink：时间从 date 字段中获取，按照 IP 地址进行汇总，每 5 秒钟汇总一次，汇总方式就是对 count 字段求和。\n之后定义计算结果如何输出，在这个例子中，我们直接把结果打印到控制台上就好了。\n这样就完成了一个流计算任务的定义。可以看到，定义一个计算任务的代码还是非常简单的，如果我们要自己写一个分布式的统计程序来实现一样的功能，代码量和复杂度肯定要远远超过上面这段代码。\n总结下来，无论是使用 Flink、Spark 还是其他的流计算框架，定义一个流计算的任务基本上都可以分为：定义输入、定义计算逻辑和定义输出三部分，通俗地说，也就是：数据从哪儿来，怎么计算，结果写到哪儿去，这三件事儿。\n我把这个例子的代码上传到了 GitHub 上，你可以在这里下载，关于如何设置环境、编译并运行这个例子，我在代码中的 README 中都给出了说明，你可以下载查看。\n执行计算任务打印出的计算结果是这样的：\n1\u0026gt; 18:40:10 192.168.1.2 23\r4\u0026gt; 18:40:10 192.168.1.4 16\r4\u0026gt; 18:40:15 192.168.1.4 27\r3\u0026gt; 18:40:15 192.168.1.3 23\r1\u0026gt; 18:40:15 192.168.1.2 25\r4\u0026gt; 18:40:15 192.168.1.1 21\r1\u0026gt; 18:40:20 192.168.1.2 21\r3\u0026gt; 18:40:20 192.168.1.3 31\r4\u0026gt; 18:40:20 192.168.1.1 25\r4\u0026gt; 18:40:20 192.168.1.4 26\r 对于流计算的初学者，特别不好理解的一点是，我们上面编写的这段代码，**它只是“用来定义计算任务的代码”，而不是“真正处理数据的代码”。**对于普通的应用程序，源代码编译之后，计算机就直接执行了，这个比较好理解。而在 Flink 中，当这个计算任务在 Flink 集群的计算节点中运行的时候，真正处理数据的代码并不是我们上面写的那段代码，而是 Flink 在解析了计算任务之后，动态生成的代码。\n这个有点儿类似于我们在查询 MySQL 的时候执行的 SQL，我们提交一个 SQL 查询后，MySQL 在执行查询遍历数据库中每条数据时，并不是对每条数据执行一遍 SQL，真正执行的其实是 MySQL 自己的代码。SQL 只是告诉 MySQL 我们要如何来查询数据，同样，我们编写的这段定义计算任务的代码，只是告诉 Flink 我们要如何来处理数据而已。\nJob 是如何在 Flink 集群中执行的？ 那我们的计算任务是如何在 Flink 中执行的呢？在讲解这个问题之前，我们先简单看一下 Flink 集群在运行时的架构。\n下面这张图来自于Flink 的官方文档。\n这张图稍微有点儿复杂，我们先忽略细节看整体。Flink 的集群和其他分布式系统都是类似的，集群的大部分节点都是 TaskManager 节点，每个节点就是一个 Java 进程，负责执行计算任务。另外一种节点是 JobManager 节点，它负责管理和协调所有的计算节点和计算任务，同时，客户端和 Web 控制台也是通过 JobManager 来提交和管理每个计算任务的。\n我们编写好计算任务的代码后，打包成 JAR 文件，然后通过 Flink 的客户端提交到 JobManager 上。计算任务被 Flink 解析后，会生成一个 Dataflow Graph，也叫 JobGraph，简称 DAG，这是一个有向无环图（DAG），比如我们的这个例子，它生成的 DAG 是这样的：\n图中的每个节点是一个 Task，每个 Task 就是一个执行单元，运行在某一个 TaskManager 的进程内。你可以想象一下，就像电流流过电路图一样，数据从 Source Task 流入，进入这个 DAG，每流过一个 Task，就被这个 Task 做一些计算和变换，然后数据继续流入下一个 Task，直到最后一个 Sink Task 流出 DAG，就自然完成了计算。\n对于图中的 3 个 Task，每个 Task 对应执行了什么计算，完全可以和我们上面定义计算任务的源代码对应上，我也在源代码的注释中，用\u0026rdquo;//Task n\u0026quot;的形式给出了标注。第一个 Task 执行的计算很简单，就是连接日志服务接收日志数据，然后将日志数据发往下一个 Task。第二个 Task 执行了两个 map 变换，把文本数据转换成了结构化的数据，并添加 Watermark（水印）。Watermark 这个概念可以先不用管，主要是用于触发按时间汇总的操作。第三个 Task 执行了剩余的计算任务，按时间汇总日志，并输出打印到控制台上。\n这个 DAG 仍然是一个逻辑图，它到底是怎么在 Flink 集群中执行的呢？你注意到图中每个 Task 都标注了一个 Parallelism（并行度）的数字吗？这个并行度的意思就是，这个 Task 可以被多少个线程并行执行。比如图中的第二个任务，它的并行度是 4，就代表 Task 在 Flink 集群中运行的时候，会有 4 个线程都在执行这个 Task，每个线程就是一个 SubTask（子任务）。注意，如果 Flink 集群的节点数够多，这 4 个 SubTask 可能会运行在不同的 TaskManager 节点上。\n建立了 SubTask 的概念之后，我们再重新回过头来看一下这个图中的两个箭头。第一个箭头连接前两个 Task，这个箭头标注了 REBALANCE（重新分配），因为第一个 Task 并行度是 1，而第二个 Task 并行度是 4，意味着从第一个 Task 流出的数据将被重新分配给第二个 Task 的 4 个线程，也就是 4 个 SubTask（子任务）中，这样就实现了并行处理。这和消息队列中每个主题分成多个分区进行并行收发的设计思想是一样的。\n再来看连接第二、第三这两个 Task 的箭头，这个箭头上标注的是 HASH，为什么呢？可以看到，第二个 Task 中最后一步业务逻辑是：keyBy(“ip”)，也就是按照 IP 这个字段做一个 HASH 分流。你可以想一下，第三个 Task，它的并行度是 4，也就是有 4 个线程在并行执行汇总。如果要统计每个 IP 的日志条数，那必须得把相同 IP 的数据发送到同一个 SubTask（子任务）中去，这样在每个 SubTask（子任务）中，对于每一条数据，只要在对应 IP 汇总记录上进行累加就可以了。\n反之，要是相同 IP 的数据被分到多个 SubTask（子任务）上，这些 SubTask 又可能分布在多个物理节点上，那就没办法统计了。所以，第二个 Task 会把数据按照 IP 地址做一个 HASH 分流，保证 IP 相同的数据都发送到第三个 Task 中相同的 SubTask（子任务）中。这个 HASH 分流的设计是不是感觉很眼熟？我们之前课程中讲到的，严格顺序消息的实现方法：通过 HASH 算法，让 key 相同的数据总是发送到相同的分区上来保证严格顺序，和 Flink 这里的设计就是一样的。\n最后在第三个 Task 中，4 个 SubTask 并行进行数据汇总，每个 SubTask 负责汇总一部分 IP 地址的数据。最终打印到控制台上的时候，也是 4 个线程并行打印。你可以回过头去看一下输出的计算结果，每一行数据前面的数字，就是第三个 Task 中 SubTask 的编号。\n到这里，我们不仅实现并运行了一个流计算任务，也理清了任务在 Flink 集群中运行的过程。\n小结 流计算框架适合对实时产生的数据进行实时统计分析。我们通过一个“按照 IP 地址统计 Web 请求的次数”的例子，学习了 Flink 实现流计算任务的原理。首先，我们用一段代码定义了计算任务，把计算任务代码编译成 JAR 包后，通过 Flink 客户端提交到 JobManager 上。\n这里需要注意的是，我们编写的代码只是用来定义计算任务，和在 Flink 节点上执行的真正做实时计算的代码是不一样的。真正执行计算的代码是 Flink 在解析计算任务后，动态生成的。\nFlink 分析计算任务之后生成 JobGraph，JobGraph 是一个有向无环图，数据流过这个图中的节点，在每个节点进行计算和变换，最终流出有向无环图就完成了计算。JobGraph 中的每个节点是一个 Task，Task 是可以并行执行的，每个线程就是一个 SubTask。SubTask 被 JobManager 分配给某个 TaskManager，在 TaskManager 进程中的一个线程中执行。\n通过分析 Flink 的实现原理，我们可以看到，流计算框架本身并没有什么神奇的技术，之所以能够做到非常好的性能，主要有两个原因。一个是，它能自动拆分计算任务来实现并行计算，这个和 Hadoop 中 Map Reduce 的原理是一样的。另外一个原因是，流计算框架中，都内置了很多常用的计算和统计分析的算子，这些算子的实现都是经过很多大神级程序员反复优化过的，不仅能方便我们开发，性能上也比大多数程序员自行实现要快很多。\n思考 我们在启动 Flink 集群之前，修改了 Flink 的一个配置：槽数 taskmanager.numberOfTaskSlots。请你课后看一下 Flink 的文档，搞清楚这个槽数的含义。然后再想一下，这个槽数和我们在计算任务中定义的并行度又是什么关系呢？\n学习开源代码该如何入手   官方文档：入手最佳的方式。\n 优先整体了解，快速地掌握软件整体的结构，它有哪些功能特性，它涉及到的关键技术、实现原理和它的生态系统等等，推荐从目录标题入手，翻阅需要的内容；然后再去看它的源代码，才不会完全摸不着头脑。 尽量不要去网上搜一些翻译的中文文档，因为这些开源软件，特别是一些社区活跃的软件迭代很快，即使是自带官方中文翻译的项目，它的中文文档很多都会落后于英文版，非官方的翻译，可能还会出现一些错漏的地方。或者，根据标题找到你需要阅读的文章后，可以在网上搜一下对应的中文版本，先看一遍中文版，然后再对着英文原版过一遍，弥补中文版可能过时或翻译不准确的问题。    开源社区经过这么多年的发展，它已经形成一个相对比较成熟的文化。每个开源软件，代码如何管理、社区成员如何沟通、如何协作这些都已经形成了一个比较固定的套路。大多数开源软件，它的官网和技术文档也是有一个相对比较固定的结构的。接下来我们以Kafka 的官网为例子，来说下怎么来看它的文档\n QuickStart：对这个项目完全不了解，没用过这个软件，你首先需要看的文档是Quick Start，按照 Quick Start 中的指导快速把它的环境搭起来，把它运行起来，这样你会对这个项目有个感性认识，也便于你在后续深入学习的时候“跑”一些例子。 Introduction：然后你需要找一下它的Introduction，一般里面会有项目的基本介绍。这里面很重要的一点是，你需要找到这个项目用到的一些基本概念或者名词的介绍文档，在 Kafka 的文档中，这些内容就在 Introduction 里面，比如 Topic、Producer、 Consumer、Partition 这些概念在 Kafka 中代表的含义。有些开源项目会单独有一个 Basic Concepts 文档来讲这些基础概念。这个文档非常重要，因为这些开源社区的开发者都有个很不好的爱好：发明概念。很多开源项目都会自己创造一些名词或者概念，了解这些基本概念才有可能看懂它项目的其他文档。 对项目有个基本的了解之后呢，接下来你可以看一下它的使用场景、功能特性以及相关的生态系统的介绍。在 Kafka 中功能相关的内容在Use cases和EcoSystem两篇文章中，有些项目中会有类似名为 Features 的文档介绍功能和特性。其中项目的生态系统，也就是 EcoSystem，一般会介绍它这个项目适用的一些典型的使用场景，在某个场景下适合与哪些其他的系统一起来配合使用等。如果说你的系统不是特别特殊或者说冷门的话，你大概率可以在 EcoSystem 里面找到和你类似的场景，可以少走很多的弯路。 读完上面这些文档之后，对这个项目的整体应该会有一个比较全面的了解了，比如说：这个项目是干什么的？能解决哪些问题？适合在哪些场景使用？有哪些功能？如何使用？对这些问题有一个初步的答案之后，接下来你就可以去深入学习它的实现原理了。这是不是意味着，你可以立即去看它的源码呢？这样做或许可行，但并不是最好的方法。知道大部分开源项目都是怎么诞生的吗？一般来说是这样的：某个大学或者大厂的科学家，某天脑海里突然出现了一个改变世界的想法，科学家们会基于这个想法做一些深入的研究，然后写了一篇论文在某个学术期刊或者会议上发表。论文发表后在业内获得很多的赞，这时候就轮到像 Google、Facebook 这样的大厂出手了：这个论文很有价值，不如我们把它实现出来吧？一个开源项目就这样诞生了。所以，对于这样的开源项目，它背后的这篇论文就是整个项目的灵魂，你如果能把这篇论文看完并且理解透了，这个项目的实现原理也就清楚了。对于 Kafka 来说，它的灵魂是这篇博文：The Log: What every software engineer should know about real-time data’s unifying abstraction，对应的中文译稿在这里：《日志：每个软件工程师都应该知道的有关实时数据的统一抽象》。这篇博文被评为程序员史诗般必读文章，无论你是不是想了解 Kafka 的实现原理，我都强烈推荐你好好读一下上面这篇博文。 学习完项目灵魂，就可以开始阅读源码了。用以点带面的方式来阅读源码，需要注意的是，你在读源码的时候，千万不要上来就找 main 方法这样泛泛地去看，为什么？你可以想一下，一篇文章，它是一个线性结构，你从前往后读就行了。一本书呢？如果我们看目录的话，可以认为是个树状结构，但大多数的书的内容还是按照线性结构来组织的，你可以从前往后读，也可以通过目录跳着读。那程序的源代码是什么结构？那是一个网状结构，关系错综复杂，所以这种结构是非常不适合人类去阅读的。你如果是泛泛去读源代码，很容易迷失在这个代码织成的网里面。那怎么办？  推荐大家阅读源码的方式是，**带着问题去读源码，最好是带着问题的答案去读源码。**你每次读源码之前，确定一个具体的问题，比如：RocketMQ 的消息是怎么写到文件里的？Kafka 的 Coordinator 是怎么维护消费位置的？类似这种非常细粒度的问题，粒度细到每个问题的答案就是一两个流程就可以回答，这样就可以了。如果说你就想学习一下源代码，或者说提不出这些问题怎么办呢？答案还是，看文档。 确定问题后，先不要着急看源代码，而是应该先找一下是否有对应的实现文档，一般来说，核心功能都会有专门的文档来说明它的实现原理，比如在 Kafka 的文档中，DESIGN和IMPLEMENTATION两个章节中，介绍了 Kafka 很多功能的实现原理和细节。一些更细节的非核心的功能不一定有专门的文档来说明，但是我们可以去找一找是否有对应的 Improvement Proposal。（Kafka 的所有 Improvement Proposals 在这里。）这个 Improvement Proposal 是什么呢？你可以认为它是描述一个新功能的文档，一般开源项目需要增加一个新的功能或者特性的时候，都会创建一个 Improvement Proposal，一般标题都是\u0026quot;xIP- 新功能名称\u0026rdquo;，其中 IP 就是 Improvement Proposal 的缩写，x 一般就是这个开源项目的名称的首字母，比如 Kafka 中 Improvement Proposal 的标题就都是以 KIP 来开头。每个 Improvement Proposal 都是有固定格式的，一般要说明为什么需要增加这个功能，会对系统产生那些影响和改变，还有我们最关心的设计和实现原理的简述。 读完讲解实现的文档再去看源代码，也就是我刚刚说的，不只是带着问题去读，而是带着答案去读源码。这样你在读源码的时候，不仅仅是更容易理解源代码，还可以把更多的精力放在一些实现细节上，这样阅读源码的效果会更好。使用这种以问题为阅读单元的方式来读源代码，你每次只要花很短的时间，阅读很少的一部分源码，就能解决一个问题，得到一些收获。这种方式其实是通过一个一个的问题，在网状的源代码中，每次去读几个点组成的那一两条线。随着你通过阅读源码了解的问题越来越多，你对项目源码的理解也会越来越全面和深入。      小结  如果你想了解一个开源项目，学习它的代码，最佳的切入点就是去读它的官方文档，这些文档里面，最重要的灵魂就是项目背后的那篇论文，它一般是这个开源项目的理论基础。 在阅读源码的时候呢，最佳的方式是带着问题去阅读，最好是带着问题的答案去读，这样难度低、周期短、收获快。不要想着一定要从总体上去全面掌握一个项目的所有源代码，也没有必要。 精选留言：在还没阅读开源项目代码之前，会先把Java的一些常用的源代码先看完，感觉所有的源代码都是这些基础的砖块叠加上去的，最近也在撸rocketmq源码，也是先把用到的一些其他框架先熟悉如netty、logger，然后看其官方文档，对整体的框架和基本的概念有比较感性的认知，再者写一些demo。然后先把rocketmq中的公共的模块的源代码先看，logger\u0026hellip;等  思考  建议你找一个你熟悉的开源项目，可以是消息相关的，也可以是无关的开源项目，确定一个问题，用这节课中讲到的“带着问题和答案去读源码”的方法，去读一点源码。然后，最重要的是，把主要的流程用流程图或者时序图画出来，把重点的算法、原理用文字写出来。  如何选择消息队列？  开源：开源意味着，如果有一天你使用的消息队列遇到了一个影响你系统业务的 Bug，你至少还有机会通过修改源代码来迅速修复或规避这个 Bug，解决你的系统火烧眉毛的问题，而不是束手无策地等待开发者不一定什么时候发布的下一个版本来解决。 社区活跃：必须是近年来比较流行并且有一定社区活跃度的产品。流行的好处是，只要你的使用场景不太冷门，你遇到 Bug 的概率会非常低，因为大部分你可能遇到的 Bug，其他人早就遇到并且修复了。你在使用过程中遇到的一些问题，也比较容易在网上搜索到类似的问题，然后很快的找到解决方案 流行：还有一个优势就是，流行的产品与周边生态系统会有一个比较好的集成和兼容，比如，Kafka 和 Flink 就有比较好的兼容性，Flink 内置了 Kafka 的 Data Source，使用 Kafka 就很容易作为 Flink 的数据源开发流计算应用，如果你用一个比较小众的消息队列产品，在进行流计算的时候，你就不得不自己开发一个 Flink 的 Data Source 最后，作为一款及格的消息队列产品，必须具备的几个特性包括：  消息的可靠传递：确保不丢消息； Cluster：支持集群，确保不会因为某个节点宕机导致服务不可用，当然也不能丢消息； 性能：具备足够好的性能，能满足绝大多数场景的性能要求    RabbitMQ  Erlang 语言编写的，它最早是为电信行业系统之间的可靠通信设计的，也是少数几个支持 AMQP 协议的消息队列之一 优点  轻量级、迅捷，是它的 Slogan。Messaging that just works，“开箱即用的消息队列”。也就是说，RabbitMQ 是一个相当轻量级的消息队列，非常容易部署和使用 RabbitMQ 一个比较有特色的功能是支持非常灵活的路由配置，和其他消息队列不同的是，它在生产者（Producer）和队列（Queue）之间增加了一个 Exchange 模块，你可以理解为交换机。这个 Exchange 模块的作用和交换机也非常相似，根据配置的路由规则将生产者发出的消息分发到不同的队列中。路由的规则也非常灵活，甚至你可以自己来实现路由规则。基于这个 Exchange，可以产生很多的玩儿法，如果你正好需要这个功能，RabbitMQ 是个不错的选择 RabbitMQ 的客户端支持的编程语言大概是所有消息队列中最多的，如果你的系统是用某种冷门语言开发的，那你多半可以找到对应的 RabbitMQ 客户端   缺点：  RabbitMQ 对消息堆积的支持并不好，在它的设计理念里面，消息队列是一个管道，大量的消息积压是一种不正常的情况，应当尽量去避免。当大量消息积压的时候，会导致 RabbitMQ 的性能急剧下降 RabbitMQ 的性能是我们介绍的这几个消息队列中最差的，根据官方给出的测试数据综合我们日常使用的经验，依据硬件配置的不同，它大概每秒钟可以处理几万到十几万条消息。其实，这个性能也足够支撑绝大多数的应用场景了，不过，如果你的应用对消息队列的性能要求非常高，那不要选择 RabbitMQ RabbitMQ 使用的编程语言 Erlang，如果你想基于 RabbitMQ 做一些扩展和二次开发什么的，建议你慎重考虑一下可持续维护的问题    RocketMQ  RocketMQ 是阿里巴巴在 2012 年开源的消息队列产品，后来捐赠给 Apache 软件基金会。有着不错的性能，稳定性和可靠性，具备一个现代的消息队列应该有的几乎全部功能和特性，并且它还在持续的成长中 优点  非常活跃的中文社区，大多数问题你都可以找到中文的答案 Java 语言开发，贡献者大多数都是中国人，源代码相对也比较容易读懂，你很容易对 RocketMQ 进行扩展或者二次开发 RocketMQ 对在线业务的响应时延做了很多的优化，大多数情况下可以做到毫秒级的响应，如果你的应用场景很在意响应时延，那应该选择使用 RocketMQ RocketMQ 的性能比 RabbitMQ 要高一个数量级，每秒钟大概能处理几十万条消息。   缺点  作为国产的消息队列，相比国外的比较流行的同类产品，在国际上还没有那么流行，与周边生态系统的集成和兼容程度要略逊一筹    Kafka  由 LinkedIn 开发，目前也是 Apache 的顶级项目，最初的设计目的是用于处理海量的日志。早期的版本中，为了获得极致的性能，在设计方面做了很多的牺牲，比如不保证消息的可靠性，可能会丢失消息，也不支持集群，功能上也比较简陋，这些牺牲对于处理海量日志这个特定的场景都是可以接受的。这个时期的 Kafka 甚至不能称之为一个合格的消息队列。随后的几年 Kafka 逐步补齐了这些短板，你在网上搜到的很多消息队列的对比文章还在说 Kafka 不可靠，其实这种说法早已经过时。当下的 Kafka 已经发展为一个非常成熟的消息队列产品，无论在数据可靠性、稳定性和功能特性等方面都可以满足绝大多数场景的需求 优点：  Kafka 与周边生态系统的兼容性是最好的没有之一，尤其在大数据和流计算领域，几乎所有的相关开源软件系统都会优先支持 Kafka 使用 Scala 和 Java 语言开发，设计上大量使用了批量和异步的思想，这种设计使得 Kafka 能做到超高的性能。Kafka 的性能，尤其是异步收发的性能，是三者中最好的，但与 RocketMQ 并没有量级上的差异，大约每秒钟可以处理几十万条消息   缺点  但是 Kafka 这种异步批量的设计带来的问题是，它的同步收发消息的响应时延比较高，因为当客户端发送一条消息的时候，Kafka 并不会立即发送出去，而是要等一会儿攒一批再发送，在它的 Broker 中，很多地方都会使用这种“先攒一波再一起处理”的设计。当你的业务场景中，每秒钟消息数量没有那么多的时候，Kafka 的时延反而会比较高。所以，Kafka 不太适合在线业务场景。    其它  ActiveMQ是最老牌的开源消息队列，是十年前唯一可供选择的开源消息队列，目前已进入老年期，社区不活跃。无论是功能还是性能方面，ActiveMQ 都与现代的消息队列存在明显的差距，它存在的意义仅限于兼容那些还在用的爷爷辈儿的系统 ZeroMQ 并不能称之为一个消息队列，而是一个基于消息队列的多线程网络库，如果你的需求是将消息队列的功能集成到你的系统进程中，可以考虑使用 ZeroMQ Pulsar 是一个新兴的开源消息队列产品，最早是由 Yahoo 开发，目前处于成长期，流行度和成熟度相对没有那么高。与其他消息队列最大的不同是，Pulsar 采用存储和计算分离的设计，我个人非常喜欢这种设计，它有可能会引领未来消息队列的一个发展方向，建议你持续关注这个项目。  总结  在了解了上面这些开源消息队列各自的特点和优劣势后，我相信你对于消息队列的选择已经可以做到心中有数了。我也总结了几条选择的建议供你参考。 如果说，消息队列并不是你将要构建系统的主角之一，你对消息队列功能和性能都没有很高的要求，只需要一个开箱即用易于维护的产品，我建议你使用 RabbitMQ。 如果你的系统使用消息队列主要场景是处理在线业务，比如在交易系统中用消息队列传递订单，那 RocketMQ 的低延迟和金融级的稳定性是你需要的。 如果你需要处理海量的消息，像收集日志、监控信息或是前端的埋点这类数据，或是你的应用场景大量使用了大数据、流计算相关的开源产品，那 Kafka 是最适合你的消息队列。 如果我说的这些场景和你的场景都不符合，你看了我之前介绍的这些消息队列的特点后，还是不知道如何选择，那就选你最熟悉的吧，毕竟这些产品都能满足大多数应用场景，使用熟悉的产品还可以快速上手不是？  ​\n","date":"0001-01-01","permalink":"https://yuanyatianchi.github.io/post/it.mq.mq/","tags":["it","mq"],"title":"MQ"},{"content":"mysql https://www.mysql.com/ ，https://www.mysql.com/cn/\n 3306 sql  DDL，数据定义语言。定义和管理数据对象，如数据库，数据表等。CREATE、DROP、ALTER DML，数据操作语言。用于操作数据库对象中所包含的数据。| NSERT、UPDATE、DELETE DQL，数据查询语言。用于查询数据库数据。SELECT DCL，数据控制语言。用来管理数据库的语言，包括管理权限及数据更改。GRANT、COMMIT、ROLLBACK    client go $ go get -u github.com/go-sql-driver/mysql\r 连接 db/db.go import (\r\u0026quot;database/sql\u0026quot;\r_ \u0026quot;github.com/go-sql-driver/mysql\u0026quot; //mysql驱动，不使用其提供的方法，通过'_'加载其初始化函数即可\r)\rvar (\r/*DB是一个数据库(操作)句柄，代表一个具有零到多个底层连接的连接池，它可以安全的被多个goroutine同时使用。\rsql包会自动创建和释放连接，它也会维护一个闲置连接的连接池。*/\rMyDB *sql.DB\rerr error\r)\rfunc init() {\r/*Open：创建一个DB实例。Open过程只验证参数，不参创建与数据库的连接，自然也不会校验账号密码数据源名称中账号密码等数据正确性*/\rMyDB, err = sql.Open(\u0026quot;mysql\u0026quot;, \u0026quot;root:yuanya@tcp(192.168.31.108:3306)/yuanya\u0026quot;)\rif err != nil {\rpanic(err)\r}\rdefer MyDB.Close()\r/*配置\rMyDB.SetMaxOpenConns(0) //设置最大连接数，默认为0(\u0026lt;=0时无限制)\rMyDB.SetConnMaxLifetime(0) //设置最大闲置连接数，默认为0(\u0026lt;=0时不保留闲置连接)\rMyDB.SetMaxIdleConns(-1) //设置连接池大小，默认为-1(\u0026lt;=0时)*/\r/*尝试与数据库建立连接。将检查数据源的名称是否合法*/\rif err := MyDB.Ping(); err != nil {\rpanic(err.Error())\r}\r}\r 操作 dao/UserDao.go type User struct {\rId int\rUsername string\rPassword string\r}\r/*row：查询一行结果*/\rfunc getById(id int) (user *User, err error) {\rsql := \u0026quot;select * from user where id=?\u0026quot;\r/*sql拼接*/\rrow := db.MyDB.QueryRow(sql, id) //QueryRow返回一行结果\rerr = row.Scan(\u0026amp;user.Id, \u0026amp;user.Username, \u0026amp;user.Password) //Scan将该行查询结果各列分别保存进dest参数指定的值中\rreturn\r}\r/*rows：查询多行结果*/\rfunc listByUsernameLike(usernameLike string) (users []*User, err error) {\rsql := \u0026quot;select * from user where username like ?\u0026quot;\r/*sql拼接\rrows, err := db.MyDB.Query(sql, usernameLike) //Query返回多行结果\rif err != nil {\rfmt.Println(err)\r}*/\r/*sql预处理。任何时候都不应该拼接SQL语句，而是要进行预处理，以防SQL注入(如\u0026quot;or 1=1#\u0026quot;等)*/\rstmt, err := db.MyDB.Prepare(sql)\rif err != nil {\rfmt.Println(\u0026quot;Prepare异常\u0026quot;, err)\r}\rrows, err := stmt.Query(usernameLike) //stmt方法与db基本一致，只是不用再传入sql\rif err != nil {\rfmt.Println(\u0026quot;Query异常\u0026quot;, err)\r}\r/*迭代遍历行*/\rfor rows.Next() {\ruser := User{}\rrows.Scan(\u0026amp;user.Id, \u0026amp;user.Username, \u0026amp;user.Password)\rusers = append(users, \u0026amp;user)\r}\rreturn\r}\r/*增*/\rfunc insert(user *User) (err error) {\rsql := \u0026quot;insert into user(username, password) value(?,?)\u0026quot;\rstmt, err := db.MyDB.Prepare(sql)\rif err != nil {\rfmt.Println(\u0026quot;Prepare异常\u0026quot;, err)\r}\r_, err = stmt.Exec(user.Username, user.Password) //执行\rif err != nil {\rfmt.Println(\u0026quot;Exec异常\u0026quot;, err)\r}\rreturn\r}\r/*事务。db.Begin开启事务，tx.Rollback()回滚事务，tx.Commit()提交事务\r如果数据库具有单连接状态的概念，该状态只有在事务中被观察时才可信。一旦调用BD.Begin,返回的Tx会绑定到单个连接\r当调用事务Tx的Commit或Rollback后，该事务使用的连接会归还到DB的闲置连接池中*/\rfunc tx() {\rtx, err := db.MyDB.Begin() //开启事务\rif err != nil {\rif tx != nil {\rtx.Rollback() //回滚事务\r}\rfmt.Printf(\u0026quot;begin trans failed, err:%v\\n\u0026quot;, err)\rreturn\r}\rsql1 := \u0026quot;Update user set age=30 where id=?\u0026quot;\rret1, err := tx.Exec(sql1, 2)\rif err != nil {\rtx.Rollback()\rfmt.Printf(\u0026quot;exec sql1 failed, err:%v\\n\u0026quot;, err)\rreturn\r}\raffRow1, err := ret1.RowsAffected()\rif err != nil {\rtx.Rollback()\rfmt.Printf(\u0026quot;exec ret1.RowsAffected() failed, err:%v\\n\u0026quot;, err)\rreturn\r}\rsql2 := \u0026quot;Update user set age=40 where id=?\u0026quot;\rret2, err := tx.Exec(sql2, 3)\rif err != nil {\rtx.Rollback()\rfmt.Printf(\u0026quot;exec sql2 failed, err:%v\\n\u0026quot;, err)\rreturn\r}\raffRow2, err := ret2.RowsAffected()\rif err != nil {\rtx.Rollback()\rfmt.Printf(\u0026quot;exec ret1.RowsAffected() failed, err:%v\\n\u0026quot;, err)\rreturn\r}\rfmt.Println(affRow1, affRow2)\rif affRow1 == 1 \u0026amp;\u0026amp; affRow2 == 1 {\rfmt.Println(\u0026quot;事务提交啦...\u0026quot;)\rtx.Commit() //提交事务\r} else {\rtx.Rollback()\rfmt.Println(\u0026quot;事务回滚啦...\u0026quot;)\r}\rfmt.Println(\u0026quot;exec trans success!\u0026quot;)\r}\r CREATE TABLE user(\rid INT PRIMARY KEY AUTO_INCREMENT,\rusername VARCHAR(15) UNIQUE NOT NULL,\rpassword VARCHAR(15) NOT NULL,\r)ENGINE=InnoDB AUTO_INCREMENT=1 DEFAULT CHARSET=utf8mb4;\r java  jdbc  public class JdbcDemo {\rstatic final String DRIVER = \u0026quot;com.mysql.jdbc.Driver\u0026quot;;\rstatic final String URL = \u0026quot;jdbc:mysql://localhost:3306/yuanya_mybatis?useUnicode=true\u0026amp;characterEncoding=utf8\u0026amp;allowMultiQueries=true\u0026quot;;\rstatic final String USERNAME = \u0026quot;root\u0026quot;;\rstatic final String PASSWORD = \u0026quot;root\u0026quot;;\r@Test\rpublic void StatementDemo() {\r//...\r}\r@Test\rpublic void PreparedStatementDemo() {\r//...\r}\r@Test\rpublic void batchDemo() {\r//...\r}\r}\r Statement 无预编译，每次需要经历完整的过程，都较慢\n@Test\rpublic void QueryStatementDemo() {\rConnection conn = null;\rStatement stmt = null;\rList\u0026lt;User\u0026gt; users = new ArrayList();\rtry {\r//STEP 2: 注册mysql的驱动\rClass.forName(DRIVER);\r//STEP 3: 获得一个连接\rconn = DriverManager.getConnection(URL, USERNAME, PASSWORD);\r//STEP 4: 创建一个查询\rString name = \u0026quot;yuanya\u0026quot;;\rString sql = \u0026quot;SELECT * FROM user WHERE name='\u0026quot; + name + \u0026quot;'\u0026quot;; //加'防sql注入\rstmt = conn.createStatement(); //Statement\rResultSet rs = stmt.executeQuery(sql); //执行查询，获得结果集\r//STEP 5: 从结果集中获取数据并转化成bean\rwhile (rs.next()) {\rUser user = new User();\ruser.setId(rs.getInt(\u0026quot;id\u0026quot;));\ruser.setName(rs.getString(\u0026quot;name\u0026quot;));\ruser.setAge(rs.getInt(\u0026quot;age\u0026quot;));\ruser.setDeleteFlag(rs.getString(\u0026quot;delete_flag\u0026quot;));\rusers.add(user);\r}\r// STEP 6: 关闭连接\rrs.close();\rstmt.close();\rconn.close();\r} catch (SQLException se) {\r// Handle errors for JDBC\rse.printStackTrace();\r} catch (Exception e) {\r// Handle errors for Class.forName\re.printStackTrace();\r} finally {\r// finally block used to close resources\rtry {\rif (stmt != null)\rstmt.close();\r} catch (SQLException se2) {\r// nothing we can do\r}\rtry {\rif (conn != null)\rconn.close();\r} catch (SQLException se) {\rse.printStackTrace();\r}\r}\r}\r PreparedStatement 有预编译，所以第一次查询很慢，但是之后很快\n@Test\rpublic void QueryPreparedStatementDemo() {\rConnection conn = null;\rPreparedStatement stmt = null;\rList\u0026lt;User\u0026gt; users = new ArrayList();\rtry {\r// STEP 2: 注册mysql的驱动\rClass.forName(DRIVER);\r// STEP 3: 获得一个连接\rconn = DriverManager.getConnection(URL, USERNAME, PASSWORD);\r// STEP 4: 创建一个查询\rString sql = \u0026quot;SELECT * FROM user WHERE name=? \u0026quot;; //占位符，有预处理，可以避免sql注入\rstmt = conn.prepareStatement(sql); //\rstmt.setString(1, \u0026quot;yuanya or 1=1\u0026quot;);\r/*得到的sql将是\u0026quot;SELECT * FROM user WHERE name='yuanya or 1=1'\u0026quot;而不是\u0026quot;SELECT * FROM user WHERE name=yuanya or 1=1\u0026quot;\r拼接到sql中的字符串变量将作为一个整体'yuanya or 1=1'，从而防止sql注入*/\rResultSet rs = stmt.executeQuery();\r// STEP 5: 从resultSet中获取数据并转化成bean\rwhile (rs.next()) {\rUser user = new User();\ruser.setId(rs.getInt(\u0026quot;id\u0026quot;));\ruser.setName(rs.getString(\u0026quot;name\u0026quot;));\ruser.setAge(rs.getInt(\u0026quot;age\u0026quot;));\ruser.setDeleteFlag(rs.getString(\u0026quot;delete_flag\u0026quot;));\rusers.add(user);\r}\r// STEP 6: 关闭连接\rrs.close();\rstmt.close();\rconn.close();\r} catch (SQLException se) {\r// Handle errors for JDBC\rse.printStackTrace();\r} catch (Exception e) {\r// Handle errors for Class.forName\re.printStackTrace();\r} finally {\r// finally block used to close resources\rtry {\rif (stmt != null)\rstmt.close();\r} catch (SQLException se2) {\r// nothing we can do\r}\rtry {\rif (conn != null)\rconn.close();\r} catch (SQLException se) {\rse.printStackTrace();\r}\r}\r}\r batch操作 批量操作，只连接了一次数据库\n@Test\rpublic void batchDemo() {\rConnection conn = null;\rPreparedStatement stmt = null;\rtry {\r// STEP 2: 注册mysql的驱动\rClass.forName(DRIVER);\r// STEP 3: 获得一个连接\rconn = DriverManager.getConnection(URL, USERNAME, PASSWORD);\r// STEP 4: 启动手动提交。写操作需要用到事务，读操作不需要\rconn.setAutoCommit(false);\r// STEP 5: 创建一个更新\rString sql1 = \u0026quot;UPDATE user SET delete_flag= '1' WHERE name='yuanya'\u0026quot;;\rString sql2 = \u0026quot;UPDATE user SET delete_flag= '1' WHERE name='tianchi'\u0026quot;;\rstmt.addBatch(sql1);\rstmt.addBatch(sql2);\rSystem.out.println(stmt.toString()); //打印sql\rint[] ret = stmt.executeBatch(); //影响行数是一个int[]，对应每个sql\rSystem.out.println(\u0026quot;影响数据库行数: \u0026quot; + ret);\r// STEP 6: 手动提交数据\rconn.commit();\r// STEP 7: 关闭连接\rstmt.close();\rconn.close();\r} catch (SQLException se) {\r// Handle errors for JDBC\rtry {\rconn.rollback();\r} catch (SQLException e) {\r// TODO Auto-generated catch block\re.printStackTrace();\r}\rse.printStackTrace();\r} catch (Exception e) {\rtry {\rconn.rollback();\r} catch (SQLException e1) {\r// TODO Auto-generated catch block\re1.printStackTrace();\r}\re.printStackTrace();\r} finally {\r// finally block used to close resources\rtry {\rif (stmt != null)\rstmt.close();\r} catch (SQLException se2) {\r}// nothing we can do\rtry {\rif (conn != null)\rconn.close();\r} catch (SQLException se) {\rse.printStackTrace();\r}\r}\r}\r shell #查看最大连接数\rshow variables like '%max_connections%'; #设置全局最大连接数\rset GLOBAL max_connections = 999;\r#列出全局变量\rshow global variable\r#列出系统会话变量\rshow session variable\r#定义变量\rdeclare 变量名1,变量名2... 类型1,类型2... [default 值]\r database  mysql初始database：information_schema、sysql、perfomance_schema  #列出\rshow databases\r#创建\rcreate database 库名\r#删除\rdrop database 库名\r#连接\ruse 库名\r table  操作表之前需要先通过 use 命令选择要操作的数据库  #列出\rshow tables\r#创建信息\rshow create table 表名\r#列信息\rshow columns from 表名\r#索引信息\rshow index from 表名\r#创建\rcreate table 表名(列名1 数据类型(长度) 约束1 约束2, 列名2 数据类型(长度) 约束1 约束2...);\rcreate table `doctor_patient_group` (\r`id` int(11) unsigned NOT NULL AUTO_INCREMENT,\r`created_at` datetime NOT NULL DEFAULT CURRENT_TIMESTAMP COMMENT '创建时间',\r`updated_at` datetime NOT NULL DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP COMMENT '更新时间',\rPRIMARY KEY (`id`),\rkey `idx_created_at` (`created_at`)\r) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_unicode_ci COMMENT='hahah';;\r#复制表\rcreate table 表名2 as(select * from 表名1)\r#复制表结构\rcreate table 表名2 like 表名1\r#修改表名\rrename table 旧表名 to 新表名\ralter table 旧表名 rename as 新表名\r#修改表的属性\ralter table 表名 change 旧列名 新列名 数据类型(长度) 约束1 约束2\ralter table 表名 add column 列名 数据类型(长度);\ralter table 表名 drop column 列名;\r#删除\rdorp table 表名\r #查看Mysql数据库管理系统的性能及统计信息\rshow table status like [FROM db_name] [LIKE 'pattern'] \\G;\r 数据类型 MySQL常用数据类型  字符串型 ：VARCHAR(varchar(10)：可变长度的字符串\u0026quot;abc\u0026rdquo;)、 CHAR(char(10)：固定长度的字符串\u0026quot;abc \u0026ldquo;) 大数据类型：BLOB（存储大的二进制数据）、TEXT（存储海量的文本数据） 数值型：TINYINT 、SMALLINT、INT、BIGINT、FLOAT、DOUBLE 逻辑性 ：BIT 日期型：DATE、TIME、DATETIME、TIMESTAMP  字段约束\n 列名 数据类型 约束1 约束2 非空约束：not null 唯一约束：unique 主键约束：primary key，非空，唯一，alter table tablename drop primary key删除主键 主键自增：auto_increment 外键添加：  列名 类型(长度) [constraint fk_外键名] references 表名2(外键名) [constraint fk_外键名] foreign key(外键名) references 表名2(外键名) slter table 表名 add [constraint kf_外键名] foreign ke(外键名) reference 表名2(外键名)    crud select #查询所有*\rselect * from 表名\r#查询指定列\rselect 列名1, 列名2, 函数名3(列名3)... from 表名\r as #查询字段定义别名as，as可以省略\rselect 列名 as '别名' from 表名\r 聚合函数 #聚合函数。sum()和，avg()均值，count()个数，max()最大值，min()最小值\rselect 列名, 函数名(列名) as '别名' from 表名\r distinct #去重查询distinct。distinct对其后列名去重\rselect distinct 列名 from 表名\r limit #分页查询limit。分页查询，index从0开始\rselect * from 表名 limit index,pageSize\r where #条件查询where\rselect * from 表名 where 条件式\r  where：where 条件式  日期：WHERE date\u0026lt;'999-9-9\u0026rsquo;，日期date在999年9月9日之前 and：where num\u0026gt;=3 and num\u0026lt;=9，num在3-9之间 or：WHERE num=10 OR num=30，num为3或者为9 not：WHERE NOT(num\u0026gt;2000)，num不大于2000 between\u0026hellip;and：where sal between3 AND 9，num在3-9之间 in(值1,值2\u0026hellip;)：where num in(3,9\u0026hellip;)，num为3或9 length(列名)：where length(name)=5，名字长度为5    #模糊查询like，_代替一个字符，%代替任意长度字符，转义字符有效\rselect * from 表名 where 列名 like '_xxx%'\r group #分组查询，group by\rgroup by 列名\r#分组查询，group by ...... having\rgroup by 列名 having 条件式\r order #排序查询，order by，默认升序，asc升序，desc降序\rorder by 列名1 desc, 列名2 asc\r 顺序小结 #顺序小结：(S-F-W-G-H-O) select ... from ... where ... group by... having... order by ... ; SELECT id,count(*) AS '个数'\rFROM user\rWHRER name LIKE '_xxx%' #筛选在分组之前\rGROUP BY rank #分组在再筛选之前，因为分组得到的数据count被再筛选使用\rHAVING count(*)\u0026gt;1 #再筛选：having对分组之后的数据进行再筛选\rORDER BY count(*) DESC; #排序在最后，因为筛选完才能排序\r 关联查询 #内连接where，有无外键约束查询都有效，对删改有约束效果\rwhere：select 列名1,列名2... from 表名1,表名2 where 表名1.列名=表名2.列名 and empno=7788\r#内连接inner join\rselect 列名1,列名2... from 表名1 inner join 表名2 on 表名1.列名=表名2.列名 and empno=7788\r#左外连接left outer join。学生表(student)、必修表(required)、选修表(elective)，查询学生的id、name、选修科目数量、必修科目数量、科目总数\rselect s.student_id,s.student_name, ifnull(r.rCount,0) as rCount, ifnull(e.eCount,0) as eCount, ifnull(r.rCount,0)+ifnull(e.eCount,0)*1 as amount from student as s left join ( select student_id, count(1) as rCount from required group by student_id ) as r on s.student_id = r.student_id left join (\rselect student_id,count(1) as eCount from elective group by student_id ) e on p.student_id = e.student_id where r.rCount\u0026gt;0 or e.eCount\u0026gt;0 order by activity desc limit 0,9;\r#右外连接：lright outer join\r#全外连接：full outer join(mysql不支持)\r 嵌套查询 外层的查询块称为父查询，内层的查询块称为子查询\n关联子查询 子查询不可以单独运行，依赖于福查询中的数据\n#exists。exists(子查询)返回true或false，为true则查询符合子查询条件的父查询内容\rselect * from 表1 where exists(select * from 表名2 where 表1.外键名=外键名)\r 非关联子查询 子查询可以单独运行\n#单行单列子查询：子查询结果返回的值只有一个，\u0026lt; 小于，\u0026gt; 大于，\u0026lt;= 小于等于，\u0026gt;=大于等于，=等于，!=或\u0026lt;\u0026gt;不等于\rselect 列名1 from 表名1 where 列名\u0026lt;(select 函数名(列名) from 表名2 where 条件式; );\rselect student_id from score where count(course_i)\r#单列多行子查询：子查询结果返回的值有多个。any、all、in、not in\rselect 列名1 from 表名1 where 列名 \u0026gt; any (select 函数名(列名) from 表名2 where 条件式; );\rselect 列名1 from 表名1 where 列名 \u0026lt; all (select 函数名(列名) from 表名2 where 条件式; );\rselect 列名1 from 表名1 where 列名 in (select 函数名(列名) from 表名2 where 条件式; );\r#查询参加算计考试且不参加英语和数学考试的学生的所有信息\rselect t\rwhere id in(\rselect stu_id\rfrom scor\rwhere c_name='计算机' and stu_id not in(\rselect stu_id\rfrom score\rwhere c_name='英语' or c_name='数学'\r)\r)\r#多行多列子查询：子查询的结果是一个表\r 合并查询 #合并查询：union(去重合并)，union all(无去重合并)\rselect 列名1,列名3 from 表1 union select 列名2,列名4 from 表2\r insert  存在的判断，需要插入内容包含主键或唯一索引（字段只要被UNIQUE修饰就有唯一索引），且3种操作无论是否成功插入都会使自增的主键+1  #如果已存在则报错，如果不存在则插入\rinsert into 表名(列名1, 列名2...) values (值1,值2,...),(值1,值2,...);\r#已存在行则忽略，不存在行则插入\rinsert ignore into ...\r#已存在行则替换，不存在行则插入\rreplace into ...\r update update 表名 set 列名1=值1, 列名2=值2... where 条件式\r delete delete from 表名 where 条件式\r 视图 一个或多个表的整个或部分的映射，一般只用来查询\n#创建\rcreate view 视图名 as 查询语句\r 存储过程 无返回值，但有输出参数，call调用\n#创建函数(in，out，inout)\rcreate procedure 函数名( in 输入参数名 varchar(18), out 输出参数名 varchar(18) )\rbegin\rdeclare 变量名1 varchar(18);\rdeclare 变量名2 varchar(18);\rset 变量名1='值'; #赋值，无set为判断相等\rselect 列名1,列名2 into 变量名1,变量名2 from 表名 where 列名='值' #从表中查询然后赋值给变量\rselect 变量1; #输出\rif 条件式 then .......;\relseif 条件式 then .......;\relse 条件式 then .......;\rend if;\rcase 变量名\rwhen 值1 then ......;\rwhen 值2 then ......;\rend case;\rdeclare i int default 9\rrepeat #循环\rset num=num-1;\runtll num\u0026lt;1 end repeat;\rwhile i\u0026gt;0 do\r......;\rend while\r标签名:while(true) do\rif 条件式 then iterate 标签名; #leave相当于continue\rif 条件式 then leave 标签名; #leave相当于break\rend while\rend\rcall 函数名(); #执行\rdrop procedure 存储; #删除函数\r 函数 select 列名1，if (列名2='值1','值1时显示值','非值1显示值') 称呼 from 表名; #根据不同值输出不同显示值\rselect 列名1，case 列名2 when '值1' then '显示值1' when '值2' then '显示值2' then ' ' from 表名; #根据不同值输出不同显示值\rselect ifnull('值1','值2') ; #值1为null则返回值2，否则返回值1\rselect nullif('值1','值2'); #值1=值2返回值1，否则返回null\rselect concate('值1','值2','值3'); #拼接字符串\rconcate_ws(sep,s1,s2...,sn); #将s1,s2...,sn连接成字符串，并用sep字符间隔\rsubstring（被截取字段，从第几位开始截取，截取长度）\rTRIM(str) #去除字符串首部和尾部的所有空格\rUUID() #生成具有唯一性的字符串\rLASTINSERTID() #返回最后插入的id值\rCURDATE()或CURRENT_DATE() #返回当前的日期\rCURTIME()或CURRENT_TIME() #返回当前的时间\rDATE_ADD(date,INTERVAL int unit) #返回日期date加上间隔时间int的结果(int必须按照关键字进行格式化),如：SELECT DATE_ADD(CURRENT_DATE,INTERVAL 6 MONTH);\rDATE_FORMAT(date,fmt) #依照指定的fmt格式格式化日期date值，如：select DATE_FORMAT(CURDATE(),'%Y-%m-%d')\rDATE_SUB(date,INTERVAL int unit) #返回日期date减去间隔时间int的结果(int必须按照关键字进行格式化),如：SELECT DATE_SUB(CURRENT_DATE,INTERVAL 6 MONTH);\rNOW() #返回当前的日期和时间\r 自定义函数\nCREATE FUNCTION sp_name ([param_name type[,...]])\rRETURNS type -- 定义返回值类型\rBEGIN\rroutine_body\rEND\r#删除\rDROP function [IF EXISTS] sp_name;\r#查看\rSHOW CREATE FUNCTION sp_name;\r#使用\rSELECT db_name.sp_name;\r 触发器 #为表创建在 增/删/改 之前或之后 执行的触发器\rcreate trigger 触发器名 before/after insert/delete/update 表名 for each ROW\rBEGIN\r#触发器内容，例插入数据时检验输入性别是否正确，无效数据将性别设置为女\rif new.student_age\u0026lt;0 and new.student_age\u0026gt;150 then\rset new.student_age=old.student_age;\rend if;\rEND\rinsert into student VALUES('student013','小十三',13,'男男女女')\r 条件处理器 declare [continue/exit/undo] handler for 条件\r#条件\rmysql_error_code | SQLSTATE [VALUE] sqlstate_value\r| condition_name | SQLWARNING | NOT FOUND | SQLEXCEPTION\r 游标 -- 根据课程号查询不及格名单(用游标)\rcreate procedure pro (in couid)\rbegin\rdeclare stuid varchar(18);\rdeclare score varchar(18);\rdeclare flag int default false;\rdeclare cur cursor for select * from score where score\u0026lt;60 and course_id=couid; -- 定义游标\rdeclare continue handler for not found set flag=true; -- 定义处理器\ropen cur; -- 打开游标\rw:while true DO\rif flag then\rleave w\rend if\rfetch cur into stuid,couid,score;\rselect stuid,couid,score;\rend while w;\rclose cur; -- 关闭游标\rend\r 事务 BEGIN #显式地开启一个事务；\rSTART TRANSACTION #显式地开启一个事务；\rCOMMIT #也可以使用COMMIT WORK，不过二者是等价的。COMMIT会提交事务，并使已对数据库进行的所有修改称为永久性的；\rROLLBACK #有可以使用ROLLBACK WORK，不过二者是等价的。回滚会结束用户的事务，并撤销正在进行的所有未提交的修改；\rSAVEPOINT identifier #SAVEPOINT允许在事务中创建一个保存点，一个事务中可以有多个SAVEPOINT；\rRELEASE SAVEPOINT identifier #删除一个事务的保存点，当没有指定的保存点时，执行该语句会抛出一个异常；\rROLLBACK TO identifier #把事务回滚到标记点；\rSET TRANSACTION #用来设置事务的隔离级别\r#方式一：（用 BEGIN, ROLLBACK, COMMIT来实现）\rBEGIN #开始一个事务\rROLLBACK #事务回滚\rCOMMIT #事务确认\r#方式二：（直接用 SET 来改变 MySQL 的自劢提交模式）\rSET AUTOCOMMIT=0 #禁止自劢提交\rSET AUTOCOMMIT=1 #开吭自劢提交\r 备份 #备份\rmysqldump -u userName -p [arguments] \u0026gt; file_name.sql\r#备份服务器上所有数据库\rmysqldump -u userName -p --all-databases \u0026gt; file_name.sql\r#备份指定数据库\rmysqldump -u userName -p --databases \u0026gt; file_name.sql\r#还原\rmysql -u userName -p \u0026lt; file_name.sql\r 性能 指标 TPS  TPS：TransactionsPerSecond（每秒传输的事物处理个数），这是指服务器每秒处理的事务数，支持事务的存储引擎如InnoDB等特有的一个性能指标。只适用于InnoDB，因为MyiISAM没有事务 TPS = COM_COMMIT + COM_ROLLBACK) /UPTIME，（事务的提交次数+回滚次数）/mysql server运行时长，运行时长在mysql server重启后会刷新  use information_schema;\rselect VARIABLE_VALUE into @num_com from GLOBAL_STATUS where VARIABLE_NAME='COM_COMMIT';\rselect VARIABLE_VALUE into @num_roll from GLOBAL_STATUS where VARIABLE_NAME='COM_ROLLBACK' ;\rselect VARIABLE_VALUE into @uptime from GLOBAL_STATUS where VARIABLE_NAME='UPTIME';\rselect (@num_com+@num_ro11)/@uptime;\r 注：从mysql5.7.6开始information_schema.global_status已经开始被舍弃，为了兼容性，此时需要打开 show_compatibility_56。https://blog.csdn.net/itwxming/article/details/97897589\nQPS  QPS: Queries Per Second（每秒查询处理量）同时适用与InnoDB和MyISAM引擎 QPS=QUESTIONS/UPTIME，每秒查询数量 等待时间：执行Sql等待返回结果之间的等待时间  use information_schema;\rselect VARIABLE_VALUE into @num_queries from GLOBAL_STATUS where VARIABLE_NAME='QUESTIONS';\rselect VARIABLE_VALUE into @num_roll from GLOBAL_STATUS where VARIABLE_NAME='COM_ROLLBACK' ;\rselect VARIABLE_VALUE into @uptime from GLOBAL_STATUS where VARIABLE_NAME='UPTIME';\rselect @num_queries/@uptime;\r 压测 MySqlSlap   MySqlSlap：mysql提供的官方压测工具\n  安装：MySQLSlap是从MySQL的5.1. 4版开始就开始官方提供的压力测试工具。创建schema、table、 test data；运行负载测试，可以使用多个并发客户端连接；测试环境清理(删除创建的数据、表等，断开连接)\n  参数\n \u0026ndash;create-schema=name：指定测试的数据库名，默认是mysqlslap \u0026ndash;engine=name：创建测试表所使用的存储引擎，可指定多个 \u0026ndash;concurrency=N：模拟N个客户端并发执行。可指定多个值，以逗号或者 \u0026ndash;number-of-queries=N：总的测试查询次数(并发客户数×每客户查询次数)，比如并发 是10，总次数是100，那么10个客户端各执行10个 \u0026ndash;iterations=N：迭代执行的次数，即重复的次数（相同的测试进行N次，求一 个平均值），指的是整个步骤的重复次数，包括准备数据、测 试load、清理 \u0026ndash;commit=N：执行N条DML后提交一次 \u0026ndash;auto-generate-sql, -a：# 自动生成测试表和数据，表示用mysqlslap工具自己生成的 SQL脚本来测试并发压力。 \u0026ndash;auto-generate-sql-load-type=name：# 测试语句的类型。代表要测试的环境是读操作还是写操作还是两者混合的。# 取值包括：read (scan tables), write (insert into tables), key (read primary keys), update (update primary keys), or mixed (half inserts, half scanning selects). 默认值是：mixed. \u0026ndash;auto-generate-sql-add- auto-increment：对生成的表自动添加auto_increment列 \u0026ndash;number-char-cols=name 自动生成的测试表中包含N个字符类型的列，默认1 \u0026ndash;number-int-cols=name：自动生成的测试表中包含N个数字类型的列，默认1 \u0026ndash;debug-info：打印内存和CPU的信息    案例：直接到bin目录中执行即可\n  1000个客户端，重复10次，自动生成SQL语句，总共1000个查询：\n./mysqlslap -uroot -proot \u0026ndash;concurrency=1000 \u0026ndash;iterations 10 -a \u0026ndash;auto-generate-sql-add-autoincrement \u0026ndash;engine=innodb \u0026ndash;number-of-queries=1000\n  1, 50，100， 200个客户端， 每一个测试3次， 并打印内存，CPU信息 ./mysqlslap -uroot -proot \u0026ndash;concurrency=1,50,100,200 \u0026ndash;iterations=3 \u0026ndash;number-char-cols=5 \u0026ndash;number-int-cols=5 \u0026ndash;auto-generate-sql \u0026ndash;auto-generate-sql-add- autoincrement \u0026ndash;engine=myisam,innodb \u0026ndash;create -schema='enjoytest1\u0026rsquo; \u0026ndash;debug-info\n  500个客户端，分别使用myisam,innodb两种存储引擎，比较效率：\n./mysqlslap -uroot -proot \u0026ndash;concurrency=500 \u0026ndash;iterations=3 \u0026ndash;number-char-cols=5 \u0026ndash;number-int-cols=5 \u0026ndash;auto-generate-sql \u0026ndash;auto-generate-sql-add-autoincrement \u0026ndash;engine=myisam,innodb \u0026ndash;create-schema='enjoytest1\u0026rsquo; \u0026ndash;debug-info\n    逻辑架构 这里是部分架构\n连接层  当MySQL启动（MySQL服务器就是一个进程），等待客户端连接 每一个客户端连接请求，服务器都会新建一个线程处理（如果是线程池的话，则是分配一个空的线程），每个线程独立，拥有各自的内存处理空间，如果这个请求只是查询，没关系，但是若是修改数据，显然，当两个线程修改同一块内存是会引发数据同步问题的 连接到服务器，服务器寻需要对其进行验证，也就是IP、账户、密码验证，一旦连接成功，还要验证是否具有执行某个特定查询的权限（例如，是否允许客户端对某个数据库某个表的某个操作）  SQL处理层   主要功能有：SQL语句的解析、优化；缓存的查询；MySQL内置函数的实现；跨存储引擎功能（所谓跨存储引擎就是说每个引擎都需提供的功能（引擎需对外提供接口），例如:存储过程、触发器、视图等。\n  如果是查询语句(select语句)，首先会查询缓存是否已有相应结果，有则返回结果，无则进行下一步（如果不是查询语句，同样调到下一步）\n  解析查询，创建一个内部数据结构（解析树），这个解析树主要用来SQL语句的语义与语法解析;\n  优化：优化SQL语句，例如重写查询，决定表的读取顺序，以及选择需要的索引等。这一阶段用户是可以查询的，查询服务器优化器是如何进行优化的，便于用户重构查询和修改相关配置，达到最优化。这一阶段还涉及到存储引擎，优化器会询问存储引擎，比如某个操作的开销信息、是否对特定索引有查询优化等。\n  缓存：如果缓存中有就执行缓存中的计划（如果开启了数据缓存则直接返回结果）\n 缓存sql（及其执行计划），默认开启 缓存数据，默认不开启。  数据缓存是否开启的参数：show variables like \u0026lsquo;%query_cache_type%'。一般不开启，而是用redis等专业的缓存 数据缓存大小：show variables like \u0026lsquo;%query_cache_size%'，修改大小SET GLOBAL query_cache_size=11111111;或者在mysql的配置文件my.ini中修改      查询解析器：解析成mysql识别的东西，见图7\n FROM：笛卡尔积 ON：主表保留 JOIN：不符合ON也添加。WHERE：非聚合；非SELECT别名 GROUP BY：改变对表引用 HAVING：只作用分组后 SELECT：DISTINCT ORDER BY：可使用SELECT别名 LIMIT：rows；offset    查询优化器：explan查看优化结果，即执行计划。例如执行下面两个sql可以发现它们执行计划是完全一样的，where 1=1被查询优化器优化排除了\nexplain select * from user where 1=1\rexplain select * from user\r   存储引擎  查看：通过 show engines 查看看当前 mysql 提供的存储引擎；通过 show variables like '%storage_engine%' 查看 mysql 当前默认存储引擎 存储引擎的索引  聚集索引：索引项顺序 与 数据存储顺序 一致  即聚集索引的顺序就是数据的物理存储顺序。它会根据聚集索引键的顺序来存储表中的数据，即对表的数据按索引键的顺序进行排序，然后重新存储到磁盘上。因为数据在物理存放时只能有一种排列方式，所以一个表只能有一个聚集索引 场景：查询命令的回传结果是以该字段为排序依据的；查询的结果返回一个区间的值；查询的结果返回某值相同的大量结果集。这都得益于索引排序与数据排序一致 优：select适用性高。从场景就可以发先聚集索引的查询很符合我们最普遍的逻辑。 劣：聚集索引会降低 insert，和update操作的性能，所以，是否使用聚集索引要全面衡量   非聚集索引：索引项顺序 与 数据存储顺序 一致  非聚集索引必须是稠密索引 场景：查询所获数据量较少时；某字段中的数据的唯一性比较高时；   聚簇索引：聚簇索引并不是一种单独的索引类型，而是一种数据存储方式。术语“聚族”表示数据行和相邻的键值紧凑的存储在一起。因为无法同时把数据行放在两个不同的地方，所以一个表只能有一个聚族索引。  优点：可以把相关数据保存在一起。就好像在操场上战队，一个院系一个院系的站在一起，这样要找到一个人，就先找到他的院系，然后在他的院系里找到他就行了，而不是把学校里的所有人都遍历一遍。数据访问更快。聚族索引将索引和数据保存在同一个B-Tree中，因此从聚族索引中获取数据通常比在非聚族索引中查找更快   稠密索引：每个索引键值都对应有一个索引项，索引项指向的索引键值更连续  稠密索引能够比稀疏索引更快的定位一条记录   稀疏索引：每个索引键值都对应有一个索引项，索引项指向的索引键值更分散  相对于稠密索引，稀疏索引只为某些搜索码值建立索引记录；在搜索时，找到其最大的搜索码值小于或等于所查找记录的搜索码值的索引项，然后从该记录开始向后顺序查询直到找到为止。      InnoDB   Innodb：mysql5.5 及之后版本默认使用的存储引擎。\n  存储：frm 文件存储表结构，ibd 文件存储索引+数据。可见索引与数据是存储在一个文件中的\n  索引：聚集索引，索引项顺序 与 数据存储顺序 一致\n 也使用 B+Tree 作为索引结构，索引页大小16，和表数据页共同存放在表空间中，可以看出InnoDB表数据文件本身就是按 B+Tree 组织的一个索引结构，这棵树的叶节点 data 域保存了完整的数据记录。这个索引的key是数据表的主键，因此 InnoDB 表数据文件本身就是主索引。 InnoDB默认对主键建立聚簇索引。如果你不指定主键，InnoDB会用一个具有唯一且非空值的索引来代替。如果不存在这样的索引，InnoDB会定义一个隐藏的主键，然后对其建立聚簇索引。一般来说，InnoDB 会以聚簇索引的形式来存储实际的数据，它是其它二级索引的基础。 所以mysql innodb引擎的聚集索引、聚簇索引都默认是主键索引，如果没有指定主键，就是一个具有唯一且非空值的索引，如果不存在这样的索引，就是InnoDB自定义的隐藏主键索引，并且该索引是稠密索引。 支持主外键    锁：默认采用行级锁，并发能力稍好；当然表级锁也是支持的\n  表空间：通过 innodb_ file_ per_ table 配置 innodb 的表是否使用单独数据文件。配置为 ON 时开启 独立表空间；配置为 OFF 时则是 系统表空间。\n  独立表空间：每个表使用自己单独的文件。会在对应 schame 文件夹下生成 frm 和 idb 文件，以表名为名，table_name.frm、table_name.ibd（索引+数据）。通过 optimize table table_name 可以简便的收缩系统文件，独立表空间可以同时向多个文件刷新数据。MySQL5.6 及之后默认为独立表空间。\n-- 删除大量数据，再执行OPTIMIZE TABLE table_name，可以发现.idb文件大小变小了很多，类似于磁盘整理，这就是收缩系统文件\rDELETE user WHERE id!=1\rOPTIMIZE TABLE user\r   系统表空间：每个表没有单独的文件，所有 innodb 的表共用一个数据文件。schame 文件夹下仍然会生成其 frm 文件，table_name.frm，但是索引和数据将存放在data/目录下的idata文件中，是一个系统级的数据文件。无法简单的收缩文件大小，随着数据量增大，系统表空间会存在 IO 瓶颈。mysql5.6 之前默认为系统表空间\n  建议：Innodb 建议使用 独立表空间\n    事务：Innodb 是一种事务性存储引擎，完全支持事务得 ACID 特性，Redo Log 和 Undo Log\n  缓存：缓存索引+数据。对内存要求较高，而且内存大小对性能有决定性的影响\n  其它：支持全文检索（mysql5.6 及之后）；支持数据压缩，如前面 optimize 操作；\n  适用场景：Innodb 适合于大多数 OLTP（on-line transaction processing，联机事务处理）应用\n  MyISAM  MyISAM：mysql5.5 之前默认使用的存储引擎 存储：frm 文件存储表结构，MYI 文件存储索引，MYD 文件存储数据。可见索引与数据是分开存储的 索引：非聚集索引，索引项顺序 与 数据存储顺序 不一致  索引文件仅保存数据记录的地址，主索引和辅助索引无区别 索引页正常大小为1024字节，索引页存放在.MYI 文件中 使用 B+Tree 作为索引结构，叶节点的 data 域存放的是数据记录的地址 不支持主外键   锁：只支持表级锁，并发能力自然一般 事务：不支持 缓存：只缓存索引，不缓存数据 其它  支持全文检索 支持数据压缩：由 bin/myisampack.exe 程序支持，通过命令 myisampack -b -f table_name.MYI 可以压缩数据文件，压缩后旧文件会变成 .OLD 文件。注意如果删除 .OLD 文件，查询可以正常查询，但是增删改可能会出问题，通过 CHECK table product info 可以检查是否存在问题，通过 REPAIR table product info 可以恢复原文件。   适用场景：临时表、非事务型应用（如数据仓库、日志数据、报表等）；只读类应用，查询快；空间类应用(空间函数，坐标)，详见文档mysl5空间扩展。  Memory  Memory：也称 HEAP 存储引擎，数据保存在内存中，所有连接有效，重启mysql就没了 索引：支持 HASH 索引和 BTree 索引 锁：表级锁 其它：所有字段都是固定长度varchar(10) = char(10)；不支持Blog和Text等大字段；最大大小由max_ heap_ table_size参数决定 使用场景：  临时表：保存一些临时数据  临时表只在一个连接中有效 临时表都是由 mysql 系统自己创建和维护的，通过 create temporary table table_name{...} 可以创建临时表 mysql 自己就是使用 Memory 或 Myisam 存储引擎来实现临时表。临时表大小不超过配置限制，则使用 Memory 表；超过限制将使用 Myisam 表。Memory 表存储在内存，Myisam 表存储在磁盘，可见临时表过大将严重影响性能   hash 索引用于查找或者是映射表（邮编和地区的对应表） 用于保存数据分析中产生的中间表 用于缓存周期性聚合数据的结果表 数据可再生。memory数据易丢失，所以要求数据可再生    CSV  存储：frm 文件存储表结构，csm 文件存储表得元数据如表状态和数据量，csv 文件存储数据。数据以文本的方式存储 特点：以csv格式进行数据存储；所有列都不能为null；不支持索引(不适合大表，不适合在线处理)；可以对数据文件直接编辑(保存文本文件内容，每一行要以回车\\n结尾)：需要flush tables; 刷新表  Archive  存储：frm 文件存储表结构，ARZ 文件存储数据。是以 zlib 对表数据进行了压缩，磁盘I/O更少 其它特性：只支持insert和select操作，只允许在自增ID列上加索引 使用场景：日志和数据采集应用（当然一般如果用到了 mongodb 就使用 mongodb 即可）  Ferderated  其它：提供了访问远程MySQL服务器上表的方法；本地不存储数据，数据全部放到远程服务器上；本地需要保存表结构和远程服务器的连接信息 适用场景：偶尔的统计分析及手工查询 开启：通过 show engine 查看是否开启，默认不开启，需要手动开启，在配置文件 my.ini 中加上配置 ferderated=1 也可以开启  --local_fed要与下面remote_fed表结构一致\rCREATE TABLE local_fed (\r`id` int(11) NOT NULL AUTO_ INCREMENT,\r`c1` varchar (10) NOT NULL DEFAULT '',\r`c2` char(10) NOT NULL DEFAULT '',\rPRIMARY KEY (`id`)\r) ENGINE=federated CONNECTION ='mysq1:/ / root:root@127.0.0.1:3306/remote/remote_fed'\r 锁   行级锁：\n 优劣：开销大，加锁慢；锁定粒度最小，发生锁冲突的概率最低，并发度也最高，但会出现死锁。 适用：适合于有大量按索引条件并发更新少量不同数据，同时又有并发查询的应用。如一些在线 OLTP（on-line transaction processing，联机事务处理）系统。    表级锁：\n 优劣：开销小，加锁快；锁定粒度大，发生锁冲突的概率最高，并发度最低，但不会出现死锁 适用：适合于以查询为主，只有少量按索引条件更新数据的应用。如OLAP（On-Line Analytical Processing，联机分析处理）系统    页面锁\n 优劣：开销和加锁时间界于表锁和行锁之间:会出现死锁:锁定粒度界于表锁和行锁之间，并发度一般。    仅从锁的角度来说（很难笼统地说哪种锁更好，只能就具体应用的特点来说哪种锁更合适）\n  表锁   表锁：就是对表的读写锁\n  Table Read Lock：表读锁。一个 session 对一个 table 的读操作会导致 表读锁。因为是存在读共享的，所以也称为表共享读锁\n  其它session操作被锁table：读无阻塞共享，写会等待阻塞\n  上锁session操作被锁table：读无阻塞，写err\n  上锁session操作其它table：上锁session被锁在表中，对其它表执行任何读写操作都将err（注意：被锁表取别名也将被识别为其它表）\n  通过 lock table 表名 read 可以手动上表读锁\nlock table user read; --上表共享读锁\rupdate user set id=2 where id=1; --报错\rupdate user_info set id=2 where id=1; --对另一张表执行写操作也报错，是session写 被关在表中了\rselect * user; --成功，读无阻塞共享\rselect u.* user fom user u; --如果是别名也会报错，a会被识别为非user表\r     lock table user as u Read; \u0026ndash;如果非要用别名，那也必须要用同名别名来上锁 ```\n```sql\r \u0026ndash;另一个session。user被读锁锁住期间，在另一个session中执行写操作，不会报错但是会等待，直到user释放锁则成功，否则阻塞时间过长将超时 update user set id=2 where id=1; \u0026ndash;等待 ```\n  Table Write Lock：表写锁，表独占写锁。一个session对一个table的写操作会导致 表独占写锁（手动上锁：lock table 表名 write）。写锁，即对表的写操作导致的锁，因为读写都是独占的，所以被称为独占\n 其它session操作被锁table：读写均等待阻塞 上锁session操作被锁table：独占，读写均可成功 上锁session操作其它table：上锁 session 被锁在表中，对其它表执行任何读写操作都将err（注意：被锁表取别名也将被识别为其它表） 通过 lock table 表名 write 可以手动上表写锁      锁表因素：\n 非索引查询会导致表锁，但 MySQL 做了优化，对于不满足条件的记录，会在判断后释放锁，最终持有的，是满足条件的记录上的锁 表结构修改会导致表锁    行锁   MySQL的InnoDB引擎支持行锁和表锁。数据库使用锁是为了支持更好的并发，提供数据的完整性和一致性。InnoDB是一个支持行锁的存储引擎，锁的类型有：共享锁（S）、排他锁（X）、意向共享（IS）、意向排他（IX）。为了提供更好的并发，InnoDB提供了非锁定读：不需要等待访问行上的锁释放，读取行的一个快照。该方法是通过InnoDB的一个特性：MVCC来实现的。\n  行锁：不一定是一行，可能是多行。如页锁 where id\u0026gt;3 and id\u0026lt;9；如间隙锁 where id=3 or id=9。提交或回滚事务可以释放锁。\n  共享锁：又称读锁。多个事务的查询语句可以共用某数据的共享锁，读无阻塞共享\n 如果只有一个事务拿到了共享锁，则该事务可以对数据进行 UPDATE DETELE 等写操作； 如果有多个事务拿到了共享锁，则所有事务都不能对数据进行 UPDATE DETELE 等写操作。  --关闭自动提交\rbegin --开启事务\rselect * from user where id=1 lock in share mode; --上共享锁\rcommit; --提交或回滚事务可以释放锁\r   排它锁：又称写锁。只有一个事务能获取某数据的排它锁，该事务可以读写该数据，但其它事务对该数据读写等待阻塞。\nselect from user where id=1 for update; --上排它锁\r     意向锁：意向锁的存在是为了协调行锁和表锁的关系，支持多粒度（表锁与行锁）的锁并存\n 意向共享锁：IS锁。一个事务给一个数据行加共享锁时，必须先获得表的IS锁。如果事务B要给表上一个表独占锁，判断意向锁就知道表中已经有在使用共享锁，就会阻塞 意向排他锁：IX锁。一个事务给一个数据行加排他锁时，必须先获得表的IX锁。如果事务B要给表上一个表锁，判断意向锁就知道表中已经有在使用排他锁了，就会阻塞    行锁的实现算法：https://www.oschina.net/question/4154815_2315824，https://www.cnblogs.com/zhoujinyi/p/3435982.html\n  Record Lock：记录锁，单个行记录上的锁。\n Record Lock总是会去锁住索引记录，如果InnoDB存储引擎表建立的时候没有设置任何一个索引，这时InnoDB存储引擎会使用隐式的主键来进行锁定    Gap Lock：间隙锁，锁定一个范围，但不包含记录本身，锁定已有记录之间的还未存在的值范围，或者第一条索引记录之前的范围，又或者最后一条索引记录之后的范围\n  产生间隙锁的条件（RR事务隔离级别下）：使用普通索引（非唯一索引）锁定；使用多列唯一索引；使用唯一索引锁定多行记录。使用单个唯一索引不会产生间隙锁，比如id=5，只会有记录锁\n  可以防止同一事务的两次当前读，解决幻读问题\n  通过 show variables like 'innodb_locks_unsafe_for_binlog' 查看是否禁用间隙锁，该参数是只读模式，通过配置文件 my.ini 配置可以禁用\n[mysqld]\rinnodb_locks_unsafe_for_binlog = 1\r     Next-Key Lock：临键锁=行锁+间隙锁，锁定一个范围，并且锁定记录本身。这个锁算法实际上就是对行锁和间隙锁的灵活应用，有时候只使用行锁，有时候只使用间隙锁，有时候同时一起使用\n 在Repeatable Read隔离级别下，Next-key Lock 算法是默认的行记录锁定算法。如果把事务的隔离级别降级为RC，临键锁则也会失效 范围是前开后闭的 ( ] 。比如存在记录 id=3,5 ，那么可能被锁定的范围就是(-, 3]、(3, 5]、(5, +]，此时查询 id=5 时，即锁定 (3, 5]。 当查询的索引有唯一属性时，InnoDB 会对 Next-Key Lock 进行优化，将其降为 Record Lock，即仅锁住索引本身，而不是范围；但是如果唯一索引由多个列组成，而查询仅是查找多个列中的一个，那么查询其实是range 类型查询，而不是 point 类型查询，故 InnoDB 仍然使用 Next-Key Lock 进行锁定 聚集索引和辅助索引同时存在时，当sql语句通过辅助索引列进行查询，会对两个索引分别进行锁定，满足辅助索引查询条件的行中的聚集索引使用 Record Lock 锁定，辅助索引使用 Next-Key Lock 锁定，且 InnoDB还会对辅助索引的下一个相等键值加上 Gap Lock，这样才能避免幻读。因为辅助索引可以是不唯一的，即使锁住了已有记录，其它事务仍然可以插入同值的记录，导致幻读  比如一个表 CREATE TABLE z (a INT,b INT,PRIMARY KEY(a), KEY(b)); INSERT INTO z SELECT 3,2; INSERT INTO z SELECT 5,4; INSERT INTO z SELECT 7,6; ，唯一索引a，辅助索引b 当使用辅助索引 b 查询时 WHERE b=4 ，记录为 5,4，对于唯一索引 a 将使用 Record Lock 锁定 a=5，对于辅助索引 b 将使用 Next-Key Lock 锁定 b=(2,4] ，并且s使用 Gap Lock 锁定 b=(4,6)。因为 InnoDB 的 insert 操作会检查插入记录的下一条记录是否被锁定，如果被锁定则不允许查询，自然也无法执行后续的插入了。所以如果不加这个 Gap Lock，即使已经锁定了已有的 b=4 的记录，b=5 并没有被锁定，其它事务仍然可以而插入一个 b=4 的记录，这将导致条件 where b=4 在同一事务中的再次查询出现幻读        注意：两个事务不能锁同一个索引。insert、delete、update操作在事务中都会自动默认加上排它锁。\n  面试题：系统运行一段时间，数据量已经很大，这时候系统升级，有张表A需要增加个字段，并发量白天晚上都很大，请问怎么修改表结构。面试考点：修改表结构会导致表锁,数据量大修改数据很长，导致大量用户阻塞，无法访问!\n 首先创建一个和你要执行的alter操作的表一样的空的表结构 执行我们赋予的表结构的修改，然后copy原表中的数据到新表里面。 在原表上创建一个触发器，在数据copy的过程中，将原表的更新数据的操作全部更新到新的表中来。 copy完成之后，用rename table新表代替原表，默认删除原表。    物理结构修改-pt-online-schema-change，一个表结构修改工具\n 下载安装 perl 环境：http://www.perl.org/get.html 下载 percona-toolkit 工具集合：https://www.percona.com/doc/percona-tookit ppm install DBI：依赖 ppm install DBD::mysql：安装mysq|驱动依赖 修改表结构（其实就是完成了上面面试题答案的步骤）：pt-online-schema-change h=127.0.0.1,u=root,D=database_name,t=table_name \u0026ndash;alter \u0026ldquo;modify colunm_name varchar(150) not null default ' ' \u0026quot; -execute    事务 现在的很多软件都是多用户，多程序，多线程的，对同一个表可能同时有很多人在用，为保持 数据的一致性，所以提出了事务的概念。 A给B要划钱，A的账户-1000元，B 的账户就要+1000元，这两个update语句必须作为一一个整体 来执行，不然A扣钱了，B没有加钱这种情况很难处理。\nshow engines，mysql中只有只有innodb存在事务\n1.查看数据库下面是否支持事务( InnoDB支持) ? show engines; 2.查看mysql当前默认的存储引擎? show variables like \u0026lsquo;%storage_ _engine%'; 3.查看某张表的存储引擎? show create table表名; 4.对于表的存储结构的修改? 建立InnoDB表: Create table \u0026hellip; type=InnoDB; Alter table table_ name type=InnoDB;\n事务的特性 事务应该具有4个属性:原子性、- -致性、隔离性、持久性。这四个属性通常称为ACID特性。 ◆原子性(atomicity) 。-一个事务是一个不可分割的工作单位(最小单元)，事务中包括的诸操作要么都做，要么都不做。一个人转给另一个人100块，A-100和B+100是一个一个整体必须都发生 ◆一致性(consistency)。事务必须是使数据库从一个\u0026ndash;致性状态变到另一个\u0026ndash;致性状态。一致性与原子性是密切相关的。A-100，B必须只能+100，不能出现如B+100后系统卡顿，没有得到响应，又让B+100。说简单一点就是要与事务所期望的数据结果完全一致，主要体现在不能让其它事务干扰修改数据导致数据错误 ◆隔离性(isolation) 。一个事务的执行不能被其他事务干扰。即一个事务内部的操作及使用的数据对并发的其他事务是隔离的，并发执行的各个事务之间不能互相干扰。 ◆持久性(durability) 。持久性也称永久性( permanence)，指一个事务- -旦提交，它对数据库中数据的改变就应该是永久性的。接下来的其他操作或故障不应该对其有任何影响。持久性不能完全通过数据库本身解决，需要做如主从、容灾等（几地几中心）\n  事务并发问题\n 脏读：一个事务读取到了其它事务修改前的数据。问题源于其它事务对数据的并发修改 不可重复读：一次事务中重复读取同一数据，多次读取之间数据不一致，违背了事务一致性状态，因此不可重复读。问题源于其它事务对数据的并发修改。行锁可以解决问题 幻读：一个事务中年重复读取一些数据，非第一次读取时读到了之前没有返回的记录，如幻影一般。问题源于其它事务对数据的并发增删。几个解决思路如下  next key lock：等于record Lock + gap Lock，锁定记录本身并锁定一个范围，可以完美控制当前读的幻读问题 表锁：简单粗暴 MVCC：https://blog.csdn.net/w2064004678/article/details/83012387。Multi-Version Concurrency Control，多版本并发控制，类似于乐观锁的一种实现方式。通过保存数据在某个时间点的快照，以快照读的方式读取数据，快照读从根本上就不存在幻读问题，但并不能阻止其它事务对表进行写操作，等于自欺欺人      事务隔离级别（重点）：事务隔离级别越高，越能保证数据的完整性和一致性，但是对并发性能的影响也越大，对于多数应用程序，可以优先考虑把数据库系统的隔离级别设为ReadCommitted,它能够避免脏读取，而且具有较好的并发性能。\nshow variables like '%tx_isolation%'; --查看当前事务隔离级别mysq|默认的事务隔离级别为repeatable-read\r--修改全局隔离级别可以通过配置文件\rset SESSION TRANSACTION ISOLATION LEVEL READ UNCOMMITTED; --修改当前session隔离级别\r  READ UNCOMMITED：读未提交。一个事务可以读取其它事务未提交的数据。存在问题：脏读、不可重复度、幻读  读未提交：如事务B修改了一行数据，还未提交，随之事务A成功读取到B未提交的修改过的数据，即读未提交 脏读：但是A读取到B修改过的数据之后，B回滚了数据，A读到的数据与数据库真实数据不符，该数据被称为脏数据，产生脏读   READ COMMITED：读已提交。一个事务可以读取其它事务已经提交的数据。存在问题：不可重复读、幻读。  读已提交：如A事务将对同一行数据执行2次读取，但是A读了一次之后，事务B修改了该行数据并完成提交，那么事务A第二次读取到的数据是事务B已提交的新数据，即可以读取其它事务已经提交的数据 不可重复读：因为上面两次读取到的数据不一致，不满足事务的一致性状态，即不可重复读 在 READ COMMITED 下，除了唯一性的约束检查及外键约束检查需要 Gap Lock，InnoDB 不会使用 Gap Lock 锁算法 READ COMMITED 下，通过 MVCC 解决了不可重复读。如果事务执行时间较短，其实没太多影响，因为本来就是以事务开始的时间作为节点去查询这个节点上的数据；但是如果执行时间特别长，甚至是一些失误写出的超级慢慢sql，那就可能会一定程度破坏数据一致性，就有些本质上治标不治本了   REPEATABLE READ：可重复读。存在问题：幻读。  可重复度：如事务A将对同一行数据执行2次读取，但是A读了一次之后，事务B修改了该行数据并完成提交，事务A第二次读取到的数据仍然将与第一次的数据一致，而读取不到B提交的新数据，满足了事务的一致性状态，即可重复读； 幻读：会话A开启事务A→会话B开启事务B→事务A修改数据,并查询到2条数据(A的读取到的第二条数据是A添加的数据)→事务B添加一条数据,并查询到2条数据(B的读取到的第二条数据是B添加的数据)→事务A提交→事务B提交→会话A再次查询数据发现有3条记录，就像幻觉→会话B再次查询数据发现有3条记录，就像幻觉。A和B均产生幻读 产生锁：事务隔离级别为可重复读时  行锁\u0026amp;页锁：如果where涉及索引（包括主键索引），以索引列为条件更新数据，会存在间隙锁间、行锁、页锁的问题，从而锁住一些行 表锁：如果where条件不涉及索引，更新数据时会锁住整张表 行锁升级表锁：索引失效     SERIALIZABLE：序列化，串行化。事务序列化串行执行。不存在任何问题  锁  表锁：事务隔离级别为串行化时，读写数据都会锁住整张表        事务操作语法\n  事务自动提交关闭\nset autocommit=0 --或者修改配置文件my.ini\r   事务开启\nbegin\rbegin work\rstart transaction --推荐\r   事务回滚\nrollback --回滚整个事务\rrollback to savepoint sp_name --回滚到某个还原点\r   事务提交\nconmmit\r   事务还原点设置：还原到某个点\ninsert into user values(1,1,1) --添加第1条记录\rsavepoint sp1 --设置还原点sp1\rinsert into user values(2,2,2) --添加第2条记录\rsavepoint sp2 --设置还原点sp2\rinsert into user values(3,3,3) --添加第2条记录\rsavepoint sp3 --设置还原点sp3\rselect * from user --表中有3条记录\rrollback to savepoint sp1 --回滚到还原点sp1\rselect * from user --表中有1条记录\r     业务设计 逻辑设计 范式设计   数据库设计三大范式：除了第一大范式，其它每个范式都是建立在前一个范式基础之上的\n  第一范式（1NF）：列的原子性，最小属性不可再分化。数据表的每一列，必须是不可拆分的最小单元\n  错误示范\n user    id name-age     1 yuanya-23        正确示范\n user    id name age     1 yuanya 23          第二范式（2NF）：满足1NF的前提下。行的唯一性，非主键列完全依赖主键列，不是部分依赖关键字\n  错误示范：一个订单有多个产品\n order    id product_id time     1 1 2020-02-02   1 2 2020-02-02        正确示范\n  order\n   id time     1 2020-02-02      order_product\n   id order_id product_id     1 1 1   2 1 2          第三大范式：满足1NF的前提下。非主键列直接依赖主键列，不是间接依赖。数据库表中不包含已在其它表中已包含的非主关键字信息\n  错误示范\n order_product（这非常不利于增改，增改都要多考虑一个列，但是其实有时候为了查询方便会设计这样的冗余字段，这是一种反范式设计）    id order_id product_id product_name     2 1 1 飞机   2 1 2 大炮            例子\n 用户表：用户名 密码 手机号 姓名 注册时间 在线状态 出生日期。符合三大范式 商品表：ID 商品名称 分类名称 出版社名称 图书价格 图书表述 作者。不符合，一个图书通常有多种分类，应该设计成多对多的形式 出版社表：出版社名称 地址 电话 联系人 银行账号。符合 出版社表：出版社名称 地址 电话 联系人 银行账号 银行支行。不符合，因为 银行账号、银行支行 之间存在关联关系 订单表：订单编号(pK) 下单用户名 下单日期 订单金额 订单商品分类 订单商品名(pk) 订单商品单价 订单商品数量 支付金额 物流单号。有多个业务主键，不符合第二范式；订单商品单价、订单数量、订单金额 存在传递依赖关系，不符合第三范式。这样计算得到的订单金额会随着商家升降价改变，这是不被允许的，所以可见完全遵循三大范式有时候并不可行    订单\n  表设计\n 订单表：订单编号、下单用户名、下单日期、支付金额、物流单号 订单商品关联表：订单编号、商品分类ID、订单商品数量 商品信息表：商品名称、出版社名称、图书价格、图书表述、作者 商品分类关联表：商品分类ID、商品名称、分类名称 商品分类信息表：分类名称、分类描述 供应商信息表：出版社名称、地址、电话、联系人、银行账号 用户表：用户名、密码、手机号、姓名、注册时间、在线状态、出生日期    sql\n  编写SQL查询出每一个用户的订单总金额\nSELECT a.单用户名, sum(d.商品价格 * b.商品数量)\rFROM 订单表 a\rJOIN 订单分类关联表 b ON a.订单编号 = b.订单编号\rJOIN 商品分类关联表 c ON c.商品分类ID = b.商品分类ID\rJOIN 商品信息表 d ON d.商品名称 = c.商品名称\rGROUP BY a.下单用户名\r   编写SQL查询出下单用户和订单详情\nSELECT a.订单编号, e.用户名, e.手机号, d.商品名称, c.商品数量, d.商品价格\rFROM 订单表 a\rJOIN 订单分类关联表 b ON a.订单编号 = b.订单编号\rJOIN 商品分类关联表 c ON c.商品分类ID = b.商品分类ID\rJOIN 商品信息表 d ON d.商品名称 = c.商品名称\rJOIN 用户信息表 e ON e.用户名 = a.下单用户\r     问题：完全符合范式化的设计有时并不能得到良好得SQL查询性能，大量的表关联非常影响查询的性能\n    优点：\n 可以尽量得减少数据冗余 范式化的更新操作比反范式化更快 范式化的表通常比反范式化的表更小    缺点：\n  对于查询需要对多个表进行关联\n  更难进行索引优化\n    反范式设计   反范式设计：所谓得反范式化就是为了性能和读取效率得考虑而适当得对数据库设计范式得要求进行违反。允许存在少量得冗余，换句话来说反范式化就是使用空间来换取时间。主要是将一些经常多表关联查询的字段在被查表中做冗余\n  表设计：\n  商品\n ID 商品名称 分类名称 出版社名称 图书价格 图书表述 作者 分类信息：商品名称、分类名称    订单\n 订单表：订单编号 下单用户名 下单日期 支付金额 物流单号 订单商品关联表：订单编号 订单商品数量 订单商品名 订单商品分类 订单单价    sql\n  编写SQL查询出每一个用户的订单总金额\nSELECT 下单用户名, sum(订单金额)\rFROM 订单表\rGROUP BY 下单用户名;\r   编写SQL查询出下单用户和订单详情\nSELECT a.订单编号, a.用户名, a.手机号, b.商品名称, b.商品单价, b.商品数量\rFROM 订单表 a\rJOIN 订单商品关联表 b ON a.订单编号 = b.订单编号\r       优点：\n 可以减少表的关联 可以更好的进行索引优化    缺点\n 存在数据冗余及数据维护异常 对数据的修改需要更多的成本    物理设计  物理设计  定义数据库、表及字段的命名规范 选择合适的存储引擎 为表中的字段选择合适的数据类型 建立数据库结构    命名规范   数据库、表、字段的命名要遵守可读性原则 使用大小写来格式化的库对象名字以获得良好的可读性 例如：使用custAddress而不是custaddress来提高可读性。\n  数据库、表、字段的命名要遵守表意性原则 对象的名字应该能够描述它所表示的对象 例如： 对于表，表的名称应该能够体现表中存储的数据内容；对于存储过程 存储过程应该能够体现存储过程的功能。\n  数据库、表、字段的命名要遵守长名原则 尽可能少使用或者不使用缩写\n  存储引擎选择    对比项 MyISAM InnoDB     主外键 不支持 支持   事务 不支持 支持   行表锁 表锁，即使操作一条记录也会锁住整个表不适合高并发的操作 行锁,操作时只锁某一行，不对其它行有影响适合高并发的操作   缓存 只缓存索引，不缓存真实数据 不仅缓存索引还要缓存真实数据，对内存要求较高，而且内存大小对性能有决定性的影响   表空间 小 大   关注点 性能 事务   默认安装 Y Y    数据类型选择   当一个列可以选择多种数据类型时\n 优先考虑数字类型 其次是日期、时间类型 最后是字符类型 对于相同级别的数据类型，应该优先选择占用空间小的数据类型    浮点类型\n   列类型 存储空间 是否精确类型     FlOAT 4个字节 否，存在精度丢失   DOUBLE 8个字节 否，存在精度丢失   DECIMAL 每4个字节存9个数字，小数点占1个字节 是，如财务数据优先考虑      时间类型\n   类型 大小（字节） 范围 格式 用途 时区     DATETIME 8 1000-01-01 00:00:00 ~ 9999-12-31 23:59:59 YYYY-MM-DD HH:MM:SS 混合日期和时间值 时区无关，修改时区无改变   TIMESTAMP 4 1970-01-01 00:00:01 UTC ~ 2038-01-19 03:14:07 UTC YYYY-MM-DD HH:MM:SS 混合日期和时间值，时间戳 时区有关，修改时区会改变   DATE 3 1000-01-01 ~ 9999-12-31 YYYY-MM-DD     TIME 3 -838:59:59 ~ 838:59:59 HH:MM:SS     YEAR 1 1901 ~ 2155 YYYY-        慢查询 慢查询定义及作用 慢查询日志，顾名思义，就是查询慢的日志，是指mysql记录所有执行超过long_query_time参数设定的时间阈值的SQL语句的日志。该日志能为SQL语句的优化带来很好的帮助。默认情况下，慢查询日志是关闭的，要使用慢查询日志功能，首先要开启慢查询日志功能。\n配置 set global slow_query_log=1 --单位秒,超过1秒的操作将被记录日志\rshow variable like '%slow_query_log%'\r   常用配置\n slow_query_log 启动停止技术慢查询日志 slow_query_log_file 指定慢查询日志得存储路径及文件（默认和数据文件放一起，mysql/data目录下） long_query_time 指定记录慢查询日志SQL执行时间得伐值（单位：秒，默认10秒） log_queries_not_using_indexes 是否记录未使用索引的SQL log_output 日志存放的地方【FILE,TABLE】，不要用table，用file即可    记录符合条件得SQL\n 查询语句 数据修改语句 已经回滚得SQL    解读\n# User@Host: root [root] @ localhost [127.0.0.1] Id: 10 --用户名 、用户的IP信息、线程ID号\r# Query_time: 0.001042 --执行花费的时间【单位：毫秒】\r#Lock_time: 0 .000000 --执行获得锁的时间\r#Rows_sent: 2 --获得的结果行数\r#Rows_examined: 2 --扫描的数据行数\rSET timestamp=1535462721; --这SQL执行的具体时间\rSELECT * FROM myarchive LIMIT 0，1000; --具体的SQL语句\r   分析 常用的慢查询日志分析工具\n mysqldumpslow：mysql/bin目录下的exe文件，登录到mysql服务端才能访问  汇总除查询条件外其他完全相同的SQL，并将分析结果按照参数中所指定的顺序输出。 语法  mysqldumpslow -s r -t 10 /slow-mysql.log  -s order (c,t,l,r,at,al,ar) c:总次数 t:总时间 l:锁的时间 r:总数据行 at,al,ar :t,l,r平均数 【例如：at = 总时间/总次数】   -t top 指定取前面几条作为结果输出     pt_query_digest：详细内容见扩展阅读  per1 .\\pt-query-digest \u0026ndash;explain h=127.0.0.1, u=root,p=password /slow-mysql.log。需要per1环境 汇总的信息【总的查询时间】、【总的锁定时间】、【总的获取数据量】、【扫描的数据量】、【查询大小】 结果解析  Response: 总的响应时间。 time: 该查询在本次分析中总的时间占比。 calls: 执行次数，即本次分析总共有多少条这种类型的查询语句。 R/Call: 平均每次执行的响应时间。 Item : 查询对象   还有执行计划    索引与执行计划 索引  索引是什么  MySQL官方对索引的定义为：索引（Index）是帮助MySQL高效获取数据的数据结构。可以得到索引的本质：索引是数据结构。   索引得分类  普通索引：即一个索引只包含单个列，一个表可以有多个单列索引 唯一索引：索引列的值必须唯一，但允许有空值 复合索引：即一个索引包含多个列 聚集索引：就是索引与数据块顺序对应，比如id主键。索引键值的逻辑顺序与索引所服务的表中相应行的物理顺序相同的索引，被称为聚集索引，反之为非聚集索引，索引一般使用二叉树排序索引键值的，聚集索引的索引值是直接指向数据表对应元组的，而非聚集索引的索引值仍会指向下一个索引数据块，并不直接指向元组，因为还有一层索引进行重定向，所以非聚集索引可以拥有不同的键值排序而拥有多个不同的索引。而聚集索引因为与表的元组物理顺序一一对应，所以只有一种排序，即一个数据表只有一个聚集索引。  聚簇索引(聚集索引)：并不是一种单独的索引类型，而是一种数据存储方式。具体细节取决于不同的实现，InnoDB的聚簇索引其实就是在同一个结构中保存了B-Tree索引(技术上来说是B+Tree)和数据行。 聚集索引一般是表中的主键索引，如果表中没有显示指定主键，则会选择表中的第一个不允许为NULL的唯一索引，如果还是没有的话，就采用Innodb存储引擎为每行数据内置的6字节RowID作为聚集索引   非聚簇索引：不是聚簇索引，就是非聚簇索引  show global variables like \u0026ldquo;%datadir%\u0026quot;;     基础语法  查看索引：SHOW INDEX FROM table_name\\G 创建修改索引：创建过多索引，会占用更多空间，并且虽然查询很快，但是增删改的效率都会收到影响，因为维护数据的同时还要维护索引。一般来说一个表索引不能超过5个  CREATE [UNIQUE ] INDEX indexName ON mytable(columnname(length)); ALTER TABLE 表名 ADD [UNIQUE ] INDEX [indexName] ON (columnname(length))   删除索引：DROP INDEX [indexName] ON mytable;   什么情况建索引：  某一列相对来说较唯一 经常用来查询显示的列 经常用来关联的列，where条件中用到的列，以及join on用到的列   问题  MySQL中myisam与innodb的区别? redo和undo干什么用的? hash索弓|是什么，什么存储弓|擎支持?有什么优缺点? btree和b + tree有什么样的区别，对于范围检索来说，b+ tree好在哪里? 全文索引是怎么回事? MySQL中InnoDB支持的四种事务隔离级别是什么?有什么区别 MYSQL中的间隙锁是怎么回事，有几种方式产生间隙锁? 能谈谈mysq|实现读写分离的原理吗，和存储弓|擎有什么关系?    执行计划   什么是执行计划\n 执行计划是什么 使用EXPLAIN关键字可以模拟优化器执行SQL查询语句，从而知道MySQL是如何处理你的SQL语句的。分析你的查询语句或是表结构的性能瓶颈    语法 Explain + SQL语句\n  执行计划的作用\n 表的读取顺序 数据读取操作的操作类型 哪些索引可以使用 哪些索引被实际使用 表之间的引用 每张表有多少行被优化器查询    执行计划详解\n  id：select查询的序列号,包含一组数字，表示查询中执行select子句或操作表的顺序\n id相同，执行顺序由上至下 id不同，如果是子查询，id的序号会递增，id值越大优先级越高，越先被执行 id相同不同，同时存在    select_type：查询的类型，主要是用于区别 普通查询、联合查询、子查询 等的复杂查询\n  SIMPLE：简单的 select 查询，查询中不包含子查询或者UNION\nexplain select * from t1\r   PRIMARY：查询中若包含任何复杂的子部分，最外层查询则被标记为PRIMARY\nexplain select t1.*, (select t2.id from t2 where t2.id = 1 ) from t1\r   SUBQUERY：在SELECT或WHERE列表中包含了的子查询被标记为SUBQUERY\nexplain select t1.*, (select t2.id from t2 where t2.id = 1 ) from t1\r   DERIUED：在FROM列表中包含的子查询被标记为DERIVED(衍生)，MySQL会递归执行这些子查询, 把结果放在临时表里。\nexplain select t1.* from t1 , (select t2.* from t2 where t2.id =1 ) s2 where t1.id = s2.id --s2是衍生\r   UNION：若第二个SELECT出现在UNION之后，则被标记为UNION； 若UNION包含在FROM子句的子查询中,外层SELECT将被标记为：DERIVED\nexplain select * from t1 UNION select * from t2\r   UNI0N RESULT：从UNION表获取结果的SELECT\nexplain select * from t1 UNION select * from t2\r     table：显示这一行的数据是关于哪张表的\n  type：type显示的是访问类型，是较为重要的一个指标，结果值从最好到最坏依次是\n  system \u0026gt; const \u0026gt; eq_ref \u0026gt; ref \u0026gt; fulltext \u0026gt; ref_or_null \u0026gt; index_merge \u0026gt; unique_subquery \u0026gt; index_subquery \u0026gt; range \u0026gt; index \u0026gt; ALL\n  重点：system\u0026gt;const\u0026gt;eq_ref\u0026gt;ref\u0026gt;range\u0026gt;index\u0026gt;ALL\n  system：表只有一行记录（等于系统表），这是const类型的特列，平时不会出现，这个也可以忽略不计\n  const：表示通过索引一次就找到了。const用于比较primary key或者unique索引。因为只匹配一行数据，所以很快。如将主键置于where列表中，MySQL就能将该查询转换为一个常量\n  eq_ref：唯一性索引扫描，对于每个索引键，表中只有一条记录与之匹配。常见于主键或唯一索引扫描\nEXPLAIN SELECT * from t1, t2 where t1.id=t2.id --通过id进行关联，id是索引且唯一，找id时出现eq_ref\r   ref：优化最多基本就到这里。非唯一性索引扫描，返回匹配某个单独值的所有行。本质上也是一种索引访问，它返回所有匹配某个单独值的行，然而，它可能会找到多个符合条件的行，所以他应该属于查找和扫描的混合体\n  range：最少优化到这里。只检索给定范围的行,使用一个索引来选择行。key 列显示使用了哪个索引 一般就是在你的where语句中出现了between、\u0026lt;、\u0026gt;、in等的查询。这种范围扫描索引扫描比全表扫描要好，因为它只需要开始于索引的某一点，而结束语另一点，不用扫描全部索引。\n  index：当查询的结果全部是索引列时，也是全表扫描，会扫面整个索引文件，但不扫描非索引数据\n  ALL：Full Table Scan，将遍历全表以找到匹配的行。即全表扫描\n    possible_keys：可能用到的索引\n  key：实际使用的索引。如果为NULL，则没有使用索引\n 查询中若使用了覆盖索引，则该索引和查询的select字段重叠  --除了id，有2个联合索引index_col1_col2、index_col1_col2_col3\rEXPLAIN select col1, col2 from t1 --possible_keys为null，key为index_col1_col2\rEXPLAIN select col1, col2 from t1 where col1=1 --possible_keys为index_col1_col2、index_col1_col2_col3，key为index_col1_col2。可见possible_keys会将包含有col1的所有索引列入\r   key_len\n  表示索引中使用的字节数，可通过该列计算查询中使用的索引的长度。在不损失精确性的情况下，长度越短越好\n  key_len显示的值为索引字段的最大可能长度，并非实际使用长度，即key_len是根据表定义计算而得，不是通过表内检索出的。如varchar(255)，即使只是用了1个字符，key_len也是255\n  key_len表示索引使用的字节数，根据这个值，就可以判断索引使用情况，特别是在组合索引的时候，判断所有的索引字段是否都被查询用到。\n  char和varchar跟字符编码也有密切的联系，latin1占用1个字节，gbk占用2个字节，utf8占用3个字节。（不同字符编码占用的存储空间不同）\n  字符串类型：CHAR和VARCHAR类型类似,但它们保存和检索的方式不同。它们的最大长度和是否尾部空格被保留等方面也不同。在存储或检索过程中不进行大小写转换。这里我们只需要考虑CHAR、VARCHAR，因为不太可能用TEXT、BLOB来建立索引，大字段建索引想崩了？\n   类型 大小 用途     CHAR 0-255字节 定长字符串   VARCHAR 0-65535字节 变长字符串   TINYBLOB 0-255字节 不超过255个字符的二进制字符串   TINYTEXT 0-255字节 短文本字符串   BLOB 0-65 535字节 二进制形式的长文本数据   TEXT 0-65 535字节 长文本数据   MEDIUMBLOB 0-16 777 215字节 二进制形式的中等长度文本数据   MEDIUMTEXT 0-16 777 215字节 中等长度文本数据   LONGBLOB 0-4 294 967 295字节 二进制形式的极大文本数据   LONGTEXT 0-4 294 967 295字节 极大文本数据    explain select * from s2 where name= 'enjoy' ; --比如 utf8 下 name char(10)，其它编码计算方法一致，把3换成对应字节数即可\r  索引字段为char类型+不可为Null时：key_len=3*10=30 索引字段为char类型+允许为Null时：key_len=3*10+1=31，1个字节记录该字段可为null 索引字段为varchar类型+不可为Null时：key_len=3*10+2=32，varchar是可变长度的，2个字节记录varchar实际使用了多少字节 索引字段为varchar类型+允许为Null时：key_len=3*10+2+1=32，varchar是可变长度的，2个字节记录varchar实际使用了多少字节，1字节记录该字段可为null    数值类型：key_len=类型字节数+可为null标记等\n   类型 大小 范围(有符号) 范围(无符号) 用途     TINYINT 1字节 (-128, 127) (0,255) 小整数值   SMALLINT 2字节 (-32 768，32 767) (0，65535) 大整数值   MEDIUMINT 3字节 (-8388608,8388607) (0, 16777 215) 大整数值   INT或INTEGER 4字节 (-2147483648，2147 483 647) (0，4294 967 295) 大整数值   BIGINT 8字节 (-9 233 372036 854775 808，9223 372036 854 775 807) (0, 18446 744 073709 551 615) 极大整数值   FLOAT 4字节 (-3.402 823 466E+38 , -1.175 494 351E-38)，0, (1.175 494351 E-38，3.402 823466 351 E+38) 0，(1.175 494 351 E-38，3.402 823 466E+38) 单精度浮点数值   DOUBLE 8字节 (-1.797 693 134 8623157 E+308 , -2.225073 858507201 4 E-308), 0, (2.225 073858 507201 4 E-308 ,1.797 693 134 862 3157 E+308) 0, (2.225 073 858507 201 4 E-308 ,1.797 693 134 8623157 E+308) 双精度浮点数值      时间类型：\n   类型 大小(字节) 范围 格式 用途     DATE 3 1000-01-01~9999-12-31 YYYY-MM-DD 日期值   TIME 3 -838:59:59~838:59:59 HH:MM:SS 时间值或持续时间   YEAR 1 1901~2155 YYYY 年份值   DATETIME 5.6版本及以后为5字节，之前为8字节 1000-01-01 00:00:00~9999-12-31 23:59:59 YYYY-MM-DDHH:MM:SS 混合日期和时间值   TIMESTAMP 4 1970-01-01 00:00:00~2037年某时 YYYYMMDDHHMMSS 混合日期和时间值,时间戳      总结：NOT NULL=字段本身的字段长度，NULL=字段本身的字段长度+1(因为需要有是否为空的标记，这个标记需要占用1个字节)\n 变长字段需要额外的2个字节（VARCHAR值保存时只保存需要的字符数，另加一个字节来记录长度(如果列声明的长度超过255，则使用两个字节)，所以VARCAHR索引长度计算时候要加2），固定长度字段不需要额外的字节。 而NULL都需要1个字节的额外空间,所以索引字段最好不要为NULL，因为NULL让统计更加复杂并且需要额外的存储空间。 复合索引有最左前缀的特性，如果复合索引能全部使用上，则是复合索引字段的索引长度之和，这也可以用来判定复合索引是否部分使用，还是全部使用。      ref：显示索引的哪一列被使用了，如果可能的话，是一个常数。哪些列或常量被用于查找索引列上的值\n const：及表示常量，如where name=\u0026quot;yuanya\u0026rdquo; 索引名：格式为 schema_name.table_name.column_name，如yuanya.user.id    rous：根据表统计信息及索引选用情况，大致估算出找到所需的记录所需要读取的行数。同一个sql，有无索引，扫描的行数明显将不一样，有索引可以扫描行数更少，扫描行数越少即执行效率越高\n  Extra：包含不适合在其他列中显示但十分重要的额外信息\n  Using filesort：说明mysql会对数据使用一个外部的索引排序，而不是按照表内的索引顺序进行读取。 MySQL中无法利用索引完成的排序操作称为“文件排序\u0026rdquo;\nEXPLAIN select cl from tl whe re c1='ac' order by c3 --索引是c1_c2和c1_c2_c3，没有用到索引排序，将Using filesort\rEXPLAIN select cl from tl whe re c1='ac' order by c2,c3 --优化，将用到c1_c2_c3索引排序\r   Using temporary：使了用临时表保存中间结果，MySQL在对查询结果排序时使用临时表。常见于排序 order by 和分组查询 group by\nEXPLAIN SELECT coll FROM tl WHERE col1 IN ('ac', 'ab', 'aa' ) GROUP BY c2 --Using temporary\rEXPLAIN SELECT coll FROM tl WHERE col1 IN ('ac', 'ab', 'aa' ) GROUP BY c1, c2 --优化，最左前缀原则\r   USING index：是否用了覆盖索引。表示相应的select操作中使用了覆盖索引(Covering Index)，避免访问了表的数据行，效率不错！如果同时出现using where，表明索引被用来执行索引键值的查找;\n 聚集索引（主键索引）：聚集索引就是按照每张表的主键构造一颗B+树，同时叶子节点中存放的即为整张表的记录数据。聚集索引的叶子节点称为数据页，聚集索引的这个特性决定了索引组织表中的数据也是索引的一部分。 辅助索引（二级索引）：非主键索引，叶子节点=键值+书签。Innodb存储引擎的书签就是相应行数据的主键索引值。 覆盖索引（索引覆盖）  就是select的数据列只用从索引中就能够取得，不必从数据表中读取，换句话说查询列要被所使用的索引覆盖 索引是高效找到行的一个方法，当能通过检索索引就可以读取想要的数据，那就不需要再到数据表中读取行了。如果一个索引包含了（或覆盖了）满足查询语句中字段与条件的数据就叫 做覆盖索引。 是非聚集组合索引的一种形式，它包括在查询里的Select、Join和Where子句用到的所有列（即建立索引的字段正好是覆盖查询语句[select子句]与查询条件[Where子句]中所涉及的字段，也即，索引包含了查询正在查找的所有数据）。 不是所有类型的索引都可以成为覆盖索引。覆盖索引必须要存储索引的列，而哈希索引、空间索引和全文索引等都不存储索引列的值，所以MySQL只能使用B-Tree索引做覆盖索引      Using where：表明使用了where过滤\n  Using join buffer：使用了连接缓存，即使用join on连接查询，连接查询会使用join buffer\n  Impossible where：where子句的值总是false，不能用来获取任何元组\n      sql优化 假设联合索引 index_c1_c2_c3\n  尽量全值匹配：即尽量使联合索引中更多的列被使用\nEXPLAIN SELECT * FROM staffs WHERE c1 = 'July'; --根据索引第一个列找到后就在该值下全扫描了\rEXPLAIN SELECT * FROM staffs WHERE c1 = 'July' AND c2 = 25; --第一个列找到后继续folow索引第二个列\rEXPLAIN SELECT * FROM staffs WHERE c1 = 'July' AND c2 = 25 AND c3 = 'dev'; --同理继续第三个列\r   最佳左前缀法则：如果索引了多列，要遵守最左前缀法则。指的是查询从索引的最左前列开始并且不跳过索引中的列。最左前缀原则其实就是防止索引失效的原则\n--没有问题，c1 c2 c3依序索引生效，and连接的条件是无序的，谁前谁后都不影响索引\rEXPLAIN SELECT * FROM user where c1=\u0026quot;xxx\u0026quot; and c2=\u0026quot;xxx\u0026quot; where c3=\u0026quot;xxx\u0026quot;\r--c1被砍掉了，等同于失去head的链表无从循迹，失去火车头的火车跑不起来，索引失效\rEXPLAIN SELECT * FROM user where c2=\u0026quot;xxx\u0026quot; where c3=\u0026quot;xxx\u0026quot;\r--c1仍然可以索引生效，但是没有c2，c3也无从生效\rEXPLAIN SELECT * FROM user where c1=\u0026quot;xxx\u0026quot; where c3=\u0026quot;xxx\u0026quot;\r   不在索引列上做任何操作（计算、函数、(自动or手动)类型转换），会导致索引失效而转向全表扫描\n--c1仍然生效，c2做了操作，将失效，c3自然也失效\rEXPLAIN SELECT * FROM user where c1=\u0026quot;xxx\u0026quot; and left(c2,3)=23 where c3=\u0026quot;xxx\u0026quot;\r   范围条件放最后：存储引擎不能使用索引中范围条件右边的列。in查询是范围查询的一种\n--c1仍然生效，查询c2为23的过程c2是作为索引生效了的，因为b+tree的结构，在c1=\u0026quot;xxx\u0026quot;上c2是有序的，索引到23之后，开始转为扫描c2\u0026gt;23所有数据，c2开始失效，c3则完全失效\rEXPLAIN SELECT * FROM user where c1=\u0026quot;xxx\u0026quot; and c2\u0026gt;23 where c3=\u0026quot;xxx\u0026quot;\r   覆盖索引尽量用：尽量使用覆盖索引（只访问索引的查询，即索引列和查询列一致），减少select *。覆盖索引可以仅遍历索引就获得数据，过程中不需要取访问非索引数据内容\nEXPLAIN SELECT c1, c2, c3 FROM user where c2=23\r   不等于要慎用：mysql 在使用不等于(!= 或者\u0026lt;\u0026gt;)的时候无法使用索引会导致全表扫描\nEXPLAIN SELECT c1, c2, c3 FROM c1, c2, c3 where c2!=23 --但是覆盖索引可以使使用!=的语句仍使用索引\r   Null/Not Null对索引可能有影响：\nEXPLAIN SELECT * FROM user where c1 is null --如果c1的定义是可以为null，那么索引生效，如果c1的定义是not null那么，索引不生效\rEXPLAIN SELECT * FROM user where c1 is not null --索引失效\rEXPLAIN SELECT c1, c2, c3 FROM use where c1 is not null --覆盖索引可以使使用null/Not Null的语句仍索引生效\r   Like查询要当心：like以通配符开头('%abc\u0026hellip;')mysql索引失效会变成全表扫描的操作\nEXPLAIN SELECT * FROM user where c1 like \u0026quot;%asd%\u0026quot;; --失效\rEXPLAIN SELECT c1, c2 FROM user where c1 like \u0026quot;%asd%\u0026quot;; --覆盖索引可使索引生效\rEXPLAIN SELECT * FROM user where c1 like \u0026quot;asd%\u0026quot;; --生效，b+tree是有序的，从前往后的\r   字符类型加引号：字符串不加单引号索引失效\nEXPLAIN SELECT * FROM user where c1=917; --失效。覆盖索引。。。算了不说了\rEXPLAIN SELECT * FROM user where c1='917'; --生效\r   OR改UNION效率高\nEXPLAIN SELECT * FROM user where c1='x' or c1=\u0026quot;xxx\u0026quot;; --失效。覆盖索引……\rEXPLAIN SELECT * FROM user where c1='x' UNION SELECT * FROM user where c1=\u0026quot;xxx\u0026quot;; --生效。大概in查询也可以这么玩？如果in中数据很多？\r   insert语句优化   提交前关闭自动提交\n  尽量使用批量insert语句\n  可以使用MyISAM存储引擎\n  例：有一个sql文件，里面几百万insert语句，用java dbc写入，读取sql文件可以\n  读一行插入一行，效率极低\n  使用batch，关闭自动提交，每几千行执行一次这样，效率稍高，但是batch实际上还是一条一条执行的\n  将数据库改为MyISAM存储引擎，将sql拼接成如下形式，可以极大节省效率，同样几千行执行一次即可，执行完后再将数据库改回innodb，注意仅导入数据时这么做，生产环境一定还是不能MyISAM\ninsert into user(c1,c2) value(1,2)(2,3)(3,4)\r   LOAD DATA INFLIE：使用LOAD DATA INFLIE ,比一般的insert语句快几十倍不止，几百万数据也就几秒完成\nselect * into OUTFILE 'D:\\\\product.txt' from t1; --导出t1到D:\\\\product.txt\rcreate table t2 select * from t1 where 1=2; --仅拷贝表结构，如果1=1或者不加where则包含数据\rload data INFILE 'D:\\\\product.txt' into table t2; --导入D:\\\\product.txt到t2\r     最后一题\n100w数据，索引是date_str，25秒左右\nSELECT a.date_str, a.shopCode, a.add_car_pv\rFROM dp_car_copy a\rWHERE 1=1\rAND (\r(\ra.date_str \u0026gt;= '2018-07-01'\rAND a.date_str \u0026lt;= '2018-08-30'\r)\r)\rORDER BY a.date_str, a.shopCode, a.add_car_pv desc\r  索引覆盖，将3个列建为联合索引，15秒左右 查看执行计划，发现type已经是range了，但是extra中using filesort，所以删除desc，使用索引本身的顺序，1秒左右 如果确实需要desc，现在可能会想到索引反着建，但是如果下次又要正着查询呢，所以还是选择再动索引，而是在后台代码中处理即可，因为返回的是一个list，所以倒着取list不就好了。这里主要要学会变通，sql当然需要满足需求，但是不要忽视后台代码或者其它因素  limit 10000 10：他不会只扫描10000-10010，而是扫描前10010条数据，所以当数据很多时，取后面的数据效率很低，这时候就要通过其他方式解决，如假如id是自增无断缺的，那么可以where id\u0026gt;9999 and id\u0026lt;10011 limit 0,10\nmysql 事务 set autocommit=0; --关闭自动提交\rcommit; --提交事务\r 任何数据库层面的优化都抵不上应用系统的优化\n  Client：基于不同的语言不同的脚本都可以实现，甚至可以写socket直接去连接Server\n  Server：即我们通常所说的MySQL。架构见图01、02\n  SQL_Layer\n 初始化模块：Server启动时，加载默认参数，进行初始化。 连接管理模块：启动完成后server将监听请求 连接进程模块：请求进来之后分发到连接进程模块（connection pool，有任务进来了，有没有线程活跃，有则去服务这个连接，没有就创建一个新的连接。连接池不止是数据库外的Client有，Server内也有），完成数据库连接 用户模块：鉴权，校验令牌，是否有权限 命令分发器：辨别请求（请求有两种，Query、commond），如果发现查询缓存模块种有，那么就直接返回结果了，不会前往到下面的模块。没有缓存则继续往下交给命令解析器解析后分发 查询缓存模块：query cache，它类似于一个hashmap，把Sql语句和参数做一个hash作为key，value就是结果值，这就是首次查询比重复查询慢的多（查询都有时间报告，可以注意到）。所以调大cache也是优化的一种，只对查询有效 日志记录模块： 命令解析器（parser）：解析完之后，分发到不同的模块处理 以下是被分发的模块  查询优化器：被分发select这类请求。这里的select包括crud 表变更模块：被分发dml这类请求 表维护模块：ddl 复制模块：rep，主从复制等 状态模块：status   访问控制模块：上面所有被分发模块最终都到访问控制模块 表管理模块： 存储引擎接口：关键。与Storage Engines打交道 还有独立的两个模块：  核心API：内存管理、小IO、数字及字符串等，类似于jvm或是一个manager的一个功能 网络交互：网络监听、交互协议处理等      Storage Engines。很多，主要关注innodb和myisam这两个存储引擎\n 文件存储形式：每一个schama（mysql种即database）都有一个文件夹，不同的引擎创建的文件格式和数量都不一样，innodb创建的一个表是两个文件，user.frm、user.ibd(inno database dater)两个文件格式，myisam是frm、MYD、MYI格式共3个存储文件     inno db MySIAM     存储文件 .frm表定义文件\n.ibd数据文件（索引和数据文件集在一起。聚集索引，基于主键展开的b+tree） SYWWA.frm表定义文件.myd数据文件.myi索引文件（也是一个索引tree，每一个索引都有一个指向，就像一个文件系统一样，指向来快速定位数据）   锁 表锁、行锁 表锁   事务 ACID 不支持   CRDU 读、写 读多   count 扫表 专门存储的地方，每个表中有多少行是被专门存储下来的，   索引结构 B+TREE B+TREE        性能\n 影响性能的因素  人为因素：需求，是否是不是一定要这么去做  count(*)：论坛分页需要用到，数据小于100w这样的数据，count(*)还ok，但是如果是1000w上e，可能count(*)就会成为一个瓶颈。这时候就要考虑数据是：实时、准实时、可以有误差。我们知道MySIAM每个表中有多少行是被专门存储，count(*)只需要直接取出来就ok，那么我们也可以学习它专门找一个地方来存储，如果数据只需要准实时、或者可以有误差，就可以这么做   程序员因素：比如太过面向对象  有一个相册、相册里有一个相片，相片有评论数。要看到相册里前三个照片的评论总数。可能会这么做：方案1，在相片中limit出来前n，然后再遍历这n个pohoto_id去评论中count出来，这很面向对象，但进行了1+n次数据库io；方案2，limit出来前n，再通过pohoto_id的集合去评论中做一次 count(*) group by pohoto_id集合，无论n是多少都是2次io，要好一些；   cache：  server内部的查询cache：最多也就放大一点，不怎么管得到。 外部有redis、menmerycache等，如果设计的不够好，所有的请求都打到数据库层面，也会有性能问题   可扩展：过度追求，如过度设计冗余字段等  先看一个合理的扩展设计：如一个user_id给另一个user_id发了一个msg，那么就是两个id和一个msg内容 就是表结构，因为我们在msg外部界面 消息列表 只需要知道谁给谁发了消息，列表可能很多，msg内容可能是一个大字段，只是看一个简略列表就要连带到这么多大字段内容，很影响性能，那么为了点进去才需要知道msg的具体内容，msg内容就可以再拆出来一个表，之前的表就变成两个userid和一个msgid，需要看消息内容再根据id去查 过度：如订单表，首先一般是有userid的，要不要冗余一个用户姓名，可能会展示订单的时候更方便，这就属于没有必要，因为查看订单详情的时候连带查询一个用户信息并不是很耗费性能的事情   表范式 应用场景：  OLTP：On-Line Transaction Processioning 链接事务处理。一般的普通系统基本都是  特点  数据量大 每次访问数据比较少 数据离散 活跃数据占比不大   优化  活跃数据占比不大：扩大内存容量将活跃数据cache住 IO频繁 IOPS（per second）：频繁，一般单次数据量就不会很大，Not 吞吐量，不需要关注吞吐量 并发大：CPU要强劲 与客户端交互频繁：网络设备扛流量的能力不能差     OLAP：On-Line Analysis Processioning 链接分解处理。一般是数据仓库，数据分析的系统  优化  数据量大 并发不高 单次检索数据量多 数据访问集中 没有明显的活跃数据   优化  磁盘单位容量要大 单次检索数据量多：关注IO吞吐量，Not IOPS 并发少：CPU要求没那么高 计算量大时间长、并行要求高：做集群，集群同步会网络通讯要求高。这就是为什么大数据都是集群\u0026hellip;         提高性能：  索引：一种数据结构，高效定位数据。select * from table where id =1，如果没有索引，就会去数据文件轮询每一条数据，对比得到，没有索引第一个读到的不一定是1，因为物理磁盘上的数据存储不是逻辑上那么连续的，所以就需要索引。如文件系统(柱面，磁道，扇区)：我们去找文件的时候，我们的机械硬盘磁头去找到文件，是我们文件系统有记录，通过柱面、磁道、扇区定位到文件在磁盘哪个地方哪个扇区，磁头就可以直接去到文件位置读取。索引table.ibd也是类似 索引。衡量索引，主要考虑IO渐进复杂度，即IO越来越多，索引是否能还是那么高效。命名好习惯：idx_作为索引命名前缀，还有如唯一约束以uni_开头等  种类  Hash：有冲突，只能做等值查询无法做范围查询如where id\u0026gt;1 Full Text：做全文搜索 前缀索引：如一个string，截取其前面几个字符做索引 R-Tree：空间索引，如3公里以内的商店。替代方案GEOHash B+Tree：红黑树高度不可控，那么查询就非常耗时  联合索引：（id,age）作为索引，且遵循最左前缀匹配的规则，先从左边id开始寻找索引，如果where age=10 and id=1，这样是找不到索引的。联合索引还有一个值得注意的点，B+Tree下，因为id相同的索引是聚集一起的一定区域，在id相同的情况下，第二列age也是是排序的，这和mysql中B+Tree插入联合索引的设计有关，那么这个时候我们通过where id=1 order by age，得到的结果是不用经过排序直接取出来的，因为本身就是按序的。这就是索引对分组排序的效率音响的点     pros  提高检索效率 降低排序成本：排序分组主要小号的是内存和cpu资源   cons  更新索引的IO量 调整索引所致的计算量 存储空间   是否创建索引  较频繁的作为查询条件的字段应该创建索引 唯一性太差的字段不适合单独创建索引 更新非常频繁的字段不适合创建索引 不会出现在where子句中的字段不该创建索引            锁  row-level：  pros  粒度小   cons  获取、释放所做的工作更多 容易发生死锁：如果事务A、B的第一个操作同时执行，导致t1和t2的id=1同时被行锁  事务A：update table1 where id=1，t1的id=1行锁了，update table2 where id=1，要等待t2的id=1解锁 事务B：update table2 where id=1，t2的id=1行锁了，update table1 where id=1，要等待t1的id=1解锁 相互等待构成死锁     实现Innodb的锁，以下锁都是系统底层将帮我们做，以下只是现实演示，实际中不会用这些sql的  共享锁：  读锁：lock table user read;上读锁，读可以无阻塞共享，只有写需要等待阻塞，unlock tables释放所有锁 写锁：lock table user write;，只有写可以无阻塞同时进行，   排他锁 间隙锁：通过在指向数据记录的第一个索引键之前和最后一个索引键之后的空域空间上标记锁定信息来实现的。是一个范围的行锁，如update user set nick name = \u0026ldquo;Xxx\u0026rdquo; where id \u0026gt;1 and id\u0026lt;4;将锁住 1-4之间的行(假如id是递增的) 锁优化  尽可能让所有的数据检索都通过索引来完成 合理设计索引 减少基于范围的数据是检索过滤条件 尽量控制事务的大小 业务允许的情况下，尽量使用较低级别的事务隔离       table-level：  pros  实现逻辑简单 获取、释放快 避免死锁：因为直接获取不到整张表。。。没讲清楚，好像是两个表都锁了的情况下，先去另一个表的请求获取不到表直接out不会wait，它之前锁的表就解锁了，另一个事务的后一个请求就可以继续操作了   cons：粒度太大，并发不够高 实现MyISAM   页锁：一个表很大是会分为多个page，所以page是table的一部分，包含大于1个row。innodb不支持page锁      共享锁(S) 排他锁(X) 意向共享锁(IS) 意向排他锁(IX)     共享锁(S) 兼容 冲突 兼容 冲突   排他锁(X) 冲突 冲突 冲突 冲突   意向共享锁(IS) 兼容 冲突 兼容 兼容   意向排他锁(IX) 冲突 冲突 兼容 兼容    优化   原则\n 永远用小结果集驱动大结果集  join表   只取出自己需要的columns  数据量 排序占用空间：max length for sort data   仅仅使用最有效的过滤条件：key length 尽可能避免复杂的join和子查询：会锁资源    QEP：query excution plan，查询执行计划，由查询优化器选择得到的查询最佳路径\n 使用：在select语句前面加个explain就可以查看该sql的执行计划  如explain SELECT * FROM user order by user.id 如果是update user where id=5，只需要换成select语句即可，explain select * from user where id=5，因为修改也是要先找到才修改嗷 字段：  ID: Query Opt imizer所选定的执行计划中查询的序列号，一个计划的唯一标识符，一个计划可能有多行组成（比如join的查询），所以id是可重复的 select_type：查询类型，类型很多具体百度对照  DEPENDENT SUBQUERY: 子查询中内层的第一个SELECT,   table：当前行执行的表是哪个表，同一个计划多行中涉及的表可能也不同（比如join的查询），一般会有一个驱动表，驱动表即第一个要操作查询的表，这是查询优化器帮我们选出来的，也是满足小结果集驱动大结果集，如计划中涉及一个table有100个数据， 另一个table有1w的数据，那么第一个table是被选作驱动表的，在计划中也是第一行，除了第一行计划的多行并不是按顺序的 type：重中之重。对表所使用的访问方式，类型很多，具体介绍百度一下或者官网看看。依次从好到差: System、const、eq_ref、ref、fulltext、ref or null、unique_subquery , index_subquery , range、index_merge、index、ALL possible_ keys：重点。可能命中的索引有哪些 key：最终用到的是哪些索引，多个则是组合索引，满足最左前缀的点 key_len：用到的索引长度，将选key_len最短的 ref：过滤形式 rows：该计划行查询数据所读取的行数，这就是为什么要小表驱动大表，小表100行，那么计划第一行最多读取100行就能完成计划第一行筛选，如果大表1w行作为驱动，那么计划第一行最多可能要1w读取1w行才能完成第一部分筛选 filtered： Extra：用到了的资源，如where关键词、索引、排序、临时表之类的，如两表关联order by，两表需要关联并拷贝到一个临时表中进行排序        profiling\n 先set profiling=1;开启， 然后执行任意查询语句select * from user order by age， 然后再执行show profiles，可以看到一个表，是我们最近一次查询语句执行查询的过程操作查询，这些操作数据行都有id， 可以通过id来执行最后的show profile cpu,block io for query 75，75即我们要查操作行的id，可以看到该操作的每一个过程状态所持续的时间，如初始化、排序之类的    join：如A.id=B.id，A表是驱动表（是底层查询优化器自己选出来的），那么则遍历A筛选出来的这些A.id，到B中又遍历去找到对应的B.id值，即一个双层for循环，然后拼凑出结果到临时表。小结果集驱动大结果集，如果A是小表只筛选出来100条，那么只需要轮询B100次，如果A是大表筛选出来1w条，那么要轮询B1w次，虽然m*n和n*m是一样的，但是对B表的读取扫描次数是不一样的。show variables like \u0026lsquo;ioin_ %';查看\n Nested Loop Join 优化  永远用小结果集驱动大的结果集，这是查询优化器筛选的，我们只能改变强制使用某些索引，来一定程度影响筛选，筛选还是不可控的 保证被驱动表上的Join条件字段已经被索引：这样每次扫描都会更快，也会放大小结果集驱动大结果集的优化效果 Join Buffer：join_buffer_size，当join表时，一些中间结果是缓存在Join Buffer中的，如三表join，A和Bjoin得到一个结果缓存到Join Buffer形成一个中间结果表，中间表再与C表join得到结果集，Join Buffer的使用是由底层决定的，并不是多表join一定会用到，调大join_buffer_size是一种预优化，给可能用到容量缓存的时候准备。因为如果Join Buffer容量不够就会缓存到磁盘分段，产生io      order by\n 实现  有序，命中索引：因为B+tree索引是有序的结构，所以根据索引order by，执行计划的Extra中将不会出现用到排序的信息，因为有序直接拿出来就完了，不进行排序 无序，未命中索引：show variables like \u0026lsquo;%sort%\u0026rdquo; ;可以找到sort_buffer_size。下面两种方式是底层选择的，当sort_buffer_size足够承载要返回的数据，那么则选用第二种，否则第一种。这也是为什么取数据尽量只取需要的Colum，而不要用select *，当然这只是一个点  一：只把要排序的字段和指针copy到sort buffer中去排序，这一列指针指向数据表所在行，做完排序再根据指针去拿到数据表中的数据copy到结果集。共两次磁盘io 二：直接copy所有要返回的字段和指针到sort buffer，之后又做一次拆分，将要排序的字段和指针copy出来，这时候指针就是指向copy出来的那些行了，之后与第一种一样。这样除了第一次copy的一次io，之后都在内存中操作了。节省io，但是耗费内存 ，空间换时间     优化  索引顺序致的话不需要再排序：可以看到索引是最主要的，可以直接索引没有排序后面的优化都不需要了 加大max length for sort data从而使用第二种排序方法（排序只针对需要排序的字段） 内存不充足时去掉不必要的返回字段 增大sort buffer size：减少在排序过程中对需要排序的数据进行分段      group by：基于order by，先排序后分组，所以适用于order by的优化也适用于group by\n  distinct：基于group by，先排序后分组再去重，所以。。。不说了\n  limit：当offset很大时，很慢。因为limit执行取出的数据不是size条，而是offset+size条\n 自增无空缺的id可以SELECT * FROM user where id\u0026gt;10000 limnit 10;，总之就是根据业务通过其它方式筛选后再limit    slow sql\n  配置\n[mysqld]\rslow_query_log=1\rslow_query_log_file=/path/to/file\rlong_query_time=0.2\rlog_output=FILE\r   show full processlist;，然后slow sql来定位sql，最后explain查看分析\n    建索引的几大原则\n 最左前缀匹配原则，非常重要的原则, mysq|会直向右匹配直到遇到范围查询(\u0026gt;、\u0026lt;、 between、 like)就停止匹配，比如a= 1 andb= 2andc\u0026gt; 3 andd = 4如果建立(a,b,c,d)顺序的索引, d是用不到索引的，如果建立(a,b,d,c)的索引则都可以用到，a,b,d的顺序可以任意调整。 =和in可以乱序，比如a= 1 andb= 2andc= 3建立(a,b,c)索引可以任意顺序, mysql的查询优化器会帮你优化成索引可以识别的形式 尽量选择区分度高的列作为索引,区分度的公式是count(distinct co)/count(*)，表示字段不重复的比例,比例越大我们扫描的记录数越少，唯一键的区分度是1 ，而一些状态、性别字段可能在大数据面前区分度就是0 ,那可能有人会问，这个比例有什么经验值吗?使用场景不同，这个值也很难确定,一般需要join的字段我们都要求是0.1以上，即平均1条扫描10条记录 索引列不能参与计算,保持列\u0026quot;干净”, 比如from_ unixtime(create_ time) =’2014-05-29\u0026rsquo; 就不能使用到索引,原因很简单，b+树中存的都是数据表中的字段值，但进行检索时, 需要把所有元素都应用函数才能比较,显然成本太大。所以语句应该写成create_ time = unix_ timestamp(\u0026rsquo; 2014-05-29\u0026rsquo; ); 尽量的扩展索引,不要新建索引。比如表中已经有a的索引,现在要加(a,b)的索引，那么只需要修改原来的索引即可    ","date":"0001-01-01","permalink":"https://yuanyatianchi.github.io/post/it.db.mysql/","tags":["it","db"],"title":"MySQL"},{"content":"RabbitMQ https://www.rabbitmq.com/\nclient go go get -u github.com/streadway/amqp\r package rabbitmq\rconst URL = \u0026quot;amqp://guest:guest@127.0.0.1:5672/\u0026quot; //格式：amqp://账号:密码@地址:端口/虚拟主机\rtype RabbitMQ struct {\rurl string //连接信息\rconn *amqp.Connection //连接\rchannel *amqp.Channel //通道\rqueueName string //queue名\rexchangeName string //exchange名\rroutingKey string //routing key\r}\r/*RabbitMQ实例化*/\rfunc NewRabbitMQ(queueName string, exchangeName string, routingKey string) *RabbitMQ {\rrabbitMQ := \u0026amp;RabbitMQ{url: URL, queueName: queueName, exchangeName: exchangeName, routingKey: routingKey}\rvar err error\rrabbitMQ.conn, err = amqp.Dial(rabbitMQ.url) //获取connection\rrabbitMQ.failOnError(err, \u0026quot;创建连接错误！\u0026quot;)\rrabbitMQ.channel, err = rabbitMQ.conn.Channel() //获取channel\rrabbitMQ.failOnError(err, \u0026quot;获取channel失败\u0026quot;)\rreturn rabbitMQ\r}\r/*断开通道和连接*/\rfunc (r *RabbitMQ) Destroy() {\rr.channel.Close()\rr.conn.Close()\r}\r/*错误处理\r包括可以将错误写入日志、数据库、发送邮件等。\r不用于代码错误处理*/\rfunc (r *RabbitMQ) failOnError(err error, message string) {\rif err != nil {\rlog.Fatalf(\u0026quot;%s:%s\u0026quot;, message, err)\rpanic(fmt.Sprintf(\u0026quot;%s:%s\u0026quot;, message, err))\r}\r}\r/*消息处理*/\rfunc handleMessages(messages \u0026lt;-chan amqp.Delivery) {\rforever := make(chan bool)\rgo func() {\rfor msg := range messages {\r//消息处理\rlog.Printf(\u0026quot;[*] Receieved a message:%s for messages, To exit press CTRL+C\u0026quot;, msg.Body)\r}\r}()\rlog.Printf(\u0026quot;[*] Waiting for messages, To exit press CTRL+C\u0026quot;)\r\u0026lt;-forever\r}\r 工作模式 simple 简单模式：生产者→消息队列→消费者。一个生产者，一个消费者。消息被消费一次就删除\npackage rabbitmq\r/*---------------------------Simple---------------------------*/\r/*Simple模式RabbitMQ实例化*/\rfunc NewRabbitMQSimple(queueName string) *RabbitMQ {\rreturn NewRabbitMQ(queueName, \u0026quot;\u0026quot;, \u0026quot;\u0026quot;) //\u0026quot;\u0026quot;表示不指定，将默认使用RabbitMQ初始自带的name=(AMQP default)的Exchange\r}\r/*Simple模式生产*/\rfunc (r *RabbitMQ) PublishSimple(message string) {\r//1.申请队列。如果队列存在则获取，如果队列不存在则自动创建并获取。保证队列存在，消息可以发送到队列\r_, err := r.channel.QueueDeclare(\rr.queueName, //name：队列名\rfalse, //durable：消息是否持久化\rfalse, //autoDelete：是否自动删除。当最后一个消费者断开连接后，是否自动删除队列中的消息\rfalse, //exclusive：是否具有排他性。true时，该队列无法被其它用户访问\rfalse, //noWait：是否不等待。发送消息时是否不等待RabbitMQ服务器响应，false则需要等待\rnil, //args：额外属性\r)\rif err != nil {\rfmt.Println(err)\r}\r//2.消息发布到队列\rr.channel.Publish(\rr.exchangeName, //exchange：exchange名\rr.queueName, //queue：queue名\rfalse, //mandatory：是否托管。true时，会根据Exchange的类型和routeKey规则，寻找符合条件的队列，找不到，会将发布的消息返还给消息生产者\rfalse, //immediate：是否直属。true时，当exchange发送消息到队列后发现队列上没有绑定消费者，则将把发送的消息返还给消息生产者\ramqp.Publishing{ //消息发布设置\rContentType: \u0026quot;text/plain\u0026quot;, //消息内容类型\rBody: []byte(message), //消息体\r},\r)\r}\r/*Simple模式消费*/\rfunc (r *RabbitMQ) ConsumeSimple() {\r//1.申请队列。如果队列存在则获取，如果队列不存在则自动创建并获取。保证队列存在，消息可以发送到队列\r_, err := r.channel.QueueDeclare(r.queueName, false, false, false, false, nil)\rif err != nil {\rfmt.Println(err)\r}\r//2.消息接收\rmessages, err := r.channel.Consume(\rr.queueName, //queue：队列名。如果不指定将随机生成队列名\r\u0026quot;\u0026quot;, //consumer：消费者。用来区分多个消费者。不指定\rtrue, //autoAck：是否自动命令正确应答，默认true。true时，消费消费消息后自动告诉RabbitMQ服务器消息正确消费完毕，mq服务器就可以去做删除消息等操作；false时，需要手动实现回调函数\rfalse, //exclusive\rfalse, //noLocal：是否非本地。true时，不能将同一个连接中发布的消息发送给该连接的消费者\rfalse, //noWait\rnil, //args\r)\rif err != nil {\rfmt.Println(err)\r}\r//3.消息处理\rhandleMessages(messages)\r}\r 测试：先启动消费者，再启动生产者\n/*生产者*/\rfunc main() {\rrabbitMQ := RabbitMQ.NewRabbitMQSimple(\u0026quot;yuanyaQueueSimple\u0026quot;)\rrabbitMQ.PublishSimple(\u0026quot;Hello yuanya！\u0026quot;)\rfmt.Println(\u0026quot;发送成功！\u0026quot;)\r}\r /*消费者*/\rfunc main() {\rrabbitMQ := RabbitMQ.NewRabbitMQSimple(\u0026quot;yuanyaQueueSimple\u0026quot;)\rrabbitMQ.ConsumeSimple()\r}\r work 工作模式：生产者→消息队列→消费者1、消费者2\u0026hellip;消费者N。一个生产者，多个消费者。一个消息只能被一个消费者消费一次。默认是轮询模式的负载均衡\n使用场景：生产消息的速度大于消费消息的速度，来增加系统的性能和处理能力，起到负载均衡\n代码实现与Simple模式一致，只是创建多创建几个消费者即可\npublish/subscribe 发布订阅模式：生产者→exchange→queue1、queue2\u0026hellip;queueN→consumer1、consumer2\u0026hellip;consumerN。\n 生产者：生产者发送消息到exchange exchange：\u0026ldquo;fanout\u0026rdquo;，发散类型，即广播。exchange广播消息给所有该queue queue：queue发送给对应消费者。默认一个queue对应一个consumer  package rabbitmq\r/*---------------------------Publish/Subscribe---------------------------*/\r/*Publish/Subscribe模式RabbitMQ实例化*/\rfunc NewRabbitMQPubSub(exchangeName string) *RabbitMQ {\rreturn NewRabbitMQ(\u0026quot;\u0026quot;, exchangeName, \u0026quot;\u0026quot;)\r}\r/*Publish/Subscribe模式生产*/\rfunc (r *RabbitMQ) PublishPubSub(message string) {\r//1.Exchange声明。将尝试创建exchange\rerr := r.channel.ExchangeDeclare(\rr.exchangeName, //name：交换名\r\u0026quot;fanout\u0026quot;, //kind：交换类型\rtrue, //durable\rfalse, //autoDelete\rfalse, //internal：内部的。\rfalse, //noWait\rnil, //args\r)\rif err != nil {\rfmt.Println(err)\r}\r//2.消息发布到队列\rr.channel.Publish(r.exchangeName, \u0026quot;\u0026quot;, false, false,\ramqp.Publishing{\rContentType: \u0026quot;text/plain\u0026quot;,\rBody: []byte(message),\r},\r)\r}\r/*Publish/Subscribe模式消费*/\rfunc (r *RabbitMQ) ConsumePubSub() {\r//1.尝试创建exchange\rerr := r.channel.ExchangeDeclare(r.exchangeName, \u0026quot;fanout\u0026quot;, true, false, false, false, nil)\rif err != nil {\rfmt.Println(err)\r}\r//2.申请队列\rqueue, err := r.channel.QueueDeclare(\u0026quot;\u0026quot;, false, false, true, false, nil)\rif err != nil {\rfmt.Println(err)\r}\r//3.绑定Queue、Exchange\rr.channel.QueueBind(queue.Name, \u0026quot;\u0026quot;, r.exchangeName, false, nil)\r//4.消息接收\rmessages, err := r.channel.Consume(queue.Name, \u0026quot;\u0026quot;, true, false, false, false, nil)\rif err != nil {\rfmt.Println(err)\r}\r//5.消息处理\rhandleMessages(messages)\r}\r 测试：先启动消费者，再启动生产者\n/*生产者*/\rfunc main() {\rrabbitMQ := RabbitMQ.NewRabbitMQSimple(\u0026quot;yuanyaQueuePubSub\u0026quot;)\rrabbitMQ.PublishSimple(\u0026quot;Hello yuanya！\u0026quot;)\rfmt.Println(\u0026quot;发送成功！\u0026quot;)\r}\r /*消费者1。可以接收消息*/\rfunc main() {\rrabbitMQ := RabbitMQ.NewRabbitMQSimple(\u0026quot;yuanyaQueuePubSub\u0026quot;)\rrabbitMQ.ConsumeSimple()\r}\r /*消费者2。可以接收消息*/\rfunc main() {\rrabbitMQ := RabbitMQ.NewRabbitMQSimple(\u0026quot;yuanyaQueuePubSub\u0026quot;)\rrabbitMQ.ConsumeSimple()\r}\r routing 路由模式：一个消息可以被多个消费者获取，并且消息的目标队列可以被生成者指定\n producer：producer发送消息到exchange exchange：\u0026ldquo;direct\u0026rdquo;，指导类型。发送消息到指定队列 queue：queue发送给对应消费者。默认一个queue对应一个consumer  package rabbitmq\r/*---------------------------Routing---------------------------*/\r/*Routing模式RabbitMQ实例化*/\rfunc NewRabbitMQRouting(exchangeName, routingKey string) *RabbitMQ {\rreturn NewRabbitMQ(\u0026quot;\u0026quot;, exchangeName, routingKey)\r}\r/*Routing模式生产*/\rfunc (r *RabbitMQ) PublishRouting(message string) {\r//1.Exchange声明\rerr := r.channel.ExchangeDeclare(r.exchangeName, \u0026quot;direct\u0026quot;, true, false, false, false, nil)\rif err != nil {\rfmt.Println(err)\r}\r//2.消息发布到队列\rr.channel.Publish(r.exchangeName, r.routingKey, false, false,\ramqp.Publishing{\rContentType: \u0026quot;text/plain\u0026quot;,\rBody: []byte(message),\r},\r)\r}\r/*Routing模式消费*/\rfunc (r *RabbitMQ) ConsumeRouting() {\r//1.Exchange声明\rerr := r.channel.ExchangeDeclare(r.exchangeName, \u0026quot;direct\u0026quot;, true, false, false, false, nil)\rif err != nil {\rfmt.Println(err)\r}\r//2.申请队列\rqueue, err := r.channel.QueueDeclare(\u0026quot;\u0026quot;, false, false, true, false, nil)\rif err != nil {\rfmt.Println(err)\r}\r//3.绑定Queue、Exchange\rr.channel.QueueBind(queue.Name, r.routingKey, r.exchangeName, false, nil, )\r//4.消息接收\rmessages, err := r.channel.Consume(queue.Name, \u0026quot;\u0026quot;, true, false, false, false, nil)\rif err != nil {\rfmt.Println(err)\r}\r//5.消息处理\rhandleMessages(messages)\r}\r 测试：先启动消费者，再启动生产者\n/*生产者*/\rfunc main() {\rrabbitMQ1 := RabbitMQ.NewRabbitMQRouting(\u0026quot;yuanyaExchangeRouting\u0026quot;, \u0026quot;yuanyaRoutingKey1\u0026quot;)\rrabbitMQ2 := RabbitMQ.NewRabbitMQRouting(\u0026quot;yuanyaExchangeRouting\u0026quot;, \u0026quot;yuanyaRoutingKey2\u0026quot;)\rrabbitMQ1.PublishRouting(\u0026quot;Hello yuanya！1\u0026quot;)\rrabbitMQ2.PublishRouting(\u0026quot;Hello yuanya！2\u0026quot;)\rfmt.Println(\u0026quot;发送成功！\u0026quot;)\r}\r /*消费者1。可以接收yuanyaRoutingKey1对应的queue的消息*/\rfunc main() {\rrabbitMQ := RabbitMQ.NewRabbitMQRouting(\u0026quot;yuanyaExchangeRouting\u0026quot;, \u0026quot;yuanyaRoutingKey1\u0026quot;)\rrabbitMQ.ConsumeRouting()\r}\r /*消费者2。yuanyaRoutingKey1和2对应的queue的消息都可以接收*/\rfunc main() {\rrabbitMQ1 := RabbitMQ.NewRabbitMQRouting(\u0026quot;yuanyaExchangeRouting\u0026quot;, \u0026quot;yuanyaKey2\u0026quot;)\rrabbitMQ2 := RabbitMQ.NewRabbitMQRouting(\u0026quot;yuanyaExchangeRouting\u0026quot;, \u0026quot;yuanyaKey1\u0026quot;)\rgo func() {rabbitMQ1.ConsumeRouting()}()\rrabbitMQ2.ConsumeRouting()\r}\r topic 主题模式：一个消息被多个消费者获取，消息的目标Queue可用RoutingKey以通配符的方式指定，如*.orange.*、lazy.#，#表示一个或多个词，*表示一个词，Exchange的Type必须是\u0026quot;topic\u0026rdquo;。比如消费者指定routing key为#，则接收所有消息，生产者随便定设置routing key如a.b.c都能被消费者接收。实现时除了把生产者、消费者获取Exchange时type指定为topic，其它代码一摸一样\npackage rabbitmq\r/*---------------------------Topic---------------------------*/\r/*Topic模式RabbitMQ实例化*/\rfunc NewRabbitMQTopic(exchangeName, routingKey string) *RabbitMQ {\rreturn NewRabbitMQ(\u0026quot;\u0026quot;, exchangeName, routingKey)\r}\r/*Topic模式生产*/\rfunc (r *RabbitMQ) PublishTopic(message string) {\r//1.Exchange声明\rerr := r.channel.ExchangeDeclare(r.exchangeName, \u0026quot;topic\u0026quot;, true, false, false, false, nil)\rif err != nil {\rfmt.Println(err)\r}\r//2.消息发布到队列\rr.channel.Publish(r.exchangeName, r.routingKey, false, false,\ramqp.Publishing{\rContentType: \u0026quot;text/plain\u0026quot;,\rBody: []byte(message),\r},\r)\r}\r/*Topic模式消费*/\rfunc (r *RabbitMQ) ConsumeTopic() {\r//1.Exchange声明\rerr := r.channel.ExchangeDeclare(r.exchangeName, \u0026quot;topic\u0026quot;, true, false, false, false, nil)\rif err != nil {\rfmt.Println(err)\r}\r//2.申请队列\rqueue, err := r.channel.QueueDeclare(\u0026quot;\u0026quot;, false, false, true, false, nil)\rif err != nil {\rfmt.Println(err)\r}\r//3.绑定Queue、Exchange\rr.channel.QueueBind(queue.Name, r.routingKey, r.exchangeName, false, nil, )\r//4.消息接收\rmessages, err := r.channel.Consume(queue.Name, \u0026quot;\u0026quot;, true, false, false, false, nil)\rif err != nil {\rfmt.Println(err)\r}\r//5.消息处理\rhandleMessages(messages)\r}\r 测试：先启动消费者，再启动生产者\n/*生产者*/\rfunc main() {\rrabbitMQ := RabbitMQ.NewRabbitMQSimple(\u0026quot;yuanyaQueueSimple\u0026quot;)\rrabbitMQ.PublishSimple(\u0026quot;Hello yuanya！\u0026quot;)\rfmt.Println(\u0026quot;发送成功！\u0026quot;)\r}\r /*消费者*/\rfunc main() {\rrabbitMQ := RabbitMQ.NewRabbitMQSimple(\u0026quot;yuanyaQueueSimple\u0026quot;)\rrabbitMQ.ConsumeSimple()\r}\r hello 安装 linux 安装Erlang，因为RabbitMQ依赖Erlang\nwget https://www.rabbitmq.com/releases/erlang/erlang-19.0.4-1.el7.centos.x86_64.rpm #下载\rrpm -ivh erlang-19.0.4-1.el7.centos.x86_64.rpm #安装\r 安装RabbitMQ\nwget https://www.rabbitmq.com/releases/rabbitmq-server/v3.6.15/rabbitmq-server-3.6.15-1.el7.noarch.rpm #下载\rrpm -ivh rabbitmq-server-3.6.15-1.el7.noarch.rpm #安装\rsystemctl start rabbitmq-server #启动\rrabbitmqctl stop #停止\r docker 带有management的是带有web控制台的\ndocker pull rabbitmq:3.7.24-management docker run -d -p 5672:5672 -p 15672:15672 rabbitmq:3.7.24-management\r API 插件 插件列表：显示的插件列表中带有E*或e*的表示已启用的插件，大写表示主插件，如[E*] rabbitmq_management\nrabbitmq-plugins list #rabbitmq插件列表\r 启用插件\nrabbitmq-plugins enable rabbitmq_management\r 停用插件\nrabbitmq-plugins disable rabbitmq_management\r 控制台 http://127.0.0.1:15672，缺省账号密码guest\n Overview：概览 Connections：连接。用于连接rabbitmq的客户端和服务器。rabbitmq-client通过格式为\u0026quot;amqp://账号:密码@地址:端口/虚拟主机\u0026rdquo; 的url来获取服务器的Connection Channels：通道。用于客户端与服务器通信。客户端通过一个有效Connection获取Channel Exchanges：交换。等于根据规则转发信息的交换机。Exchange根据其Type对应规则转发给正确的Queue。未指定Exchange时，则将默认使用RabbitMQ初始自带的Name=\u0026rdquo;(AMQP default)\u0026ldquo;的Exchange转发给指定的Queue  Name：Exchange名字。  \u0026ldquo;(AMQP default)\u0026quot;：RabbitMQ初始自带的其中一个Exchange，Type为\u0026quot;direct\u0026rdquo;。该Exchange不能绑定Queue，但是可以通过QueueName转发消息给任何Queue   Type：Exchange类型  \u0026ldquo;direct\u0026rdquo;：指导。转发给该Exchange下的指定RoutingKey对应的所有Queue。\u0026ldquo;direct\u0026quot;类型下，必须指定RoutingKey \u0026ldquo;fanout\u0026rdquo;：发散。转发给该Exchange下的所有Queue，即广播。无需指定Queue \u0026ldquo;topic\u0026rdquo;：主题。 \u0026ldquo;headers\u0026rdquo;   Bindings：绑定。绑定Queue与Exchange  To：QueueName，队列名。即当前Exchange绑定的Queue Routing key：路由键。等于对当前Exchange绑定的Queue进行分组的分组标识。一个Queue只能对应一个RoutingKey。RoutingKey只作用于Exchange内，不影响其它Exchange的RoutingKey Arguments：参数     Queues：队列。如果不指定Queue。未指定Queue时，将根据Exchange的配置来决定是否报错、是否发散转发、是否指导转发等  Name：Queue名字 Bindings：同Exchange的Bindings   Admin：  Users：用户 Virtual Hosts：虚拟主机。可以有效隔离开发、测试等各种环境， Feature Flags Policies Limits Cluster    注意：传入空字符串\u0026quot;\u0026ldquo;表示不指定\n核心   Connection：连接\n  Channel：通道。用来通信。一个连接可对应多个通道\n  Exchange：交换。等于根据规则转发信息的交换机。producer发布消息，消息首先发送到Exchange，   Queue：队列\n  流程\n producer发布消息，可以指定queueName、exchangeName、routingKey  消息首先根据exchangeName发送到指定的Exchange。如果未指定Exchange（exchangeName=\u0026rdquo;\u0026quot;），则将使用默认的Name为(AMQP default)、Type为direct的Exchange Exchange根据其Type、绑定的routingKey，将消息转发给正确的Queue。      ","date":"0001-01-01","permalink":"https://yuanyatianchi.github.io/post/it.mq.rabbitmq/","tags":["it","mq"],"title":"RabbitMQ"},{"content":"redis https://redis.io/，http://www.redis.cn/\n Redis的特性  速度快  数据存在内存（主要原因） c语言编写（50000line，单机的核心代码只有23000line） 单线程：   持久化：断电不丢数据。Redis所有数据保持在内存中,对数据的更新将异步地保存到磁盘上  RDB AOF   多种数据结构：key:value，value支持以下结构  基本  Strings/Blobs/Bitmaps Hash Tables (objects!) Linked Lists Sets Sorted Sets   衍生  BitMaps：位图。本质是字符串实现 HyperLogLog：超小内存唯一值计数。本质是字符串实现 GEO：地理信息定位。本质是集合实现     支持多种编辑语言 功能丰富  发布订阅 Lua脚本 事务 pipeline   \u0026ldquo;简单\u0026rdquo;  不依赖外部库(like libevent) 单线程模型   主从复制 高可用、分布式  高可用Redis Sentinel(v2.8)支持高可用 分布式Re dis Cluster(v3.0)支持分布式     典型使用场景  缓存系统  user - AppServer - cache - Stoage：如果缓存中有直接返回，没有则到Storage中取（并存入cache）。redis则充当cache的角色   计数器  如点赞转发评论数，可以在单线程下非常高效的进行计数   消息队列系统  redis提供了发布订阅、阻塞队列来实现类似的模型，可用于实现一些对消息队列功能需求不是很强的系统时，可以直接使用redis，节省技术成本   排行榜  有序集合   社交网络  粉丝数、关注数、共同关注、时间轴列表，等都可以用redis实现   实时系统  如使用位图实现类似布隆过滤器这样的功能，实现对一些垃圾邮件过滤、实时系统处理等非常有帮助      client https://redis.io/clients ，redis客户端每种语言都有很多，自行选择\ngo redigo $ go get github.com/gomodule/redigo\r .go c, err := redis.Dial(\u0026quot;tcp\u0026quot;, \u0026quot;127.0.0.1:6379\u0026quot;)\rif err != nil {\rfmt.Println(err)\rreturn\r}\rdefer c.Close()\r/*set*/\rV, err := c.Do(\u0026quot;SET\u0026quot;, \u0026quot;hello\u0026quot;, \u0026quot;world\u0026quot;)\rif err!=nil{\rfmt.Println(err)\rreturn\r}\rfmt.Println(v)\r/*get*/\rv, err = redis.String(c.Do(\u0026quot;GET\u0026quot;, \u0026quot;hello\u0026quot;))\rif err!=nil{\rfmt.Println(err)\rreturn\r}\r/*其它命令大都类似*/\r java jedis .pom \u0026lt;!--redis-client：jedis--\u0026gt;\r\u0026lt;dependency\u0026gt;\r\u0026lt;groupId\u0026gt;redis.clients\u0026lt;/groupId\u0026gt;\r\u0026lt;artifactId\u0026gt;jedis\u0026lt;/artifactId\u0026gt;\r\u0026lt;version\u0026gt;3.2.0\u0026lt;/version\u0026gt;\r\u0026lt;/dependency\u0026gt;\r\u0026lt;!--还可以导入jackson-databind进行redis string序列化相关操作--\u0026gt;\r Jedis直连  jedis直连：TCP连接。适用于少量长期连接的场景。存在每次新建/关闭TCP开销，资源无法控制,存在连接泄露 的可能，Jedis对象线程不安全  生成一个Jedis对象，这 个对象负责和指定Redis节点进行通信：Jedis jedis = new Jedis(\u0026rdquo; 127.0.0.1\u0026rdquo;, 6379);  构造函数很多，介绍一个：Jedis(String host, int port, int connectionTimeout, int soTimeout)  host：Redis节点的所在机器的IP port：Redis节点的端口 connectionTimeout：客户端连接超时 soTimeout：客户端读写超时     jedis执行set操作：jedis.set(\u0026ldquo;hello\u0026rdquo;, \u0026ldquo;world\u0026rdquo;); jedis执行get操作，value= \u0026ldquo;world\u0026rdquo;：String value = jedis.get(\u0026ldquo;hello\u0026rsquo; ); 关闭连接：jedis.close(); 所有 jedis 操作函数名都和 redis api 操作命令一致    jedis连接池   jedis连接池：Jedis预先生成,降低开销使用，连接池的形式保护和控制资源的使用。相对于直连，使用相对麻烦, 尤其在资源的管理上需要很多参数来保证, 一旦规划不合理也会出现问题。和一般的连接池一样，需要合理规划防止连接池爆满阻塞或者大量连接闲置等\n//初始化Jedis连接池，通常来讲JedisPool是单例的。\rGenericObjectPoolConfig config = new GenericObjectPoolConfig();\rconfig.setMaxTotal(10);\rJedisPool jedisPool = new JedisPool(config, \u0026quot;127.0.0.1\u0026quot;, 6379)\rJedis jedis = null;\rtry {\r// 1.从连接池获取jedis对象\rjedis = jedisPool.getResource();\r// 2.执行操作\rjedis.set(\u0026quot;hello\u0026quot; ，\u0026quot;world\u0026quot; );\r} catch (Exception e) {\re.printStackTrace();\r} finally {\rif (jedis != nulI) {\r//如果使用JedisPool,close操作不是关闭连接，代表归还到连接池\rjedis. close();\r}\r}\r   配置优化  commons-pool配置(1)-资源数控制  maxTotal：资源池最大连接数，缺省值8。  建议：比较难确定的，举个例子 :1.命令平均执行时间0.1ms = 0.001S；2.业务需要50000 QPS；3.maxTotal理论值= 0.001 * 50000 = 50个。实际值要偏大一些。或者通过jmx来统计后设置 业务希望Redis并发量 客户端执行命令时间 Redis资源:例如nodes(例如应用个数) * maxTotal是不能超过redis的最大连接数。(config get maxclients)   maxIdle：资源池允许最大空闲连接数，缺省值8。后面讨论  建议maxIdle = maxTotal，减少创建新连接的开销。   minIdle：资源池确保最少空闲连接数，缺省值0。后面讨论  建议预热minIdle，减少第一次启动后的新连接开销。在连接池初始化的时候提前去getResource一下   jmxEnabled：是否开启jmx监控,可用于监控，缺省值true。建议开启   借还参数  blockWhenExhausted：当资源池用尽后,调用者是否要等待。只有当为true时，下面的maxWaitMillis才会生效。缺省值true。建议使用默认值 maxWaitMillis：当资源池连接用尽后,调用者的最大等待时间(单位为毫秒)。缺省值-1，表示永不超时。不建议使用默认值 testOnBorrow：向资源池借用连接时是否做连接有效性检测(ping)，无效连接会被移除。缺省值false。建议false testOnReturn：向资源池归还连接时是否做连接有效性检测(ping)，无效连接会被移除。缺省值false。建议false    常见问题  redis.clients.jedis. exceptions. JedisConnectionException: Could not get a resource from the pool  获取空闲连接超时：Caused by: java.util.NoSuchElementException: Timeout waiting for idle object。连接池没有空闲资源了 池枯竭，资源耗尽：Caused by: java.util.NoSuchElementException: Pool exhausted。如maxIdle和minIdle不等时，   常见解决思路：  慢查询阻塞：出现一个连接查询慢，其它连接阻塞等待。需要根据业务场景合理设置超时时间 资源池参数不合理：例如QPS高、池子小。调大池子 连接泄露(没有close())：此类问题比较难定位，可通过client list、netstat等，最重要的是代码 DNS异常等    python redis-py hello redis安装 wget http://download.redis.io/releases/redis-3.0.7.tar.gz #下载\rtar -xzf redis-3.0.7.tar.gz #解压\rln -s redis-3.0.7 redis #软连接，便于升级\rcd redis #进入软连接目录\rmake #编译\rmake install # #安装\rcd src/\rll | grep redis- #查看redis-开头的文件\r   可执行文件：src目录\n redis-server：Redis服务器 redis-cli：Redis命令行客户端 redis-benchmark：Redis性能测试工具 redis-check-aof：AOF文件修复工具 redis-check-dump：RDB文件检查工具 redis-sentinel：Sentinel服务器(2.8以后)    启动方法:\n  最简启动：默认。直接执行redis-server\n  动态参数启动：指定一些动态参数。redis-server \u0026ndash;port 6380\n  配置文件启动：推荐。redis-server 。生成环境选择配置启动，单机多实例配置文件可以用端口区分开\ncd redis #进入redis目录\rmkdir config #创建config目录，可能启动很多redis，为了统一管理\rcp redis.conf config/redis-6379.conf #拷贝默认配置文件到config，以对应端口号命名\r#删掉所有注释和空格并重定向到redis-6382.conf\rcat config/redis-6381.conf| grep -v \u0026quot;#\u0026quot; | grep -v \u0026quot;^$\u0026quot; \u0026gt; config/redis-6382.conf\rvim config/redis-6382.conf #编辑\r #暂时只需要这些参数，其它都删掉\rdaemonize yes #是否以守护进程的方式启动，默认no，一般设置yes\rport 6382\rdir \u0026quot;/app/it/database/redis/data\u0026quot;\rLogfile \u0026quot;redis-6382.log\u0026quot;\r src/redis-server config/redis-6382.conf #启动\rps -ef | grep redis-server | grep 6382 #检查\rcat data/redis-6382.log #查看日志\r     启动验证\n 查看进程的方式：ps -ef | grep redis-server、ps -ef | grep redis-server | grep -v grep 查看端口是否时lesten的状态：netstat -antpl| grep redis 直接ping：redis-cli -h ip -p port ping    Redis客户端连接\nredis-cli -h 10.10.79.150 -p 6380 #连接，缺省参数为127.0.0.1和6379\rping #连接成功的话可以收到响应：PONG\rset hello world #响应：OK\rget hello #响应：\u0026quot;world\u0026quot;\r   响应\n 状态回复，如：ping→PONG 错误恢复，如：hget hello field → (error) WRONGTYPE Operation against 整数回复，如：incr hello → (integer) 1 字符串回复，如：get hello → \u0026ldquo;world\u0026rdquo; 多行字符串回复，如：mget hello foo → 1) \u0026ldquo;world\u0026rdquo; 2) \u0026ldquo;bar\u0026rdquo;    常用配置\n daemonize：是否是守护进程(no|yes) port：Redis对外端口号。缺省值6379，对应手机9宫格键盘的MERZ，取自意大利歌女Alessia Merz的名字 logfile：Redis系统日志，仅指定日志名 dir：Redis工作目录，日志、持久化文件的存储目录 RDB config、AOF config、slow Log config、maxMemory等等，后续章节介绍    docker安装 $ docker pull redis:5.0.9\r$ docker run --name redis01 -d -p 6379:6379 redis:5.0.9 redis-server --appendonly yes #run并启用持久化\r$ docker exec -it redis01 /bin/bash\r$ redis-cli #使用客户端\r$ set hello \u0026quot;world\u0026quot;\r$ get hello\r$ del hello\r api  Redis API使用和理解  通用命令  通用命令 数据结构和内部编码 单线程架构   字符串类型 哈希类型 列表类型 集合类型 有序集合类型    通用命令 info info memory：查看redis内存使用信息\nkeys O(n)\n 用于展示key。  keys命令一般不要在生产环境使用  是一个O(n)的命令 redis是单线程，会阻塞其它命令   怎么用  热备从节点：在热备从节点上使用 scan      set hello world\rset php good\rset java best\rkeys * #遍历所有key\rdbsize #计算key的总数\r mset hello world hehe haha php good phe his #批量插入数据\rkeys he* #展示以he开头的key，hehe、hello\rkeys he[h-l]* #第三个字母是h到l的范围\rkeys ph? #一个?代表一位，即长度为3位的ph开头的key\r dbsize O(1)\n 可以在生产环境使用。redis内置有计数器，实时记录key的总数，O(1)  dbsize #计算key的总数\r exists key O(1)，一般来说可以随意使用\nset a b\rexists a #存在响应：(integer) 1\rdel a\rexists a #不存在响应：(integer) 0\r del key [key ..] O(1)\nset a b\rget a #响应：\u0026quot;b\u0026quot;\rdel a #删除。删除成功响应：(integer) 1\rget a #响应：(nil)\rdel key [a k1 k2] #批量删除。删除失败响应：(integer) 0\r expire、ttl、persist O(1)\nexpire key 3 #key在3秒后过期\rttl key #查看key剩余的过期时间。-2表示key已经过期（已经不存在），-1表示key存在并且没有设置过期时间\rpersist key #去掉key的过期时间\r type key O(1)\ntype key #返回key的类型。string、hash、list、set、zset、none（不存在的key）\r 数据结构和内部编码  key：key的数据结构有 string、hash、list、set、zset  string：string内部编码有 raw、int、embstr  raw int embstr   hash：string内部编码有 hashtable、ziplist  hashtable：hash表 ziplist：压缩列表   list：string内部编码有 linkedlist、ziplist  linkedlist ziplist   set：string内部编码有 hashtable、intset  hashtable intset   zset：string内部编码有 skiplist、ziplist  skiplist ziplist     redisObject  数据类型(type)：用户只需要知道数据结构即可，而不需要知道编码方式，类似于面向接口编程 编码方式(encoding) 数据指针(ptr) 虚拟内存(vm) 其他信息    单线程  命令串行执行，类似于队列 单线程为什么这么快？  纯内存：主要原因，其它都是辅助原因 非阻塞IO：阻塞IO和非阻塞IO的区别 https://www.cnblogs.com/ynyhl/p/9792699.html 避免线程切换和竞态消耗   使用  一次只运行一条命令 拒绝长(慢)命令：keys、flushall、flushdb、slow lua script、mutil/exec、operate big value(collection) 其实不是单线程，如 fysnc file descriptor、close file descriptor 这样的操作会有独立的线程来做，但是整体模型大致是单线程的，只有极个别例外    字符串 结构和命令\n  Up to 512MB：最大512MB，但是一般不要存太大（考虑网络开销、单线程等），所以一般100KB以内差不多了，一般也够用，最多几MB吧\n  key:value\n hello world：value可以是字符串的 counter 1：value可以是数字的，是可以内部数字、字符串转换的 bits 10111101：value是二进制的 还有json、xml等序列化或者压缩的value，实际上本质上所有的key value的value都是二进制的    场景\n 缓存 计数器 分布式锁 等等    命令\n set：O(1)。设置key-value。set hello \u0026ldquo;world\u0026rdquo; get：O(1)。获取key对应的value。get hello del：O(1)。删除key-value。del hello incr：O(1)。key自增1，如果key不存在，自增后get(key)=1，即创建key并0开始计算并加1。incr count。因为redis天然单线程无竞争，不会因为大并发量记错数。所以也可以用这种方式实现自增的分布式id decr：O(1)。key自减1，如果key不存在，自减后get(key)=-1，即创建key并0开始计算并减1。decr count incrby：O(1)。key自增k，如果key不存在，自增后get(key)=k。incrby count 3 decrby：O(1)。key自减k，如果key不存在，自减后get(key)=-k。decrby count 3    实战\n  记录网站每个用户个人主页的访问量\n incr userid:pageview：如userid为1，pageview为index。则 incr 1:index string实现用户信息  一个key:value记录一个用户，如key是\u0026quot;user:1\u0026rdquo;，value是json串。使用时必须全部取出，且需要序列化开销 多个key:value记录一个用户，一个key:value记录一个属性，key命名\u0026quot;user:1:age\u0026rdquo;，value是属性值，key较多，内存开销多一些，且key教分散的      缓存视频的基本信息（数据源在MySQL中）伪代码\n  如用户通过vedio_id获取视频信息，先到redis中找，找到则返回，找不到则到mysql中找，返回给用户同时也将视频信息存入redis\n//伪代码\rpublic VideoInfo get(long id) {\rString redisKey = redisPrefix + id;\rVideoInfo videoInfo = redis.get(redisKey).desocialization(); //有反序列化的过程，因为redis中存的是二进制，转为VideoInfo对象需要反序列化\rif (videoInfo == nul) {\rvideoInfo = mysql.get(id);\rif (videoInfo != nul) {\rredis.set(redisKey, videoInfo.serialize()); //序列化并存入redis\r}\r}\rreturn videoInfo;\r}\r       补充：\n 选择操作  set：set key value #不管key是否存在，都设置 setnx：setnx key value #key不存在，才设置。类似于新增操作 set xx：set key value xx #key存在，才设置。类似于更新操作 无论setnx还是set xx本质上都是set命令，nx和xx类似于选项，如还有set ex，这是一个set和expire的组合命令，可以在set的同时设置过期时间，且是一个原子操作，这在分布式锁的时候非常有帮助   批量  mget key1 key2 key3\u0026hellip;：O(n)。批量获取key，原子操作。相比于 n次get = n次网络时间+n次命令时间，网络时间是巨大的开销，因为我们的程序服务器与redis可能在不同的机器、不同的机房、甚至不同的地区城市。而1次mget = 1次网络时间 + n次命令时间，节省网络时间，但要注意mget一次携带太多数据也会产生负面影响（数据量很大一般拆分开来，如1000个kv使用一次mget），且要注意时间复杂度是O(n) mset key1 value1 key2 value2 key3 value3：O(n)。批量设置key-value   more  getset key newvalue：O(1)。set key newvalue并返回旧的value append key value：O(1)。将value追加到旧的value strlen key：O(1)。返回字符串的长度（注意中文，redis中utf8的一个中文是2字节）。之所以是O(1)，是因为在存储的字符串内部也实时记录了该字符串的长度，不需要遍历字符计算长度 incrbyfloat key 3.5：O(1)。增加key对应的值3.5 getrange key start end：O(1)。获取字符串指定下标所有的值 setrange key index value：O(1)。设置指定下标所有对应的值，即给指定下标设置新的值      hash   一个key→多个field:value：类似于一个Map\u0026lt;String,Map\u0026lt;String,String\u0026raquo;\n user:1:info  name:Ronaldo age:40 Date:201 viewCounter:50      命令：\n  hget：O(1)。hget key field #获取hash key对应的field的value\n hgetall #获取所有field和value    hset：O(1)。hset key field value #设置hash key对应field的value\n  hdel：O(1)。hdel key field #删除hash key对应field的value\n  hexists：O(1)。hexists key field #判断hash key是否有field\n  hlen：O(1)。hlen key #获取hash key field的数量\n  hmget：O(n)。hmget key field1 field2\u0026hellip;fieldN #批量获取hash key的一批field对应的值\n  hmset：O(n)。hmset key field1 value1 field2 value2\u0026hellip;fieldN valueN #批量设置hash key的一批field value、\n    实战\n  记录网站每个用户个人主页的访问量\n hash记录用户信息就简单的多了。key为\u0026quot;user:1:info\u0026rdquo;，多个field和对应value做属性名和值。直观，可以部分更新，且节省内存。而且使用ziplist编码可以更加节省内存。但是ttl不好控制，因为过期时间无法设置到个体fied，只能针对key，想要实现只能自己写管理或删除的逻辑  hincrby user:1:info pageview count\r   缓存视频的基本信息(数据源在mysqI中)\n//伪代码，相比字符串，无须序列化和反序列化过程\rpublic VideoInfo get(long id) {\rString redisKey = redisPrefix + id;\rMap \u0026lt; String,String\u0026gt; hashMap = redis .hgetAll(redisKey);\rVideoInfo videoInfo = transferMap ToVideo(hashMap);\rif (videoInfo == nulI) {\rvideoInfo = mysql.get(id);\rif (videoInfo != nul) {\rredis.hmset(redisKey, transferVideo ToMap(videoInfo));\r}\r}\r}\r     补充：\n  hgetall：O(n)。hgetall key #返回hash key对应所有的field和value。小心使用，牢记redis是单线程，如果field过多，获取过大数据量影响很大\n  hvals：O(n)。hvals key #返回hash key对应所有field的value\n  hkeys：O(n)。hkeys key #返回hash key对应所有field\n  hsetnx：O(1)。类似setnx，hsetnx key field value #设置hash key对应field的value(如field已经存在，则失败)\n  hincrby：O(1)。类似incrby。hincrby key field intCounter #hash key对应的field的value自增intCounter\n  hincrbyfloat：O(1)。hincrbyfloat key field floatCounter #hincrby浮点数版\n    list 有序，可以重复\n 命令  rpush：O(1~n)。rpush key value1 value2\u0026hellip;valueN #从列表 右 端插入值(1-N个) lpush：O(1~n)。lpush key value1 value2\u0026hellip;valueN #从列表 左 端插入值(1-N个) linsert：O(n)。linsert key before|after value newValue #在list指定的值前|后插入newValue lpop：O(1)。lpop key #从列表 左 侧弹出一个item rpop：O(1)。rpop key #从列表 右 侧弹出一个item Irem：O(n)。Irem key count value #根据count值，从列表中删除所有value相等的项  (1) count\u0026gt;0，从左到右，删除最多count个value相等的项 (2) count\u0026lt;0，从右到左，删除最多Math.abs(count)个value相等的项 (3) count=0，删除所有value相等的项   ltrim：O(n)。ltrim key start end #按照索弓|范围修剪列表，即保留start-end，其它都删掉（不含start和end） lrange：O(n)。lrange key start end (包含end) #获取列表指定索引|范围所有item  list还有一个从右到左的索引，如从左到右索引是0~5，则从右到左则是-1~6，所以lrange key 1 5 与 lrange key 1 -1 是等价的   lindex：O(n)。lindex key index #获取列表指定索引|的item llen：O(1)。llen key #获取列表长度 lset：O(n)。Iset key index newValue #设置列表指定索弓|值为newValue   实战  TimeLine：如微博，它会将你关注用户的动态按时间排序，你的timeline中存按序存放着微博动态的id（具体内容可以存为hash或者sting之类的用id进行外联即可），关注的人有新的动态就push进来   补充  blpop：O(1)。blpop key timeout #lpop阻塞版本，timeout是阻塞超时时间，timeout=0将一直阻塞，如对空的list执行lpop将立刻return，但是如果使用blpop它就会阻塞等待，直到list不为空，然后pop一个出来。对生产者消费者模式或者消息队列有帮助 brpop：O(1)。   数据结构实现：  LRUSH + LPOP = Stack LPUSH + RPOP = Queue LPUSH + LTRIM = Capped Collection LPUSH + BRPOP = Message Queue    set  元素：无序、不可重复 命令   sadd：O(1)。sadd key element #向集合key添加element(如果element已经存在添加失败)\n  srem：O(1)。srem key element #将集合key中的element移除掉\n  scard ber (1)。scard user:1:follow = 4 #计算集合大小\n  sismember ：O(1)。sismember user:1:follow it = 1(存在) #判断it是否在集合中\n  srandmem ：O(1)。srandmember user:1:follow count= his #从集合中随机挑count个元素\n  spop：spop user:1:follow = sports #从集合中随机弹出一个元素\n  smembers：O：O(1)。smembers user:1:follow = music his sports it #获取集合所有元素，返回结果无序，数据较多时谨慎使用\n  sscan：O(1)。使用游标获取集合中的值。即扫集合中的元素，根据游标\n   实战  转发抽奖：将转发者存入集合，spop弹出 赞、踩：将点赞者存入集合，scard获取数量，当然其它数据结构也可以做 标签：给用户添加标签和给标签添加用户属于同一个事务  给用户添加标签：sadd user:1:tags tag1 tag2 tag5，sadd user:2:tags tag2 tag3 tag5，sadd user:k:tags tag1 tag2 tag4 给标签添加用户：sadd tag1:users user:1 user:3，sadd tag2:users user:1 user:2 user:3，sadd tagk:users user:1 user:2     补充：   sdiff：sdiff user:1:follow user:2:follow = music his #差集\n  sinter：sinter user:1:follow user:2:follow = it sports #交集，如共同关注\n  sunion：sunion user:1:follow user:2:follow = it music his sports news ent #并集\n  sdif | sinter | suion + store destkey .. #将差集、交集、并集结果保存在destkey中\n   TIPS  SADD = Tagging SPOP/SRANDMEMBER = Random item SADD + SINTER = Social Graph    zset 有序集合，通过打分score来实现有序，无重复element\n 一个key，多个score:element 命令  zadd：o(logN)。zadd key score element(可以是多对) #添加score和element zrem：o(1)。zrem key element(可以是多个) #删除元素 zscore：o(1)。zscore key element #返回元素的分数 zincrby：o(1)。zincrby key increScore element #增加或减少(负数)元素的分数 zcard：o(1)。zcard key #返回元素的总个数 zrank：o(n)。zrank key #获取元素从低到高的排名 zrange：o(log(n)+m)，n指元素总数，m指要获取的范围个数。zrange key start end [WITHSCORES] #返回指定索引|范围内的升序元素[分值]，WITHSCORES可选，如果写上会带上分值 zrangebyscore：o(log(n)+m)。zrangebyscore key minScore maxScore [WITHSCORES] #返回指定分数范围内的升序元素[分值] zcount：o(log(n)+m)。zcount key minScore maxScore #返回有序集合内在指定分数范围内的个数 zremrangebyrank：o(log(n)+m)。zremrangebyrank key start end #删除指定排名内的升序元素 zremrangebyscore：o(log(n)+m)。zremrangebyscore key minScore maxScore #删除指定分数内的升序元素   实战  排行榜：score设定: timeStamp saleCount followCount   补充  zrevrank：zrevrank key #获取元素从低到高的排名，与zrank相反 zrevrange：从高到低，与zrange相反 zrevrangebyscore：从高到低，与zrangebyscore相反 zinterstore：计算交集并存储 zunionstore：计算并集并存储    更多功能 慢查询  一个查询的生命周期：client 发送命令→阻塞等待排队执行（redis单线程，命令执行类似于队列）→执行命令→返回结果  慢查询发生在执行命令阶段，如hgetAll一个很大的hash 客户端超时不一定是因为慢查询，但慢查询是客户端超时的一个可能因素，因为4个阶段任意一个阶段慢了都可能导致超时   一个列表：慢查询列表，一个redis list实现的队列  如果一个命令执行时间超过了所配置的慢查询时间阈值，将作为慢查询被列入慢查询列表 慢查询列表是固定长度：由slowlog-max-len配置 慢查询列表保存在内存中   两个配置：修改配置文件（一般仅第一次启动时这样做），启动后建议动态配置即可  slowlog-max-len：配置慢查询列表大小  查看：config get slowlog-max-len。缺省值128。 动态配置：config set slowlog-max-len 1000   slowlog-log-slower-than：慢查询时间阈值。单位：微秒。为0时记录所有命令（开发调试可能用到吧，为了查看某个具体的命令临时设置一次）。  查看：config get slowlog-log-slower-than。缺省值10000。 动态配置：config set slowlog-log-slower-than 1000     三个命令  slowlog get [n]：获取慢查询队列，n指定条数 slowlog len：获取慢查询队列长度 slowlog reset：清空慢查询队列   运维经验  slowlog-max-len不要设置过小，通常设置1000左右 slowlog-log-slower-than不要设置过大，默认10ms，通常设置1ms。当然实际场景还是根据qps适当调整 理解命令生命周期 定期持久化慢查询：需要自己或者通过其它开源工具实现    pipeline   pipeline：流水线。用于打包一批命令，批量执行\n 一次网络命令通信模型：客户端传输命令（网络）→服务器计算（执行命令）→返回结果（网络）。即1次时间= 1次网络时间（1、3）+ 1次命令时间（2） 批量网络命令通信模型：n次时间= n次网络时间+ n次命令时间。 流水线通信模型：1次pipeline(n条命令) = 1次网络时间+ n次命令时间。命令时间很快（redis命令时间是微秒级的），但是网络时间可能很大（跨地区跨城市），也许会想到mget、mset，但是如果是hash结构，不存在mhmset，包括也许还想要同时统一发送get、hget，流水线就是为此存在的，将一批命令批量打包一次发送，再在服务端批量计算，按顺序将结果批量打包一次性返回 redis命令时间是微秒级的；pipeline每次条数要控制（网络因素，一次性传递巨大的数据包是不太好的）    客户端实现\n//hset 10000次。这样就只需要10次网络时间\rJedis jedis = new Jedis(\u0026quot;127.0.0.1\u0026quot; 6379);\rfor (inti = 0;i \u0026lt; 10=; i++) {\rPipeline pipeline = jedis. pipelined();\rfor (intj= i* 1000;j \u0026lt; (i+ 1) * 1000;j++) {\rpipeline.hset(\u0026quot;hashkey:\u0026quot; + j, \u0026quot;field\u0026quot; + j, \u0026quot;value\u0026quot; + j);\rpipeline.syncAndReturnAll();\r}\r}\r   对比原生操作（即m操作，如mset）\n m命令：是原子的，是redis原生支持的命令，在操作队列中就是一个命令 pipeline命令：是非原子的，将根据其中打包的命令，被重新拆为一堆pipeline子命令，然后才挨个进入命令操作队列，中间是可以穿插其它操作命令的    使用建议\n 注意每次pipeline携带数据量 pipeline每次只能作用在一个Redis节点 上    发布订阅   发布订阅：redis仅提供发布订阅的功能，但没有消息堆积的功能，新的订阅者无法从redis获取到历史消息\n  角色：发布者publisher、订阅者subscriber、频道channel\n  API\n  publish：publish channel message\nredis\u0026gt; publish yuanya:channel \u0026quot;hello\u0026quot; #向\u0026quot;yuanya:channel\u0026quot;频道发布消息\r(integer) 3 #订阅者个数\r   subscribe：subscribe [channel] #一个或多个\nredis\u0026gt; subscribe yuanya:channel #从yuanya:channel订阅消息\r1) \u0026quot;subscribe\u0026quot;\r2) \u0026quot;yuanya:channel\u0026quot;\r3) (integer) 1\r1) \u0026quot;message\u0026quot;\r2) \u0026quot;yuanya:channel\u0026quot;\r3) \u0026quot;hello\u0026quot;\r   unsubscribe：unsubscribe [channel] #一个或多个\n    补充\n psubscribe [pattern..] #订阅模式。psubscribe v* #订阅v开头的频道 punsubscribe [pattern..] #退订指定的模式。 pubsub channels #列出至少有一个订阅者的频道。pubsub numsub [channel..] #列出给定频道的订阅者数量，pubsub numpat #列出被订阅模式的数量      消息队列：消费者是竞争关系，每一条消息只被一个消费者消费一次。可以使用list来实现\n  Bitmap   位图：如 \u0026ldquo;big\u0026rdquo; 对应 ascii 码的二进制为：01100010 01101001 01100111，通过位图可以获取其每一个位的值\nset hello big #OK\rgetbit hello 0 #(integer) 0\rgetbit hello 1 #(integer) 1\r   命令\n  setbit：setbit key offset value #给位图指定索弓|设置值（值只能是0或1）。返回结果是之前该位置对应的值（如果没有值就是0），offset 跳过的中间未设置值的位将补0，所以最大offset不要突然变大，否则补0影响性能\nsetbit key 0 1 #(integer) 0。当前位图：1\rsetbit key 7 1 #(integer) 0。当前位图：10000001\rsetbit key 9 1 #(integer) 0。当前位图：1000000101\r   bitcount：bitcount key [start end]，获取位图指定范围（start到end，单位为字节，如果不指定就是获取全部）位值为1的个数\n  bitop：bitop op destkey key [key\u0026hellip;.，做多个Bitmap的and(交集)、or(并集)、 not(非)、 xor(异或) 操作并将结果保存在destkey中\n  bitpos：bitpos key targetBit [start] [end]，算位图指定范围(start到end，单位为字节，如果不指定就是获取全部)第一个偏移量对应的值等于targetBit的位置\n    实战\n  独立用户统计：总共有1亿用户\n  假如每天5千万独立访问用户，使用set和Bitmap对比\n   数据类型 每个userid占用空间 需要存储的用户量 全部内存量（一天） 一个月 一年     set 32位(假设userid用的是整型，实际很多网站用的是长整型) 50,000,000 32位 * 50,000,000= 200MB 6G 72G   Bitmap 1位 100,000,000 1位 * 100,000,000 = 12.5MB 375M 4.5G      假如每天只有10万独立用户，合适的场景使用合适的技术\n   数据类型 每个userid占用空间 需要存储的用户量 全部内存量（一天）     set 32位(假设userid用的是整型，实际很多网站用的是长整型) 1,000,000 32位 * 1,000,000= 4MB   Bitmap 1位 100,000,000 1位 * 100,000,000 = 12.5MB          经验\n 要注意的是位图type=string，所以最大是512M，大部分情况都可以满足一天的统计量，无法满足时，拆分为多个位图即可 注意setbit时的偏移量,可能有较大耗时 位图不是绝对好    HyperLogLog   基于HyperLogLog算法：极小空间完成独立数量统计。本质还是字符串\n  命令\n pfadd：pfadd key element [element .. #向hyperloglog添加元素 pfcount：pfcount key [key .. #计算hyperloglog的独立总数 pfmerge：pfmerge destkey sourcekey [sourcekey \u0026hellip;] #合并多个hyperloglog  redis\u0026gt; pfadd 2017_03_06:unique:ids \u0026quot;uuid-1\u0026quot; \u0026quot;uuid-2\u0026quot; \u0026quot;uuid-3\u0026quot; \u0026quot;uuid-4\u0026quot;\r(integer) 1\rredis\u0026gt; pfcount 2017_03._06:unique:ids\r(integer) 4\rredis\u0026gt; pfadd 2017_ 03_06:unique:ids \u0026quot;uuid-1\u0026quot; \u0026quot;uuid-2\u0026quot; \u0026quot;uuid-3\u0026quot; \u0026quot;uuid-90\u0026quot;\r(integer) 1\rredis\u0026gt; pfcount 2017_03_06:unique:ids\r(integer) 5\r  内存消耗(百万独立用户)：1天 15KB    经验\n 是否能容忍错误? (官方给出错误率: 0.81%) 是否需要单条数据?不能单独取出    GEO  GEO(地理信息定位)：存储经纬度,计算两地距离,范围计算等 应用场景：实现如微信摇一摇这种类似功能、计算附近一定距离的酒店  5个城市经纬度  北京，116.28，39.55，beijing 天津，117.12，39.08，tianjin 石家庄，114.29，38.02，shijiazhuang 唐山，118.01，39.38，tangshan 保定，115.29，38.51，baoding     api  geo：geo key longitude latitude member [longitude latitude member...] #增加地理位置信息 geopos：geopos key member [member...] #获取地理位置信息 geodist：geodist key member1 member2 [unit] #获取两个地理位置的距离  unit：m(米)、 km(千米)、mi(英里)、ft(尺)   georadius：georadius key longitude latitude radiusm|km|ft|mi [withcoord] [withdist] [withhash] [COUNT count] [asc|desc] [store key] [storedist key] georadiusbymember：georadiusbymember key member radiusm|km|ft|mi [withcoord] [withdist] [withhash] [COUNT count] [asc|desc] [store key] [storedist key] #获取指定位置范围内的地理位置信息集合  withcoord：返回结果中包含经纬度。 withdist：返回结果中包含距离中心节点位置。 withhash：返回结果中包含geohash COUNT count：指定返回结果的数量。 asc|desc：返回结果按照距离中心节点的距离做升序或者降序。 store key：将返回结果的地理位置信息保存到指定键。 storedist key：将返回结果距离中心节点的距离保存到指定键      持久化  持久化：redis所有数据保持在内存中，对数据的更新将异步地保存到磁盘上  快照：将数据备份。如 MySQL Dump、Redis RDB 日志：记录所有数据更新操作到日志，数据丢失时，完整重走一边日志记录的操作即可恢复。如 MySQL Binlog、Hbase HLog、Redis AOF    RDB 在硬盘创建RDB文件（二进制）备份数据，redis启动的时候载入RDB文件。主从复制中也可以用RDB文件做复制媒介\n触发机制 触发机制-主要三种方式\n  save：该命令是同步的。主动创建RDB文件，执行时将阻塞redis。不会消耗额外内存\n 文件策略：如存在老的RDB文件，将替换 复杂度：O(n)    bgsave：异步。主动创建RDB文件，将调用fork()（fork非常快，会消耗一定内存，使用不当还是可能发生阻塞）开一个子进程来执行，除了fork本身，之后不会阻塞redis客户端命令\n 文件策略：如存在老的RDB文件，将替换 复杂度：O(n)    自动：达到某些条件时自动触发RDB文件生成（bgsave方式），save配置。自动生成的文件为./dump.rdb\n  以下是redis默认的3条save配置，满足任意一个即可触发\n   配置 seconds changes 说明     save 900 1 900秒改变（增删改等）了1条数据，生成rdb文件。即使每900秒只改变1条，也会每900秒生成rdb文件   save 300 10 300秒改变了10条数据，生成rdb文件   save 60 10000 60秒改变了10000条数据，生成rdb文件      无法控制生成rdb的频率，写操作大的情况可能会很高\n  默认配置\nsave 900 1\rsave 300 10 save 60 10000\rdbfilename dump.rdb #rdb文件文件名\rdir ./ #rdb文件保存目录\rstop-writes-on-bgsave-error yes #如果bgsave发生error，是否停止写入\rrdbcompression yes #是否采用压缩格式\rrdbchecksum yes #是否进行校验和检验\r   推荐配置\n#删掉默认save配置\rdbfilename dump-${port}.rdb #因为一台机器可能有多个redis\rdir /bigdiskpath #选择一个较大的磁盘路径，甚至可能根据不同端口上的redis分盘\rstop-writes-on-bgsave-error yes\rrdbcompression yes\rrdbchecksum yes\r     触发机制补充-不容忽略方式\n 全量复制：如主从复制时，主会自动生成RDB文件 debug reload：debug级别的重启，不清空内存的重新加载，这也会触发生成RDB文件 shutdown：执行shutdown时会执行一个shutdownsave，也会生成RDB文件    试验\n  save阻塞：执行save同时在另一个窗口get一下，可以发现在save完成之前是阻塞的\n  bgsave fork：执行bgsave时立马查看进程，可以发现有多出一个子进程（执行完后消失），文件夹多出一个临时rdb文件（执行完后用替换掉原来的dump-6379.rdb）\ndata ps -ef | grep redis- | grep -V \u0026quot;redis-cli\u0026quot; | grep -V \u0026quot;grep\u0026quot;\r501 36775 1 0 10:22下午 ?? 0:17.91 redis-server *:6379\r501 36954 36775 0 10:28下午 ?? 0:02.81 redis-rdb-bgsave *:6379 #子进程\r\u0026gt; data ll\rtotal 1126064\r-rw-r--r-- 1 carlosfu staff 1.9K 10 6 22:29 6379.10g\r-rw-r--r-- 1 carlosfu staff 459M 10 6 22:28 dump-6379.rdb\r-rw-r--r-- 1 carlosfu staff 90M 10 6 22:29 temp-36985.rdb #临时文件\r   真的自动?是的\n  RDB长啥样?二进制文件，看不懂\n     总结与问题   总结\n  RDB是Redis内存到硬盘的快照,用于持久化。\n save通常会阻塞Redis。 bgsave不会阻塞Redis ,但是会fork新进程。 save自动配置满足任一就会被执行。 有些触发机制不容忽视    问题\n 耗时、耗性能  O(n)数据:耗时 fork() :消耗内存，copy-on-write策略 DiskI/O : IO性能   不可控、丢失数据  T1：执行多个写命令 T2：满足RDB自动创建的条件 T3：再次执行多个写命令 T4：宕机，T3的命令将丢失      AOF   AOF文件：记录所有写命令到AOF文件。\n 如客户端发送set hello world命令→rerdis保存set hello world命令到AOF文件。 如果宕机了，重启redis时载入aof文件进行恢复    三种策略 三种策略：aof并不是直接写入磁盘，是写在磁盘的缓冲区中，缓冲区根据策略刷新到磁盘（为了提高写入）\n always：每条命令都fsync到硬盘。不会丢失数据，io开销大，一般sata盘只有几百tps everysec：默认策略。每秒把缓冲区fsync到硬盘。有可能丢失最后1秒的命令。一般就使用everysec no：操作系统os决定fsync。不用管，不可控。一般不使用  AOF重写 AOF重写：对redis当前在内存中的数据进行一次回溯，回溯成一个新aof文件，并覆盖旧aof文件\n  因为要记录每条写命令，长期下来aof文件会变得很大，恢复和写入都有一定性能影响。进行重写优化，来减少硬盘占用量、加速恢复速度\n  例子：例子只是作用效果举例，不是真的对原aof文件重写，实际是内存数据回溯\n 假如有set hello world、set hello java、set hello hehe是aof中的记录，实际上只有最后一条是有意义的，AOF重写就会优化为只记录最后一条命令set hello hehe 如incr counter、incr counter优化为set counter 2（假如incr了1e次，将优化了太多） 如rpush mylist a、rpush mylist b、rpush mylist c，优化为rpush mylista bc 设置过期时间并已经过期的数据，其相关操作直接优化为无了    实现\n bgrewriteaof：异步，fork一个子进程执行 AOF重写配置：aof重写自动触发相关配置  配置  auto-aof-rewrite-min-size：AOF文件重写需要的尺寸 auto-aof-rewrite-percentage：AOF文件增长率   统计  aof_current_size：AOF当前尺寸(单位:字节) aof_base_size：AOF_上次启动和重写的尺寸(单位:字节)   触发时机：以下条件需要同时满足  当前尺寸\u0026gt;AOF文件重写需要的尺寸：aof_current_size \u0026gt; auto-aof-rewrite-min-size 当前增长率\u0026gt;AOF文件增长率：aof_current_size - aof_base_size / aof_base_size \u0026gt; auto-aof-rewrite-percentage        流程：bgrewriteaof命令发送给redis父进程。父进程fork一个子进程，去执行回溯；同时父进程任然会将新收到的写命令写入缓冲aof_buf并同步到旧aof文件；同时还会将新收到的写命令写入另一个缓冲当中aof_rewrite_buf，这个buf会在新aof文件生成完毕之后补充进去。最后新aof文件覆盖旧aof文件\n  配置\nappendonly yes #默认值是no。开启以使用aof重写相关功能\rappendfilename \u0026quot;appendonly-${port}.aof\u0026quot; #命名\rappendtsync everysec #同步策略\rdir /bigdiskpath #选一个大磁盘目录，甚至在比较严格情况情况下分盘\rno-appendtsync-on-rewrite yes #在aof重写的时候，是否不执行旧aof文件的append同步操作，yes即不做这个操作。如果是完全不允许丢失数据的场景，就设置no，因为万一aof重写时宕机，这段时间的数据可能就丢失了\rauto-aof-rewrite-percentage 100\rauto-aof-rewrite-min-size 64mb\raof-load-truncated yes #重启加载aof文件时，如果出现错误是否忽略，因为可能重写时只刷入一半缓存时宕机，造成文件内容不完整\r   aof文件内容：\ndata\u0026gt; head appendonly.aof\r*2 # *指定接下来的一个命令是几个参数。下面这个命令有2个参数\r$6 # $指定接下来的一个参数是几个字节。下面这个参数有6个字节\rSELECT #确实是6个字节\r$1\r0 #select 0即选中了0号数据库\r*3\r$3\rset\r$5\rhello\r$5\rworld\r   取舍和选择   对比\n   命令 RDB AOF     启动优先级（如果都配置了，优先启动谁） 低 高   体积 小 大   恢复速度 快 慢   数据安全性 丢数据 根据策略决定   轻重 重（每次从内存全部写入硬盘） 轻（每次只追加缓冲内的）      RDB最佳策略\n \u0026ldquo;关\u0026rdquo;，但是启动时主从复制时全量复制还是会用到RDB，所以是无法彻底关闭的 集中管理：用于备份数据是不错的选择 主从，从开?：有时候需要在从节点开启RDB，以保存历史的RDB文件，但要控制自动生成的力度，不要太频繁，因为redis一般都是混合部署，单机多部署，而RDB是一个重操作    AOF最佳策略\n \u0026ldquo;开\u0026rdquo;：缓存和存储。因为一般是每秒追加缓存同步，如果丢失数据，从数据源加载即可，redis毕竟大多只作缓存，不是数据源。如果对数据源压力不大，redis也只是起到一定缓存作用，关掉也无妨，因为AOF每秒追加缓存到磁盘确实是有一定开销的 AOF重写集中管理：一般分配百分之六七十内存给redis，剩下的要留给做类似fork这样的操作，因为单机多部署，可能AOF重写集中发生，产生大量fork，就可能内存爆满等情况 everysec：建议使用everysec策略    最佳策略\n 小分片（没听懂，对fork不熟）：使用max memory（最大内存）对redis进行规划，如每个redis的max memory只设置4g，这样fork、rdb复制传输、等操作都只会产生较小的开销。但也有缺点，分布式下，产生更多的redis进程可能对cpu占用更多 缓存、存储：根据存储和缓存的特性来决定是否使用那种的策略 监控(硬盘、内存、负载、网络) 足够的（冗余）内存    开发运维常见问题 （很多没搞懂，对fork和linux不熟）\n fork操作：fork操作本身，不包括fork后产生的子进程  fork操作本身，是一个在主进程中完成的同步操作，只是做一个内存页的拷贝而不是完全做内存的拷贝，所以大部分情况下速度是非常快的。但如果fork操作本身较慢，则会阻塞redis主进程 与内存量息息相关：内存越大，耗时越长(与机器类型有关) info: latest fork_ usec：查询上一次fork操作消耗的微秒数，如果对此要关注，可以做一些监控或相对应的告警 改善fork  优先使用物理机或者高效支持fork操作的虚拟化技术 控制Redis实例最大可用内存: maxmemory 合理配置Linux内存分配策略: vm.overcommit _memory= 1。默认值0，如果没有足够内存做内存分配的时候（内存较低时）就不去分配，这将造成fork阻塞 降低fork频率：例如放宽AOF重写自动触发时机，不必要的全量复制     进程外开销：子进程开销和优化  CPU：  开销：RDB和AOF文件生成，属于CPU密集型（写入是一个集中的过程） 优化：不做CPU绑定（如果绑定，会和主进程集中消耗cpu，可能对主进程造成很大资源影响），不和CPU密集型应用部署在一起   内存：  开销: fork内存开销，copy-on-write。因为子进程是通过fork来产生的，理论上占用内存是等于父进程，但是linux有一个显式复制的机制copy-on-write，父子进程共享相同的物理内存页，当父进程有写请求的时候，会创建一个副本，相当于这是才会消耗内存，而在整个期间子进程会共享fork时父进程的内存的快照，即在做如aof重写fork产生子进程过程中，如果父进程有大量内存写入，就证明子进程的内存会开销比较大，因为它会做一个副本，如果父进程没什么写入，实际上子进程也就开销不了多少内存 优化: echo never \u0026gt; /sys/kernel/mm/transparent hugepage/enabled   硬盘  开销: AOF和RDB文件写入，可以结合iostat、iotop等工具分析 优化  不要和高硬盘负载服务部署一 起:存储服务、消息队列等 no-appendfsync-on-rewrite = yes 根据写入量决定磁盘类型:例如ssd 单机多实例持久化文件目录可以考虑分盘       AOF追加阻塞  everysec流程：  主线程将命令写入aof缓冲区；主线程还会负责对比上次fsync时间，如果距离上次fsync时间小于2秒，主线程返回继续其它操作，否则阻塞直到同步完成，这是为了保证aof文件安全性的策略 同时还有一个AOF同步线程，用来每秒将缓冲区内容同步到硬盘，并且会记录最近一次fsync时间   问题：  主线程阻塞问题 aof丢失的数据，最大可达2秒   AOF阻塞定位  Redis日志：Asynchronous AOF fsync is taking too long (disk is busy?). Writing the AOF buffer without waiting for fsync to complete, this may slow down Redis info persistence：记录上述过程发生的数量，aof_ delayed fsync: 100。没发生一次+1 查看硬盘：top命令，查看是否有发生io资源紧张的时间   硬盘优化策略，在上面已经有了，根据实际情况考量即可   单机多实例部署  Redis缓存  缓存的使用与设计  缓存的受益与成本 缓存更新策略 缓存粒度控制 缓存穿透优化 无底洞问题优化 缓存雪崩优化 热点key重建优化    缓存的受益与成本  收益  加速读写  通过缓存加速读写速度：如CPU L1/L2/L3 Cache、Linux page Cache加速硬盘读写、浏览器缓存、Ehcache缓存数据库结果。   降低后端负载  后端服务器通过前端缓存降低负载：业务端使用Redis降低后端MySQL负载等     成本  数据不一致：缓存层和数据层有时间窗口不一致，和更新策略有关。如redis做缓存层，mysql做数据源，需要将数据源的数据放到缓存层进行缓存，而数据源更新时缓存需要如何更新？是立刻发出通知还是，按时间间隔轮询自动更新，还是懒加载用到时才更新。策略要根据所能容忍的时间窗口或范围来决定 代码维护成本：多了一层缓存逻辑，缓存的读写、缓存与数据库的沟通等 运维成本：例如Redis Cluster 经济成本：实体机器成本或者云计算机器成本   使用场景  降低后端负载  对高消耗的SQL：join结果集/分组统计结果缓存。比如做排行榜计算，涉及很多表，做一个实时计算，很复杂，我们不需要每次都实时计算，只需要计算某个时间点的结果进行缓存即可   加速请求响应:  利用Redis/Memcache优化IO响应时间   大量写合并为批量写  如计数器：如果真的需要将计数写入DB，我们不能每次计数都写DB，先Redis累加再批量写DB      缓存更新策略 缓存的数据通常都有生命周期，需要做定期更新或删除以保证空间在可控范围内，且保证数据的定期更新。但是缓存中的数据和真实的数据可能存在不一致，需要一个合理的更新策略来保证数据的不一致在可容忍范围内\n  LRU/LFU/FIFO算法剔除：例如maxmemory-policy，最大内存对应的策略，在达到最大内存时执行的策略\n LRU：把最近没有使用的key删除，保证不超过maxmemory，尽可能保证了数据安全。还有all keys lru，即在所有的键去执行LRU 使用场景：需要控制配置maxmemory    超时剔除：例如expire。设置过期时间，时间内访问到缓存中读来保证性能，如果时间内用户更行了一些重要的信息，expire就不太好了，所以对于不重要的信息可以expire，比如一个视频信息的说明修改了一个标点符号，可能时间内信息不一致是可以容忍的，但如果是涉及钱的金融方面的信息，肯定就不能用这样的策略了\n  主动更新：开发控制生命周期。用户信息在存储层发生了变化，如果对应的缓存层能通过业务代码或者开发一些工具能知道存储层的变化，比如订阅存储层一些消息的变化，比如更新存储层时会发布一条消息，缓存层收到消息则主动更新或者做一次重建，到数据层重新取一次数据然后缓存，来实现数据的一致性，虽然仍然不是完全一致的强一致性，而是需要一个最终一致性，最终实现一致性的时间也是比较短的，即由该更新机制保证的\n   策略 一致性 维护成本     LRU/LIRS算法剔除 最差 低   超时剔除 较差 低   主动更新 强 高      建议：\n 低一致性环境：最大内存+淘汰策略。随意往缓存里扔就行了，达到最大内存就淘汰即可，淘汰什么也无所谓，没有的话下次再到存储层读取重新构建缓存即可 高一致性情境：超时剔除、主动更新结合，超时策略为主动更新兜底，最大内存和淘汰策略做最终兜底。主动更新的代码是开发人员自己维护的，万一出了问题，没有将真正的数据删除，后期我们无法发现这样的问题，那么我们就给数据设置一个较长的过期时间，如果出了问题，最终数据也会被超时剔除，当然这里的数据指真正有生命周期的，确实要在一定时间后过期的，如果有数据真的是不会过期，就不能设置过期时间。最后是最大内存和淘汰策略兜底，因为无法保证那天监控不到位内存就上去了，保证高可用性，而不会直接爆内存导致缓存不可用  缓存粒度控制  从MySQL获取用户信息：select * from user where id= {id} 设置用户信息缓存：set user:{id} \u0026lsquo;select * from user where id= {id}'。缓存数据来源于存储层，是需要做一个两者间映射的，key需要id值，value是select * from user where id= {id}的结果值，可能需要很多操作，如果对它做一个压缩，或者不使用字符串类型，使用hash类型，将每一个属性变成hash的field和value 缓存粒度：到底是缓存select *得到的所有字段还是仅仅缓存需要的字段  全部属性：set user:{id} \u0026lsquo;select * from user where id={id} 部分重要属性：set user:{id} select importantColumn1, \u0026hellip;importantColumnK from user where id={id}\u0026rsquo;   缓存粒度控制-三个角度  通用性：全量属性更好   占用空间：部分属性更好 代码维护：表面上全量属性更好。不会出现有新的业务需求时，需要用到的字段没有，还要重新到存储层单独取字段又更新到缓存。但是大部分时候，需求和业务固定了，我们只需要缓存我们需要的属性，而不需要过多考虑扩展性，虽然考虑扩展性是一个很好的习惯，但是在使用缓存的时候要更多考虑空间占用和性能问题，因为内存空间珍贵，而缓存也更是为了解决性能问题而存在的。所以还是要综合考虑  缓存穿透优化   缓存穿透：key对应的数据在数据源并不存在，每次针对此key的请求从缓存获取不到，请求都会到数据源，从而可能压垮数据源。比如用一个不存在的用户id获取用户信息，不论缓存还是数据库都没有，若黑客利用此漏洞进行攻击可能压垮数据库。\n  缓存穿透问题-大量请求不命中：request打到cache上，如果miss未命中，就会把流量往下引到storage层，正常情况存储层拿到对应的结果回写cache并返回response，当下次再有同样的数据请求时就可以直接命中cache，不需要到存储层取了。但是如果cache中miss时，storage也miss，将响应一个空的业务，当下一次再进行同样的请求时，他仍然会先访问cache再导到storage，并任然全部miss响应空，所有的流量都会打到存储层，这就是缓存穿透。缓存穿透使得缓存失去了意义，因为本来缓存就是为了保护存储层的，如果有大量这样的请求穿透缓存直击存储层，会给存储层带来很大的隐患\n  原因\n 业务代码自身问题：比如从mysql拿了数据，结果存的时候用了空的或错的变量，导致存入了mysql原本根本不存在内容，并且还将key响应了出去供用户使用 恶意攻击、爬虫等等：我们知道虽然一般视频网站的url做了很多加密，来防止别人猜到视频的id规则，但别人仍然可以强制访问不存在的id，就可能会发生缓存穿透    发现\n 业务的响应时间：一般都会有监控系统，平时缓存扛了很大量，并且响应速度会很快，是可预期的，如果出现缓存穿透，必然会在响应时间上有所体现 业务本身问题 相关指标：总调用数、缓存层命中数、存储层命中数，比如采集每分钟的变化，可以知道有没有这样的问题    解决方法\n  缓存空对象：如果缓存穿透，存储层返回了null，我们仍然将null当作结果，配合请求的id值将其回写到缓存层。\n 使用：当然请求的数据可能是真的不可用，也可能比如存储层对外提供了接口，该节点暂时不可用，等等原因，那么就可以将null结果的cache设置过期时间，就可以在过期时间内暂缓缓存穿透对存储层带来的影响 两个问题：  需要更多的键：会占用额外的空间，为了不使key越堆越多，设置过期时间就是一个较好的方法 缓存层和存储层数据\u0026quot;短期\u0026quot;不一致。在null缓存过期时间内都是不一致的，可以订阅消息，尽量使存储层恢复后能马上同步回写数据刷新null缓存的这个key，甚至可以使用消息队列单独定位到某个key或者某个业务，来解决不一致问题。当然仍然不是强一致的，永远会存在短期的不一致   总的来说是一个不错的解决方式  //java伪代码。具体的过期时间还有逻辑要根据实际开发需求进行变动\rpublic String getPassThrough(String key) {\rString cacheValue = cache.get(key);\rif (StringUtils.isBlank(cacheValue)) {\rString storageValue = storage.get(key);\rcache.set(key, storageValue);\r//如果存储数据为空，需要设置一个过期时间(300秒)\rif (StringUtils.isBlank(storageValue)) {\rcache.expire(key, 60 * 5);\r}\rreturn storageValue;\r} else {\rreturn cacheValue;\r}\r}\r   布隆过滤器拦截：通过很小的内存来实现对数据的过滤。比如巨大的电话本，10E行，判断一个电话是否在这个电话本里。电话本如此巨大不可能放在内存中，bloom filter就是解决类似问题的，可以通过一些算法将电话本这样的大数据放在bloom filter预热一遍，当下次访问需要判断一个电话是否在这个电话本里的时候，可以用很小的内存来解决这个问题。bloom filter后面说明\n bloom filter在cache前再做一次拦截，如果在bloom filter这里被过滤了，就不认为请求的数据是有效的，没被过滤才能到cache层去。问题就在于bloom filter如何去生成？离线去生成或者如何去做？对于固定的数据很好解决，对于频繁更新的数据要如何解决就有很多问题了。比如一个非实时的推荐服务，根据用户前一天的日志给出第二天的结果，一般就可以在夜间去计算，存储在如hbase上，对于第二天来说这就是一个比较固定的数据，有每个用户的行为，可以把每个用户的key做成布隆过滤器，就会是一个相对可信的布隆过滤器，而如果是现在大多数的实时推荐，需要实时更新布隆过滤器，就相对不可信，可能会不完全符合实时。所以说布隆过滤器是有一定局限性的，需要特殊使用场景，布隆过滤器本身也需要单独维护代码，实现过滤也是需要额外的空间来完成的       缓存击穿 缓存击穿：key对应的数据存在，但在redis中过期，此时若有大量并发请求过来，这些请求发现缓存过期一般都会从后端DB加载数据并回设到缓存，这个时候大并发的请求可能会瞬间把后端DB压垮。业界比较常用的做法，是使用mutex。简单地来说，就是在缓存失效的时候（判断拿出来的值为空），不是立即去load db，而是先使用缓存工具的某些带成功操作返回值的操作（比如Redis的SETNX或者Memcache的ADD）去set一个mutex key，当操作返回成功时，再进行load db的操作并回设缓存；否则，就重试整个get缓存的方法\n 解决：通过redis的setnx（不存在才set）设置key做互斥锁，设置成功的才去数据库加载数据  缓存雪崩优化  缓存雪崩：当缓存服务器重启或者大量缓存集中在某一个时间段失效，这样在失效的时候，也会给后端系统(比如DB)带来很大压力。加锁或者请求队列，还有尽量不要使大量数据的过期时间在同一个时间段 缓存失效时的雪崩效应对底层系统的冲击非常可怕！大多数系统设计者考虑用加锁或者队列的方式保证来保证不会有大量的线程对数据库一次性进行读写，从而避免失效时大量的并发请求落到底层存储系统上。还有一个简单方案就是，将缓存失效时间分散开，比如我们可以在原有的失效时间基础上增加一个随机值，比如1-5分钟随机，这样每一个缓存的过期时间的重复率就会降低，就很难引发集体失效的事件  https://zhuanlan.zhihu.com/p/75588064\n无底洞问题优化   问题描述\n 2010年, Facebook有了3000个Memcache节点。 发现问题：\u0026ldquo;加\u0026quot;机器性能没能提升，反而下降。节点过多了    关键点：批量操作的变化，如mget，单机是一个原子操作O(1)，集群则不是，前面提到过了\n 更多的机器!=更高的性能 批量接口需求(mget,mset等) 数据增长与水平扩展需求：服务端水平扩容无非就是加节点、加机器，而客户端又需要更高的性能    优化思路：如redis这种命令本身很快的，就更多考虑优化网络时间，但是如果是mysql这种可能命令会执行极慢的关系型数据库，可能就需要更多考虑优化sql本身。不过一般情况下mysql本来也不需要像redis那么高的性能。要学会根据不同类型数据库调整不同的优化思路\n 命令本身优化：例如慢查询keys、hgetall bigkey 减少网络通信次数 降低接入成本：例如客户端长连接/连接池、NIO等    优化方法：跟前面一样，不再赘述\n 串行mget 串行IO 并行IO hash_tag     热点key重建优化   缓存重建：到cache中获取数据，miss则到数据源获取，这个过程就是缓存重建\n  问题：比如一个微博大V在重要的时间节点发布了一个重要消息会落在一个重要的key上，成为一个有极大访问量的热点key，在重建完成之前，有巨大的多线程的访问打过来，那么每个线程都miss，都都要去数据源取，即很多线程都参与重建，重建可能很慢，比如是一个复杂的sql、一个很慢的api，这就产生了问题，每个线程都要执行一遍重建过程，会很浪费时间，会对数据源造成巨大压力，\n  三个目标\n 减少重缓存的次数 数据尽可能一致 减少潜在危险：比如死锁，线程池被阻塞等    解决\n  互斥锁(mutex key)将重建的过程上互斥锁，其它线程则阻塞等待，重建好之后，其它线程则可以直接获取缓存。个人理解：为了防止大量线程都阻塞等待，可以直接响应一些提示内容\n 问题：重建过程中需要其它线程都阻塞等待  //java伪代码\rString get(String key){\rString value = redis.get(key);\rif (value == nul) {\rString mutexKey = \u0026quot; mutex:key.\u0026quot; + key;\rif (redis.set(mutexKey, \u0026quot;1\u0026quot;, \u0026quot;ex 180\u0026quot;, \u0026quot;nx\u0026quot;)) { //设置一个互斥锁，只有1个线程可以进去执行，为了保证这个锁会被删掉，设置一个过期时间180秒。ex和nx是一个组合命令，set exnx，保证命令的原子性\rvalue = db.get(key);\rredis.set(key, value);\rredis.delete(mutexKey);\r} else {\r//其他线程休息50毫秒后重试\rThread.sleep(50);\rget(key);\r}\rreturn value;\r}\r}\r   永远不过期：\n 缓存层面：不设置过期时间(没有用expire)。 功能层面：为每个value添加逻辑过期时间，但发现超过逻辑过期时间后，会使用单独的线程去构建缓存。 问题：存在数据不一致，虽然数据永远不会过期，任何时刻到来的线程都可以无需等待获取数据，但某一时刻一个线程A发现数据逻辑时间到期，则单独开启一个线程去完成重建并设置新的过期时间，而重建过程中，线程A和之后时间的线程获取到的数据等于是旧的数据  String get(final String key){\rV v = redis.get(key);\rString value = v.getValue();\rlong logicTimeout = v.getLogicTimeout();\rif (logicTimeout \u0026gt;= System.currentTimeMillis()) {\rString mutexKey = \u0026quot; mutex🔑\u0026quot; + key;\rif (redis set(mutexKey, \u0026quot;1\u0026quot;, \u0026quot;ex 180\u0026quot;, \u0026quot;nx\u0026quot;)) {\r//异步更新后台异常执行\rthreadPool.execute(new Runnable() {\rpublic void run() {\rString dbValue = db.get(key);\rredis.set(key, (dbValue,newLogicTimeout));\rredis.delete(keyMutex);\r}\r});\r}\r}\rreturn value;\r}\r      方案 优点 缺点     互斥锁 思路简单保证一致性 代码复杂度增加存在死锁的风险   永远不过期 基本杜绝热点key重建问题 不保证一致性逻辑过期时间增加维护成本和内存成本      总结  缓存收益：加速读写、降低后端存储负载。 缓存成本：缓存和存储数据不一致性、代码维护成本、运维成本。 推荐结合剔除、超时、主动更新三种方案共同完成。 穿透问题：使用缓存空对象和布隆过滤器来解决，注意它们各自的使用场景和局限性。 无底洞问题:分布式缓存中,有更多的机器不保证有更高的性能。有四种批量操作方式:串行命令、串行IO、并行IO、hash_tag。 雪崩问题:缓存层高可用、客户端降级、提前演练是解决雪崩问题的重要方法。 热点key问题:互斥锁、“永远不过期”能够在一-定程度 上解决热点key问题，开发人员在使用时要了解它们各自的使用成本。  RedisBloomFilter 基于Redis的分布式布隆过滤器\n 引出布隆过滤器 布隆过滤器原理 布隆过滤器误差率 本地布隆过滤器 Redis单机布隆过滤器 Redis分布式布隆过滤器  引出布隆过滤器  问题：现有50亿个电话号码，现有10万个电话号码，要 快速、准确 判断这些电话号码是否已经存在?  通过数据库查询：实现快速有点难。假设在mysql中，写个in？肯定跑飞了；循环？也肯定快不起来 数据预放在集合中：50亿*8字节(long型)≈40GB，内存浪费或不够。比如java集合？jvm一般也不可能开这么大。hbase？可能会开一些比较大的堆栈，但为了这么一个小功能也太过于浪费 hyperloglog：准确有点难。很好，但是违背准确性   类似问题很多  垃圾邮件过滤 文字处理软件(例如word )错误单词检测 网络爬虫重复ur|检测 Hbase行过滤   布隆过滤器：1970年伯顿.布隆提出,用很小的空间,解决上述类似问题  实现原理：一个很长的二进制向量和若干个哈希函数  内容  二进制向量：比如000000000000000000000000000000000000000000000 哈希函数：如F1、F2、\u0026hellip;Fn 过滤的内容：如 p=185xxxxxxxx。   构建过程  p通过F1进行hash，假设落在第3位，则将第3位变成1，001000000000000000000000000000000000000000000 p通过F2进行hash，假设落在第9位，则将第9位变成1，001000001000000000000000000000000000000000000 \u0026hellip;一直到Fn全部执行完         布隆过滤器原理  构建  参数  m个二进制向量：00000\u0026hellip;0，m个初始的0 k个hash函数：一般8个即可 n个预备数据：比如那5E个电话号码   过程  n个预备数据全部走一遍上面过程  如果m值较小，比如只有10个0，而n是50，就算只有少量hash函数，这个二进制向量也可能直接全部变为1了，所以二进制向量的1的密度跟m和n比例是很有关系的 当然和hash函数的个数也有关系，不过hash函数个数越多对判断的提高是有帮助的       判断元素存在：让元素走一遍同样的过程：如果得到的位置在二进制向量上都是1，则在表示存在，反之表示不存在  布隆过滤器误差率   误差率：肯定存在误差，对的肯定是对的，错的可能是对的，错的也可能恰好都命中了\n  只管因素：m/n的比率，hash函数的个数\n  实际误差率公式\n  1个元素，1个hash函数，任意一个比特为1的概率为1/m，依然为0的概率为1 - 1/m\n  k个函数， 任意一个比特依然为0的概率为 $$ (1-1/m)^k $$ n个元素，任意一个比特依然为0的概率为(1-M)nk $$ (1-1/m)^{kn} $$\n  任意一个比特位被设置为1的概率 $$ 1-(1-1/m)^{kn} $$\n  新元素全中的概率为 $$ (1-(1-1/m)^{kn})^k≈(1-e^{-kn/m})k $$\n    m/n与误差率成反比，k与误差率成反比\n     本地布隆过滤器  现有库：guava，https://github.com/google/guava 本地布隆过滤器的问题  容量受限制：受限于容器，比如jvm，或者tomcat（也是jvm）等web容器。在n的体量较大、或者期望的误差率较低，还是会受限于容量，单机也会成为限制 多个应用存在多个布隆过滤器，构建同步复杂：多个应用都需要在本地构建布隆过滤器，它们之间会产生布隆过滤器同步问题，而前端发来的请求可能通过一些负载均衡是落在不同的应用上的，布隆过滤器的范围类似于session，无法跨应用（container），需要去实现同步，比如session集中存储，就采用一个独立的session，让所有的container去访问它，或者笨一点让该用户的请求一定要打在之前的容器上，但这违背了负载均衡 使用redis来实现布隆过滤器的集中存储    Redis单机布隆过滤器  基于位图：布隆过滤器的二进制向量天然吻合redis位图，redis位图提供了setbit、getbit等功能 实现  定义布隆过滤器构造参数：m、n、k、误差概率 定义布隆过滤器操作函数：add和contain 封装Redis位图操作 开发测试样例    BloomFilter package com.yuanya.bloomFilter;\rimport java.util.List;\rimport java.util.Map;\r/**\r* 布隆过滤器接口\r*\r* @param \u0026lt;T\u0026gt;\r* @author yuanya\r* 2017年12月23日 下午10:03:12\r*/\rpublic interface BloomFilter\u0026lt;T\u0026gt; {\r/**\r* 添加\r*\r* @param object\r* @return 是否添加成功\r*/\rboolean add(T object);\r/**\r* 批量添加\r*\r* @param objectList\r* @return\r*/\rMap\u0026lt;T, Boolean\u0026gt; batchAdd(List\u0026lt;T\u0026gt; objectList);\r/**\r* 是否包含\r*\r* @param object\r*/\rboolean contains(T object);\r/**\r* 批量是否包含\r*\r* @param object\r*/\rMap\u0026lt;T, Boolean\u0026gt; batchContains(T object);\r/**\r* 预期插入数量\r*/\rlong getExpectedInsertions();\r/**\r* 预期错误概率\r*/\rdouble getFalseProbability();\r/**\r* 布隆过滤器总长度\r*/\rlong getSize();\r/**\r* hash函数迭代次数\r*/\rint getHashIterations();\r/**\r* 获取子布隆过滤器个数\r*/\rint getChildNum();\r}\r RedisBloomFilter /**\r* 布隆过滤器接口\r* * @param \u0026lt;T\u0026gt;\r* @author yuanya\r* 2019年4月15日 下午10:06:54\r*/\rpublic class RedisBloomFilter\u0026lt;T\u0026gt; implements BloomFilter\u0026lt;T\u0026gt; {\rprivate Logger logger = LoggerFactory.getLogger(RedisBloomFilter.class);\rprivate BloomFilterBuilder config;\rpublic RedisBloomFilter(BloomFilterBuilder bloomFilterBuilder) {\rthis.config = bloomFilterBuilder;\r}\rpublic boolean add(T object) {\rif (object == null) {\rreturn false;\r}\r//偏移量列表\rList\u0026lt;Integer\u0026gt; offsetList = this.hash(object);\rif (offsetList == null || offsetList.isEmpty()) {\rreturn false;\r}\r//设置偏移量到二进制向量位图\rfor (Integer offset : offsetList) {\rJedis jedis = null; try {\rjedis = getJedisPool().getResource();\rjedis.setbit(getName(), offset, true);\r} catch (Exception e) {\rlogger.error(e.getMessage(), e);\rreturn false;\r} finally {\rif (jedis != null) {\rjedis.close();\r}\r}\r}\rreturn true;\r}\rpublic Map\u0026lt;T, Boolean\u0026gt; batchAdd(List\u0026lt;T\u0026gt; objectList) {\rif (objectList == null || objectList.isEmpty()) {\rreturn Collections.emptyMap();\r}\rMap\u0026lt;T, Boolean\u0026gt; resultMap = new HashMap\u0026lt;T, Boolean\u0026gt;();\rfor (T object : objectList) {\rboolean result = this.add(object);\rresultMap.put(object, result);\r}\rreturn resultMap;\r}\rpublic boolean contains(T object) {\rif (object == null) {\rreturn false;\r}\r//偏移量列表\rList\u0026lt;Integer\u0026gt; offsetList = hash(object);\rif (offsetList == null || offsetList.isEmpty()) {\rreturn false;\r}\rfor (int offset : offsetList) {\rJedis jedis = null;\rtry {\rjedis = getJedisPool().getResource();\rboolean result = jedis.getbit(getName(), offset);\rif (!result) { //如果是0，即没有在列表中\rreturn false;\r}\r} catch (Exception e) {\rlogger.error(e.getMessage(), e);\r} finally {\rif (jedis != null) {\rjedis.close();\r}\r}\r}\rreturn true; //所有位都是1则返回true\r}\rpublic Map\u0026lt;T, Boolean\u0026gt; batchContains(T object) {\rreturn null;\r}\rpublic BloomFilterBuilder getConfig() {\rreturn config;\r}\rpublic long getExpectedInsertions() {\rreturn getConfig().getExpectedInsertions();\r}\rpublic double getFalseProbability() {\rreturn getConfig().getFalseProbability();\r}\rpublic long getSize() {\rreturn getConfig().getTotalSize();\r}\rpublic int getHashIterations() {\rreturn getConfig().getHashIterations();\r}\rpublic int getChildNum() {\rreturn getConfig().getChildNum();\r}\rpublic String getName() {\rreturn getConfig().getName();\r}\rpublic JedisPool getJedisPool() {\rreturn getConfig().getJedisPool();\r}\rpublic HashFunction getHashFunction() {\rreturn getConfig().getHashFunction();\r}\rpublic List\u0026lt;Integer\u0026gt; hash(Object object) {\rbyte[] bytes = object.toString().getBytes();\rreturn getHashFunction().hash(bytes, (int) getSize(), getConfig().getHashIterations());\r}\r}\r BloomFilterBuilder /**\r* 布隆过滤器构造器\r*\r* @author yuanya\r* 2019年4月15日 下午10:05:57\r*/\rpublic class BloomFilterBuilder {\rprivate Logger logger = LoggerFactory.getLogger(RedisBloomFilter.class);\rprivate static final long MAX_SIZE = Integer.MAX_VALUE * 100L;\r/**\r* 需要的JedisPool\r*/\rprivate JedisPool jedisPool;\r/**\r* 需要的JedisCluster\r*/\r/**\r* 布隆过滤器名(位图的key)\r*/\rprivate String name;\r/**\r* 位图总长度\r*/\rprivate long totalSize;\r/**\r* hash函数循环次数\r*/\rprivate int hashIterations;\r/**\r* 预期插入条数\r*/\rprivate long expectedInsertions;\r/**\r* 预期错误概率\r*/\rprivate double falseProbability;\r/**\r* 子布隆过滤器个数\r*/\rprivate int childNum;\r/**\r* 子布隆过滤器m。根据需求设置即可\r*/\rprivate int childMaxSize = 1000000000;\r/**\r* hash函数:默认murmur3\r* 自己随便找一个hash也都行\r*/\rprivate HashFunction hashFunction = new MurMur3HashFunction();\r/**\r* 是否完成\r*/\rprivate boolean done = false;\rpublic BloomFilterBuilder(JedisPool jedisPool, String name, long expectedInsertions, double falseProbability) {\rthis.jedisPool = jedisPool;\rthis.name = name;\rthis.expectedInsertions = expectedInsertions;\rthis.falseProbability = falseProbability;\r}\rpublic BloomFilterBuilder setHashFunction(HashFunction hashFunction) {\rthis.hashFunction = hashFunction;\rreturn this;\r}\rpublic \u0026lt;T\u0026gt; RedisBloomFilter\u0026lt;T\u0026gt; build() {\rcheckBloomFilterParam();\rreturn new RedisBloomFilter\u0026lt;T\u0026gt;(this);\r}\r/**\r* 检查布隆过滤器参数\r*/\rprivate void checkBloomFilterParam() {\rif (done) {\rreturn;\r}\rif (name == null || \u0026quot;\u0026quot;.equals(name.trim())) {\rthrow new\rIllegalArgumentException(\u0026quot;Bloom filter name is empty\u0026quot;);\r}\rif (expectedInsertions \u0026lt; 0 || expectedInsertions \u0026gt; MAX_SIZE) {\rthrow new\rIllegalArgumentException(\r\u0026quot;Bloom filter expectedInsertions can't be greater than \u0026quot; + MAX_SIZE + \u0026quot; or smaller than 0\u0026quot;);\r}\rif (falseProbability \u0026gt; 1) {\rthrow new IllegalArgumentException(\u0026quot;Bloom filter false probability can't be greater than 1\u0026quot;);\r}\rif (falseProbability \u0026lt; 0) {\rthrow new IllegalArgumentException(\u0026quot;Bloom filter false probability can't be negative\u0026quot;);\r}\r//计算布隆过滤器(位图)长度\rtotalSize = optimalNumOfBits();\rlogger.info(\u0026quot;{} optimalNumOfBits is {}\u0026quot;, name, totalSize);\rif (totalSize == 0) {\rthrow new IllegalArgumentException(\r\u0026quot;Bloom filter calculated totalSize is \u0026quot; + totalSize);\r}\rif (totalSize \u0026gt; MAX_SIZE) {\rthrow new IllegalArgumentException(\u0026quot;Bloom filter totalSize can't be greater than \u0026quot; + MAX_SIZE\r+ \u0026quot;But calculated totalSize is\u0026quot; + totalSize);\r}\r// hash函数迭代次数\rhashIterations = optimalNumOfHashFunctions();\rlogger.info(\u0026quot;{} hashIterations is {}\u0026quot;, name, hashIterations);\rchildNum = (int) (totalSize / childMaxSize + 1);\rdone = true;\r}\r/**\r* 根据预期插入条数和概率计算布隆过滤器(位图)长度\r*/\rprivate long optimalNumOfBits() {\rif (falseProbability == 0) {\rfalseProbability = Double.MIN_VALUE;\r}\rreturn (long) (-expectedInsertions * Math.log(falseProbability) / (Math.log(2) * Math.log(2)));\r}\r/**\r* 根据布隆过滤器长度与预期插入长度之比，计算hash函数个数\r*/\rprivate int optimalNumOfHashFunctions() {\rreturn Math.max(1, (int) Math.round((double) totalSize / expectedInsertions * Math.log(2)));\r}\rpublic Logger getLogger() {\rreturn logger;\r}\rpublic static long getMaxSize() {\rreturn MAX_SIZE;\r}\rpublic JedisPool getJedisPool() {\rreturn jedisPool;\r}\rpublic String getName() {\rreturn name;\r}\rpublic long getTotalSize() {\rreturn totalSize;\r}\rpublic int getHashIterations() {\rreturn hashIterations;\r}\rpublic long getExpectedInsertions() {\rreturn expectedInsertions;\r}\rpublic double getFalseProbability() {\rreturn falseProbability;\r}\rpublic int getChildNum() {\rreturn childNum;\r}\rpublic int getChildMaxSize() {\rreturn childMaxSize;\r}\rpublic HashFunction getHashFunction() {\rreturn hashFunction;\r}\rpublic boolean isDone() {\rreturn done;\r}\r}\r 存在的问题  速度慢：比本地慢，输在网络  解决：单独部署，与应用同机房甚至机架部署   容量受限：Redis最大字符串为512MB，通过拆分成多个子串解决，但仍然收Redis单机容量限制  解决：基于Redis Cluster实现    Redis分布式布隆过滤器  实现方法：基于单机改改即可  个布隆过滤器：二次路由。输入参数不变，m比如100E，预定好多少个布隆过滤器，比如一个布隆过滤器最大是1E，即需要100份，设计的时候对key做一个二次路由即可 基于pipeline提高效率：不然分布式布隆过滤器性能下降还是会比较厉害    主从   单机存在的问题：机器故障（分布式问题）、容量瓶颈（分布式问题）、QPS瓶颈（高可用问题）\n  主从复制：一个master可以有多个slave。主复制到从，类似于数据备份的作用，多副本，高可用，读写分离，读分流负载均衡\n 一个master可以有多个slave 一个slave只能有一-个master 数据流向是单向的, master到slave slave要保证只读：因为主从复制只能从主到从，从如果写入数据，主不会知道。且主节点宕机，由某个从节点补上，要保证数据一致 作用  数据副本；做rdb也可以在从节点做，减轻master负载 扩展读性能      配置\n  命令：无需重启，但不便于管理\n slaveofs：在从节点执行 slaveof 主节点ip port。是异步的 slaveof no one：不成为任何节点的从节点，即辞掉从节点地位，与主节点断开连接。如果再次成为其它节点的slave，主节点会清除掉从节点所有旧的数据    配置：需要重启，但可以统一配置管理\nslaveof ip port #在从节点配置的，指向主节点的ip和port\rslave-read-only yes #从节点只读\r     操作\n  查看节点状态，在成为从节点之前每个节点都是master\nsys\u0026gt; redis-cli info #查看所有信息\rsys\u0026gt; redis-cli -p 6380 info replication #在客户端外查看\r127.0.0.1:6379\u0026gt; info replication #客户端内查看\r     全量复制和部分复制\n  run_id：每个redis启动都会有一个run_id作为标识。例子，如果从节点发现主节点run_id发生了变化（如主节点重启，或者从节点第一次连接主节点，即第一次获得主节点的run_id标识），这种变化可以引起从节点全量复制主节点数据\nredis-cli -p 6379 info server | grep run #查看run_id\r   偏移量：是主从之间实时同步的一个指标，如果主从之间偏移量差值过大，可能是主从同步复制发生了问题\nredis-cli -p 6379 info replication #其中master_repl_offset:1865\r   全量复制：master将当前状态RDB文件同步给slave，在同步期间产生的新数据也将被记录，通过对比偏移量，再同步给slave\n  流程\n 从节点发送命令 psync ? 1 给主节点：psync（2.8之前是sync）可以完成全量复制和部分复制的功能。参数1是run_id，参数2是偏移量，首次复制还不知道主节点的run_id和自己的偏移量是多少，run_id以?占位，偏移量以-1 master收到命令，从参数可见从节点都不知道run_id和偏移量，肯定就全量复制了，并会返回run_id和offset slave保存mster的信息 run_id、offset mster执行bgsave，主从复制期间，master中新增的命令被保存在repl_back_buffer（用于记录最新写入的命令）中 master向slave send RDB master向slave send buffer 从节点flush old data（删除原来的数据） 从节点加载RDB和buffer    开销\n bgsave时间 RDB文件网络传输时间 从节点清空数据时间 从节点加载RDB的时间 可能的AOF重写时间：如果开启了AOF将进行一次AOF重写，保证AOF是最新的状态    问题：如果网络主从之间网络抖动，从节点无法及时同步数据：2.8以前，会直接再进行一次全量复制，会消耗较大资源；2.8之后\n master在会写命令到复制缓冲区repl_back_buffer（默认1mb） 当slave再次连上master时，slave向mster发送pysnc {offset} {runId} mster对比偏移量，如果发现slave错过的数据还在缓冲区记录范围内 master发送continue表示继续部分复制 然后send partial data      故障处理\n 自动故障转移：当一个机器挂掉了，另一台机器自动顶上，之后再对故障机器或服务进行处理，保证高可用 主从结构故障转移：假设一主（读写）二从（只读）  slave宕掉：宕一个，如果另一个从节点有足够性能冗余，将宕掉从节点的客户端改到另一个上即可 master宕掉：从节点仍可以正常服务，然后发送命令slaveof no one让其中一个成为master，让另一个成为新mster的从节点slaveof new master 但是都没有实现自动故障转移，等待redis-sentinel出场吧      常见问题\n 读写分离：读流量分摊到从节点  可能遇到问题：  复制数据延迟 读到过期数据：删除过期数据有两种策略  lazy：去操作key的时候才判断该key是否过期，过期则返回空 定时任务：每次去采样一些key，看它是否过期，但是如果采样速率低于数据产生速率，可能会发生很多过期数据没有删除，然后在主从复制中，slave没有删除数据的资格，如果mster没有及时将删除命令同步给slave时，slave就可能读到脏（过期）数据，但是redis3.2已经解决了这个问题   从节点故障，需要迁移其客户端到其它从节点进行数据读取，如果很多应用使用这个节点，迁移成本就很高了     主从配置不一致  例如maxmemory不一致：丢失数据。比如mster配置4g、slave配置2g，主从复制可以正常进行，mster传输RDB，slave加载RDB，但这就可能RDB过大，触发从节点maxmemory的触发策略，将数据进行淘汰（这个过程有可能产生OOM），如果触发策略是以剔除过期数据优先的策略，就会剔除过期数据，对于数据这就已经没有真正实现副本的功能了，也不会报任何错误，如果主节点再宕机，从节点顶替为主节点，从节点无法挽回那些剔除的数据，数据丢失。比较好的办法是使用一些标准工具安装上对其做监控 例如数据结构优化参数：内存不一致。例如主节点设置hash-max-ziplist-entries优化值，从节点没优化，就会造成主从节点内存不一致   规避全量复制  第一次全量复制：第一次不可避免  小主节点：数据分片，maxmemory不要设置太大，这样bgsave、传输、加载、fork等速度都会更快，开销相对更小 低峰：在访问量较低的时候（如夜间）进行   节点运行ID不匹配：主节点重启run_id改变，主从节点记录的master run_id不一致，会认为数据不安全，将全量复制  redis4.0中提供了psyncto的规则可以有效解决这类问题 故障转移，例如哨兵或集群，让从节点顶替成为新的master   复制积压缓冲区（repl_back_buffer，实际上是一个队列，默认大小1mb）不足：网络中断，部分复制可能无法满足  增大复制缓冲区配置rel_ backlog_ size，根据实际情况设置，假如网络故障一般是几分钟，就根据统计每分钟传输字节数*分钟数来计算大小 网络\u0026quot;增强\u0026rdquo;     规避复制风暴：感觉最好的办法还是直接让slave顶替为master这种高可用的方式，后面讲，这里讲重启方式  单主节点复制风暴，如果一个mster上挂载了很多slave，如果master宕掉重启，所有slave都要做主从复制，虽然redis有优化，master只生成一次rdb，但要做多次传输，对master开销极大  更换复制拓扑：树形。不是所有slave都挂在master上，可以master上挂一个slave1，这个slave上再挂载其它几个salve，这样就将传输压力分担到了一个slave上。但是这会产生新的问题，如果slave1故障了\u0026hellip;如何处理故障或者如何故障转移   单机器（机器上很多mster）复制风暴：机器宕机后，大量全量复制  主节点分散多机器        RedisSentinel   主从复制高可用？：主从复制存在问题\n 手动故障转移：虽然也可以用脚本监控master，出现问题就让slave顶替上来，但这就很复杂，比如怎么判定master是有问题的，怎么通知客户端更改为服务端为新的master，还有保证事务，所以有了Redis Sentinel这样高可用的实现服务 来完成这些事情 写能力和存储能力受限    架构说明\n client从sentinel获取redis信息，即 sentinel挡在redis集群前面，监控其后每一个mster和slave。sentinel也是多个的，且一套sentinel可以同时监控多套mster及其slave，每套mster及其slave将以master-name这个配置作为标识    故障自动转移：即sentinel将作为redis客户端来自动进行操作\n 多个sentinel发现并确认master有问题 选举出一个sentinel作为领导 选出一个slave作为master 通知其余slave成为新的master的slave 通知客户端主从变化 等待老的master复活成为新master的slave：依然会对老的mster进行监控（死的时候就已经配置为slave了，复活直接复制master的数据）    安装配置\n  配置开启主从节点\nport 7002\rdaemonize yes\rpidfile /var/run/redis-7002.pid\rlogfile \u0026quot;7002. log\u0026quot;\rdir \u0026quot;/opt/soft/redis/redis/data/\u0026quot;\rslaveof 127 .0.0.1 7000 #主节点不用配置\r   配置开启sentinel监控主节点。(redis-sentinel是特殊的redis，不存储数据，支持的redis命令非常有限，主要功能就完成监控、故障转移、通知)\ndaemonize yes #以守护进程方式启动\rport ${port} #默认26379\rdir \u0026quot;/opt/soft/redis/data/\u0026quot;\rlogfile \u0026quot;${port}.log'\rsentinel monitor mymaster 127.0.0.1 7000 2 #监控的 主节点名字、ip、端口，2表示至少几个sentinel发现该出master出现问题才会进行故障转移 sentinel down-after-milliseconds mymaster 30000 #故障判定时间，30000毫秒无法连通则判定master为故障\rsentinel parallel-syncs mymaster 1 #故障转移后其它slave会对新master进行复制，这个配置指定同时复制的个数，1表示每次只复制一个，减轻master压力\rsentinel failover-timeout mymaster 180000 #故障转移超时时间\r#可以发现只有master的配置，因为sentinel通过对master执行info并解析就可以获取对应从节点信息并自动添加配置，以及会去掉一些与默认配置相同的配置以及添加一些配置\r   多个sentinel实际应该多机器，演示仅一台机器\nredis-sentinel redis-sentinel-26379.conf #启动\rredis-cli -p 26379 #连接\rping\rinfo #命令和redis一样，只是支持的命令更少\rsed \u0026quot;s/26379/ 26380/g\u0026quot; redis-sentinel-26379.conf \u0026gt; redis-sentinel-26380.conf #sed命令偷懒copy出26380和26381即可\r     客户端连接：现在要从直连redis换为连接sentinel，这样才能达到客户端也高可用\n  过程：client会获取到Sentinel节点集合 + masterName，遍历Sentinel节点集合，获取一个可用的Sentinel节点，将发送命令sentinel get-master-addr-by-name masterName，然后sentinel返回mster节点信息，获取到mster执行role 或者role replication以进行验证，然后master会返回节点角色信息\n  如何通知：类似于发布订阅模式，客户端订阅某一个频道，假如该频道master变化，客户端将收到通知，与新的master进行连接\n  接入流程：Sentinel地址集合，masterName，不是代理模式而是发布订阅模式\n//不管jedis还是其它实现模式都是一样的，只是封装程度不同\rString masterName =\u0026quot; mymaster\u0026quot; ;\rSet\u0026lt;String\u0026gt; sentinels = new HashSet \u0026lt;String\u0026gt;();\rsentinels. add(\u0026quot;127.0.0.1:26379\u0026quot;);\rsentinels. add(\u0026quot;127.0.0.1:26380\u0026quot;);\rsentinels. add(\u0026quot;127.0.0.1:26381\u0026quot;);\rJedisSentinelPool sentinelPool = new JedisSentinelPool(masterName, SentinelSet, poolConfig, timeout); //本质还是连接master的\rJedis jedis = null;\rtry{\rjedis = redisSentinelPool.getResource();\r//jejedis command\r} catch (Exception e) {\rlogger.error(e.getMessage(, e);\r} finally {\rif (jedis != null){\rjedis.close();\r}\r}4\r     实现原理：\n 故障转移演练：在java程序中循环执行jedis命令，然后将master shutdown  客户端高可用观察：mster宕掉以后，程序开始报错，但是一会儿之后，恢复了正常 服务端日志分析：数据节点和sentinel节点  主节点7000：因为直接kill掉了，日志停止在kill掉的瞬间 从节点7001日志：首先与master失联（不断尝试连接master并失败），收到user request（可以肯定是来自sentinel）希望让它自己成为master，并进行了配置重启，然后发现7002要复制它的数据（说明已经被选为master了），然后开始bgsave，然后去完成复制的过程 从节点7002日志首：首先和7001一样与master失联，然后收到user request希望它slave of 7001，然后复制7001数据的过程 sentinel-26379：（日志量很大，只是大概过程）先有sdown master mymaster 127.0.0.1 7000表示该master下线了（一个人的意见），后有odown\u0026hellip;表示sentinel达成条件（多个人的意见，具体是多少，根据前面的配置）认为它该做下线了。然后是sentinel的选举 +vote-for-leader 尝试去做领导者，然后记录了26380、26381都投票给它，然后选择7001成为master，选择7001成为slave，然后还是用odown\u0026hellip;对7000做下线标识，然后正式标识新的master和slave（应该是会自动更变配置内容）        三个定时任务：为了对redis做失败判定、故障转移，redis sentinel有3个定时任务来作为实现这些过程的基础\n 每10秒每个sentinel对master和slave执行info  发现slave节点 确认主从关系   每2秒每个sentinel通过master节点的channel交换信息(pub/sub)  通过sentinel_ :hello频道交互，类似于发布订阅，准确一点大概是一个广播网络 交互对节点的\u0026quot;看法”和自身信息   每1秒每个sentine|对其他sentinel和redis执行ping    主观下线和客观下线\n#通过命令配置，前面配置文件已经见过了\rsentinel monitor \u0026lt;masterName\u0026gt; \u0026lt;ip\u0026gt; \u0026lt;port\u0026gt; \u0026lt;quorum\u0026gt;\rsentinel monitor myMaster 127.0.0.1 6379 2 #一般配置ceil(n/2)，n是sentinel节点总数（集群一般是单数）\rsentinel down-after-milliseconds \u0026lt; masterName\u0026gt; \u0026lt; timeout\u0026gt;\rsentinel down-after-milliseconds mymaster 30000\r  主观下线：每个sentinel节点对Redis节点失败的\u0026quot;偏见\u0026rdquo;。slave被主观下线可以直接让其下线，如果是对master主观下线，则对其它所有节点发出命令sentinel is-master-down-by-addr 客观下线：所有sentinel节点对Redis节点失败\u0026quot;达成共识”( 超过quorum个统一)，达成共识的交互手段就是通过命令sentinel is-master-down-by-addr，然后选举出leader控制mster进行客观下线    leader选举\n 原因：只一个sentinel节点来完成故障转移 选举：通过sentinel is-master-down-by-addr命令，是的，这个命令也具有自荐为leader的作用  每个做主观下线的Sentinel节点向其他Sentinel节点发送命令，要求将它设置为领导者 收到命令的Sentinel节点如果没有同意通过其他Sentinel节点发送的命令，那么将同意该请求，否则拒绝 如果该Sentinel节点发现自己的票数已经超过Sentinel集合半数且超过quorum，那么它将成为领导者。 如果此过程有多个Sentinel节点成为了领导者，那么将等待一段时间重 新进行选举 虽然可能某个短时间内几乎同时有多个节点发出命令，但是同意总有先后      故障转移：前提是sentinel leader节点选举完成\n 从slave节点中选出一一个“合适的”节点作为新的master节点  选择slave-priority(slave节点优先级，一般没配置)最高的slave节点，如果存在则返回,不存在则继续 选择复制偏移量最大的slave节点(复制的最完整) ,如果存在则返回，不存在则继续 选择runId最小的slave节点   对上面的slave节点执行slaveof no one命令让其成为master节点。 向剩余的slave节点发送命令, 让它们成为新master节点的slave节点,复制规则和parallel-syncs参数有关。 更新对原来master节点配置为slave ,并保持着对其\u0026quot;关注\u0026rdquo;，当其恢复后命令它去复制新的master节点。    常见开发运维问题\n 节点运维  节点下线  原因  机器下线：例如过保等情况 机器性能不足：例如CPU、内存、硬盘、网络等 节点自身故障：例如服务不稳定等   主节点：sentinel failover ，手动在某个sentinel上执行命令让该sentinel执行指定master的故障转移 从节点：考虑临时下线还是永久下线，例如是否做一些清理工作（配置、日志、数据、RDB文件等）。还要考虑读写分离的情况，如转移连接该节点的客户端到其它节点等。   节点上线  主节点：sentinel failover  进行替换。 从节点：slaveof即可, sentinel节点可以感知。 sentinel节点：参考其他sentinel节点启动即可     高可用读写分离  JedisSentinelPool的实现：客户端高可用  从构造方法进入，先初始化sentinels，初始化和前面讲过的客户端原理一致，循环sentinel集合，对sentinel发出命令让其根据mastername获取master地址来检验连接，直到找到一个可用的sentinel则跳出循环。 然后去订阅频道，MasterListener（继承了Thread）来监听，连接每一个sentinel，订阅master更变的通道，如果mster更变将重新初始化连接池，完成客户端高可用   从节点的作用  副本:高可用的基础 扩展:读能力   三个\u0026quot;消息\u0026rdquo;：可以把所有slave看作一个池子，客户端通过监听三个\u0026quot;消息（通道）\u0026quot;，监听到变化则重新初始化redis连接池，完成高可用读写分离  switch-master：切换主节点(从节点晋升主节点) convert-to-slave：切换从节点(原主节点降为从节点) sdown：主观下线   高可用读写分离相对复杂，一般来说真正需要高扩展性的高可用redis集群，都通过redis-cluster（redis集群版本）来完成    总结 Redis Sentinel是Redis的高可用实现方案：故障发现、故障自动转移、配置中心、客户端通知\nRedis Sentinel从Redis2.8版本开始才正式生产可用，之前版本生产不可用\n尽可能在不同物理机上部署Redis Sentinel所有节点，建议放在一个网络中，减少误差\nRedis Sentinel中的Sentinel节点个数应该为大于等于3，且最好为奇数。\nRedis Sentinel中的数据节点与普通数据节点没有区别\n客户端初始化时连接的是Sentinel节点集合，不再是具体的Redis节点，但Sentinel只是配置中心不是代理。\nRedis Sentinel通过三个定时任务实现了Sentinel节点对于主节点、从节点、%其余Sentinel节点的监控。\nRedis Sentinel在对节点做失败判定时分为主观下线和客观下线\n看懂Redis Sentinel故障转移日志对于Redis Sentinel以及问题排查非常有帮助\nRedis Sentinel实现读写分离高可用可以依赖Sentinel节点的消息通知，获取Redis数据节点的状态变化\nRedisCluster  呼唤集群 数据分布 搭建集群 集群伸缩 客户端路由 集群原理 开发运维常见问题  呼唤集群  为什么呼唤集群  1.并发量。redis宣称：10万/每秒。业务需要100万/每秒呢? 2.数据量。机器内存: 16~256G。业务需要500G呢? 3.网络流量。网卡：千兆网卡。业务需要万兆呢   解决方法：  配置\u0026quot;强悍”的机器:超大内存、牛x CPU等，但单机上限远远比不上集群 分布式集群，才是正确的解决方法   集群：规模化需求  并发量: OPS 数据量: \u0026ldquo;大数据 网络流量    数据分布   分布式数据库-数据分区：全量数据→分区规则→子集-1、子集-2\u0026hellip;子集-n\n   分布方式 特点 典型产品     哈希分布 数据分散度高键值分布业务无关无法顺序访问支持批量操作 一致性哈希MemcacheRedis Cluster其他缓存产品   顺序分布 数据分散度易倾斜键值业务相关可顺序访问支持批量操作 BigTableHBase     分序分布（区）：1~100→1~33、34~ 66、67~ 100 哈希分布（例如节点取模）：1~100→hash(key)%3→(3,6\u0026hellip;99 )、(1,4\u0026hellip;100)、( 2,5.. .98)  节点取余：hash(key)%nodes。nodes只节点数。  扩容：扩展节点时对所有数据重新计算，并根据结果将其迁移到应该去的机器（如果结果不属于原来的机器的话）。好像是。。。。。：数据迁移并不是从一个节点直接到另一个节点，而是在计算以后如果应该到新的节点上取，发现没有数据，则会到数据源取，然后再放入新节点，这样就完成了\u0026quot;迁移\u0026rdquo;，其实是一个访问数据源回写的过程。  比如从3个节点到4个节点，会发现数据的迁移率（从原来的节点迁移的另一个节点的数据的比例）较高（计算为80%）。 多倍扩容：3个节点变为6个节点，则只有50%迁移率   建议  客户端分片：哈希+取余，简单、原始，迁移率较大会产生庞大的数据源访问和数据回写到内存，不建议使用 节点伸缩：数据节点关系变化,导致数据迁移 迁移数量和添加节点数量有关：建议翻倍扩容     一致性哈希：见图  客户端分片：哈希+顺时针(优化取余) 节点伸缩：只影响邻近节点，但是还是有数据\u0026quot;迁移\u0026rdquo; 翻倍伸缩：保证最小迁移数据和负载均衡（因为访问量大的可能一直只是某些段，比如老用户访问少且id较小，新用户访问多而id都再较大数字的段）   虚拟槽：Redis Cluster就是这种分区方式。不同的操由不同的节点管理，计算key得到value将知道属于哪个槽，然后任意分配给集群的一个节点，如果不属于该节点负责的槽，则由该节点返回目标节点给客户端（因为edis cluster中节点之间都有槽信息共享），让客户端去到正确的节点，但这种方式性能不高，节点较多时利用率较低，所以需要让客户端知道所有槽-节点。每个节点负责的槽是固定的，新节点将负责新的槽，不会有数据迁移  预设虚拟槽:每个槽映射一个数据子集, 一般比节点数大 良好的哈希函数:例如CRC16 服务端管理节点、槽、数据:例如Redis Cluster        搭建集群   基本架构\n 单机架构：主从复制，只有主可以写 分布式架构：都可以读写    Redis Cluster架构\n 节点：配置cluster-enabled:yes，是否是集群模式启动 meet：一个命令。节点之间完成通信的基础。A meet B、A meet C，就可以使B和C找到对方，这样就能完成所有节点的相互通信 指派槽：redis cluster指定有16384个slot， 复制：也有主从复制，每个主节点都有从节点，集群中有很多主节点。且无需redis sentinel 高可用 分片    安装\n  原生命令安装：就是按上面架构流程。无需记住，仅用来理解架构，实际生产基本还是使用安装工具安装\n  配置开启节点\nport ${port}\rdaemonize yes\rdir \u0026quot;/opt/redis/redis/data/\u0026quot;\rdbfilename \u0026quot;dump-${port}.rdb\u0026quot;\rlogfile \u0026quot;${port}.log\u0026quot;\rcluster-enabled yes #集群模式启动\r#下面是集群主要配置\rcluster-enabled yes\rcluster-node-timeout 1 5000\rcluster-config-file \u0026quot;nodes.conf\u0026quot;\rcluster-require-full-coverage yes #是否需要集群内所有节点都能提供服务才能认为该集群是正确的，即有一个节点出现问题集群就不可用，默认yes，肯定要改为no\r redis-server redis-7000.conf\rredis-server redis-7001.conf\rredis-server redis-7002.conf\rredis-server redis-7003.conf\rredis-server redis-7004.conf\rredis-server redis-7005.conf\r   meet：cluster meet ip port\nredis-cli -h 127.0.0.1 -p 7000 cluster meet 127.0.0.1 7001\rredis-cli -h 127.0.0.1 -p 7000 cluster meet 127.0.0.1 7002\rredis-cli -h 127.0.0.1 -p 7000 cluster meet 127.0.0.1 7003\rredis-cli -h 127.0.0.1 -p 7000 cluster meet 127.0.0.1 7004\rredis-cli -h 127.0.0.1 -p 7000 cluster meet 127.0.0.1 7005\r   指派槽：cluster addslots slot [slot ..]\nredis-cli -h 127.0.0.1 -p 7000 cluster addslots {0...5461}\rredis-cli -h 127.0.0.1 -p 7001 cluster addslots {5462...10922}\rredis-cli -h 127.0.0.1 -p 7002 cluster addslots {10923...16383}\r   主从：cluster replicate node-id\nredis-cli -h 127.0.0.1 -p 7003 cluster replicate ${node-id-7000}\rredis-cli -h 127.0.0.1 -p 7004 cluster replicate ${node-id-7001}\rredis-cli -h 127.0.0.1 -p 7005 cluster replicate ${node-id-7002}\r   操作\n配置文件\nport 7000\rdaemonize yes\rdir \u0026quot;/opt/soft/redis/data\u0026quot;\rlogfile \u0026quot;7000. log\u0026quot;\rdbfilename \u0026quot;dump-7000.rdb\u0026quot;\rcluster-enabled yes\rcluster-config-file nodes-7000.conf\rcluster-require-full-coverage no\r 快速sed配置文件\nsed 's/7000/7001/g' redis-7000.conf \u0026gt; redis-7001.conf\rsed 's/7000/7002/g' redis-7000.conf \u0026gt; redis-7002.conf\rsed 's/7000/7003/g' redis-7000.conf \u0026gt; redis-7003.conf\rsed 's/7000/7004/g' redis-7000.conf \u0026gt; redis-7004.conf\rsed 's/7000/7005/g' redis-7000.conf \u0026gt; redis-7005.conf\r 启动和查看\nredis-server redis-7000.conf\rredis-server redis-7001.conf\rredis-server redis-7002.conf\rredis-server redis-7003.conf\rredis-server redis-7004.conf\rredis-server redis-7005.conf\rps -ef | grep redis #查看服务\rredis-cli -p 7000 cluster nodes #查看节点信息，这种查看和redis-cli -p 7000客户端先连接，后再执行cluster nodes是一个效果\rredis-cli -p 7000 cluster info #查看集群信息\r meet\nredis-cli -p 7000 cluster meet 127.0.0.1:7001\rredis-cli -p 7000 cluster meet 127.0.0.1:7002\rredis-cli -p 7000 cluster meet 127.0.0.1:7003\rredis-cli -p 7000 cluster meet 127.0.0.1:7004\rredis-cli -p 7000 cluster meet 127.0.0.1:7005\rredis-cli -p 7005 cluster info\r 分配槽：脚本\nvim addslots.sh #vim可以自动创建\r#内容\rstart=$1\rend=$2\rport=$3\rfor slot in `seq ${start} ${end}`\rdo\recho \u0026quot;slot:${slot}\u0026quot; redis-cli -p ${port} cluster addslots ${slot}\rdone\r#执行，3个主节点（现在还不是）\rsh addslots.sh 0 5461 7000\rsh addslots.sh 5461 10922 7001\rsh addslots.sh 10922 16383 7002\r 主从\nredis-cli -p 7000 cluster nodes #查看\rredis-cli -p 7003 cluster replicate 560598cc5f6b13663ee7aaa9ff403e799e7d4918 #主节点id\rredis-cli -p 7004 cluster replicate xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\rredis-cli -p 7005 cluster replicate xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\rredis-cli -p 7000 cluster nodes #查看\rredis-cli -p 7000 cluster slots #查看槽分配信息\rredis-cli -c -p 7000 #客户端连接redis cluster，可以api操作了！\r 主从可以这样设计：左主右从，错开设计，减少机器\n10.0.0.1:7000 10.0.0.2:7003 10.0.0.2:7001 10.0.0.3:7004 10.0.0.3:7002 10.0.0.1:7005\n    官方工具安装：安装工具简单、高效、准确，但是如果真的是几百上千的超大集群，最好还是需要专门的平台来管理\n  官方工具安装提供了Ruby安装脚本，需要准备Ruby环境\n  下载、编译、安装Ruby\nwget https://cache.ruby-lang.org/pub/ruby/2.3/ruby-2.3.1.tar.gz\rtar -xvf ruby-2.3.1.tar.gz\r./configure -prefix=/usr/ocal/ruby\rmake\rmake install cd /usr/local/ruby\rcp bin/ruby/usr/local/bin\r   安装rubygem redis：ruby的一个客户端\nwget http://rubygems.org/ downloads/redis-3.3.0.gem\rgem install -l redis-3.3.0.gem\rgem list --check redis gem\r     安装redis-trib.rb：官方工具安装\n#配置开启Redis\rredis-server redis-8000.conf\rredis-server redis-8001.conf\rredis-server redis-8002.conf\rredis-server redis-8003.conf\rredis-server redis-8004.conf\rredis-server redis-8005.conf\r#一键开启，前3个是主节点，后3个是按序对应的从节点，如果每个主节点需要2个从节点，写6个即可，不符合标准的话会给你返回错误的\r./redis-trib.rb create --replicas 1 127.0.0.1:8000 127.0.0.1:8001 \\127.0.0.1:8002127.0.0.1:8003 127.0.0.1:8004 127.0.0.1:8005\rcp ${REDIS_ HOME}/src/redis-trib.rb /usr/local/bin\r     可视化部署：开发专门的管理平台来进行\n    集群伸缩   伸缩原理：集群伸缩=槽和数据在节点之间的移动\n  扩容集群：主要作用是：为它迁移槽和数据实现扩容，作为从节点负责故障转移\n  手动操作\n  准备新节点\n  集群模式\n  配置和其它节点统一\n  启动后是孤儿节点\nredis-server conf/redis-6385.conf\rredis-server conf/redis-6386.conf\r     加入集群：让集群中的节点去meet这些孤立节点\n127.0.0.1:6379\u0026gt; cluster meet 127.0.0.1 6385\r127.0.0.1:6379\u0026gt; cluster meet 127.0.0.1 6386\r   迁移槽和数据\n  槽迁移计划：一般使平分16383个槽即可\n  迁移数据：非常复杂\n 对目标节点发送：cluster setslot {slot} importing {sourceNodeId}命令，让目标节点准备导入槽的数据。目标节点准备导入槽{slot} 对源节点发送：cluster setslot {slot} migrating {targetNodeId}命令，让源节点准备迁出槽的数据。通知{slot}被目标节点负责 源节点循环执行cluster getkeysinslot {slot} {count}命令，每次获取count个属于槽的健。批量迁移相关键的数据 在源节点上执行migrate {targetIp} {targetPort} key 0 {timeout}命令把指定key迁移。0是对应的数据库，不过redis cluster中只有db 0 没有其它db。获取slot下{count}个健 重复执行步骤3~4直到槽下所有的键数据迁移到目标节点。5:循环迁移键 向集群内所有主节点发送cluster setslot {slot} node {targetNodeId}命令，通知槽分配给目标节点。原节点准备导出槽{slot}  #python伪代码\rdef move_slot(source , target, slot):\r#目标节 点准备导入槽slot\rtarget.cluster(\u0026quot;setslot\u0026quot;,slot,\u0026quot;importing\u0026quot;,source.nodeID) ;\r#目标节点准备全出槽slot\rsource. cluster(\u0026quot; setslot\u0026quot;,slot,\u0026quot;migrating\u0026quot;,target.nodeId);\rwhile true :\r#批量从源节点获取键\rkeys = source. cluster(\u0026quot;getkeysinslot\u0026quot;,slot,pipeline_size);\rif keys .length == 0:\r#键列表为空时，退出循环\rbreak;\r#批量迁移键到目标节点\rsource.call( \u0026quot;migrate\u0026quot;,target.host,target.port,\u0026quot;\u0026quot;,0,timeout,\u0026quot;keys\u0026quot;,keys]);\r#向集群所有主节点通知槽slot被分配给目标节点\rfor node in nodes:\rif node.flag == \u0026quot;slave\u0026quot;:\rcontinue;\rnode. cluster(\u0026quot;setslot\u0026quot;,slot,\u0026quot;node\u0026quot;,target.nodeId);\r   添加从节点\n      官方工具redis-trib.rb操作。官方工具操作时会检测你要加入的节点是否是新节点，能够避免新节点已经加入了其他集群，造成故障。所以一般用官方工具比较好，因为如果将两个集群混合在一起了，可能造成严重后果\n  redis-trib.rb add-node new_host.new_ port existing_ host:existing. port \u0026ndash;slave \u0026ndash;master-id \n  redis-trib.rb add-node 127.0.0.1:6385 127.0.0.1:6379\n  迁移数据\nredis-trib.rb reshard 127.0.0.1: 7000 #迁移数据，7000是要找个主节点，执行过程中会提示要求更多参数，按照提示填即可\r#假如数据源选择了all，则将从7000、7001、7002中各区同等数量的一部分，最终效果则是4个主节点基本平分16383个槽\r       缩容集群：与扩容类似\n  下线迁移槽：迁移槽操作与扩容一致\nredis-trib.rb reshard --from 172d689c3ed7b3721afa71a1ca20450ad0147ebb --to 97ea959862c796988810c2a4f13ee245d318942b --slots 1366 127.0.0.1: 7006 #from 7006的id to 7000的id，迁移1366个槽\rredis-trib.rb reshard --from 172d689c3ed7b3721afa71a1ca20450ad0147ebb --to xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx --slots 1365 127.0.0.1: 7006 #from 7006的id to 7001的id，迁移1366个槽\rredis-trib.rb reshard --from 172d689c3ed7b3721afa71a1ca20450ad0147ebb --to xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx --slots 1365 127.0.0.1: 7006 #from 7006的id to 7002的id，迁移1365个槽\r   忘记节点：让其它节点忘记它。redis-cli\u0026gt; cluster forget {downNodeId}，60s有效，也就是要在60内让所有节点都忘记它，否则有一个节点还保留有它的信息，会让所有节点都记起它\n  关闭节点\n#用工具忘记和关闭一起。先下从节点，再下主节点，7007是从，7006是主\rredis-trib.rb del-node 127.0.0.1:7000 c94d20dedf3d5c365286b733ef5d6bd23c3c33eb #7007的id\rredis-trib.rb del-node 127.0.0.1:7000 xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx #7006的id\r     客户端路由   moved重定向：moved表示已经确定不在本节点了，完成式，已经迁移完毕了\n 客户端发送键命令（如set php best）给任意节点 节点根据key计算槽和对应节点（cluster keyslot php可以计算该hash值），如果指向自身就执行命令，否则回复moved异常给客户端 客户端重定向发送命令给目标节点（客户端不会自己发送，需要自己去写这个逻辑）  redis-cli -c -p 7000 #-c集群模式\r127.0.0.1:7000\u0026gt; cluster keyslot hello\r(integer) 866\r127.0.0.1:7000\u0026gt; set hello world\rOK\r127.0.0.1:7000\u0026gt; cluster keyslot php\r(integer) 9244\r127.0.0.1:7000\u0026gt; set php best\r-\u0026gt; Redirected to slot [9244] located at 127.0.0.1:7001 #集群模式下会帮我们自动重定向\rOK\r127.0.0.1:7001\u0026gt; get php\r\u0026quot;best\u0026quot;\rredis-cli -p 7000 #非集群模式连接\r127. 0.0.1:7000\u0026gt; cluster keyslot php\r(integer) 9244\r127.0.0.1:7000\u0026gt; set php best\r(error) MOVED 9244 127.0.0.1:7001 #非集群模式下仅返回moved异常\r   ask重定向：redis正在从源节点到目标节点迁移slot的情况，因为槽迁移是遍历槽中的key逐步执行migrate的过程，这很慢。ask则表示在槽迁移过程中\n 源节点正在与目标节点执行槽迁移 客户端已经从其它节点得到槽属于当前这个源节点，然后发送命令 源节点发现该槽其实已经迁移给目标节点了，然后回复ask转向异常给客户端，意思是这个槽确实本属于源节点，但是已经迁移到目标节点去了 客户端执行asking命令，然后发送键命令给目标节点，也是一个在客户端重定向的过程 目标节点返回    smart客户端\n  smart客户端原理：目标是追求性能。所以尽量不使用代理的模式（在集群前加一层代理，由代理梳理节点、槽、键的关系，客户端连接代理来找到目标节点，每次都要moved或者akd后进行重定向，非常损耗性能。而直连的性能就要高的多，所以需要实现客户端直连目标槽、节点，当然万一碰到moved和ask异常也还是得兼容\n 从集群中选一个可运行节点,使用cluster slots初始化槽和节点映射 将cluster slots的结果映射到本地（比如一个map），为每个节点创建JedisPool. 准备执行命令。key-\u0026gt; slot-\u0026gt; node得关系是知道得，所以可以JedisCluster直接计算然后连接目标节点  如果出现连接出错，就随机发送命令给一个活跃节点，然后回归到moved或者ask那种代理的过程 返回moved或ask（或者直接命中也有可能，但集群中这种可能较小）后获得响应，然后要记得重新初始化slot→node的缓存（更新map） 如果连接出错的过程反复超过了5次，会有Too many cluster redirection!的错误      JedisCluster源码：\n 比如随便选一个set方法：返回了一个new JedisClusterCommand{}.run(key) run方法：判断key是否为null，是则返回一个异常，否则执行runWithRetries方法 runWithRetries方法：参数attempts是一个初始化尝试的次数，默认为5次，attempts小于0就会异常；tryRandomNode表示是否尝试随机一个节点，前面能看到传入的是false；asking表示是否是ask，也是false。判断asking状态和tryRandomNode。都是false，然后这样connection = connectidnHandler. getConnecti onF romSlot(Jedi sClusterCRC16. getSlot(key));拿到连接，connectidnHandler就是在jedis初始化的时候梳理节点和槽的关系，通过key计算槽拿到对应节点连接 然后excute方法执行命令：执行过程忽略，找到异常的处理。  捕获到没有节点可达的异常则直接抛出异常 捕获连接异常，则释放当前连接，attempts\u0026lt;=1时才会connectidnHandler.renewSlotCache来刷新缓存（因为不要轻易刷新缓存，保证效率），然后attempts\u0026ndash;并重新调用runWithRetries JedisRedirectionException重定向异常，然后分别判断moved和ask异常  如果是moved异常会直接connectidnHandler.renewSlotCache来刷新缓存 释放jedis连接 如果ask异常，会设置asking为true，然后重新拿到目标节点连接 调用runWithRetries重试        smart客户端使用: JedisCluster\n  JedisCluster基本使用\nSet\u0026lt;HostAndPort\u0026gt; nodeList = new HashSet \u0026lt;HostAndPort\u0026gt;();\rnodeList.add(new HostAndPort(HOST1, PORT1));\rnodeList.add(new HostAndPort(HOST2, PORT2));\rnodeList.add(new HostAndPort(HOST3, PORT3));\rnodeList.add(new HostAndPort(HOST4, PORT4));\rnodeList.add(new HostAndPort(HOST5, PORT5));\rnodeList.add(new HostAndPort(HOST6, PORT6));\rJedisCluster redisCluster = new JedisCluster(nodeList, timeout, poolConfig);redisCluster.command... //执行命令就好了，归还连接都不需要，jediscluster帮我们做了\r  使用技巧：  保证单例：内置了所有节点的连接池，从节点也有连接，保证故障转移后也有完整连接池。单例保证资源唯一性，并且不至于资源浪费 无需手动借还连接池 合理设置commons- pool      整合spring\npublic class JedisClusterFactory {\rprivate JedisCluster jedisCluster;\rprivate List\u0026lt;String\u0026gt; hostPortList;\rprivate int timeout; //单位是毫秒\rprivate Logger logger = LoggerFactory.getLogger(JedisClusterFactory.class);\r//初始化方法\rpublic void init() {\rJedisPoolConfig jedisPoolConfig = new JedisPoolConfig();//用于设置相关参数\rSet \u0026lt;HostAndPort\u0026gt; nodeSet = new HashSet\u0026lt;HostAndPort\u0026gt;();\rfor(String hostPort : hostPortList) {\rString[] arr = hostPort.split(\u0026quot;:\u0026quot;);\rif (arr.length != 2) {\rcontinue;\r}\rnodeSet.add(new HostAndPort(arr[0], Integer .parseInt(arr[1])));\r}\rtry {\rjedisCluster = new JedisCluster(nodeSet, timeout, jedisPoolConfig) ;\r} catch (Exception e) {\rlogger.error(e.getMessage(), e);\r}\r}\r//销毁方法\rpublic void destroy() {\rif (jedisCluster != null) {\rtry {\rjedisCluster.close();\r} catch (I0Exception e) {\rlogger.error(e.getMessage(), e);\r}\r}\r}\rpublic JedisCluster getJedisClusterO) {\rreturn jedisCluster;\r}\rpublic void setHostPortList(List\u0026lt;String\u0026gt; hostPortList) {\rthis. hostPortList = hostPortList;\r}\rpublic void setTimeout(int timeout) {\rthis. timeout = timeout;\r}\r}\r \u0026lt;bean id= \u0026quot;jedisClusterFactory\u0026quot; class= \u0026quot;com.carlosfu.redis.factory.JedisClusterFactory\u0026quot; init-method= \u0026quot;init\u0026quot; destroy-method=\u0026quot;destroy\u0026quot;\u0026gt;\r\u0026lt;property name= \u0026quot;hostPortList \u0026quot;\u0026gt;\r\u0026lt;list\u0026gt;\r\u0026lt;value\u0026gt;127.0.0.1:7000\u0026lt;/value\u0026gt;\r\u0026lt;value\u0026gt;127.0.0.1:7001\u0026lt;/value\u0026gt;\r\u0026lt;value\u0026gt;127.0.0.1:7002\u0026lt;/value\u0026gt;\r\u0026lt;value\u0026gt;127.0.0.1:7003\u0026lt;/value\u0026gt;\r\u0026lt;value\u0026gt;127.0.0.1:7004\u0026lt;/value\u0026gt;\r\u0026lt;value\u0026gt;127.0.0.1:7005\u0026lt;/value\u0026gt;\r\u0026lt;/list\u0026gt;\r\u0026lt;/property\u0026gt;\r\u0026lt;property name= \u0026quot;timeout\u0026quot; value= \u0026quot;1000\u0026quot; /\u0026gt;\r\u0026lt;/bean\u0026gt;\r\u0026lt;bean id= \u0026quot;jedisCluster\u0026quot; factory-bean= \u0026quot;jedisClusterFactory\u0026quot; factory-method= \u0026quot;getJedisCluster\u0026quot;/\u0026gt;\u0026lt;/bean\u0026gt;\r\u0026lt;bean id= \u0026quot;redisClusterService\u0026quot; class= \u0026quot;xxx.RedisClusterServiceImpl\u0026quot;\u0026gt;\u0026lt;/bean\u0026gt;\r public class RedisClusterServiceImpl implements RedisService { //RedisService省略\rprivate Jedi sCluster jedi sCluster;\rpublic String set(String key, String value) {\rreturn jedisCluster . set(key, value);\r}\rpublic String get(String key) {\rreturn jedisCluster . get(key);\r}\rpublic void setJedisCluster(Jedi sCluster jedisCluster) {\rthis. jedisCluster = jedisCluster;\r}\r}\r //测试类\rpublic class Jedi sClusterSpringTest extends BaseTest {\r@Resource(name = \u0026quot; redi sClusterService\u0026quot; )\rprivate Redi sService redi sClusterService;\r@Test\rpublic void testNotNullO {\rassertNotNul I(redi sClusterService);\rredi sClusterService . set(\u0026quot;hello\u0026quot;, \u0026quot;world\u0026quot;);\rassertTrue(\u0026quot;world\u0026quot;.equals(redisClusterServicel.get(\u0026quot;hello\u0026quot;)));\r}\r}\r 非常简单，无需去用spring data jedis的库\n  多节点命令实现\n//获取所有节点的JeidsPool\r//获取主节点即可，获取到所有主节点连接就可以进行多节点操作了\r//一般来说是try catch处理，这里仅简单演示，写的时候注意用try catch即可\rMap \u0026lt; String, JedisPool\u0026gt; jedisPoolMap = jedisCluster.getClusterNodes0;\rfor (Entry\u0026lt;String, JedisPool\u0026gt; entry : jedisPoolMap.entrySet() {\r//获取每个节点的Jedis连接\rJedis jedis = entry.getValue().getResource();\r//只删除主节点数据\rif (!isMaster(jedis)) { //判断主节点的方法，只需执行一个row（听起来是这个，具体拼写不知道）命令，会返回master还是sleve。如果客户端不支持这个命令，infoReplication中row的字段来判断\rcontinue;\r}\r// finally close\r}\r   批量命令实现：mget mset必须在一个槽，这样的条件很苛刻，将用以下4中方法解决\n 串行mget：for循环遍历所有key，分别操作。n次网络操作，n次网络时间，效率很差 串行IO：做一个聚合，将CRC16hash计算槽，根据槽、节点对应关系，将属于相同节点的key放入同一子集合来mget，多个节点的mget操作可以并行，节点数次网络操作，节点数次网络时间 并行IO：做一个聚合，将CRC16hash计算槽，根据槽、节点对应关系，将属于相同节点的key放入同一子集合来mget，多个节点的mget操作可以多线程并行，使用节点数的网络操作次数，1次网络时间 hash_tag：将key进行hash_tag的包装，{tag}key1、{tag}key2\u0026hellip;{tag}keyN，让所有key都落到一个redis节点上，每次mget就可以只去一个节点取即可     方案 优点 缺点 网络IO     串行mget 编程简单少量keys满足需求 大量keys请求延迟严重O(keys) O(keys)   串行IO 编程简单少量节点满足需求 大量node延迟严重 O(nodes)   并行IO 利用并行特性延迟取决于最慢的节点 (多线程)编程复杂(多线程)超时定位问题难 O(max_slow(node))   hash_tag 性能最高 读写增加tag维护成本tag分布易出现数据倾斜 O(1)          故障转移 故障发现   通过ping/pong消息实现故障发现:不需要sentinel  主观下线：定义:某个节点认为另一一个节点不可用，\u0026ldquo;偏见\u0026rdquo;。节点A定期发送ping给节点B，成功节点B回复PONG，节点A记录与节点B的最后通信时间，如果ping失败了则通信异常断开连接，在下一次执行ping时，如果与节点B的最后通信时间超过了node-timeout则表示节点B为pfail状态，即主观下线 客观下线：定义:当半数以上持有槽的主节点都标记某节点主观下线，只有主节点才有读写和槽维护的工作，从节点只做复制的作用。节点B接收到其它节点发来的ping消息，它包含了pfail消息，会将该主观下线的信息添加到其自身维护的一个故障链表中，该故障链表中包含了当前节点收到的每个节点对其它节点的信息，能知道每个节点对每个节点的看法，链表是有周期的，周期为cluster-node-timeout*2，保证很久之前的故障消息不再生效，保证客观下线的公平性和有效性  尝试客观下线：计算有效下线报告数量，大于持有槽的主节点总数的一般，则更新为客观下线，向集群广播下线的节点的fail消息；否则退出尝试下线方法 下线通知：通知集群内所有节点标记故障节点为客观下线；通知故障节点的从节点触发故障转移流程    故障恢复  资格检查：对多个从节点进行资格检查，在资格审查范围内的从节点才有资格去做故障恢复的工作  每个从节点检查与故障主节点的断线时间。 超过cluster-node-timeout * cluster-slave-validity-factor取消资格。cluster-node-timeout默认15000毫秒，cluster-slave-validity-factor默认10   准备选举时间：为了使偏移量值最大的从节点具备优先成为主节点的条件，因为偏移量最大，保证与原主节点数据一致性最高  offset较大会优先去进行选举，比如offset最大的节点rank=0，第二rank=1，第三rank=2，那么rank=0的节点拥有最小的准备选举时间，延迟1秒就去进行选举，rank=1的节点延迟2秒才去选举，rank=2的节点延迟3秒才去选举   选举投票：让其它非客观下线的主节点进行投票，选票大于主节点一半数即该从节点选举为新的主节点，是否投票的机制与前面主从故障转移是一样的 替换主节点：  1.当前从节点取消复制变为主节点。(slaveof no one)。 2.执行clusterDelSlot撤销故障主节点负责的槽，并执行clusterAddSlot把这些槽分配给自己。 3.向集群广播自己的pong消息,表明已经替换了故障从节点    故障转移演练 1.执行kill -9节点模拟宕机\n2.观察客户端故障恢复时间：大概不到20秒\n3.观察各个节点的日志\n 7000：被kill后没有任何日志，重启后日志记录连接到了新的master 7003：7000原来的从节点  与原来的master7000失联了，很多连接失败的信息 接收到FAIL消息，客观下线 打印offset， 成为新的master 清除7000主节点的一些信息，如FAIL信息 与其它从节点同步   7002：清除7000主节点的一些信息，如FAIL信息；等等，不再多看，有兴趣自己看  开发运维常见问题 集群完整性  cluster-require-full-coverage，默认为yes  集群中16384个槽全部可用：保证集群完整性 如果节点故障或者正在故障转移，将(error) CLUSTERDOWN The cluster is down，整个集群不可用 大多数业务无法容忍，cluster-require-full-coverage建议设置为no    带宽消耗  问题  官方建议: 1000个节点 PING/PONG消息 不容忽视的带宽消耗   三方面：  消息发送频率:节点发现与其它节点最后通信时间超过cluster-node-timeout/2时会直接发送ping消息 消息数据量: slots槽数组(2KB空间)和整个集群1/10的状态数据(10个节点状态数据约1KB) 节点部署的机器规模:集群分布的机器越多且每台机器划分的节点数越均匀,则集群内整体的可用带宽越高。   一个例子：规模，节点200个、20台物理机(每台10个节点)  cluster-node-timeout = 15000，ping/pong带宽为25Mb cluster-node-timeout = 20000，ping/pong带宽低于15Mb   优化  避免\u0026quot;大\u0026quot;集群：避免多业务使用一个集群，一个大业务也可以多集群（比如一个大的推荐服务，根据不同推荐类型使用不同集群，因为不同推荐类型之间没有相关性） cluster-node-timeout：带宽和故障转移速度的均衡尽量均匀分配到多机器上:保证高可用和带宽 尽量均匀分配到多机器上：保证高可用和带宽。可以自己去开发一套分配规则来达到负载的均衡（使用工具）    Pub/Sub广播  问题：publish在集群每个节点广播，加重带宽。对任意节点执行publish发布消息，publish将广播到整个集群 解决：单独\u0026quot;走\u0026quot;一套Redis Sentinel。因为发布订阅的功能本身比较独立  集群倾斜  集群倾斜  数据倾斜:内存不均  节点和槽分配不均，导致数据倾斜  redis-trib.rb info ip:port查看节点、槽、键值分布 redis-trib.rb rebalance ip:port进行自动均衡节点和槽：redis内部自己的算法。谨慎使用，建议自己做计划手动迁移，而不要直接rebalance   不同槽对应键值数量差异较大  CRC16正常情况下比较均匀。 可能存在hash_tag：通常是主要原因 cluster countkeysinslot {slot}获取槽对应键值个数   包含bigkey：  因为数据只能以key为单位存储在某个节点上，比如一个hash、set等非常大，有几百万元素，就会造成数据倾斜 从节点: redis-cli \u0026ndash;bigkeys，发现bigkey，建议在从节点执行 优化：优化数据结构。拆分该大个数据，二次hash拆分等   内存相关配置不一致：我们知道如hash、list、zset等都有一些内存优化的配置参数，比如整数集合的优化等对于redis来说会节省一些内存，假如项目中大量使用了集合、hash的时候，我们做了一些优化，但没有在所有节点做优化，就会出现倾斜；还有假如某个节点的客户端缓冲区比较高，比如节点被执行了moniter命令，也会出现数据倾斜的情况；还有我们知道redis的键值对是存在一个hash表中的，如果key较多，该表也会扩容，扩容如果刚好触发到某一个数值，需要做rehash，hash表也可能会占用很大内存  hash-max- ziplist-value、set-max-intset-entries等 优化：定期\u0026quot;检查\u0026quot;配置一致性     请求倾斜：热点  热点key：重要的key或者bigkey。比如redis某个节点有一个重要的key，比如一个微博大V在重要的时间节点发布了一个重要消息会落在一个重要的key上，就会存在热点问题 优化：后面会讲缓存设计与优化，这里不详细讲  避免bigkey 热键不要用hash_tag 当一致性不高时，可以用本地缓存 + MQ。比如java的本地缓存，没有网络时间，肯定比redis效率高，但就需要考虑gc的问题，还需要达到一致性，就可以使用mq，比如redis中做了更新就去更新本地缓存达到一致性        读写分离  只读连接：集群模式的从节点不接受任何读写请求  如果对从节点请求，它将重定向到负责槽的主节点 readonly命令可以使从节点只读：连接级别命令，即连接断掉以后，需要重新对该从节点执行readonly来连接   读写分离：更加复杂  与单机同样的问题：复制延迟、读取过期数据、从节点故障 需要修改客户端：redis-cluster提供了cluster slaves {nodeId}命令，可以根据主节点id获取其从节点，所以需要实现自己的客户端，很复杂，要维护一个slave的池子，要知道从节点和槽的关系   建议：redis集群使用读写分离成本很高，尽量选择扩展集群规模，而不是利用从节点来实现读写分离。实在没有集群扩展成本，也可以考虑自己实现客户端来实现读写分离  数据迁移   官方迁移工具: redis-trib.rb import\n./redis-trib.rb import --from 127 .0.0.1:6388 --copy 127.0.0.1:7000 #这里copy，也有replace参数。\r#\r  只能从单机迁移到集群 不支持在线迁移：source需要停写，否则源节点在迁移期间的写入的数据可能不会被记录。redis-trib.rb使用的是一个scan的模式，所有被scan到的数据都会被copy，scan期间源节点有新的数据，可能也会正好被scan到，有的数据加入时可能scan的游标已经到更后面了，就不会被scan到 不支持断点续传：迁移过程中断不会记录之前迁移了的过程，只能重新重头迁移 单线程迁移：影响速度。数据较大可能迁移比较慢    在线迁移工具：都是在GitHub开源的\n 唯品会：redis-migrate-tool 豌豆荚：redis-port    集群vs单机  集群限制  key批量操作支持有限：例如mget、mset必须在一 个slot Key事务和Lua支持有限：如果使用事务或者Lua，操作的key必须在一个节点，虽然可以使用hash_tag使它们落在一个节点上，但本质上还是受限的 key是数据分区的最小粒度：不支持bigkey分区 不支持多个数据库：集群模式下只有一个db 0。当然在一般的单机下也不建议使用多db模式（最多16） 复制只支持一层：不支持树形复制结构   思考：分布式Redis不一定好  Redis Cluster：满足容量和性能的扩展性，很多业务”不需要\u0026rdquo;  大多数时客户端性能会\u0026quot;降低\u0026rdquo;：比如批量操作再怎么优化也不如单节点原子操作 命令无法跨节点使用：mget、keys、 scan、flush、sinter等 Lua和事务无法跨节点使用。 客户端维护更复杂: SDK和应用本身消耗(例如更多的连接池)。   很多场景Redis Sentinel已经足够好    总结  Redis cluster数据分区规则采用虚拟槽方式(16384个槽) ,每个节点负责一部分槽和相关数据,实现数据和请求的负载均衡。 搭建集群划分四个步骤:准备节点、节点握手、分配槽、复制。redis-trib.rb工具用于快速搭建集群。 集群伸缩通过在节点之间移动槽和相关数据实现。  扩容时根据槽迁移计划把槽从源节点迁移到新节点。 收缩时如果下线的节点有负责的槽需要迁移到其它节点,再通过cluster forget命令让集群内所有节点忘记被下线节点。   使用smart客户端操作集群达到通信效率最大化,客户端内部负责计算维护键-\u0026gt;槽-\u0026gt;节点的映射,用于快速定位到目标节点。 集群自动故障转移过程分为故障发现和节点恢复。节点下线分为主观下线和客观下线,当超过半数主节点认为故障节点为主观下线时标记它为客观下线状态。从节点负责对客观下线的主节点触发故障恢复流程,保证集群的可用性。 开发运维常见问题包括：超大规模集群带宽消耗，pub/sub广播问题，集群倾斜问题，单机和集群对比等  开发规范 键值设计   key设计\n  可读性和可管理性：以业务名(或数据库名)为前缀(防止key冲突) , 用冒号分割，比如业务名:表名:id ,如: ugc:video:1、database:table:1\n  简洁性：保证语义的前提下，控制key的长度，当key较多时，内存占用也不容忽视( redis3：39字节embstr，这里的意思是redis3版本的embstr编码可以很大程度节省内存) , 如 user:{uid}:friends:messages:{mid}简化为 u:{uid}:f:m :{mid}\ntype #对外可能都是string类型\robject encoding key #查看key的编码。内部编码各不相同：int、raw、embstr等，大于39个字节的字符串是raw，embstr则是用于存储小于等于39个字节的字符串。int会有int的优化，embstr也会有一些优化，redis在分配内存的时候，除了给redis对象分配内存，还会给其存储的字符串内容分配内存，embstr则是将对象和其字符串内容进行一次性分配，并且是连续的内存，可以节省分配内存的次数，也能节省一定的空间。在redis4版本之后对redis内部的sds实现做了一些优化，embstr的阈值更变为44个字节了\r Redis3 embstr测试\n   key-value个数 39字节 40字节     10万 15.69M 18.75M   100万 146.29M 176.81M   1000万 1.47G 1.77G   1亿 14.6G 17.7G      不要包含特殊字符。反例:包含空格、换行、单双引号以及其他转义字符\n    value设计\n  拒绝bigkey\n  阈值（并非绝对，只是参考标准）\n string类型控制在10KB以内 hash、list、set、zset元素 个数不要超过5000    反例：一个包含几百万个元素的list、hash等， 一个巨大的json字符串\n  危害：\n 网络阻塞：比如千兆网，最大流量为128m，一个热点key的qps为10w，那么key为10kb，也能差不多瞬间将网络吃满 Redis阻塞：慢查询，hgetall、lrange、 zrange(例如几十万)等全量操作，因为redis单线程，所以会阻塞其它命令 集群节点数据不均衡 频繁序列化：应用服务器CPU消耗    bigkey发现\n  应用异常：报错，如一个bigkey操作，其它命令将阻塞然后time out\n  官方工具：redis-cli \u0026ndash;bigkeys\n  自己实现：scan + debug object\ndebug object key #获取序列化长度。如果bigkey很大，debug object也可能阻塞redis，可以使用zcat、hlen、strlen等，比如长度大于多少来判定是否为bigkey\r   主动报警：网络流量监控、客户端监控\n  改造redis源码内核：内核热点key问题优化，在redis中维护一个堆来记录redis对key大小的变化\n    bigkey删除：删除也可能会阻塞redis\n  阻塞:注意隐性删除(过期、rename等)：也是删除，可能会阻塞redis，且过期、删除的命令不会记录在redis的慢查询当中，redis慢查询只记录客户端的行为，过期、删除只会记录在redis的license（一个记录延迟事件的记录）中，但是这些命令会同步给slave，可以在从节点找到对应慢查询\n  Redis 4.0 : lazy delete (unlink命令)，后台删除，单独开启线程去执行删除，不会阻塞redis了\n  通过scan删除\npublic void delBigHash(String host, int port, String password, String bigHashKey) {\rJedis jedis = new Jedis(host, port);\rif (password != null \u0026amp;\u0026amp; !\u0026quot;\u0026quot; .equals(password)) {\rjedis.auth(password);\r}\rScanParams scanParams = new ScanParams().count(100);\rString cursor = \u0026quot;0\u0026quot;;\rdo {\rScanResult\u0026lt;Entry\u0026lt;String, String\u0026gt;\u0026gt; scanResult = jedis.hscan(bigHashKey,cursor, scan); //不全，不知道对不对\rList\u0026lt;Entry\u0026lt;String, String\u0026gt;\u0026gt; entryList = scanResult. getResult();\rif (entryList != null \u0026amp;\u0026amp; !entryList. isEmpty()) {\rfor (Entry\u0026lt;String, String\u0026gt; entry : entryList) {\rjedis.hdel(bigHashKey, entry . getKey());\r}\r}\rcursor = scanResult . getStringCursor();\r} while (!\u0026quot;0\u0026quot; .equals(cursor));\rjedis.del(bigHashKey);//删除bigkey\r}\r     bigkey预防\n 优化数据结构：例如二级拆分 物理隔离或者万兆网卡：治标不治本 命令优化：例如hgetall-\u0026gt; hmget、hscan 报警和定期优化    bigkey总结\n 牢记Redis单线程特性 选择合理的数据结构和命令 清楚自身OPS 了解bigkey的危害        选择合适的数据结构   实体类型(数据结构内存优化:例如ziplist ,注意内存和性能的平衡)\n  反例 set user:1:name tom; set user:1:age 19; set user:1:favor football;\n  正例：使用hash，前面有讲好处 hmset user:1 name tom age 19 favor football\n  一个例子、三种方案：需求: picId= \u0026gt; userId (100万)\n  全部string：set picId userId。常规方法。使用内存116m\n 优点：编程简单 缺点：浪费内存；全量获取复杂     Key Value     pic:1 user:1   \u0026hellip;\u0026hellip; \u0026hellip;\u0026hellip;   pic:1000000 user:1000000      一个hash：hset allPics picId userId。形成一个bigkey，不合理的设计。使用内存129m\n 优点：无 缺点：浪费内存；形成bigkey     Key Field Value     allPics pic:1\u0026hellip;..pic:1000000 user:1\u0026hellip;..user:1000000      若干个小hash：hset picId/100 picId%100 userId。使用内存26m\n 优点：节省内存 缺点：编程复杂；超时问题；ziplist性能问题     Key Field Value     segment:0 pic:1\u0026hellip;..pic:100 user:1\u0026hellip;..user:100   \u0026hellip;\u0026hellip; \u0026hellip;\u0026hellip;    segment:99999 pic:1\u0026hellip;..pic:100 user:999901\u0026hellip;..user:1000000        内存\n 配置(支持动态修改)：当元素个数少于512、元素value小于64字节时，redis默认使用zplist来存储hash结构  hash-max-ziplist-entries 512 hash-max-ziplist-value 64   ziplist：有好有坏，需要衡量，所以才配置为元素个数较少、元素value较小时使用  连续内存：节省内存 读写有指针位移，最坏O(n2)，影响读写效率 新增删除有内存重分配，影响增删效率      过期设计   键值生命周期：Redis不是垃圾桶，不要什么东西都扔给redis，还不控制缓存时间\n 周期数据需要设置过期时间，object idle time可以找到超出指定闲置时间的垃圾kv，即多久没有使用的kv 过期时间不宜集中：缓存穿透和雪崩等问题，比如每三小时集体过期，并通过数据库重建缓存    命令优化  [推荐] O(N)以上命令关注N的数量：例如，hgetall、Irange、 smembers、zrange、 sinter等并非不能使用,但是需要明确N的值。有遍历的需求可以使用hscan、sscan、 zscan代替。即主要考虑n的大小 [推荐]禁用命令：禁止线上使用keys、flushall、 flushdb等，通过redis的rename机制禁掉命令，或者使用scan的方式渐进式处理。 [推荐]合理使用select：选择redis数据库，可以用redis的db对一些数据进行隔离或测试环境区分等。但redis的多数据库较弱，使用数字进行区分（db 0~15），很多客户端对redis数据库的操作支持较差；注意同时多业务用多数据库实际还是单线程处理，还是会有干扰，频繁切换redis数据库也有一定消耗 [推荐] Redis事务功能较弱，不建议过多使用：Redis的事务功能较弱(不支持回滚)，需要时借助外部来实现，比如java的事务实现等；而且集群版本(自研和官方)要求一次事务操作的key必须在一个slot上(可以使用hashtag功能解决，但也可能造成数据不均匀问题)。使用时不要太过信赖，只是一个辅助功能而已 [推荐] Redis集群版本在使用Lua上有特殊要求：不要过度使用，虽然可以实现很多功能。或者单独开一个redis来做对应的功能，避免一些集群问题  所有key，必须在1个slot上,否则直接返回error，同样通过hashtag可以解决 \u0026lsquo;-ERR eval/evalsha command keys must in same slot\\r\\n\u0026rdquo;   [建议] 必要情况下使用monitor命令时，要注意不要长时间使用。monitor可以查看redis监控redis执行的命令，但是如果并发量过高，会有大流量打在monitor-client（monitor也是一个redis客户端）上，会造成monitor无法实时消费大量的流量，会导致输出缓冲区暴增，会占用redis内存，撑爆redis内存  客户端优化 Java客户端优化\n  [推荐] 避免多个应用使用一个Redis实例\n 问题  业务之间key冲突，可以通过select redis数据库来解决 单线程redis，业务A的命令会影响到业务B   解决  不相干的业务拆分：不要使用一个redis实列，可以在一个机器上启动多个redis实例合理利用多核机器 公共数据做服务化：有公共数据，比如视频、音频等，不论底层使用redis还是mysql，对外只暴露服务接口即可，如http接口，让其作为单独的微服务应用对外提供服务      [推荐] 使用连接池，标准使用方式\nJedis jedis = null;\rtry {\rjedis = jedisPool,getResource();\r//具体的命令\rjedis. executeCommand()\r} catch (Exception e) {\rlogger.error(\u0026quot;op key {} error: \u0026quot; + e.getMessage(), key, e);\r} finally {\r//注意这里不是关闭连接，在JedisPool模式下，Jedis会被归还给资源池。\rif (jedis != null)\rjedis. close();\r}\r   开发运维常见坑 Redis内存管理 RedisCacheCloud https://github.com/sohutv/cachecloud\nRedis云平台CacheCloud。老师自己在原来公司参与开发的\n Redis规模化运维 快速构建 机器部署 应用接入 用户功能 运维功能  Redis规模化运维  遇到的问题  发布构建繁琐，私搭乱盖 节点\u0026amp;机器等运维成本 监控报警初级   CacheCloud  一键开启Redis。 (Standalone、 Sentinel、 Cluster) 机器、应用、实例监控和报警。 客户端:透明使用、性能上报。 可视化运维:配置、扩容、Failover、机器/应用/实例上下线。 已存在Redis直接接入和数据迁移。   使用规模  300+亿commands/day 3TB Memory Total 1300+ Instances Total 200+ Machines Total   使用场景  全量视频缓存(视频播放API) :跨机房高可用 消息队列同步(RedisMQ中间件) 分布式布隆过滤器(百万QPS) 计数系统:计数(播放数) 其他:排行榜、社交(直播)、实时计算(反作弊)等。    快速构建 https://github.com/sohutv/cachecloud/wiki\n机器部署 应用接入 用户功能 运维功能 ","date":"0001-01-01","permalink":"https://yuanyatianchi.github.io/post/it.db.redis/","tags":["it","db"],"title":"Redis"},{"content":"所有操作都只在 行首 起效，所有操作的 空格 不可缺省。\n基本 标题 \u0026lsquo;### 标题内容\u0026rsquo;：有几个 \u0026lsquo;#\u0026rsquo; 即是几级标题，同时也是几号字体。\n有序列表 \u0026lsquo;1. '\n无序列表 \u0026lsquo;- ' 或 \u0026lsquo;+ '\n插入 代码块 \u0026lsquo;```语言名\u0026rsquo; 或 \u0026lsquo;~~~语言名\u0026rsquo;：代码块\n公式块 行内公式 x^{2}+y^{2}=z^{2}\n行间公式 x^{2}+y^{2}=z^{2}\n带编号的公式 \\begin{equation} a^2+b^2=c^2 \\end{equation}\n","date":"0001-01-01","permalink":"https://yuanyatianchi.github.io/post/tool.typora/","tags":["tool"],"title":"Typora"},{"content":"Windows环境 database mysql https://dev.mysql.com/downloads/\n  下载并解压：https://dev.mysql.com/downloads/mysql/ ，c++ 2019可再发行软件包(运行库)：https://aka.ms/vs/16/release/vc_redist.x64.exe\n  创建my.ini：E:\\it\\database\\mysql\\Mysql，不要自己创建data目录！\n[mysql]\r#设置mysql客户端默认字符集\rdefault-character-set=utf8\r[mysqld]\r#设置端口\rport = 3306\r#设置安装目录\rbasedir=E:\\it\\database\\mysql\\Mysql\r#设置数据的存放目录\rdatadir=E:\\it\\database\\mysql\\Mysql\\data\r#设置最大连接数\rmax_connections=200\r#设置mysql服务端使用的字符集默\rcharacter-set-server=utf8\r#设置创建新表时使用的默认存储引擎\rdefault-storage-engine=INNODB\r   配置，以管理员身份进入cmd\nE:\rcd it\\database\\mysql\\Mysql\\bin #进入到mysql的bin目录\rmysqld install #安装mysql服务\rmysqld --initialize-insecure #不安全(无root密码)初始化，并自动生成了data目录\rnet start mysql #启动服务\rmysql -u root -p #登录mysql，无密码，回车直接登入\ruse mysql; #进到mysql这个database\rALTER USER `root`@`localhost` IDENTIFIED BY '新密码'; #设置密码\rexit #退出\r   front-end Node   下载并解压：https://nodejs.org/zh-cn/download/releases/\n  环境变量：node.exe所在目录，如E:\\it\\front-end\\Node\n  创建文件夹并配置：node_global、node_cache\nnode -v # 检查node\rnpm -v # 检查npm\rnpm config set cache \u0026quot;E:\\it\\front-end\\Node\\node_cache\u0026quot; # 配置缓存目录路径\rnpm config set prefix \u0026quot;E:\\it\\front-end\\Node\\node_global\u0026quot; # 配置全局目录路径\rnpm config set registry \u0026quot;https://registry.npm.taobao.org\u0026quot; # 淘宝镜像\rnpm config -g ls # 检查config\rnpm install -g windows-build-tools # 安装winows构建工具，python2.7，c++相关等\r   version-control Git  下载并安装：https://git-scm.com/download/win，叉掉GUI，叉掉创建开始菜单，然后全部Next  环境变量 增变量    key value     path E:\\it\\front-end\\NodeE:\\it\\go\\Go\\binE:\\it\\java\\Java\\jdk1.8.0_231\\binE:\\it\\java\\Java\\jdk1.8.0_231\\jre\\binE:\\it\\python\\Python27E:\\it\\version-control\\Git\\cmd   GOPATH E:\\it\\go\\GoProjects    原变量    key value     ComSpec %SystemRoot%\\system32\\cmd.exe   DriverData C:\\Windows\\System32\\Drivers\\DriverData   NUMBER OF_ PROCESSORS 16   OS Windows_ NT   path %SystemRoot%%SystemRoot%\\system32%SystemRoot%\\System32\\Wbem%SYSTEMROOT%\\System32\\WindowsPowerShell\\v1.0%SYSTEMROOT%\\System32\\OpenSSH   PATHEXT .COM;.EXE,;,BAT;,CMD;,VBS;VBE;JS;JSE;:WSF;WSH;MSC   PROCESSOR_ARCHITECTURE AMD64   PROCESSOR_ IDENTIFIER AMD64 Family 23 Model 113 Stepping 0, AuthenticAMD   PROCESSOR_ LEVEL 23   PROCESSOR_ REVISION 7100   PSModulePath %ProgramFiles%\\WindowsPowerShell\\Modules%SystemRoot%\\system32\\WindowsPowerShell\\v1.0\\Modules   TEMP %SystemRoot%\\TEMP   TMP %SystemRoot%\\TEMP   USERNAME SYSTEM   windir %SystemRoot%    ","date":"0001-01-01","permalink":"https://yuanyatianchi.github.io/post/it.base.winenv/","tags":["it","base"],"title":"WinEnv"},{"content":"MyDocker Docker 是一个使用了 Linux Namespace Cgroups 的虚拟化工具。\nLinux Namespace Linux Namespace 是 Kernel 的一个功能，它可以隔离一系列的系统资源，比如 PIO ( Process ID ）、User ID、Network 等。可能会想到 chroot 命令，就像 chroot 允许把当前目录变成根目录一样（被隔离开来的）。Namespace 也可以在一些资源上，将进程隔离起来，这些资源包括进程树、网络接口、挂载点等。\n使用 Namespace ，就可以做到 UID 级别的隔离，可以以 UID 为 n 的用户虚拟化出来一个 Namespace，在这个 Namespace 里面，用户 n 是具有 root 权限的 。但是，在真实的物理机器上，他还是那个以 UID 为 n 的用户，这样就解决了用户之间隔离的问题。\n除了 User Namespace，PID 也是可以被虚拟的。命名空间建立系统的不同视图，从用户的角度来看，每一个命名空间应该像一台单独的 Linux 一样，有自己的 init 进程（PID 为 1），其他进程的 PID 依次递增。子命名空间 A 和 B 空间都有 PID 为 1 的 init 进程，子命名空间的进程映射到父空间的进程上（如：A 和 B 的 init 进程分别映射到父空间 PID为 3 和 4 的进程），父命名空间可以知道每一个子命名空间的运行状态，而子命名空间与子命名空间是隔离的。\n   Namespace 类型 系统调用参数 内核版本     Mount Namespace CLONE NEWNS 2.4.19   UTS Namespace CLONE NEWUTS 2.6.19   IPC Namespace CLONE NEWIPC 2.6.19   PID Namespace CLONE NEWPID 2.6.24   Network Namespace CLONE NEWNET 2.6.29   User Namespace CLONE NEWUSER 3.8    Namespace API 主要使用如下 3 个系统调用\n clone()：创建新进程。根据系统调用参数来判断哪些类型的 Namespace 被创建，而且它们的子进程也会被包含到这些 Namespace 中。 unshare()：将进程移出某个 Namespace setns()：将进程加入到 Namespace 中。  UTS Namespace UTS Namespace 主要用来隔离 nodename 和 domainname 两个系统标识，在UTS Namespace中，每个 Namespace 允许有自己的 hostname\n用 Go 创建 UTS Namespace 的例子：使用 syscall.CLONE_NEWUTS 这个标识符去创建 UTS Namespace，Go 帮我们封装了对 clone() 函数的调用。\nfunc main() {\r// 指定被 fork 来的新进程内的初始命令，默认使用sh来执行\rcmd := exec.Command(\u0026quot;sh\u0026quot;)\r// 系统调用参数\rcmd.SysProcAttr = \u0026amp;syscall.SysProcAttr{\rCloneflags: syscall.CLONE_NEWUTS,\r}\rcmd.Stdin = os.Stdin\rcmd.Stdout = os.Stdout\rcmd.Stderr = os.Stderr\rif err := cmd.Run(); err != nil {\rlog.Fatal(err)\r}\r}\r $ go run main.go\r 容器机：执行 main.go 后进入到 一个sh 运行环境中，是一个交互式环境\n# 查看系统中执行 main.go 的进程关系\r$ pstree -pl | grep main\r|-gnome-terminal-(20132)-+-bash(20139)---go(23753)-+-main(23802)-+-sh(23807)-+-grep(23809)\r# 查看当前 sh 环境的 PID\r$ echo $$\r23807\r 查看父进程 main 和子进程 sh 的 UTS Namespace，可以看到它们在两个不同的 UTS Namespace 中\n$ readlink /proc/23802/ns/uts\ruts:[4026531838]\r$ readlink /proc/23807/ns/uts\ruts:[4026532191]\r 由于 UTS Namespace 对 hostname 做了隔离 所以在这个环境内修改 hostname 也不影外部主机\n$ hostname - b myhostname\r$ hostname\rmyhostname\r 宿主机：启动另一个 shell 可以看到外部的 hostname 并没有被内部修改所影响\n$ hostname\rMiWiFi-R3A-srv\r IPC Namespace IPC Names 用来隔离 System V IPC（） 和 POSIX message queues。每个 IPC Namespace 都有自己的 System V IPC 和 POSIX message queue。\n增加 syscall.CLONE_NEWIPC 标识表示希望创建 IPC Namespace，\nfunc main() {\rcmd := exec.Command(\u0026quot;sh\u0026quot;)\rcmd.SysProcAttr = \u0026amp;syscall.SysProcAttr{\r// 增加 CLONE_NEWIPC 标识\rCloneflags: syscall.CLONE_NEWUTS | syscall.CLONE_NEWIPC,\r}\rcmd.Stdin = os.Stdin\rcmd.Stdout = os.Stdout\rcmd.Stderr = os.Stderr\rif err := cmd.Run(); err != nil {\rlog.Fatal(err)\r}\r}\r $ go run main.go\r 宿主机\n# 查看活动的 mq，目前没有活动的 mq\r$ ipcs -q\r# 创建一个 mq\r$ ipcmk -Q\r# 可以查看到刚刚创建的 mq\r$ ipcs -q\r 容器机：看不到刚刚创建的 mq，说明 IPC Namespace 创建成功，IPC 已隔离\n$ ipcs -q\r PID Namespace PID Namespace 是用来隔离进程 ID 的。同一个进程在不同的 PID Namespace 里可以拥有不同的 PID 。这样就可以理解在 docker container 中 ps -ef 经常会发现， 在容器内，前台运行的那个进程 PID 是 1 ，但是在容器外 ，使用 ps -ef 会发同样的进程却有不同的 PID 这就是 PID Namespace 做的事情。\n增加 syscall.CLONE_ NEWPID 标识表示希望创建 PID Namespace，\nfunc main() {\rcmd := exec.Command(\u0026quot;sh\u0026quot;)\rcmd.SysProcAttr = \u0026amp;syscall.SysProcAttr{\r// 增加 CLONE_NEWIPC 标识\rCloneflags: syscall.CLONE_NEWUTS | syscall.CLONE_NEWIPC | syscall.CLONE_NEWPID,\r}\rcmd.Stdin = os.Stdin\rcmd.Stdout = os.Stdout\rcmd.Stderr = os.Stderr\rif err := cmd.Run(); err != nil {\rlog.Fatal(err)\r}\r}\r $ go run main.go\r 宿主机：查看 main.go 执行的 pid，外部 pid 是 23121\n$ pstree -pl | grep main\r|-gnome-terminal-(20132)-+-bash(20139)---go(23059)-+-main(23116)-+-sh(23121)\r 容器机：内部 pid 为 1，而不像之前 UTS Namespace 时，内外部 pid 一致\n$ pstree -pl | grep main\r|-gnome-terminal-(20132)-+-bash(20139)---go(23059)-+-main(23116)-+-sh(23121)\r$ echo $$\r1\r 这里还不能使用 ps 来查看 因为 ps、 top 等命令会使用／proc 内容，具体内容在下面的 Mount Namespace 部分会进行讲解。\nMount Namespace Mount Namespace 用来隔离各个进程看到的挂载点视图。在不同 Namespace 的进程中，看到的文件系统层次是不一样的。在 Mount Namespace 中调用 mount() 和 umount() 仅仅只会影响当前 Namespace 内的文件系统，而对全局的文件系统是没有影响的。\n到这里也许就会想到 chroot()。它也是将某一个子目录变成根节点。但是 Mount Namespace 不仅能实现这个功能，而且能以更灵活、安全的方式实现。\nMount Namespace 是 Linux 第一个实现 Namespace 类型，因此它的系统调用参数是 CLONE_NEWNS（New Namespace 的缩写）。\n增加 syscall.CLONE_NEWNS 标识表示希望创建 Mount Namespace，\nfunc main() {\rcmd := exec.Command(\u0026quot;sh\u0026quot;)\rcmd.SysProcAttr = \u0026amp;syscall.SysProcAttr{\r// 增加 CLONE_NEWIPC 标识\rCloneflags: syscall.CLONE_NEWUTS | syscall.CLONE_NEWIPC | syscall.CLONE_NEWPID | syscall.CLONE_NEWNS,\r}\rcmd.Stdin = os.Stdin\rcmd.Stdout = os.Stdout\rcmd.Stderr = os.Stderr\rif err := cmd.Run(); err != nil {\rlog.Fatal(err)\r}\r}\r $ go run main.go\r 容器机：查看一下 /proc 的文件内容，这是一个系统文件，提供额外的机制，可以通过内核和内核模块将信息发送给进程。这里会将 /proc 挂载到当前 Namespace 下进行查看。\n# 这里的 /proc 还是宿主机的 /proc，文件很多\r$ ls /proc\r# 查看进程，也仍然是宿主机的进程，进程很多\r$ ps -ef\r# 将 /proc 挂载到当前 Namespace 下\r$ mount -t proc proc /proc\r# 再次查看 /proc，已经是当前 Namespace 的 /proc 了，文件很少\r$ ls /proc\r1\tcmdline driver\tioports kpagecount modules\tschedstat sys\tversion\r8\tconsoles execdomains irq\tkpageflags mounts\tscsi\tsysrq-trigger vmallocinfo\racpi\tcpuinfo fb\tkallsyms loadavg\tmtrr\tself\tsysvipc\tvmstat\rasound\tcrypto filesystems kcore locks\tnet\tslabinfo timer_list\tzoneinfo\rbuddyinfo devices fs\tkeys mdstat\tpagetypeinfo\tsoftirqs timer_stats\rbus\tdiskstats interrupts key-users meminfo\tpartitions\tstat\ttty\rcgroups dma\tiomem\tkmsg misc\tsched_debug\tswaps\tuptime\r# 再次查看进程，同样是当前 Namespace 的进程了，进程很少\r$ ps -ef\rUID PID PPID C STIME TTY TIME CMD\rroot 1 0 0 21:30 pts/1 00:00:00 sh\rroot 9 1 0 21:40 pts/1 00:00:00 ps -ef\r 可以看到在当前 Namespace 中， sh 进程是 PID 为 1 的进程。这就说明当前的 Mount Namespace 中的 mount 和外部空间是隔离的，mount 操作并没有影响到外部。\n宿主机：记得将 /proc 挂载回来，否则 ps -ef 将报\u0026quot;Error, do this: mount -t proc proc /proc\u0026rdquo;\n$ ps -ef\rError, do this: mount -t proc proc /proc\r# 将 /proc 挂载回来\r$ mount -t proc proc /proc\r User Namespace User Namespace 主要是隔离用户的用户组 ID，即一个进程的 User ID 和 Group ID 在 User Namespace 内外可以是不同的。\n较常用的是，在宿主机上以一个非 root 用户运行创建一个 User Namespace，然后在 User Namespace 内被映射成 root 用户。这意味着这个进程在 User Namespace 内有 root 权限，但是在 User Namespace 外没有 root 权限。\n从 Linux Kernel 3.8 开始。root 进程也可以创建 User Namespace，同样在 Namespace 内被映射成 root 用户，拥有 root 权限。\nUser namespace是目前的六个namespace中最后一个支持的，并且直到Linux内核3.8版本的时候还未完全实现（还有部分文件系统不支持）。因为user namespace实际上并不算完全成熟，很多发行版担心安全问题，在编译内核的时候并未开启USER_NS,比如我们的Centos(但是在ubuntu中是默认开启的)，最明显的，在/proc/[pid]/目录下就没有gid_map和pid_map两个文件。可以通过 grubby 修改内核参数，修改后需重启。开启后可以在 /proc/[pid]/ 中看到对应进程的 gid_map 和 pid_map 文件\n# 开启\r$ grubby --args=\u0026quot;user_namespace.enable=1\u0026quot; --update-kernel=\u0026quot;$(grubby --default-kernel)\u0026quot;\rreboot\r# 关闭\r$ grubby --remove-args=\u0026quot;user_namespace.enable=1\u0026quot; --update-kernel=\u0026quot;$(grubby --default-kernel)\u0026quot;\rreboot\r# 查看内核参数，可以发现 max_user_namespaces = 0\r$ sysctl -a | grep namespace\ruser.max_ipc_namespaces = 15024\ruser.max_mnt_namespaces = 15024\ruser.max_net_namespaces = 15024\ruser.max_pid_namespaces = 15024\ruser.max_user_namespaces = 0\ruser.max_uts_namespaces = 15024\r# 所以还需要设置 max_user_namespaces 为大于 0 的值\r$ sysctl -w user.max_user_namespaces=4\r# 刷新\r$ sysctl -p\r 增加 syscall.CLONE_NEWUSER 标识表示希望创建 User Namespace\nfunc main() {\rcmd := exec.Command(\u0026quot;sh\u0026quot;)\rcmd.SysProcAttr = \u0026amp;syscall.SysProcAttr{\rCloneflags: syscall.CLONE_NEWUTS | syscall.CLONE_NEWIPC | syscall.CLONE_NEWPID | syscall.CLONE_NEWNS |\rsyscall.CLONE_NEWUSER,\r}\r//cmd.SysProcAttr.Credential = \u0026amp;syscall.Credential{Uid: uint32(1), Gid: uint32(1)} // 目前3.10内核上不支持这么设置\rcmd.Stdin = os.Stdin\rcmd.Stdout = os.Stdout\rcmd.Stderr = os.Stderr\rif err := cmd.Run(); err != nil {\rlog.Fatal(err)\r}\ros.Exit(-1)\r}\r $ go run man.go\r 宿主机：查看用户id、用户组id等\n$ id\ruid=0(root) gid=0(root) 组=0(root) 环境=unconfined_u:unconfined_r:unconfined_t:s0-s0:c0.c1023\r 容器机：uid 是不同的，说明 User N amespace 生效\n$ id\ruid=65534(nfsnobody) gid=65534(nfsnobody) 组=65534(nfsnobody)\r Network Namespace Network Namespace 是用来隔离网络设备、 IP 地址端口等网络栈的 Namespace。Network Namespace 可以让每个容器拥有自己独立的（虚拟的）网络设备，而且容器内的应用可以绑定到自己的端口，每个 Namespace 内的端口都不会互相冲突。在宿主机上搭建网桥后，就能很方便地实现容器之间的通信，而且不同容器上的应用可以使用相同的端口\n增加 syscall.CLONE_NEWNET 标识表示希望创建 Network Namespace\nfunc main() {\rcmd := exec.Command(\u0026quot;sh\u0026quot;)\rcmd.SysProcAttr = \u0026amp;syscall.SysProcAttr{\rCloneflags: syscall.CLONE_NEWUTS | syscall.CLONE_NEWIPC | syscall.CLONE_NEWPID | syscall.CLONE_NEWNS |\rsyscall.CLONE_NEWUSER | syscall.CLONE_NEWNET,\r}\r//cmd.SysProcAttr.Credential = \u0026amp;syscall.Credential{Uid: uint32(1), Gid: uint32(1)}\rcmd.Stdin = os.Stdin\rcmd.Stdout = os.Stdout\rcmd.Stderr = os.Stderr\rif err := cmd.Run(); err != nil {\rlog.Fatal(err)\r}\ros.Exit(-1)\r}\r $ go run main.go\r 宿主机：查看网络设备，可以看到有 enp0s3、lo、virbr0 等网络设备\n$ ifconfig\renp0s3: flags=4163\u0026lt;UP,BROADCAST,RUNNING,MULTICAST\u0026gt; mtu 1500\rinet 192.168.31.147 netmask 255.255.255.0 broadcast 192.168.31.255\rinet6 fe80::2bd5:35c:43f9:a946 prefixlen 64 scopeid 0x20\u0026lt;link\u0026gt;\rether 08:00:27:56:e8:02 txqueuelen 1000 (Ethernet)\rRX packets 4847 bytes 329495 (321.7 KiB)\rRX errors 0 dropped 0 overruns 0 frame 0\rTX packets 306 bytes 31373 (30.6 KiB)\rTX errors 0 dropped 0 overruns 0 carrier 0 collisions 0\rlo: flags=73\u0026lt;UP,LOOPBACK,RUNNING\u0026gt; mtu 65536\rinet 127.0.0.1 netmask 255.0.0.0\rinet6 ::1 prefixlen 128 scopeid 0x10\u0026lt;host\u0026gt;\rloop txqueuelen 1000 (Local Loopback)\rRX packets 34 bytes 2692 (2.6 KiB)\rRX errors 0 dropped 0 overruns 0 frame 0\rTX packets 34 bytes 2692 (2.6 KiB)\rTX errors 0 dropped 0 overruns 0 carrier 0 collisions 0\rvirbr0: flags=4099\u0026lt;UP,BROADCAST,MULTICAST\u0026gt; mtu 1500\rinet 192.168.122.1 netmask 255.255.255.0 broadcast 192.168.122.255\rether 52:54:00:f4:91:9d txqueuelen 1000 (Ethernet)\rRX packets 0 bytes 0 (0.0 B)\rRX errors 0 dropped 0 overruns 0 frame 0\rTX packets 0 bytes 0 (0.0 B)\rTX errors 0 dropped 0 overruns 0 carrier 0 collisions 0\r 容器机：查看网络设备，没有任何网络设备信息，说明 Network Namespace 生效\n$ ifconfig\r Linux Cgroups Linux Namespace 技术帮助进程隔离出自己单独的空间，但 Docker 如何限制每个空间的大小，保证它们不会互相争抢的呢？这就要用到 Linux Cgroups 技术\nLinux Cgroups (Control Groups) 提供了对一组进程及其将来子进程的资源限制、控制、统计的能力，这些资源包括 CPU、内存、存储、网络等。通过 Cgroups ，可以方便地限制某个进程的资源占用，并且可以实时地监控进程的监控和统计信息。\nCgroups 中的 3 个组件   cgroup：是对进程分组管理的一种机制，一个 cgroup 包含一组进程，井可以在这个 cgroup 上增加 Linux subsystem 的各种参数配置，将一组进程和 subsystem 的系统参数关联起来。\n  subsystem：是一组资源控制的模块。每个 subsystem 会关联到定义了相应限制的 cgroup 上，并对这个 cgroup 中的进程做相应的限制和控制。这些 subsystem 是逐步合并到内核中的，要查看当前的内核支持的 subsystem，可以安装 cgroup 的命令行工具（apt-get install cgroup-bin），通过 lssubsys 看到 Kernel 支持的 subsystem。一般包含如下几项：\n blkio：设置对块设备（比如硬盘）输入输出的访问控制 cpu：设置 cgroup 中进程的 CPU 被调度的策略 cpuacct：可以统计 cgroup 中进程的 CPU 占用 cpuset：在多核机器上设置 cgroup 中进程可以使用的 CPU 和内存（此处内存仅使用于 NUMA 架构） devices：控制 cgroup 中进程对设备的访问 freezer：用于挂起（suspend）和恢复（resume) cgroup 中的进程 memory：用于控制 cgroup 中进程的内存占用 net_els：用于将 cgroup 中进程产生的网络包分类，以便 Linux tc (traffic conoller）可以根据分类区分出来自某个 cgroup 的包并做限流或监控 net_prio：设置 cgroup 中进程产生的网络流量的优先级 ns：这个 subsystem 比较特殊，它的作用是使 cgroup 中的进程在新的 Namespace fork 新进程（NEWNS）时，创建出一个新的 cgroup ，这个 cgroup 包含新的 Namespace 的进程  $ yum install -y libcgroup-tools.x86_64 libcgroup\r$ lssubsys\r   hierarchy：功能是把 cgroup 串成一个树状的结构，一个这样的树便是 hierarchy，通过这种树状结构，Cgroups 可以做到继承。比如，系统对一组定时的任务进程通过 cgroup1 限制了 CPU 的使用率，然后其中有一个定时 dump 日志的进程还需要限制磁盘 IO，为了避免限制了磁盘 IO 之后影响到其他进程，就可以创建 cgroup2 ，使其继承于 cgroup2 并限制磁盘的 IO ，这样 cgroup2 便继承了 cgroup1 中对 CPU 使用率的限制，并且增加了磁盘 IO 的限制而不影响到 cgroup1 中的其他进程。\n  Cgroups 是凭借这 3 个组件的相互协作实现的\n 系统在创建了新的 hierarchy 之后，系统中所有的进程都会加入这个 hierarchy 的 cgroup 根节点，这个 cgroup 根节点是 hierarchy 默认创建的， 2.2.2 节在这个 hierarchy 中创建 cgroup 都是这个 cgroup 根节点的子节点。 一个 subsystem 只能附加到一个 hierarchy 上面 一个 hierarchy 可以附加多个 subsystem 一个进程可以作为多个 cgroup 的成员，但是这些 cgroup 必须在不同的 hierarchy 中 一个进程 fork 出子进程时，子进程将与父进程在同一个 cgroup 中，也可以根据需要将其移动到其它 cgroup 中  Kernel 接口：Cgroups 中的 hierarchy 是一种树状的组织结构，Kernel 为了使对 Cgroups 的配置更直观，是通过一个虚拟的树状文件系统配置 Cgroups 的，通过层级的目录虚拟出 cgroup 树。下面就以一个配置的例子来了解如何操作 Cgroups\n首先要创建并挂载一个 hierarchy（cgroup 树）\n$ cd ~\r# 创建一个目录作为 hierarchy 挂载点\r$ mkdir cgroupdemo\r# 挂载一个 hierarchy\r$ mount -t cgroup -o none,name=cgroup-test cgroup-test ./cgroup-test\r# 挂载后系统在目录下生成了一些默认文件\r$ ls cgroup-test/\rcgroup.clone_children cgroup.procs notify_on_release tasks\rcgroup.event_control cgroup.sane_behavior release_agent\r 这些文件就是这个 hierarchy 中 cgroup 根节点的配置项\n cgroup.clone_children：cpuset 的 subsystem 会读取这个配置文件，如果这个值是 1（默认认是 0），子 cgroup 才会继承父 cgroup 的 cpuset 的配置 cgroup.procs：是树中当前节点 cgroup 中的进程组 ID，现在的位置是在根节点，这个文件中会有现在系统中所有进程组的 ID。 notify_on_release、release agent：两项会一起使用。notify_on_release 标识当这个 cgroup 最后一个进程退出的时候是否执行了 release_agent；release_agent 则是一个路径，通常用作进程退出之后自动清理掉不再使用的 cgroup tasks：标识 cgroup 下面的进程 ID ，如果把一个进程 ID 写到 tasks 文件中，便会将相应的进程加入到这个 cgroup 中  然后在刚刚创建好的 hierarchy 的 cgroup 根节点中扩展出的两个子 cgroup。可以看到，在 cgroup 的目录下创建文件夹时， Kernel 会把文件夹标记为这个 cgroup 的子 cgroup ，它会继承父 cgroup 的属性。\n$ cd cgroup-test\r$ mkdir cgroup-1\r$ mkdir cgroup-2\r$ tree\r.\r├── cgroup-1\r│ ├── cgroup.clone_children\r│ ├── cgroup.event_control\r│ ├── cgroup.procs\r│ ├── notify_on_release\r│ └── tasks\r├── cgroup-2\r│ ├── cgroup.clone_children\r│ ├── cgroup.event_control\r│ ├── cgroup.procs\r│ ├── notify_on_release\r│ └── tasks\r├── cgroup.clone_children\r├── cgroup.event_control\r├── cgroup.procs\r├── cgroup.sane_behavior\r├── notify_on_release\r├── release_agent\r└── tasks\r 在 cgroup 中添加和移动进程：一个进程在一个 Cgroups 的 hierarchy 中，只能在一个 cgroup 节点上存在，系统的所有进程都会默认在根节点上存在，可以将进程移动到其他 cgroup 节点，只需要将进程 ID 写到要移动到的 cgroup 节点的 tasks 文件中即可。\n$ cd cgroup1\r$ echo $$\r4615\r# 将所在的终端进程移动到 cgroup1 中\r$ sh -c \u0026quot;echo $$ \u0026gt;\u0026gt; tasks\u0026quot;\r$ cat /proc/4615/cgroup\rcat /proc/4615/cgroup\r12:name=cgroup-test:/cgroup-1\r11:hugetlb:/\r10:net_prio,net_cls:/\r9:pids:/user.slice\r8:memory:/\r7:cpuset:/\r6:blkio:/\r5:freezer:/\r4:devices:/user.slice\r3:cpuacct,cpu:/\r2:perf_event:/\r1:name=systemd:/user.slice/user-0.slice/session-1.scope\r 通过 subsystem 限制 cgroup 中进程的资源：在上面创建 hierarchy 的时候，这个 hierarchy 并没有关联到任何的 subsystem ，所以没办法通过那个 hierarchy 中的 cgroup 节点限制进程的资源占用，其实系统默认已经为每个 subsystem 创建了一个默认的 hierarchy ，比如 memory hierarchy\n$ mount | grep memory\rcgroup on /sys/fs/cgroup/memory type cgroup (rw,nosuid,nodev,noexec,relatime,seclabel,memory)\r 可以看到，/sys/fs/cgroup/memory 目录便是挂在了 memory subsystem 的 hierarchy 上。下面，就通过在这个 hierarchy 中创建 cgroup ，限制如下进程占用的内存。\n$ cd /sys/fs/cgroup/memory\r# 安装 stress 工具\r$ yum install -y epel-release\r$ yum install -y stress\r# 先在不做限制的情况下，启动一个 stress 进程，看到占用了 200m 内存，然后 ctrl+c 关闭\r$ stress --vm-bytes 200m --vm-keep -m 1\r# 创建一个 cgroup\r$ mkdir test-limit-memory\r$ cd test-limit-memor\r# 设置最大 cgroup 的最大内存占用为 lOOMB\r$ sh -c \u0026quot;echo '100m' \u0026gt; memory.limit_in_bytes\u0026quot;\r# 将当前进程移动到这个 cgroup\r$ sh -c \u0026quot;echo $$ \u0026gt; tasks\u0026quot;\r# 再次启动一个 200m 内存占用的 stress 进程，发现只占用了 100m 内存，说明限制内存资源成功\r$ stress --vm-bytes 200m --vm-keep -m 1\r docker 是如何使用 Cgroups 的 Docker 是通过 Cgroups 实现容器资源限制和监控。Docker 通过为每个容器创建 cgroup 并通过 cgroup 去配置资源限制和资源监控\n$ docker pull ubuntu\r# 启动一个 ubuntu，通过 -m 设置内存限制\r$ docker run -itd -m 128m ubuntu\r50101823a6e4d35d9d9bdc3b981b8ff5d80bd666d2bf465d77c2915698ff3309\r# 进入这个 ubuntu 容器实例的 cgroup 文件\r$ cd /sys/fs/cgroup/memory/docker/50101823a6e4d35d9d9bdc3b981b8ff5d80bd666d2bf465d77c2915698ff3309\r# 查看 cgroup 的内存限制\r$ cat memory.limit_in_bytes\r134217728\r# 查看 cgroup 中进程所使用的内存大小\r$ cat memory.usage_in_bytes 557056\r 用 Go 语言实现通过 cgroup 限制容器的资源 在 Namespace 容器的 demo 基础上， 加上 cgroup 的限制， 实现一个 demo 使其能够具有限制容器内存的功能。\nconst cgroupMemoryHierarchyMount = \u0026quot;/sys/fs/cgroup/memory\u0026quot;\rfunc main() {\rif os.Args[0] == \u0026quot;/proc/self/exe\u0026quot; {\r// 容器进程\rfmt.Printf(\u0026quot;current pid %d\\n\u0026quot;, syscall.Getpid())\rcmd := exec.Command(\u0026quot;sh\u0026quot;, \u0026quot;-c\u0026quot;, `stress --vm-bytes 200m --vm-keep -m 1`)\rcmd.SysProcAttr = \u0026amp;syscall.SysProcAttr{}\rcmd.Stdin = os.Stdin\rcmd.Stdout = os.Stdout\rcmd.Stderr = os.Stderr\rif err := cmd.Run(); err != nil {\rfmt.Println(err)\ros.Exit(1)\r}\r}\r// 执行这个 cmd 就会触发上面的 if 块从而运行一个 stress 进程\rcmd := exec.Command(\u0026quot;/proc/self/exe\u0026quot;)\rcmd.SysProcAttr = \u0026amp;syscall.SysProcAttr{\rCloneflags: syscall.CLONE_NEWUTS | syscall.CLONE_NEWPID | syscall.CLONE_NEWNS,\r}\rcmd.Stdin = os.Stdin\rcmd.Stdout = os.Stdout\rcmd.Stderr = os.Stderr\rif err := cmd.Start(); err != nil {\rfmt.Println(\u0026quot;ERROR\u0026quot;, err)\ros.Exit(1)\r} else {\r// 得到 fork 出来的进程映射在外部命名空间的 pid\rfmt.Printf(\u0026quot;cmd.Process.Pid: %v\\n\u0026quot;, cmd.Process.Pid)\r// 在系统默认创建挂载了 memory subsystem 的 Hierarchy 上创建 cgroup\ros.Mkdir(path.Join(cgroupMemoryHierarchyMount, \u0026quot;testmemorylimit\u0026quot;), 0755)\r// 将容器进程加入到这个 cgroup\rerr = ioutil.WriteFile(path.Join(cgroupMemoryHierarchyMount, \u0026quot;testmemorylimit\u0026quot;, \u0026quot;tasks\u0026quot;), []byte(strconv.Itoa(cmd.Process.Pid)), 0644)\r// 限制 cgroup 进程使用\rerr = ioutil.WriteFile(path.Join(cgroupMemoryHierarchyMount, \u0026quot;testmemorylimit\u0026quot;, \u0026quot;memory.limit_in_bytes\u0026quot;), []byte(\u0026quot;100m\u0026quot;), 0644)\r}\rcmd.Process.Wait()\r}\r # 运行起来确实被限制到 100m 内存\rgo run main.go [/tmp/go-build034195069/b001/exe/main]\r31964[/proc/self/exe]\rcurrent pid 1\rstress: info: [6] dispatching hogs: 0 cpu, 0 io, 1 vm, 0 hdd\r Union File System Union File System ，简称 UnionFS，是一种为 Linux、FreeBSD、NetBSD 操作系统设计的，把其它文件系统联合到一个联合挂载点的文件系统服务。它使用 branch 把不同文件系统的文件和目录“透明地”覆盖，形成一个单一一致的文件系统，这些 branch 或是 read-only 的，或是 read-write 的，所以当对这个虚拟后的联合文件系统进行写操作时，系统是真正写到了一个新的文件中，看起来这个虚拟后的联合文件系统是可以对任何文件进行操作的，但是其实它并没有改变原来的文件，这是因为 unionfs 用到了一个重要的资源管理技术，叫做写时复制。\n写时复制（copy-on-write，下文简称 CoW），也叫隐式共享，是一种对可修改资源实现高效复制的资源管理技术。它的思想是，如果一个资源是重复的，但没有任何修改，这时并不需要立即创建一个新的资源，这个资源可以被新旧实例共享。创建新资源发生在第一次写操作，也就是对资源进行修改的时候。通过这种资源共享的方式，可以显著地减少未修改资源复制带来的消耗，但是也会在进行资源修改时增加小部分的开销。\n用一个经典例子来解释。Knoppix，一个用于 Linux 演示、光盘教学和商业产品演示的 Linux 发行版，就是将 D-ROM 或 DVD 和一个存在于可读写设备（比如 U 盘）上的叫作 knoppix.img 的文件系统联合起来的系统。这样任何对 CD/DVD 上文件的改动都会被应用在 U 盘上，而不改变原来的 CD/DVD 上的内容\nAUFS AUFS ，英文全称是 Advanced Multi-Layered Unification Filesystem 曾经也叫 Acronym Multi-Layered Unification Filesystem、Another Multi-Layered Unification Filesystem。AUFS 完全重写了早期的 UnionFS 1.x，其主要目的是为了可靠性和性能，井且引入了一些新的功能，比如可写分支的负载均衡。一些实现已经被纳入 UnionFS 2.x 版本。\nDocker 是如何使用 AUFS AUFS 时 Docker 选用的第一种存储驱动。AUFS 具有快速启动容器、高效利用存储和内存的优点。直到现在， AUFS 仍然是 Docker 支持的一种存储驱动类型。接下来介绍Docker 是如何利用 AUFS 存储 image 和 container的\nimage layer 和 AUFS 每一个 Docker image 都是由一系列 read-only layer 组成的。image layer 的内容都存储在 Docker hosts filesystem 的 /var/lib/docker/aufs/diff 目录下。而 /var/lib/docker/aufs/layers 目录，则存储着 image layer 如何堆栈这些 layer 的 metadata。\n准备一台安装了 Docker 11 .2 的机器。在没有拉取任何镜像、启动任何容器的情况下，执行如下命令\n$ docker pull ubuntu:l5.04\r$ ls /var/lib/docker/aufs/diff\r$ ls /var/lib/docker/aufs/mnt\r$ cat /var/lib/docker/aufs/layers/\r centos7默认不支持aufs，需要升级带有 aufs 的内核：https://www.jianshu.com/p/63fdb0c0659c ，https://github.com/bnied/kernel-ml-aufs\n# 查看 kenel 支持的文件系统\r$ cat /proc/filesystems | grep aufs\r# 下载源。这里的内核版本比较高，是最新的 5.xxx 版本，不过暂时用来做 demo 足够了\r$ wget https://yum.spaceduck.org/kernel-ml-aufs/kernel-ml-aufs.repo -O /etc/yum.repo.d/kernel-ml-aufs.repo\r$ yum clean all \u0026amp;\u0026amp; yum makecache\r$ yum install kernel-ml-aufs kernel-ml-aufs-devel -y\r# 重启计算机。启动时注意选择该 5.xxx 版本的内核\r$ reboot\r$ cat /proc/filesystems | grep aufs\r 我的 docker version 是20.10.6了，默认使用 overlay2，所以没有 aufs 文件夹\n# 查看docker 使用的存储文件系统\r$ docker info | grep Storage\rStorage Driver: overlay2\r 在 /etc/docker/daemon.json 增加如下配置可以修改 docker 的存储文件系统为aufs\n{\r\u0026quot;storage-driver\u0026quot;: \u0026quot;overlay2\u0026quot;\r}\r # 重启 docker 再次查看\r$ service docker restart\r$ docker info | grep Storage\rStorage Driver: aufs\r 拉取镜像后 docker pull 结果显示 ubuntu:18.04 镜像共有 3个 layer，在执行命令的结果中也3个对应的存储文件目录。\n，Docker 1.10 之后，diff 目录下的存储镜像 layer 文件夹不再与镜像 ID 相同# 拉取一个 ubuntu\r$ docker pull ubuntu:18.04\r18.04: Pulling from library/ubuntu\r6e0aa5e7af40: Pull complete d47239a868b3: Pull complete 49cbb10cca85: Pull complete Digest: sha256:122f506735a26c0a1aff2363335412cfc4f84de38326356d31ee00c2cbe52171\rStatus: Downloaded newer image for ubuntu:18.04\rdocker.io/library/ubuntu:18.04\r# layer 的 diff 文件夹\r$ls /var/lib/docker/aufs/diff\r2f70cfa4439df02ada921ed8ce1141cf9cdc9787622dd98b4a460b968c65bb14\r42f094424a3c6a75bfca9569ebe885f136e52bd2874a2fa6489f4ad79253d4c9\r52452e2eaaa2bc86f01b97c43b26676ca8606b13844bc78dd45596d36afed680\r# layer 的 挂载 文件夹\r$ ls /var/lib/docker/aufs/mnt/\r2f70cfa4439df02ada921ed8ce1141cf9cdc9787622dd98b4a460b968c65bb14\r42f094424a3c6a75bfca9569ebe885f136e52bd2874a2fa6489f4ad79253d4c9\r52452e2eaaa2bc86f01b97c43b26676ca8606b13844bc78dd45596d36afed680\r# layer 文件\r$ ls /var/lib/docker/aufs/layers/\r2f70cfa4439df02ada921ed8ce1141cf9cdc9787622dd98b4a460b968c65bb14\r42f094424a3c6a75bfca9569ebe885f136e52bd2874a2fa6489f4ad79253d4c9\r52452e2eaaa2bc86f01b97c43b26676ca8606b13844bc78dd45596d36afed680\r# 在 layer 文件记录堆栈里位于某 layer 之下的其它 layer，即 layers 之间的关系\r$ cat /var/lib/docker/aufs/layers/42f094424a3c6a75bfca9569ebe885f136e52bd2874a2fa6489f4ad79253d4c9 52452e2eaaa2bc86f01b97c43b26676ca8606b13844bc78dd45596d36afed680\r2f70cfa4439df02ada921ed8ce1141cf9cdc9787622dd98b4a460b968c65bb14\r$ cat /var/lib/docker/aufs/layers/52452e2eaaa2bc86f01b97c43b26676ca8606b13844bc78dd45596d36afed680 2f70cfa4439df02ada921ed8ce1141cf9cdc9787622dd98b4a460b968c65bb14\r# 可以看到镜像 id 不与上面任何一个相同。因为Docker 1.10 之后，diff 目录下的存储镜像 layer 文件夹不再与镜像 ID 相同\r$ docker images\rREPOSITORY TAG IMAGE ID CREATED SIZE\rubuntu 18.04 3339fde08fc3 3 weeks ago 63.3MB\r // todo: 实操\n接下来以 ubuntu:18.04 镜像为基础镜像，创建一个名为 changed-ubuntu 的镜像。这个镜像只是在镜像的 /tmp 文件夹中添加一个写了 \u0026ldquo;Hello world\u0026rdquo; 的文件。可以使用下面的 Dockerfile 来实现\nFROM ubuntu:18.04\rRUN ehco \u0026quot;Hello world\u0026quot; \u0026gt; /tmp/newfile\r # 在Dockefile 所在位置通过 docker build 构建镜像\r$ docker build -t changed-ubuntu\r# 查看镜像\r$ docker images\r# 查看镜像使用了哪些 image layer\r$ docker history changed-ubuntu\r 从输出中可以看到 9d8602c9aee1 image layer 位于最上层，只有 12B 大小，由如下命令创建 /bin/sh -c echo ” Hello world\u0026quot; \u0026gt; /tmp/newfile 。也就是说，changed-ubuntu 镜像只占用了12B 的磁盘空间，这证明了 AUFS 是如何高效使用磁盘空间的。而下面 4 层 image layer，则是共享地构成 ubuntu:18.04 镜像的 4 个 image layer。\u0026ldquo;missing\u0026rdquo; 标记的 layer，是自 Docker 1.10 之后，一个镜像的 image layer 的 image history 数据都存储在一个文件中导致的，这是 Docker官方认为的正常行为\n# 再次查看 diff、mnt 等\r$ ls /var/lib/docker/aufs/diff\r$ ls /var/lib/docker/aufs/mnt\r 可以看到 diff、mnt 目录均多了一个文件夹 xxxxxx\u0026hellip;。当使用如下命令来查看它的 metadata 时，可以看到，它前面的 layer 就是 ubuntu:18.04 镜像所使用的 image layer。进一步探查 diff 发现多了一个 tmp/newfile 文件，文件中只有一行 \u0026ldquo;Hello world\u0026rdquo;。至此就完整分析出 了 image layer 和 AUFS 之间是如何通过共享文件和文件夹来实现镜像存储的\n$ cat /var/lib/docker/aufs/layers/xxxxxx...\r$ cat /var/lib/docker/aufs/diff/xxxxxx...\r$ cat /var/lib/docker/aufs/diff/xxxxxx.../tmp/newfile\r 自己动手写 AUFS ","date":"0001-01-01","permalink":"https://yuanyatianchi.github.io/post/it.go.project.yuanyadocker/","tags":["it","go.project"],"title":"YuanyaDocker"},{"content":"什么是时序图 时序图(Sequence Diagram)，又名序列图、循序图，是一种UML交互图。它通过描述对象之间发送消息的时间顺序显示多个对象之间的动态协作。\n使用场景 时序图的使用场景非常广泛，几乎各行各业都可以使用。当然，作为一个软件工作者，我这边主要列举和软件开发有关的场景。\n1. 梳理业务流程\n一般的软件开发都是为了支撑某个具体的业务。有时候业务的流程会比较复杂，涉及到多种角色，这时就可以使用时序图来梳理这个业务逻辑。这样会使业务看起来非常清晰，代码写起来也是水到渠成的事情了。\n\n2. 梳理开源软件\n作为一个合格的程序员，阅读源代码的能力一定要过关。一般成熟框架的源代码调用深度都比较深，类之间的调用关系也比较复杂。我喜欢用时序图来梳理框架中这些对象之间的关系。比如再看Tomcat启动流程的过程中，我就时序图梳理了各个组件之间的关系，看起来层次非常清楚，也便于记忆。\n\n时序图的角色 我们在画时序图时会涉及下面7种元素：\n 角色(Actor) 对象(Object) 生命线(LifeLine) 控制焦点(Activation) 消息(Message) 自关联消息 组合片段。  其中前6种是比较常用和重要的元素，最后的组合片段元素不是很常用，但是比较复杂。我们先介绍前6种元素，再单独介绍组合片段元素。\n1. 角色(Actor)\n系统角色，可以是人或者其他系统和子系统。以一个小人图标表示。\n2. 对象(Object)\n对象位于时序图的顶部,以一个矩形表示。对象的命名方式一般有三种：\n 对象名和类名。例如：华为手机:手机、loginServiceObject:LoginService； 只显示类名，不显示对象，即为一个匿名类。例如：:手机、:LoginSservice。 只显示对象名，不显示类名。例如：华为手机:、loginServiceObject:。  3. 生命线(LifeLine)\n时序图中每个对象和底部中心都有一条垂直的虚线，这就是对象的生命线(对象的时间线)。以一条垂直的虚线表。\n4. 控制焦点(Activation)\n控制焦点代表时序图中在对象时间线上某段时期执行的操作。以一个很窄的矩形表示。\n5. 消息(Message)\n表示对象之间发送的信息。消息分为三种类型。\n  同步消息(Synchronous Message) 消息的发送者把控制传递给消息的接收者，然后停止活动，等待消息的接收者放弃或者返回控制。用来表示同步的意义。以一条实线和实心箭头表示。\n  异步消息(Asynchronous Message)\n消息发送者通过消息把信号传递给消息的接收者，然后继续自己的活动，不等待接受者返回消息或者控制。异步消息的接收者和发送者是并发工作的。以一条实线和大于号表示。\n  返回消息(Return Message) 返回消息表示从过程调用返回。以小于号和虚线表示。\n  6. 自关联消息\n表示方法的自身调用或者一个对象内的一个方法调用另外一个方法。以一个半闭合的长方形+下方实心剪头表示。\n下面举例一个时序图的列子，看下上面几种元素具体的使用方式。\n\n7. 组合片段\n组合片段用来解决交互执行的条件和方式，它允许在序列图中直接表示逻辑组件，用于通过指定条件或子进程的应用区域，为任何生命线的任何部分定义特殊条件和子进程。组合片段共有13种，名称及含义如下：\n   组合名称 组合含义     ref 引用其他地方定义的组合片段   alt 在一组行为中根据特定的条件选择某个交互   opt 表示一个可选的行为   break 提供了和编程语言中的break类拟的机制   par 支持交互片段的并发执行   seq 强迫交互按照特定的顺序执行   strict 明确定义了一组交互片段的执行顺序   neg 用来标志不应该发生的交互   region 标志在组合片段中先于其他交互片断发生的交互   ignore 明确定义了交互片段不应该响应的消息   consider 明确标志了应该被处理的消息   assert 标志了在交互片段中作为事件唯一的合法继续者的操作数   loop 说明交互片段会被重复执行    组合片段的功能平时用的不是很多，具体使用时可以参考本文最后关于组合片段的文章，这边不做深入介绍了。\n画图工具推荐 processon\n ProcessOn是一个在线作图工具的聚合平台，它可以在线画流程图、思维导图、UI原型图、UML、网络拓扑图、组织结构图等等，您无需担心下载和更新的问题，不管Mac还是Windows，一个浏览器就可以随时随地的发挥创意，规划工作； 您可以把作品分享给团队成员或好友，无论何时何地大家都可以对作品进行编辑、阅读和评论； ProcessOn不仅仅汇聚着强大的作图工具，这里还有着海量的图形化知识资源我们尽可能的将有价值的知识进行梳理，传递到您的眼前。  \n","date":"0001-01-01","permalink":"https://yuanyatianchi.github.io/post/tool.%E6%97%B6%E5%BA%8F%E5%9B%BE/","tags":["tool"],"title":"时序图"},{"content":"网络编程 ","date":"0001-01-01","permalink":"https://yuanyatianchi.github.io/post/it.base.%E7%BD%91%E7%BB%9C%E7%BC%96%E7%A8%8B/","tags":["it","base"],"title":"网络编程"},{"content":"面试大纲   算法\n ascii：A-65、a-97 dp：base case、状态、选择、dp定义 dfs：backtrack(路径, 选择列表)、结束条件-决策树底层 bfs：要扩散的节点、已扩散的节点 math：平方根Sqrt、Pow 方、Abs绝对值Ceil向上取整、Floor向下取整 string：Trim去掉两端、子序列位置In-dex、Contains匹配子序列、Replace替换n次，Split拆分    设计模式\n  网络\n tcp：可靠，字节流，3次握手、4次挥手。udp：不可靠，报文流，直接发送 rpc：远程过程调用，轻量，协议无关、序列化无关，消息头保存 http基于文本解析，http/2基于二进制解析，socket是对tcp等实现提供网络编程的方法 coocie保存再浏览器端，session保存在服务器端    go\n 类型：string(SRODATA)、array、slice(追加扩容growslice、对齐)、map(扩容)，结构、类型、初始化、访问、 函数调用栈：一个函数调用另一个函数，编译器就会生成一个call指令，call指令做两件事，一是将下一条指令的地址入栈，就是返回地址，被调用函数结束后跳回到这里，二是跳转到被调用函数入口处执行 闭包：带捕获变量的funcval，是否修改，参数、返回值作捕获变量 方法、defer、panic和recover、类型系统、接口、断言、reflect、闭包（带捕获变量的funcval） GPM：工作窃取、让出（sleep、channel（context）、io事件）、抢占（stackPreempt标识、信号、系统调用），gopark、goready ，监控线程 GC：标记清除、主体并发增量式回收、插入与删除的混合写屏障    ​ mysql\n 存储引擎：innodb、myisam、memory 的 存储、索引、锁、表空间、事务、缓存 锁：表锁（读共享、写独占）、行锁（读共享、写排他、意向共享、意向排他）、锁算法（Record Lock、Gap Lock、Next-Key Lock）、乐观锁（cas、mvcc）、悲观锁（mutex） 事务：ACID（原子一致隔离持久），脏读、不可重复读、幻读，READ UNCOMMITED、READ COMMITED、REPEATABLE READ、SERIALIZABLE 三大范式：列的原子性（最小属性不可再分化）、行的唯一性（非主键列完全依赖主键列，不是部分依赖关键字）、非主键列直接依赖主键列 慢查询：配置slow_query_log=1、mysqldumpslow工具 索引：普通索引、唯一索引、复合索引、聚集索引、聚簇索引，执行计划 Explain（select_type、key、keylen、ref、Extra(temporary、index、where、join buffer)） sql优化：全值匹配（联合索引中更多的列被使用）、最左前缀、范围条件最后、索引覆盖、慎用不等于，主要是避免索引失效变成全表扫，临时表    redis\n  API：string、hash、list、set、zset、slowlog、pipeline、pubsub、bitmap、HyperLogLog、GEO\n  持久化：RDB（触发机制：save、bgsave、配置bgsave、全量复制）、AOF（刷新策略：severysec、always、everysecno，AOF重写）\n  缓存更新策略：最大内存+LRU、过期时间超时剔除、业务上主动随数据库一起更新，高低一致性）\n  缓存穿透：key对应的数据在缓存和数据源都不存在。缓存有过期时间的空对象；布隆过滤器拦截\n  缓存击穿：key对应的数据存在，但在redis中过期。分布式锁防止并发访问db\n  缓存雪崩：大量缓存集中失效。加锁；请求队列；设计时就要将缓存失效时间分散开尽量不要使大量数据的过期时间在同一个时间段\n  无底洞：集群会使如mget这样的批操作非原子化，使更多的机器!=更高的性能。不要盲目加机器；多考虑命令优化、网络通信优化\n  布隆过滤器：二进制向量01数组、多hash、多元素，本地、远程\n  主从：全量复制、部分复制、偏移量、自动故障转移、读写分离。全量复制：需要尽量规避。第一次不可避免，\n    etcd：KV（op）、Lease（KeepAlive）、Watcher（revition）、txn，key有序，内置grpc，raft\n  mongo：bson，database、collection、document、field\n  kafka：topic、partition。消息模型、事务、丢失（三端）、重复（前置、唯一 幂等）、积压（两端 ），异步系统、异步网络io、序列化、协议、磁盘io、数据压缩，批处理，zookeeper\n  ","date":"0001-01-01","permalink":"https://yuanyatianchi.github.io/post/it.base.%E9%9D%A2%E8%AF%95%E5%A4%A7%E7%BA%B2/","tags":["it","base"],"title":"面试大纲"}]