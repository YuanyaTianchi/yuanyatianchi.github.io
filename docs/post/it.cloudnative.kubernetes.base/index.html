<!DOCTYPE html>
<html lang="en">

<head>
    
<meta charset="utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no" />
<meta name="HandheldFriendly" content="True" />
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />
<meta name="generator" content="Hugo 0.83.1" />


<link rel="shortcut icon" href="https://cdn.jsdelivr.net/gh/amzrk2/cdn-stcapi@1/favicons/favicon.ico" />


<title>Kubernetes.base - 鸢雅</title>


<meta name="author" content="DSRKafuU" />


<meta name="description" content="Kubernetes.base" />


<meta name="keywords" content="it, cloudnative" />


<meta property="og:title" content="Kubernetes.base" />
<meta name="twitter:title" content="Kubernetes.base" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://yuanyatianchi.github.io/post/it.cloudnative.kubernetes.base/" /><meta property="og:description" content="Kubernetes.base" />
<meta name="twitter:description" content="Kubernetes.base" /><meta property="og:image" content="https://yuanyatianchi.github.io/img/og.png" />
<meta name="twitter:card" content="summary_large_image" />
<meta name="twitter:image" content="https://yuanyatianchi.github.io/img/og.png" />


<style>
    @media (prefers-color-scheme: dark) {
        body[data-theme='auto'] img {
            filter: brightness(60%);
        }
    }

    body[data-theme='dark'] img {
        filter: brightness(60%);
    }
</style>



<link rel="stylesheet" href="https://yuanyatianchi.github.io/assets/css/fuji.min.css" />





</head>

<body data-theme="auto">
    <script data-cfasync="false">
  
  var fujiThemeData = localStorage.getItem('fuji_data-theme');
  
  if (!fujiThemeData) {
    localStorage.setItem('fuji_data-theme', 'auto');
  } else {
    
    if (fujiThemeData !== 'auto') {
      document.body.setAttribute('data-theme', fujiThemeData === 'dark' ? 'dark' : 'light');
    }
  }
</script>
    <header>
    <div class="container-lg clearfix">
        <div class="col-12 header">
            <a class="title-main" href="https://yuanyatianchi.github.io">鸢雅</a>
            
            <span class="title-sub">A minimal Hugo theme.</span>
            
        </div>
    </div>
</header>

    <main>
        <div class="container-lg clearfix">
            
            <div class="col-12 col-md-9 float-left content">
                
<article>
    
    <h2 class="post-item post-title">
        <a href="https://yuanyatianchi.github.io/post/it.cloudnative.kubernetes.base/">Kubernetes.base</a>
    </h2>
    <div class="post-item post-meta">
        <span><i class="iconfont icon-today-sharp"></i>&nbsp;0001-01-01</span><span><i class="iconfont icon-file-tray-sharp"></i>&nbsp;48128 words</span><span><i class="iconfont icon-pricetags-sharp"></i>&nbsp;<a href="/tags/it">it</a>&nbsp;<a href="/tags/cloudnative">cloudnative</a>&nbsp;</span>

    </div>
    
    <div class="post-content markdown-body">
        <h1 id="kubernetesbase">Kubernetes.base</h1>
<h2 id="架构设计与实现原理">架构设计与实现原理</h2>
<p>Kubernetes 为软件工程师提供了强大的容器编排能力，模糊了开发和运维之间的边界，让我们开发、管理和维护一个大型的分布式系统和项目变得更加容易</p>
<h3 id="介绍">介绍</h3>
<p>Kubernetes 被定义成一个用于自动化部署、扩容和管理容器应用的开源系统；它将一个分布式软件的一组容器打包成一个个更容易管理和发现的逻辑单元。Kubernetes 目前是 Cloud Native Computing Foundation (CNCF) 的项目并且是很多公司管理分布式系统的解决方案。</p>
<p>Kubernetes 将已经打包好的应用镜像进行编排，所以如果没有容器技术的发展和微服务架构中复杂的应用关系，其实也很难找到合适的应用场景去使用，所以在这里我们会简单介绍 Kubernetes 的两大『依赖』——容器技术和微服务架构</p>
<h4 id="容器技术">容器技术</h4>
<p>Docker 已经是容器技术的事实标准了，作者在前面的文章中 <a href="https://draveness.me/docker" target="_blank">Docker 核心技术与实现原理</a> 曾经介绍过 Docker 的实现主要依赖于 Linux 的 namespace、cgroups 和 UnionFS</p>
<p>它让开发者将自己的应用以及依赖打包到一个可移植的容器中，让应用程序的运行可以实现环境无关。</p>
<p>我们能够通过 Docker 实现进程、网络以及挂载点和文件系统隔离的环境，并且能够对宿主机的资源进行分配，这能够让我们在同一个机器上运行多个不同的 Docker 容器，任意一个 Docker 的进程都不需要关心宿主机的依赖，都各自在镜像构建时完成依赖的安装和编译等工作，这也是为什么 Docker 是 Kubernetes 项目的一个重要依赖。</p>
<h4 id="微服务架构">微服务架构</h4>
<p>如果今天的软件并不是特别复杂并且需要承载的峰值流量不是特别多，那么后端项目的部署其实也只需要在虚拟机上安装一些简单的依赖，将需要部署的项目编译后运行就可以了。</p>
<p>但是随着软件变得越来越复杂，一个完整的后端服务不再是单体服务，而是由多个职责和功能不同的服务组成，服务之间复杂的拓扑关系以及单机已经无法满足的性能需求使得软件的部署和运维工作变得非常复杂，这也就使得部署和运维大型集群变成了非常迫切的需求。</p>
<h4 id="小结">小结</h4>
<p>Kubernetes 的出现不仅主宰了容器编排的市场，更改变了过去的运维方式，不仅将开发与运维之间边界变得更加模糊，而且让 DevOps 这一角色变得更加清晰，每一个软件工程师都可以通过 Kubernetes 来定义服务之间的拓扑关系、线上的节点个数、资源使用量并且能够快速实现水平扩容、蓝绿部署等在过去复杂的运维操作。</p>
<h3 id="设计">设计</h3>
<p>这一小节我们将介绍 Kubernetes 的一些设计理念，这些关键字能够帮助了解 Kubernetes 在设计时所做的一些选择：</p>
<p><img class="img-zoomable" src="https://img.draveness.me/2018-11-25-kubernetes-design.png" alt="" />
</p>
<p>这里将按照顺序分别介绍声明式、显式接口、无侵入性和可移植性这几个设计的选择能够为我们带来什么</p>
<h4 id="声明式">声明式</h4>
<p>声明式（Declarative）的编程方式一直都会被工程师们拿来与命令式（Imperative）进行对比，这两者是完全不同的编程方法。我们最常接触的其实是命令式编程，它要求我们描述为了达到某一个效果或者目标所需要完成的指令，常见的编程语言 Go、Ruby、C++ 其实都为开发者了命令式的编程方法，</p>
<p>在 Kubernetes 中，我们可以直接使用 YAML 文件定义服务的拓扑结构和状态：</p>
<pre><code class="language-yaml">apiVersion: v1
kind: Pod
metadata:
  name: rss-site
  labels:
    app: web
spec:
  containers:
    - name: front-end
      image: nginx
      ports:
        - containerPort: 80
    - name: rss-reader
      image: nickchase/rss-php-nginx:v1
      ports:
        - containerPort: 88
</code></pre>
<p>这种声明式的方式能够大量地减少使用者的工作量，极大地增加开发的效率，这是因为声明式能够简化需要的代码，减少开发人员的工作，如果我们使用命令式的方式进行开发，虽然在配置上比较灵活，但是带来了更多的工作</p>
<pre><code class="language-sql">SELECT * FROM posts WHERE user_id = 1 AND title LIKE 'hello%';
</code></pre>
<p>SQL 其实就是一种常见的声明式『编程语言』，它能够让开发者自己去指定想要的数据是什么，Kubernetes 中的 YAML 文件也有着相同的原理，我们可以告诉 Kubernetes 想要的最终状态是什么，而它会帮助我们从现有的状态进行迁移。</p>
<p><img class="img-zoomable" src="https://img.draveness.me/2018-11-25-kubernetes-declarative-api.png" alt="" />
</p>
<p>如果 Kubernetes 采用命令式编程的方式提供接口，那么工程师可能就需要通过代码告诉 Kubernetes 要达到某个状态需要通过哪些操作，相比于更关注状态和结果声明式的编程方式，命令式的编程方式更强调过程。</p>
<p>总而言之，Kubernetes 中声明式的 API 其实指定的是集群期望的运行状态，所以在出现任何不一致问题时，它本身都可以通过指定的 YAML 文件对线上集群进行状态的迁移，就像一个水平触发的系统，哪怕系统错过了相应的事件，最终也会根据当前的状态自动做出做合适的操作。</p>
<h4 id="显式接口">显式接口</h4>
<p>第二个 Kubernetes 的设计规范其实就是 —— 不存在内部的私有接口，所有的接口都是显示定义的，组件之间通信使用的接口对于使用者来说都是显式的，我们都可以直接调用。</p>
<p><img class="img-zoomable" src="https://img.draveness.me/2018-11-25-kubernetes-external-api.png" alt="" />
</p>
<p>当 Kubernetes 的接口不能满足工程师的复杂需求时，我们需要利用已有的接口实现更复杂的特性，在这时 Kubernetes 的这一设计就不会成为自定义需求的障碍。</p>
<h4 id="无侵入性">无侵入性</h4>
<p>为了尽可能满足用户（工程师）的需求，减少工程师的工作量与任务并增强灵活性，Kubernetes 为工程师提供了无侵入式的接入方式，每一个应用或者服务一旦被打包成了镜像就可以直接在 Kubernetes 中无缝使用，不需要修改应用程序中的任何代码。</p>
<p><img class="img-zoomable" src="https://img.draveness.me/2020-09-19-16005073539151/kuberentes-non-invasive.png" alt="" />
</p>
<p>Docker 和 Kubernetes 就像包裹在应用程序上的两层，它们两个为应用程序提供了容器化以及编排的能力，在应用程序内部却不需要任何的修改就能够在 Docker 和 Kubernetes 集群中运行，这是 Kubernetes 在设计时选择无侵入带来最大的好处，同时无侵入的接入方式也是目前几乎所有应用程序或者服务都必须考虑的一点。</p>
<h4 id="可移植性">可移植性</h4>
<p>在微服务架构中，我们往往都会让所有处理业务的服务变成无状态的服务，以前在内存中存储的数据、Session 等缓存，现在都会放到 Redis、ETCD 等数据库中存储，微服务架构要求我们对业务进行拆分并划清服务之间的边界，所以有状态的服务往往会对架构的水平迁移带来障碍。</p>
<p>然而有状态的服务其实是无可避免的，我们将每一个基础服务或者业务服务都变成了一个个只负责计算的进程，但是仍然需要有其他的进程负责存储易失的缓存和持久的数据，Kubernetes 对这种有状态的服务也提供了比较好的支持。</p>
<p><strong>PersistentVolume</strong>：Kubernetes 引入了 <code>PersistentVolume</code> 和 <code>PersistentVolumeClaim</code> 的概念用来屏蔽底层存储的差异性，目前的 Kubernetes 支持下列类型的 <code>PersistentVolume</code>：</p>
<p><img class="img-zoomable" src="https://img.draveness.me/2018-11-25-kubernetes-persistent-volume.png" alt="" />
</p>
<p>这些不同的 <code>PersistentVolume</code> 会被开发者声明的 <code>PersistentVolumeClaim</code> 分配到不同的服务中，对于上层来讲所有的服务都不需要接触 <code>PersistentVolume</code>，只需要直接使用 <code>PersistentVolumeClaim</code> 得到的卷就可以了。</p>
<h3 id="架构">架构</h3>
<p><strong>架构</strong>：Kubernetes 遵循非常传统的客户端服务端架构，客户端通过 RESTful 接口或者直接使用 kubectl 与 Kubernetes 集群进行通信，这两者在实际上并没有太多的区别，后者也只是对 Kubernetes 提供的 RESTful API 进行封装并提供出来</p>
<p><img class="img-zoomable" src="https://img.draveness.me/2018-11-25-kubernetes-architecture.png" alt="" />
</p>
<p><strong>组成</strong>：每一个 Kubernetes 集群都由一组 Master 节点和一系列的 Worker 节点组成，其中 Master 节点主要负责存储集群的状态并为 Kubernetes 对象分配和调度资源。</p>
<h4 id="master">Master</h4>
<p>作为管理集群状态的 Master 节点，它主要负责接收客户端的请求，安排容器的执行并且运行控制循环，将集群的状态向目标状态进行迁移，Master 节点内部由三个组件构成：</p>
<p><img class="img-zoomable" src="https://img.draveness.me/2018-11-25-kubernetes-master-node.png" alt="" />
</p>
<p><strong>API Server</strong>：负责处理来自用户的请求，其主要作用就是对外提供 RESTful 的接口，包括用于查看集群状态的读请求以及改变集群状态的写请求，也是唯一一个与 etcd 集群通信的组件。</p>
<p><strong>Controller</strong>：管理器运行了一系列的控制器进程，这些进程会按照用户的期望状态，在后台不断地调节整个集群中的对象，当服务的状态发生了改变，控制器就会发现这个改变并且开始向目标状态迁移。</p>
<p><strong>Scheduler</strong>：调度器为 Kubernetes 中运行的 Pod 选择部署的 Worker 节点，它会根据用户的需要选择最能满足请求的节点来运行 Pod，它会在每次需要调度 Pod 时执行。</p>
<h4 id="worker">Worker</h4>
<p>其它的 Worker 节点实现就相对比较简单了，它主要由 kubelet 和 kube-proxy 两部分组成：</p>
<p><img class="img-zoomable" src="https://img.draveness.me/2018-11-25-kubernetes-worker-node.png" alt="" />
</p>
<p><strong>kubelet</strong>：是一个节点上的主要服务，它周期性地从 API Server 接受新的或者修改的 Pod 规范，并且保证节点上的 Pod 和其中容器的正常运行，还会保证节点会向目标状态迁移，该节点仍然会向 Master 节点发送宿主机的健康状况。</p>
<p><strong>kube-proxy</strong>：是一个运行在各个节点上的代理服务，负责宿主机的子网管理，同时也能将服务暴露给外部，其原理就是在多个隔离的网络中把请求转发给正确的 Pod 或者容器。</p>
<h3 id="实现原理">实现原理</h3>
<p>到现在，我们已经对 Kubernetes 有了一些简单的认识和了解，也大概清楚了 Kubernetes 的架构，在这一小节中我们将介绍 Kubernetes 中的一些重要概念和实现原理。</p>
<h4 id="对象">对象</h4>
<p><strong>Kubernetes 对象</strong>：是系统中的持久实体，它使用这些对象来表示集群中的状态，这些对象能够描述：容器化应用，应用资源，重启/升级 策略。</p>
<p><img class="img-zoomable" src="https://img.draveness.me/2018-11-25-kubernetes-object.png" alt="" />
</p>
<p>这些对象描述了哪些应用应该运行在集群中，它们请求的资源下限和上限以及重启、升级和容错的策略。每一个创建的对象其实都是我们对集群状态的改变，这些对象描述的其实就是集群的期望状态，Kubernetes 会根据我们指定的期望状态不断检查对当前的集群状态进行迁移。</p>
<pre><code class="language-go">type Deployment struct {
	metav1.TypeMeta `json:&quot;,inline&quot;`
	metav1.ObjectMeta `json:&quot;metadata,omitempty&quot; protobuf:&quot;bytes,1,opt,name=metadata&quot;`

	Spec DeploymentSpec `json:&quot;spec,omitempty&quot; protobuf:&quot;bytes,2,opt,name=spec&quot;`
	Status DeploymentStatus `json:&quot;status,omitempty&quot; protobuf:&quot;bytes,3,opt,name=status&quot;`
}
</code></pre>
<p>每一个对象都包含两个嵌套对象来描述规格（Spec）和状态（Status），对象的规格其实就是我们期望的目标状态，而状态描述了对象的当前状态，这部分一般由 Kubernetes 系统本身提供和管理，是我们观察集群本身的一个接口。</p>
<h4 id="pod">Pod</h4>
<p>Pod 是 Kubernetes 中最基本的概念，它也是 Kubernetes 对象模型中我们可以创建或者部署的最小并且最简单的单元。</p>
<p><img class="img-zoomable" src="https://img.draveness.me/2018-11-25-kubernetes-pod.png" alt="" />
</p>
<p>它将应用的容器、存储资源以及独立的网络 IP 地址等资源打包到了一起，表示一个最小的部署单元，但是每一个 Pod 中的运行的容器可能不止一个，这是因为 Pod 最开始设计时就能够在多个进程之间进行协调，构建一个高内聚的服务单元，这些容器能够共享存储和网络，非常方便地进行通信。</p>
<h4 id="controller">Controller</h4>
<p>最后要介绍的就是 Kubernetes 中的控制器，它们其实是用于创建和管理 Pod 的实例，能够在集群的层级提供复制、发布以及健康检查的功能，这些控制器其实都运行在 Kubernetes 集群的主节点上。</p>
<p>在 Kubernetes 的 <a href="https://github.com/kubernetes/kubernetes/tree/master/pkg/controller" target="_blank">kubernetes/pkg/controller/</a> 目录中包含了官方提供的一些常见控制器，我们可以通过下面这个函数看到所有需要运行的控制器：</p>
<pre><code class="language-go">func NewControllerInitializers(loopMode ControllerLoopMode) map[string]InitFunc {
	controllers := map[string]InitFunc{}
	controllers[&quot;endpoint&quot;] = startEndpointController
	controllers[&quot;replicationcontroller&quot;] = startReplicationController
	controllers[&quot;podgc&quot;] = startPodGCController
	controllers[&quot;resourcequota&quot;] = startResourceQuotaController
	controllers[&quot;namespace&quot;] = startNamespaceController
	controllers[&quot;serviceaccount&quot;] = startServiceAccountController
	controllers[&quot;garbagecollector&quot;] = startGarbageCollectorController
	controllers[&quot;daemonset&quot;] = startDaemonSetController
	controllers[&quot;job&quot;] = startJobController
	controllers[&quot;deployment&quot;] = startDeploymentController
	controllers[&quot;replicaset&quot;] = startReplicaSetController
	controllers[&quot;horizontalpodautoscaling&quot;] = startHPAController
	controllers[&quot;disruption&quot;] = startDisruptionController
	controllers[&quot;statefulset&quot;] = startStatefulSetController
	controllers[&quot;cronjob&quot;] = startCronJobController
	// ...

	return controllers
}
</code></pre>
<p>这些控制器会随着控制器管理器的启动而运行，它们会监听集群状态的变更来调整集群中的 Kubernetes 对象的状态，在后面的文章中我们会展开介绍一些常见控制器的实现原理。</p>
<h3 id="总结">总结</h3>
<p>作为 Kubernetes 系列文章的开篇，我们已经了解了它出现的背景、依赖的关键技术，同时我们也介绍了 Kubernetes 的架构设计，主节点负责处理客户端的请求、节点的调度，最后我们提到了几个 Kubernetes 中非常重要的概念：对象、Pod 和控制器，在接下来的文章中我们会深入介绍 Kubernetes 的实现原理。</p>
<h2 id="kubernetes-对象">Kubernetes 对象</h2>
<p>上一篇文章中，我们其实介绍了 Kubernetes 的对象其实就是系统中持久化的实体，Kubernetes 用这些实体来表示集群中的状态，它们描述了集群中运行的容器化应用以及这些对象占用的资源和行为。</p>
<p><img class="img-zoomable" src="https://img.draveness.me/2018-12-09-kubernetes-object.png" alt="" />
</p>
<p>不过当我们想要了解 Kubernetes 的实现原理时，绕不开的其实就是 Kubernetes 中的对象，而在 Kubernetes 中，规格（Spec）和状态（Status）是用于描述 Kubernetes 对象的两个最重要的嵌套对象，在这篇文章中会重点介绍对象的规格和状态的使用方式和实现原理。</p>
<h3 id="简介">简介</h3>
<p>在真正展开介绍对象的规格和状态之前，我们首先需要介绍 Kubernetes 中所有对象都有的三个字段 <code>apiVersion</code>、<code>kind</code> 和 <code>metadata</code>，我们从一个常见的对象描述文件来展开介绍：</p>
<pre><code class="language-yaml">apiVersion: v1
kind: Pod
metadata:
  name: nginx
  labels:
    name: nginx
spec:
  # ...
</code></pre>
<p>YAML</p>
<h4 id="api-组和类型">API 组和类型</h4>
<p><code>apiVersion</code> 和 <code>kind</code> 共同决定了当前的 YAML 配置文件应该由谁来进行处理，前者表示描述文件使用的 API 组，后者表示一个 API 组中的一个资源类型，这里的 <code>v1</code> 和 <code>Pod</code> 表示的就是核心 API 中组 <code>api/v1</code> 中的 <code>Pod</code> 类型对象。</p>
<p>除了一些 Kubernetes 的核心 API 组和资源之外，还有一些 Kubernetes 官方提供的扩展 API 组 <code>apis/batch/v1</code>、<code>apis/extensions/v1beta1</code> 等等，除此之外，我们也可以通过 <code>CustomResourceDefinition</code> 或者实现 apiserver 来定义新的对象类型。</p>
<h4 id="元数据">元数据</h4>
<p><code>apiVersion</code> 和 <code>kind</code> 描述了当前对象的一些最根本信息，而 <code>metadata</code> 能够为我们提供一些用于唯一识别对象的数据，包括在虚拟集群 <code>namespace</code> 中唯一的 <code>name</code> 字段，用于组织和分类的 <code>labels</code> 以及用于扩展功能的注解 <code>annotations</code>。</p>
<pre><code class="language-go">type ObjectMeta struct {
	Name string
	Namespace string
	Labels map[string]string
	Annotations map[string]string
	// ...
}
</code></pre>
<p>Go</p>
<p>上述的结构体嵌入在 Kubernetes 的每一个对象中，为所有的对象提供类似命名、命名空间、标签和注解等最基本的支持，让开发者能够更好地管理 Kubernetes 集群。</p>
<h5 id="标签和选择器">标签和选择器</h5>
<p>每一个 Kubernetes 对象都可以被打上多个标签，这些标签可以作为过滤项帮助我们选择和管理集群内部的 Kubernetes 对象，下面的命令可以获取到生产环境中的所有前端项目：</p>
<pre><code class="language-bash">kubectl get pods -l environment=production,tier=frontend
</code></pre>
<p>Bash</p>
<p>除了使用 kubectl 直接与 Kubernetes 集群通信获取其中的对象信息之外，我们也可以在 YAML 文件中使用选择器来达到相同的效果：</p>
<pre><code class="language-yaml">selector:
  matchLabels:
    environment: production
    tier: frontend
</code></pre>
<p>YAML</p>
<p>标签的主要作用就是对 Kubernetes 对象进行分类，这里我们可以简单给一些常见的分类方法：</p>
<p><img class="img-zoomable" src="https://img.draveness.me/2018-12-09-kubernetes-labels.png" alt="kubernetes-labels" />
</p>
<p>这些标签能够帮助我们在复杂的集群中快速选择一系列的 Kubernetes 对象，用好标签能够为管理集群带来非常大的帮助。</p>
<h5 id="命名空间">命名空间</h5>
<p>Kubernetes 支持通过命名空间在一个物理集群中划分出多个虚拟集群，这些虚拟集群就是单独的命名空间。不同的命名空间可以对资源进行隔离，它们没有办法直接使用 <code>name</code> 访问其他命名空间的服务，而是需要使用 FQDN(fully qualified domain name)。</p>
<p><img class="img-zoomable" src="https://img.draveness.me/2018-12-09-kubernetes-namespace.png" alt="kubernetes-namespace" />
</p>
<p>也就是说当我们创建一个 Service 时，它实际上在集群中加入了类似 <code>&lt;service&gt;.&lt;namespace&gt;.svc.cluster.local</code> 的 DNS 记录，在同一个命名空间中，我们可以直接使用 <code>service</code> 来访问目标的服务，但是在访问其他命名空间中的服务时却没有办法这么做。</p>
<h4 id="对象接口">对象接口</h4>
<p>Kubernetes 中的对象其实并不是 <code>struct</code>，而是一个 <code>interface</code>，其中定义了一系列的 Getter/Setter 接口：</p>
<pre><code class="language-go">type Object interface {
	GetNamespace() string
	SetNamespace(namespace string)
	GetName() string
	SetName(name string)
	GetGenerateName() string
	SetGenerateName(name string)
	GetUID() types.UID
	SetUID(uid types.UID)
	// ...
}
</code></pre>
<p>Go</p>
<p>这些 Getter/Setter 接口获取的字段基本都是 <code>ObjectMeta</code> 结构体中定义的一些字段，这也是为什么 Kubernetes 对象都需要嵌入一个 <code>ObjectMeta</code> 结构体。</p>
<h3 id="spec">Spec</h3>
<p>对象的规格（Spec）描述了某一个实体的期望状态，每一个对象的 Spec 基本都是由开发或者维护当前对象的工程师指定的，以下面的 busybox 举例：</p>
<pre><code class="language-yaml">apiVersion: v1
kind: Pod
metadata:
  name: busybox
  labels:
    app: busybox
spec:
  containers:
  - image: busybox
    command:
      - sleep
      - &quot;3600&quot;
    imagePullPolicy: IfNotPresent
    name: busybox
  restartPolicy: Always
</code></pre>
<p>YAML</p>
<p>作为一个 Pod 对象，它其实就是一个在 Kubernetes 中运行的最小、最简单的单元，所以它的 Spec 声明的就是其中包含的容器以及容器的镜像、启动命令等信息。</p>
<p>但是另一种对象 Service 就有着完全不同的 Spec 参数，作为一个一组 Pod 访问方式的抽象，它需要指定流量转发的 Pod 以及目前的端口号：</p>
<pre><code class="language-yaml">kind: Service
apiVersion: v1
metadata:
  name: nginx
spec:
  selector:
    app: nginx
  ports:
  - protocol: TCP
    port: 80
    targetPort: 80
</code></pre>
<p>YAML</p>
<p>我们可以看出，不同的 Kubernetes 对象基本上有着完全不同的 Spec，接下来我们按照 Kubernetes 项目中的源代码分别介绍如何描述几种不同的 Kubernetes 对象。</p>
<h4 id="pod-1">Pod</h4>
<p>作为一个 Kubernetes 对象，结构体 <code>Pod</code> 中嵌入了 <code>metav1.TypeMeta</code> 和 <code>metav1.ObjectMeta</code> 两个结构，这两个结构体中包含了 <code>Object</code> 接口中需要的函数：</p>
<pre><code class="language-go">type Pod struct {
	metav1.TypeMeta `json:&quot;,inline&quot;`
	metav1.ObjectMeta `json:&quot;metadata,omitempty&quot; protobuf:&quot;bytes,1,opt,name=metadata&quot;`
	Spec PodSpec `json:&quot;spec,omitempty&quot; protobuf:&quot;bytes,2,opt,name=spec&quot;`
	Status PodStatus `json:&quot;status,omitempty&quot; protobuf:&quot;bytes,3,opt,name=status&quot;`
}

type PodSpec struct {
	InitContainers []Container `json:&quot;initContainers,omitempty&quot; patchStrategy:&quot;merge&quot; patchMergeKey:&quot;name&quot; protobuf:&quot;bytes,20,rep,name=initContainers&quot;`
	Containers []Container `json:&quot;containers&quot; patchStrategy:&quot;merge&quot; patchMergeKey:&quot;name&quot; protobuf:&quot;bytes,2,rep,name=containers&quot;`
	RestartPolicy RestartPolicy `json:&quot;restartPolicy,omitempty&quot; protobuf:&quot;bytes,3,opt,name=restartPolicy,casttype=RestartPolicy&quot;`
	// ...
}
</code></pre>
<p>Go</p>
<p><code>Pod</code> 结构体中的 <code>PodSpec</code> 就是我们在 YAML 文件中定义的嵌套对象了，由于该结构体非常复杂（加上注释有 180 行），在这一节中我们只会简单介绍其中的几个字段。</p>
<p><img class="img-zoomable" src="https://img.draveness.me/2018-12-09-kubernetes-pod-containers.png" alt="kubernetes-pod-containers" />
</p>
<p><code>InitContainers</code> 是当前 Pod 启动时需要首先执行的一系列容器，这些容器没有生命周期，也没有探针，它们的主要作用就是在容器启动时进行一些资源和依赖的初始化配置；接下来的 <code>Containers</code> 数组就是 Pod 正常运行时包含的一系列容器了，这些容器会共享网络和进程，运行在同一个『虚拟机』上，所以也可以相互通信。</p>
<p>最后的 <code>RestartPolicy</code> 其实就整个 Pod 的重启策略，我们可以选择不重启 <code>Never</code>、在出现错误时重启 <code>OnFailure</code> 或者总是重启 <code>Always</code>。</p>
<h4 id="service">Service</h4>
<p>Kubernetes 中另一个常见对象 Service 的规格就有很大的不同了，虽然他们两者有着完全相同的嵌入结构 <code>metav1.TypeMeta</code> 和 <code>metav1.ObjectMeta</code> 以及字段 <code>Spec</code> 和 <code>Status</code>，但是它们的规格和状态却完全不同：</p>
<pre><code class="language-go">type Service struct {
	metav1.TypeMeta `json:&quot;,inline&quot;`
	metav1.ObjectMeta `json:&quot;metadata,omitempty&quot; protobuf:&quot;bytes,1,opt,name=metadata&quot;`
	Spec ServiceSpec `json:&quot;spec,omitempty&quot; protobuf:&quot;bytes,2,opt,name=spec&quot;`
	Status ServiceStatus `json:&quot;status,omitempty&quot; protobuf:&quot;bytes,3,opt,name=status&quot;`
}

type ServiceSpec struct {
	Ports []ServicePort `json:&quot;ports,omitempty&quot; patchStrategy:&quot;merge&quot; patchMergeKey:&quot;port&quot; protobuf:&quot;bytes,1,rep,name=ports&quot;`
	Selector map[string]string `json:&quot;selector,omitempty&quot; protobuf:&quot;bytes,2,rep,name=selector&quot;`
	// ...
}
</code></pre>
<p>Go</p>
<p>Service 都会通过选择器 <code>Selector</code> 将流量导入对应的 Pod 的指定 <code>Ports</code> 端口，这两个字段也是使用 Service 时最常用的两个字段，前者能够根据 Pod 的标签选择 Service 背后的一组 Pod，而 <code>Ports</code> 会将端口的流量转发到目标 Pod 的指定端口上。</p>
<h4 id="小结-1">小结</h4>
<p>Kubernetes 中对象的 Spec 其实描述了对象的期望状态，也就是工程师直接指定运行在 Kubernetes 集群中对象的表现和行为，同时 Kubernetes 会通过控制器不断帮助对象向期望状态迁移。</p>
<h3 id="status">Status</h3>
<p>对于很多使用 Kubernetes 的工程师来说，它们都会对对象的 Spec 比较了解，但是很多人都不太会了解对象的状态（Status）；对象的 Spec 是工程师向 Kubernetes 描述期望的集群状态，而 Status 其实就是 Kubernetes 集群对外暴露集群内对象运行状态的一个接口：</p>
<pre><code class="language-yaml">apiVersion: v1
kind: Pod
metadata:
  // ...
spec:
  // ...
status:
  conditions:
  - lastProbeTime: null
    lastTransitionTime: 2018-12-09T02:40:37Z
    status: &quot;True&quot;
    type: Initialized
  - lastProbeTime: null
    lastTransitionTime: 2018-12-09T02:40:38Z
    status: &quot;True&quot;
    type: Ready
  - lastProbeTime: null
    lastTransitionTime: 2018-12-09T02:40:33Z
    status: &quot;True&quot;
    type: PodScheduled
  containerStatuses:
  - containerID: docker://99f668a89db97342d7bd603471dfad5be262d7708b48cb6c5c8e374e9a13cf4f
    image: busybox:latest
    imageID: docker-pullable://busybox@sha256:915f390a8912e16d4beb8689720a17348f3f6d1a7b659697df850ab625ea29d5
    lastState: {}
    name: busybox
    ready: true
    restartCount: 0
    state:
      running:
        startedAt: 2018-12-09T02:40:37Z
  hostIP: 10.128.0.18
  phase: Running
  podIP: 10.4.0.28
  qosClass: Burstable
  startTime: 2018-12-09T02:40:33Z
</code></pre>
<p>YAML</p>
<p>当我们将对象运行到 Kubernetes 集群中时，Kubernetes 会将 Pod 的运行信息展示到 Status 上，接下来我们分别介绍 Pod 和 Service 的 Status 都包含哪些数据。</p>
<h4 id="pod-2">Pod</h4>
<p>每一个 Pod 的 Status 其实包含了阶段、当前服务状态、宿主机和 Pod IP 地址以及其中内部所有容器的状态信息：</p>
<pre><code class="language-go">type PodStatus struct {
	Phase PodPhase `json:&quot;phase,omitempty&quot; protobuf:&quot;bytes,1,opt,name=phase,casttype=PodPhase&quot;`
	Conditions []PodCondition `json:&quot;conditions,omitempty&quot; patchStrategy:&quot;merge&quot; patchMergeKey:&quot;type&quot; protobuf:&quot;bytes,2,rep,name=conditions&quot;`
	Message string `json:&quot;message,omitempty&quot; protobuf:&quot;bytes,3,opt,name=message&quot;`
	Reason string `json:&quot;reason,omitempty&quot; protobuf:&quot;bytes,4,opt,name=reason&quot;`
	HostIP string `json:&quot;hostIP,omitempty&quot; protobuf:&quot;bytes,5,opt,name=hostIP&quot;`
	PodIP string `json:&quot;podIP,omitempty&quot; protobuf:&quot;bytes,6,opt,name=podIP&quot;`
	StartTime *metav1.Time `json:&quot;startTime,omitempty&quot; protobuf:&quot;bytes,7,opt,name=startTime&quot;`
	InitContainerStatuses []ContainerStatus `json:&quot;initContainerStatuses,omitempty&quot; protobuf:&quot;bytes,10,rep,name=initContainerStatuses&quot;`
	ContainerStatuses []ContainerStatus `json:&quot;containerStatuses,omitempty&quot; protobuf:&quot;bytes,8,rep,name=containerStatuses&quot;`
	// ...
}
</code></pre>
<p>Go</p>
<p>上述的信息中，<code>PodCondition</code> 数组包含了一系列关于当前 Pod 状态的详情，其中包含了 Pod 处于当前状态的类型和原因以及 Kubernetes 获取该信息的时间；而另一个比较重要的 <code>ContainerStatus</code> 结构体中包含了容器的镜像、重启次数等信息。</p>
<h4 id="service-1">Service</h4>
<p>Service 的状态其实就更加的简单了，只有在当前的 Service 类型是 LoadBalancer 的时候 <code>Status</code> 字段才不会为空：</p>
<pre><code class="language-go">type ServiceStatus struct {
	LoadBalancer LoadBalancerStatus `json:&quot;loadBalancer,omitempty&quot; protobuf:&quot;bytes,1,opt,name=loadBalancer&quot;`
}

type LoadBalancerStatus struct {
	Ingress []LoadBalancerIngress `json:&quot;ingress,omitempty&quot; protobuf:&quot;bytes,1,rep,name=ingress&quot;`
}

type LoadBalancerIngress struct {
	IP string `json:&quot;ip,omitempty&quot; protobuf:&quot;bytes,1,opt,name=ip&quot;`
	Hostname string `json:&quot;hostname,omitempty&quot; protobuf:&quot;bytes,2,opt,name=hostname&quot;`
}
</code></pre>
<p>Go</p>
<p>其中可能会包含为当前负载均衡分配的 IP 地址或者 Hostname，并不会包含更加复杂的数据了。</p>
<h4 id="小结-2">小结</h4>
<p>Kubernetes 对象的 Status 不仅能够用来观察目前集群中对象的运行状态，还能帮助我们对集群中出现的问题进行排查以及修复，并能提供一些信息辅助优化集群中资源的利用率，我们在使用 Kubernetes 对象时也应该多多关注集群内的对象 Status 字段。</p>
<h3 id="总结-1">总结</h3>
<p>一个个 Kubernetes 对象组成了 Kubernetes 集群的期望状态，集群中的控制器会不断获取集群的运行状态与期望状态进行对比，保证集群向期望状态进行迁移，在接下来的文章中，我们会继续介绍 Kubernetes 集群是如何对常见的 Kubernetes 对象进行管理的。</p>
<h2 id="kubernetes-pod">Kubernetes Pod</h2>
<p>Pod、Service、Volume 和 Namespace 是 Kubernetes 集群中四大基本对象，它们能够表示系统中部署的应用、工作负载、网络和磁盘资源，共同定义了集群的状态。Kubernetes 中很多其他的资源其实只对这些基本的对象进行了组合。</p>
<p><img class="img-zoomable" src="https://img.draveness.me/2018-12-25-kubernetes-basic-objects.png" alt="kubernetes-basic-objects" />
</p>
<p>Pod 是 Kubernetes 集群中能够被创建和管理的最小部署单元，想要彻底和完整的了解 Kubernetes 的实现原理，我们必须要清楚 Pod 的实现原理以及最佳实践。</p>
<p>在这里，我们将分两个部分对 Pod 进行解析，第一部分主要会从概念入手介绍 Pod 中必须了解的特性，而第二部分会介绍 Pod 从创建到删除的整个生命周期内的重要事件在源码层面是如何实现的。</p>
<h3 id="概述">概述</h3>
<p>作为 Kubernetes 集群中的基本单元，Pod 就是最小并且最简单的 Kubernetes 对象，这个简单的对象其实就能够独立启动一个后端进程并在集群的内部为调用方提供服务。在上一篇文章 <a href="https://draveness.me/kubernetes-object-intro" target="_blank">从 Kubernetes 中的对象谈起</a> 中，我们曾经介绍过简单的 Kubernetes Pod 是如何使用 YAML 进行描述的：</p>
<pre><code class="language-yaml">apiVersion: v1
kind: Pod
metadata:
  name: busybox
  labels:
    app: busybox
spec:
  containers:
  - image: busybox
    command:
      - sleep
      - &quot;3600&quot;
    imagePullPolicy: IfNotPresent
    name: busybox
  restartPolicy: Always
</code></pre>
<p>YAML</p>
<p>这个 YAML 文件描述了一个 Pod 启动时运行的容器和命令以及它的重启策略，在当前 Pod 出现错误或者执行结束后是否应该被 Kubernetes 的控制器拉起来，除了这些比较显眼的配置之外，元数据 <code>metadata</code> 的配置也非常重要，<code>name</code> 是当前对象在 Kubernetes 集群中的唯一标识符，而标签 <code>labels</code> 可以帮助我们快速选择对象。</p>
<p>在同一个 Pod 中，有几个概念特别值得关注，首先就是容器，在 Pod 中其实可以同时运行一个或者多个容器，这些容器能够共享网络、存储以及 CPU、内存等资源。在这一小节中我们将关注 Pod 中的容器、卷和网络三大概念。</p>
<h4 id="容器">容器</h4>
<p>每一个 Kubernetes 的 Pod 其实都具有两种不同的容器，两种不同容器的职责其实十分清晰，一种是 <code>InitContainer</code>，这种容器会在 Pod 启动时运行，主要用于初始化一些配置，另一种是 Pod 在 Running 状态时内部存活的 <code>Container</code>，它们的主要作用是对外提供服务或者作为工作节点处理异步任务等等。</p>
<p><img class="img-zoomable" src="https://img.draveness.me/2018-12-25-kubernetes-pod-init-and-regular-containers.png" alt="kubernetes-pod-init-and-regular-containers" />
</p>
<p>通过对不同容器类型的命名我们也可以看出，<code>InitContainer</code> 会比 <code>Container</code> 优先启动，在 <code>kubeGenericRuntimeManager.SyncPod</code> 方法中会先后启动两种容器。</p>
<pre><code class="language-go">func (m *kubeGenericRuntimeManager) SyncPod(pod *v1.Pod, _ v1.PodStatus, podStatus *kubecontainer.PodStatus, pullSecrets []v1.Secret, backOff *flowcontrol.Backoff) (result kubecontainer.PodSyncResult) {
	// Step 1: Compute sandbox and container changes.
	// Step 2: Kill the pod if the sandbox has changed.
	// Step 3: kill any running containers in this pod which are not to keep.
	// Step 4: Create a sandbox for the pod if necessary.
	// ...

	// Step 5: start the init container.
	if container := podContainerChanges.NextInitContainerToStart; container != nil {
		msg, _ := m.startContainer(podSandboxID, podSandboxConfig, container, pod, podStatus, pullSecrets, podIP, kubecontainer.ContainerTypeInit)
	}

	// Step 6: start containers in podContainerChanges.ContainersToStart.
	for _, idx := range podContainerChanges.ContainersToStart {
		container := &amp;pod.Spec.Containers[idx]
		
		msg, _ := m.startContainer(podSandboxID, podSandboxConfig, container, pod, podStatus, pullSecrets, podIP, kubecontainer.ContainerTypeRegular)
	}

	return
}
</code></pre>
<p>Go</p>
<p>通过分析私有方法 <code>startContainer</code> 的实现我们得出：容器的类型最终只会影响在 Debug 时创建的标签，所以对于 Kubernetes 来说两种容器的启动和执行也就只有顺序先后的不同。</p>
<h4 id="卷">卷</h4>
<p>每一个 Pod 中的容器是可以通过 <a href="https://draveness.me/kubernetes-volume" target="_blank">卷（Volume）</a> 的方式共享文件目录的，这些 Volume 能够存储持久化的数据；在当前 Pod 出现故障或者滚动更新时，对应 Volume 中的数据并不会被清除，而是会在 Pod 重启后重新挂载到期望的文件目录中：</p>
<p><img class="img-zoomable" src="https://img.draveness.me/2018-12-25-kubernetes-containers-share-volumes.png" alt="kubernetes-containers-share-volumes" />
</p>
<p>kubelet.go 文件中的私有方法 <code>syncPod</code> 会调用 <code>WaitForAttachAndMount</code> 方法为等待当前 Pod 启动需要的挂载文件：</p>
<pre><code class="language-go">func (vm *volumeManager) WaitForAttachAndMount(pod *v1.Pod) error {
	expectedVolumes := getExpectedVolumes(pod)
	uniquePodName := util.GetUniquePodName(pod)

	vm.desiredStateOfWorldPopulator.ReprocessPod(uniquePodName)

	wait.PollImmediate(
		podAttachAndMountRetryInterval,
		podAttachAndMountTimeout,
		vm.verifyVolumesMountedFunc(uniquePodName, expectedVolumes))

	return nil
}
</code></pre>
<p>Go</p>
<p>我们会在 <a href="https://draveness.me/kubernetes-volume" target="_blank">后面的章节</a> 详细地介绍 Kubernetes 中卷的创建、挂载是如何进行的，在这里我们需要知道的是卷的挂载是 Pod 启动之前必须要完成的工作：</p>
<pre><code class="language-go">func (kl *Kubelet) syncPod(o syncPodOptions) error {
	// ...

	if !kl.podIsTerminated(pod) {
		kl.volumeManager.WaitForAttachAndMount(pod)
	}

	pullSecrets := kl.getPullSecretsForPod(pod)

	result := kl.containerRuntime.SyncPod(pod, apiPodStatus, podStatus, pullSecrets, kl.backOff)
	kl.reasonCache.Update(pod.UID, result)

	return nil
}
</code></pre>
<p>Go</p>
<p>在当前 Pod 的卷创建完成之后，就会调用上一节中提到的 <code>SyncPod</code> 公有方法继续进行同步 Pod 信息和创建、启动容器的工作。</p>
<h4 id="网络">网络</h4>
<p>同一个 Pod 中的多个容器会被共同分配到同一个 Host 上并且共享网络栈，也就是说这些 Pod 能够通过 localhost 互相访问到彼此的端口和服务，如果使用了相同的端口也会发生冲突，同一个 Pod 上的所有容器会连接到同一个网络设备上，这个网络设备就是由 Pod Sandbox 中的沙箱容器在 <code>RunPodSandbox</code> 方法中启动时创建的：</p>
<pre><code class="language-go">func (ds *dockerService) RunPodSandbox(ctx context.Context, r *runtimeapi.RunPodSandboxRequest) (*runtimeapi.RunPodSandboxResponse, error) {
	config := r.GetConfig()

	// Step 1: Pull the image for the sandbox.
	image := defaultSandboxImage

	// Step 2: Create the sandbox container.
	createConfig, _ := ds.makeSandboxDockerConfig(config, image)
	createResp, _ := ds.client.CreateContainer(*createConfig)

	resp := &amp;runtimeapi.RunPodSandboxResponse{PodSandboxId: createResp.ID}

	ds.setNetworkReady(createResp.ID, false)

	// Step 3: Create Sandbox Checkpoint.
	ds.checkpointManager.CreateCheckpoint(createResp.ID, constructPodSandboxCheckpoint(config))

	// Step 4: Start the sandbox container.
	ds.client.StartContainer(createResp.ID)

	// Step 5: Setup networking for the sandbox.
	cID := kubecontainer.BuildContainerID(runtimeName, createResp.ID)
	networkOptions := make(map[string]string)
	ds.network.SetUpPod(config.GetMetadata().Namespace, config.GetMetadata().Name, cID, config.Annotations, networkOptions)

	return resp, nil
}
</code></pre>
<p>Go</p>
<p>沙箱容器其实就是 <code>pause</code> 容器，上述方法引用的 <code>defaultSandboxImage</code> 其实就是官方提供的 <code>k8s.gcr.io/pause:3.1</code> 镜像，这里会创建沙箱镜像和检查点并启动容器。</p>
<p><img class="img-zoomable" src="https://img.draveness.me/2018-12-25-kubernetes-pod-network.png" alt="kubernetes-pod-network" />
</p>
<p>每一个节点上都会由 Kubernetes 的网络插件 Kubenet 创建一个基本的 <code>cbr0</code> 网桥并为每一个 Pod 创建 <code>veth</code> 虚拟网络设备，同一个 Pod 中的所有容器就会通过这个网络设备共享网络，也就是能够通过 localhost 互相访问彼此暴露的端口和服务。</p>
<h4 id="小结-3">小结</h4>
<p>Kubernetes 中的每一个 Pod 都包含多个容器，这些容器在通过 Kubernetes 创建之后就能共享网络和存储，这其实是 Pod 非常重要的特性，我们能通过这个特性构建比较复杂的服务拓扑和依赖关系。</p>
<h3 id="生命周期">生命周期</h3>
<p>想要深入理解 Pod 的实现原理，最好最快的办法就是从 Pod 的生命周期入手，通过理解 Pod 创建、重启和删除的原理我们最终就能够系统地掌握 Pod 的生命周期与核心原理。</p>
<p><img class="img-zoomable" src="https://img.draveness.me/2018-12-25-kubernetes-pod-lifecycle.png" alt="kubernetes-pod-lifecycle" />
</p>
<p>当 Pod 被创建之后，就会进入健康检查状态，当 Kubernetes 确定当前 Pod 已经能够接受外部的请求时，才会将流量打到新的 Pod 上并继续对外提供服务，在这期间如果发生了错误就可能会触发重启机制，在 Pod 被删除之前都会触发一个 <code>PreStop</code> 的钩子，其中的方法完成之后 Pod 才会被删除，接下来我们就会按照这里的顺序依次介绍 Pod 『从生到死』的过程。</p>
<h4 id="创建">创建</h4>
<p>Pod 的创建都是通过 <code>SyncPod</code> 来实现的，创建的过程大体上可以分为六个步骤：</p>
<ol>
<li>计算 Pod 中沙盒和容器的变更；</li>
<li>强制停止 Pod 对应的沙盒；</li>
<li>强制停止所有不应该运行的容器；</li>
<li>为 Pod 创建新的沙盒；</li>
<li>创建 Pod 规格中指定的初始化容器；</li>
<li>依次创建 Pod 规格中指定的常规容器；</li>
</ol>
<p>我们可以看到 Pod 的创建过程其实是比较简单的，首先计算 Pod 规格和沙箱的变更，然后停止可能影响这一次创建或者更新的容器，最后依次创建沙盒、初始化容器和常规容器。</p>
<pre><code class="language-go">func (m *kubeGenericRuntimeManager) SyncPod(pod *v1.Pod, _ v1.PodStatus, podStatus *kubecontainer.PodStatus, pullSecrets []v1.Secret, backOff *flowcontrol.Backoff) (result kubecontainer.PodSyncResult) {
	podContainerChanges := m.computePodActions(pod, podStatus)
	if podContainerChanges.CreateSandbox {
		ref, _ := ref.GetReference(legacyscheme.Scheme, pod)
	}

	if podContainerChanges.KillPod {
		if podContainerChanges.CreateSandbox {
			m.purgeInitContainers(pod, podStatus)
		}
	} else {
		for containerID, containerInfo := range podContainerChanges.ContainersToKill {
			m.killContainer(pod, containerID, containerInfo.name, containerInfo.message, nil)			}
		}
	}

	podSandboxID := podContainerChanges.SandboxID
	if podContainerChanges.CreateSandbox {
		podSandboxID, _, _ = m.createPodSandbox(pod, podContainerChanges.Attempt)
	}
	podSandboxConfig, _ := m.generatePodSandboxConfig(pod, podContainerChanges.Attempt)

	if container := podContainerChanges.NextInitContainerToStart; container != nil {
		msg, _ := m.startContainer(podSandboxID, podSandboxConfig, container, pod, podStatus, pullSecrets, podIP, kubecontainer.ContainerTypeInit)
	}

	for _, idx := range podContainerChanges.ContainersToStart {
		container := &amp;pod.Spec.Containers[idx]
		msg, _ := m.startContainer(podSandboxID, podSandboxConfig, container, pod, podStatus, pullSecrets, podIP, kubecontainer.ContainerTypeRegular)
	}

	return
}
</code></pre>
<p>Go</p>
<p>简化后的 <code>SyncPod</code> 方法的脉络非常清晰，可以很好地理解整个创建 Pod 的工作流程；而初始化容器和常规容器被调用 <code>startContainer</code> 来启动：</p>
<pre><code class="language-go">func (m *kubeGenericRuntimeManager) startContainer(podSandboxID string, podSandboxConfig *runtimeapi.PodSandboxConfig, container *v1.Container, pod *v1.Pod, podStatus *kubecontainer.PodStatus, pullSecrets []v1.Secret, podIP string, containerType kubecontainer.ContainerType) (string, error) {
	imageRef, _, _ := m.imagePuller.EnsureImageExists(pod, container, pullSecrets)
	
	// ...
	containerID, _ := m.runtimeService.CreateContainer(podSandboxID, containerConfig, podSandboxConfig)

	m.internalLifecycle.PreStartContainer(pod, container, containerID)

	m.runtimeService.StartContainer(containerID)

	if container.Lifecycle != nil &amp;&amp; container.Lifecycle.PostStart != nil {
		kubeContainerID := kubecontainer.ContainerID{
			Type: m.runtimeName,
			ID:   containerID,
		}
		msg, _ := m.runner.Run(kubeContainerID, pod, container, container.Lifecycle.PostStart)
	}

	return &quot;&quot;, nil
}
</code></pre>
<p>Go</p>
<p>在启动每一个容器的过程中也都按照相同的步骤进行操作：</p>
<ol>
<li>通过镜像拉取器获得当前容器中使用镜像的引用；</li>
<li>调用远程的 <code>runtimeService</code> 创建容器；</li>
<li>调用内部的生命周期方法 <code>PreStartContainer</code> 为当前的容器设置分配的 CPU 等资源；</li>
<li>调用远程的 <code>runtimeService</code> 开始运行镜像；</li>
<li>如果当前的容器包含 <code>PostStart</code> 钩子就会执行该回调；</li>
</ol>
<p>每次 <code>SyncPod</code> 被调用时不一定是创建新的 Pod 对象，它还会承担更新、删除和同步 Pod 规格的职能，根据输入的新规格执行相应的操作。</p>
<h4 id="健康检查">健康检查</h4>
<p>如果我们遵循 Pod 的最佳实践，其实应该尽可能地为每一个 Pod 添加 <code>livenessProbe</code> 和 <code>readinessProbe</code> 的健康检查，这两者能够为 Kubernetes 提供额外的存活信息，如果我们配置了合适的健康检查方法和规则，那么就不会出现服务未启动就被打入流量或者长时间未响应依然没有重启等问题。</p>
<p>在 Pod 被创建或者被移除时，会被加入到当前节点上的 <code>ProbeManager</code> 中，<code>ProbeManager</code> 会负责这些 Pod 的健康检查：</p>
<pre><code class="language-go">func (kl *Kubelet) HandlePodAdditions(pods []*v1.Pod) {
	start := kl.clock.Now()
	for _, pod := range pods {
		kl.podManager.AddPod(pod)
		kl.dispatchWork(pod, kubetypes.SyncPodCreate, mirrorPod, start)
		kl.probeManager.AddPod(pod)
	}
}

func (kl *Kubelet) HandlePodRemoves(pods []*v1.Pod) {
	start := kl.clock.Now()
	for _, pod := range pods {
		kl.podManager.DeletePod(pod)
		kl.deletePod(pod)
		kl.probeManager.RemovePod(pod)
	}
}
</code></pre>
<p>Go</p>
<p>简化后的 <code>HandlePodAdditions</code> 和 <code>HandlePodRemoves</code> 方法非常直白，我们可以直接来看 <code>ProbeManager</code> 如何处理不同节点的健康检查。</p>
<p><img class="img-zoomable" src="https://img.draveness.me/2018-12-25-kubernetes-probe-manager.png" alt="kubernetes-probe-manager" />
</p>
<p>每一个新的 Pod 都会被调用 <code>ProbeManager</code> 的<code>AddPod</code> 函数，这个方法会初始化一个新的 Goroutine 并在其中运行对当前 Pod 进行健康检查：</p>
<pre><code class="language-go">func (m *manager) AddPod(pod *v1.Pod) {
	key := probeKey{podUID: pod.UID}
	for _, c := range pod.Spec.Containers {
		key.containerName = c.Name

		if c.ReadinessProbe != nil {
			key.probeType = readiness
			w := newWorker(m, readiness, pod, c)
			m.workers[key] = w
			go w.run()
		}

		if c.LivenessProbe != nil {
			key.probeType = liveness
			w := newWorker(m, liveness, pod, c)
			m.workers[key] = w
			go w.run()
		}
	}
}
</code></pre>
<p>Go</p>
<p>在执行健康检查的过程中，Worker 只是负责根据当前 Pod 的状态定期触发一次 <code>Probe</code>，它会根据 Pod 的配置分别选择调用 <code>Exec</code>、<code>HTTPGet</code> 或 <code>TCPSocket</code> 三种不同的 <code>Probe</code> 方式：</p>
<pre><code class="language-go">func (pb *prober) runProbe(probeType probeType, p *v1.Probe, pod *v1.Pod, status v1.PodStatus, container v1.Container, containerID kubecontainer.ContainerID) (probe.Result, string, error) {
	timeout := time.Duration(p.TimeoutSeconds) * time.Second
	if p.Exec != nil {
		command := kubecontainer.ExpandContainerCommandOnlyStatic(p.Exec.Command, container.Env)
		return pb.exec.Probe(pb.newExecInContainer(container, containerID, command, timeout))
	}
	if p.HTTPGet != nil {
		scheme := strings.ToLower(string(p.HTTPGet.Scheme))
		host := p.HTTPGet.Host
		port, _ := extractPort(p.HTTPGet.Port, container)
		path := p.HTTPGet.Path
		url := formatURL(scheme, host, port, path)
		headers := buildHeader(p.HTTPGet.HTTPHeaders)
		if probeType == liveness {
			return pb.livenessHttp.Probe(url, headers, timeout)
		} else { // readiness
			return pb.readinessHttp.Probe(url, headers, timeout)
		}
	}
	if p.TCPSocket != nil {
		port, _ := extractPort(p.TCPSocket.Port, container)
		host := p.TCPSocket.Host
		return pb.tcp.Probe(host, port, timeout)
	}
	return probe.Unknown, &quot;&quot;, fmt.Errorf(&quot;Missing probe handler for %s:%s&quot;, format.Pod(pod), container.Name)
}
</code></pre>
<p>Go</p>
<p>Kubernetes 在 Pod 启动后的 <code>InitialDelaySeconds</code> 时间内会等待 Pod 的启动和初始化，在这之后会开始健康检查，默认的健康检查重试次数是三次，如果健康检查正常运行返回了一个确定的结果，那么 Worker 就是记录这次的结果，在连续失败 <code>FailureThreshold</code> 次或者成功 <code>SuccessThreshold</code> 次，那么就会改变当前 Pod 的状态，这也是为了避免由于服务不稳定带来的抖动。</p>
<h4 id="删除">删除</h4>
<p>当 Kubelet 在 <code>HandlePodRemoves</code> 方法中接收到来自客户端的删除请求时，就会通过一个名为 <code>deletePod</code> 的私有方法中的 Channel 将这一事件传递给 PodKiller 进行处理：</p>
<pre><code class="language-go">func (kl *Kubelet) deletePod(pod *v1.Pod) error {
	kl.podWorkers.ForgetWorker(pod.UID)

	runningPods, _ := kl.runtimeCache.GetPods()
	runningPod := kubecontainer.Pods(runningPods).FindPod(&quot;&quot;, pod.UID)
	podPair := kubecontainer.PodPair{APIPod: pod, RunningPod: &amp;runningPod}

	kl.podKillingCh &lt;- &amp;podPair
	return nil
}
</code></pre>
<p>Go</p>
<p>Kubelet 除了将事件通知给 PodKiller 之外，还需要将当前 Pod 对应的 Worker 从持有的 <code>podWorkers</code> 中删除；PodKiller 其实就是 Kubelet 持有的一个 Goroutine，它会在后台持续运行并监听来自 <code>podKillingCh</code> 的事件：</p>
<p><img class="img-zoomable" src="https://img.draveness.me/2018-12-25-kubernetes-pod-killer.png" alt="kubernetes-pod-killer" />
</p>
<p>经过一系列的方法调用之后，最终调用容器运行时的 <code>killContainersWithSyncResult</code> 方法，这个方法会同步地杀掉当前 Pod 中全部的容器：</p>
<pre><code class="language-go">func (m *kubeGenericRuntimeManager) killContainersWithSyncResult(pod *v1.Pod, runningPod kubecontainer.Pod, gracePeriodOverride *int64) (syncResults []*kubecontainer.SyncResult) {
	containerResults := make(chan *kubecontainer.SyncResult, len(runningPod.Containers))

	for _, container := range runningPod.Containers {
		go func(container *kubecontainer.Container) {
			killContainerResult := kubecontainer.NewSyncResult(kubecontainer.KillContainer, container.Name)
			m.killContainer(pod, container.ID, container.Name, &quot;Need to kill Pod&quot;, gracePeriodOverride)
			containerResults &lt;- killContainerResult
		}(container)
	}
	close(containerResults)

	for containerResult := range containerResults {
		syncResults = append(syncResults, containerResult)
	}
	return
}
</code></pre>
<p>Go</p>
<p>对于每一个容器来说，它们在被停止之前都会先调用 <code>PreStop</code> 的钩子方法，让容器中的应用程序能够有时间完成一些未处理的操作，随后调用远程的服务停止运行的容器：</p>
<pre><code class="language-go">func (m *kubeGenericRuntimeManager) killContainer(pod *v1.Pod, containerID kubecontainer.ContainerID, containerName string, reason string, gracePeriodOverride *int64) error {
	containerSpec := kubecontainer.GetContainerSpec(pod, containerName);

	gracePeriod := int64(minimumGracePeriodInSeconds)
	switch {
	case pod.DeletionGracePeriodSeconds != nil:
		gracePeriod = *pod.DeletionGracePeriodSeconds
	case pod.Spec.TerminationGracePeriodSeconds != nil:
		gracePeriod = *pod.Spec.TerminationGracePeriodSeconds
	}
	
	m.executePreStopHook(pod, containerID, containerSpec, gracePeriod)
	m.internalLifecycle.PreStopContainer(containerID.ID)
	m.runtimeService.StopContainer(containerID.ID, gracePeriod)
	m.containerRefManager.ClearRef(containerID)

	return err
}
</code></pre>
<p>Go</p>
<p>从这个简化版本的 <code>killContainer</code> 方法中，我们可以大致看出停止运行容器的大致逻辑，先从 Pod 的规格中计算出当前停止所需要的时间，然后运行钩子方法和内部的生命周期方法，最后将容器停止并清除引用。</p>
<h3 id="总结-2">总结</h3>
<p>在这篇文章中，我们已经介绍了 Pod 中的几个重要概念 — 容器、卷和网络以及从创建到删除整个过程是如何实现的。</p>
<p>Kubernetes 中 Pod 的运行和管理总是与 kubelet 以及它的组件密不可分，后面的文章中也会介绍 kubelet 究竟是什么，它在整个 Kubernetes 中扮演什么样的角色。</p>
<h2 id="kubernetes-service">Kubernetes Service</h2>
<p>在上一篇文章中，我们介绍了 <a href="https://draveness.me/kubernetes-pod" target="_blank">Kubernetes 中 Pod 的实现原理</a>，Pod 是 Kubernetes 中非常轻量的对象。</p>
<p>集群中的每一个 Pod 都可以通过 <code>podIP</code> 被直接访问的，但是正如我们所看到的，Kubernetes 中的 Pod 是有生命周期的对象，尤其是被 ReplicaSet、Deployment 等对象管理的 Pod，随时都有可能由于集群的状态变化被销毁和创建。</p>
<p><img class="img-zoomable" src="https://img.draveness.me/2019-01-01-kubernetes-pods-and-pods.png" alt="kubernetes-pods-and-pods" />
</p>
<p>这也就造成了一个非常有意思的问题，当 Kubernetes 集群中的一些 Pod 需要为另外的一些 Pod 提供服务时，我们如何为提供同一功能服务的一组 Pod 建立一个抽象并追踪这组服务中节点的健康状态。</p>
<p>这一个抽象在 Kubernetes 中其实就是 Service，每一个 Kubernetes 的 Service 都是一组 Pod 的逻辑集合和访问方式的抽象，我也可以把 Service 加上的一组 Pod 称作是一个微服务。</p>
<p>在这篇文章中，我们将分两个部分介绍 Kubernetes 中 Service 的实现原理，在第一部分我们将介绍 Kubernetes 如何处理服务的创建，第二部分会介绍它是如何转发来自节点内部和外部的流量。</p>
<h3 id="创建服务">创建服务</h3>
<p>在 Kubernetes 中创建一个新的 Service 对象需要两大模块同时协作，其中一个模块是控制器，它需要在每次客户端创建新的 Service 对象时，生成其他用于暴露一组 Pod 的 Kubernetes 对象，也就是 Endpoint 对象；另一个模块是 kube-proxy，它运行在 Kubernetes 集群中的每一个节点上，会根据 Service 和 Endpoint 的变动改变节点上 iptables 或者 ipvs 中保存的规则。</p>
<h4 id="控制器">控制器</h4>
<p>控制器模块其实总共有两个部分监听了 Service 变动的事件，其中一个是 <code>ServiceController</code>、另一个是 <code>EndpointController</code>，我们分别来看两者如何应对 Service 的变动。</p>
<p>Service</p>
<p>我们可以先来看一下 <code>ServiceController</code> 在 Service 对象变动时发生了什么事情，每当有服务被创建或者销毁时，Informer 都会通知 <code>ServiceController</code>，它会将这些任务投入工作队列中并由其本身启动的 Worker 协程消费：</p>
<pre><code class="language-mermaid">sequenceDiagram
    participant I as Informer
    participant SC as ServiceController
    participant Q as WorkQueue
    participant B as Balancer
    I-&gt;&gt;+SC: Add/Update/DeleteService
    SC-&gt;&gt;Q: Add
    Q--&gt;&gt;SC: return
    deactivate SC
    loop Worker
        SC-&gt;&gt;+Q: Get
        Q--&gt;&gt;-SC: key
        SC-&gt;&gt;SC: syncService
        SC-&gt;&gt;+B: EnsureLoadBalancer
        B--&gt;&gt;-SC: LoadBalancerStatus
    end
</code></pre>
<p>Mermaid</p>
<p>不过 <code>ServiceController</code> 其实只处理了负载均衡类型的 Service 对象，它会调用云服务商的 API 接口，不同的云服务商会实现不同的适配器来创建 LoadBalancer 类型的资源。</p>
<p><img class="img-zoomable" src="https://img.draveness.me/2019-01-01-kubernetes-cloud-provider-loadbalancer.png" alt="kubernetes-cloud-provider-loadbalance" />
</p>
<p>我们以 GCE 为例简单介绍一下 Google Cloud 是如何对实现负载均衡类型的 Service：</p>
<pre><code class="language-go">func (g *Cloud) EnsureLoadBalancer(ctx context.Context, clusterName string, svc *v1.Service, nodes []*v1.Node) (*v1.LoadBalancerStatus, error) {
	loadBalancerName := g.GetLoadBalancerName(ctx, clusterName, svc)
	desiredScheme := getSvcScheme(svc)
	clusterID, _ := g.ClusterID.GetID()

	existingFwdRule, _ := g.GetRegionForwardingRule(loadBalancerName, g.region)

	if existingFwdRule != nil {
		existingScheme := cloud.LbScheme(strings.ToUpper(existingFwdRule.LoadBalancingScheme))
		if existingScheme != desiredScheme {
			switch existingScheme {
			case cloud.SchemeInternal:
				g.ensureInternalLoadBalancerDeleted(clusterName, clusterID, svc)
			default:
				g.ensureExternalLoadBalancerDeleted(clusterName, clusterID, svc)
			}
			existingFwdRule = nil
		}
	}

	var status *v1.LoadBalancerStatus
	switch desiredScheme {
	case cloud.SchemeInternal:
		status, err = g.ensureInternalLoadBalancer(clusterName, clusterID, svc, existingFwdRule, nodes)
	default:
		status, err = g.ensureExternalLoadBalancer(clusterName, clusterID, svc, existingFwdRule, nodes)
	}
	return status, err
}
</code></pre>
<p>Go</p>
<p>上述代码会先判断是否应该先删除已经存在的负载均衡资源，随后会调用一个内部的方法 <code>ensureExternalLoadBalancer</code> 在 Google Cloud 上创建一个新的资源，这个方法的调用过程比较复杂：</p>
<ol>
<li>检查转发规则是否存在并获取它的 IP 地址；</li>
<li>确定当前 LoadBalancer 使用的 IP 地址；</li>
<li>处理防火墙的规则的创建和更新；</li>
<li>创建和删除指定的健康检查；</li>
</ol>
<p>想要了解 GCE 是如何对 LoadBalancer 进行支持的可以在 Kubernetes 中的 <a href="https://github.com/kubernetes/kubernetes/tree/master/pkg/cloudprovider/providers/gce" target="_blank">gce</a> package 中阅读相关的代码，这里面就是 gce 对于云服务商特定资源的实现方式。</p>
<p>Endpoint</p>
<p><code>ServiceController</code> 主要处理的还是与 LoadBalancer 相关的逻辑，但是 <code>EndpointController</code> 的作用就没有这么简单了，我们在使用 Kubernetes 时虽然很少会直接与 Endpoint 资源打交道，但是它却是 Kubernetes 中非常重要的组成部分。</p>
<p><code>EndpointController</code> 本身并没有通过 <code>Informer</code> 监听 Endpoint 资源的变动，但是它却同时订阅了 Service 和 Pod 资源的增删事件，对于 Service 资源来讲，<code>EndpointController</code> 会通过以下的方式进行处理：</p>
<pre><code class="language-mermaid">sequenceDiagram
    participant I as Informer
    participant EC as EndpointController
    participant Q as WorkQueue
    participant PL as PodLister
    participant C as Client
    I-&gt;&gt;+EC: Add/Update/DeleteService
    EC-&gt;&gt;Q: Add
    Q--&gt;&gt;EC: return
    deactivate EC
    loop Worker
        EC-&gt;&gt;+Q: Get
        Q--&gt;&gt;-EC: key
        EC-&gt;&gt;+EC: syncService
        EC-&gt;&gt;+PL: ListPod(service.Spec.Selector)
        PL--&gt;&gt;-EC: Pods
        loop Every Pod
            EC-&gt;&gt;EC: addEndpointSubset
        end
        EC-&gt;&gt;C: Create/UpdateEndpoint
        C--&gt;&gt;-EC: result
    end
</code></pre>
<p>Mermaid</p>
<p><code>EndpointController</code> 中的 <code>syncService</code> 方法是用于创建和删除 Endpoint 资源最重要的方法，在这个方法中我们会根据 Service 对象规格中的选择器 Selector 获取集群中存在的所有 Pod，并将 Service 和 Pod 上的端口进行映射生成一个 <code>EndpointPort</code> 结构体：</p>
<pre><code class="language-go">func (e *EndpointController) syncService(key string) error {
	namespace, name, _ := cache.SplitMetaNamespaceKey(key)
	service, _ := e.serviceLister.Services(namespace).Get(name)
	pods, _ := e.podLister.Pods(service.Namespace).List(labels.Set(service.Spec.Selector).AsSelectorPreValidated())

	subsets := []v1.EndpointSubset{}
	for _, pod := range pods {
		epa := *podToEndpointAddress(pod)

		for i := range service.Spec.Ports {
			servicePort := &amp;service.Spec.Ports[i]

			portName := servicePort.Name
			portProto := servicePort.Protocol
			portNum, _ := podutil.FindPort(pod, servicePort)

			epp := &amp;v1.EndpointPort{Name: portName, Port: int32(portNum), Protocol: portProto}
			subsets, _, _ = addEndpointSubset(subsets, pod, epa, epp, tolerateUnreadyEndpoints)
		}
	}
	subsets = endpoints.RepackSubsets(subsets)

	currentEndpoints = &amp;v1.Endpoints{
		ObjectMeta: metav1.ObjectMeta{
			Name:   service.Name,
			Labels: service.Labels,
		},
	}

	newEndpoints := currentEndpoints.DeepCopy()
	newEndpoints.Subsets = subsets
	newEndpoints.Labels = service.Labels
	e.client.CoreV1().Endpoints(service.Namespace).Create(newEndpoints)

	return nil
}
</code></pre>
<p>Go</p>
<p>对于每一个 Pod 都会生成一个新的 <code>EndpointSubset</code>，其中包含了 Pod 的 IP 地址和端口和 Service 的规格中指定的输入端口和目标端口，在最后 <code>EndpointSubset</code> 的数据会被重新打包并通过客户端创建一个新的 Endpoint 资源。</p>
<p>在上面我们已经提到过，除了 Service 的变动会触发 Endpoint 的改变之外，Pod 对象的增删也会触发 <code>EndpointController</code> 中的回调函数。</p>
<pre><code class="language-mermaid">sequenceDiagram
    participant I as Informer
    participant EC as EndpointController
    participant Q as WorkQueue
    participant SL as ServiceLister
    I-&gt;&gt;+EC: Add/Update/DeletePod
    EC-&gt;&gt;+SL: GetPodServices
    SL--&gt;&gt;-EC: []Service
    EC-&gt;&gt;Q: Add
    Q--&gt;&gt;EC: return
    deactivate EC
</code></pre>
<p>Mermaid</p>
<p><code>getPodServiceMemberships</code> 会获取跟当前 Pod 有关的 Service 对象并将所有的 Service 对象都转换成 <code>&lt;namespace&gt;/&lt;name&gt;</code> 的字符串：</p>
<pre><code class="language-go">func (e *EndpointController) getPodServiceMemberships(pod *v1.Pod) (sets.String, error) {
	set := sets.String{}
	services, _ := e.serviceLister.GetPodServices(pod)

	for i := range services {
		key, _ := controller.KeyFunc(services[i])
		set.Insert(key)
	}
	return set, nil
}
</code></pre>
<p>Go</p>
<p>这些服务最后会被加入 <code>EndpointController</code> 的队列中，等待它持有的几个 Worker 对 Service 进行同步。</p>
<p>这些其实就是 <code>EndpointController</code> 的作用，订阅 Pod 和 Service 对象的变更，并根据当前集群中的对象生成 Endpoint 对象将两者进行关联。</p>
<h4 id="代理">代理</h4>
<p>在整个集群中另一个订阅 Service 对象变动的组件就是 kube-proxy 了，每当 kube-proxy 在新的节点上启动时都会初始化一个 <code>ServiceConfig</code> 对象，就像介绍 iptables 代理模式时提到的，这个对象会接受 Service 的变更事件：</p>
<pre><code class="language-mermaid">sequenceDiagram
    participant SCT as ServiceChangeTracker
    participant SC as ServiceConfig
    participant P as Proxier
    participant EC as EndpointConfig
    participant ECT as EndpointChangeTracker
    participant SR as SyncRunner
    SC-&gt;&gt;+P: OnServiceAdd/Update/Delete/Synced
    P-&gt;&gt;SCT: Update
    SCT--&gt;&gt;P: Return ServiceMap
    deactivate P
    EC-&gt;&gt;+P: OnEndpointsAdd/Update/Delete/Synced
    ECT--&gt;&gt;P: Return EndpointMap
    P-&gt;&gt;ECT: Update
    deactivate P
    loop Every minSyncPeriod ~ syncPeriod
        SR-&gt;&gt;P: syncProxyRules
    end
</code></pre>
<p>Mermaid</p>
<p>这些变更事件都会被订阅了集群中对象变动的 <code>ServiceConfig</code> 和 <code>EndpointConfig</code> 对象推送给启动的 <code>Proxier</code> 实例：</p>
<pre><code class="language-go">func (c *ServiceConfig) handleAddService(obj interface{}) {
	service, ok := obj.(*v1.Service)
	if !ok {
		return
	}
	for i := range c.eventHandlers {
		c.eventHandlers[i].OnServiceAdd(service)
	}
}
</code></pre>
<p>Go</p>
<p>收到事件变动的 <code>Proxier</code> 实例随后会根据启动时的配置更新 iptables 或者 ipvs 中的规则，这些应用最终会负责对进出的流量进行转发并完成一些负载均衡相关的任务。</p>
<h3 id="代理模式">代理模式</h3>
<p>在 Kubernetes 集群中的每一个节点都运行着一个 kube-proxy 进程，这个进程会负责监听 Kubernetes 主节点中 Service 的增加和删除事件并修改运行代理的配置，为节点内的客户端提供流量的转发和负载均衡等功能，但是当前 kube-proxy 的代理模式目前来看有三种：</p>
<p><img class="img-zoomable" src="https://img.draveness.me/2019-01-01-kubernetes-service-proxy-mode.png" alt="kubernetes-service-proxy-mode" />
</p>
<p>这三种代理模式中的第一种 userspace 其实就是运行在用户空间代理，所有的流量最终都会通过 kube-proxy 本身转发给其他的服务，后两种 iptable 和 ipvs 都运行在内核空间能够为 Kubernetes 集群提供更加强大的性能支持。</p>
<h4 id="userspace">userspace</h4>
<p>作为运行在用户空间的代理，对于每一个 Service 都会在当前的节点上开启一个端口，所有连接到当前代理端口的请求都会被转发到 Service 背后的一组 Pod 上，它其实会在节点上添加 iptables 规则，通过 iptables 将流量转发给 kube-proxy 处理。</p>
<p>如果当前节点上的 kube-proxy 在启动时选择了 userspace 模式，那么每当有新的 Service 被创建时，kube-proxy 就会增加一条 iptables 记录并启动一个 Goroutine，前者用于将节点中服务对外发出的流量转发给 kube-proxy，再由后者持有的一系列 Goroutine 将流量转发到目标的 Pod 上。</p>
<p><img class="img-zoomable" src="https://img.draveness.me/2019-01-01-kubernetes-userspace-proxy-mode.png" alt="kubernetes-userspace-proxy-mode" />
</p>
<p>这一系列的工作大都是在 <code>OnServiceAdd</code> 被触发时中完成的，正如上面所说的，该方法会调用 <code>mergeService</code> 将传入服务 Service 的端口变成一条 iptables 的配置命令为当前节点增加一条规则，同时在 <code>addServiceOnPort</code> 方法中启动一个 TCP 或 UDP 的 Socket：</p>
<pre><code class="language-go">func (proxier *Proxier) mergeService(service *v1.Service) sets.String {
	svcName := types.NamespacedName{Namespace: service.Namespace, Name: service.Name}
	existingPorts := sets.NewString()
	for i := range service.Spec.Ports {
		servicePort := &amp;service.Spec.Ports[i]
		serviceName := proxy.ServicePortName{NamespacedName: svcName, Port: servicePort.Name}
		existingPorts.Insert(servicePort.Name)
		info, exists := proxier.getServiceInfo(serviceName)
		if exists {
			proxier.closePortal(serviceName, info)
			proxier.stopProxy(serviceName, info)
		}
		proxyPort,  := proxier.proxyPorts.AllocateNext()

		serviceIP := net.ParseIP(service.Spec.ClusterIP)
		info, _ = proxier.addServiceOnPort(serviceName, servicePort.Protocol, proxyPort, proxier.udpIdleTimeout)
		info.portal.ip = serviceIP
		info.portal.port = int(servicePort.Port)
		info.externalIPs = service.Spec.ExternalIPs
		info.loadBalancerStatus = *service.Status.LoadBalancer.DeepCopy()
		info.nodePort = int(servicePort.NodePort)
		info.sessionAffinityType = service.Spec.SessionAffinity

		proxier.openPortal(serviceName, info)
		proxier.loadBalancer.NewService(serviceName, info.sessionAffinityType, info.stickyMaxAgeSeconds)
	}

	return existingPorts
}
</code></pre>
<p>Go</p>
<p>这个启动的进程会监听同一个节点上，转发自所有进程的 TCP 和 UDP 请求并将这些数据包发送给目标的 Pod 对象。</p>
<p>在用户空间模式中，如果一个连接被目标服务拒绝，我们的代理服务能够重新尝试连接其他的服务，除此之外用户空间模式并没有太多的优势。</p>
<h4 id="iptables">iptables</h4>
<p>另一种常见的代理模式就是直接使用 iptables 转发当前节点上的全部流量，这种脱离了用户空间在内核空间中实现转发的方式能够极大地提高 proxy 的效率，增加 kube-proxy 的吞吐量。</p>
<p><img class="img-zoomable" src="https://img.draveness.me/2019-01-01-kubernetes-iptables-proxy-mode.png" alt="kubernetes-iptables-proxy-mode" />
</p>
<p>iptables 作为一种代理模式，它同样实现了 <code>OnServiceUpdate</code>、<code>OnEndpointsUpdate</code> 等方法，这两个方法会分别调用相应的变更追踪对象。</p>
<pre><code class="language-mermaid">sequenceDiagram
    participant SC as ServiceConfig
    participant P as Proxier
    participant SCT as ServiceChangeTracker
    participant SR as SyncRunner
    participant I as iptable
    SC-&gt;&gt;+P: OnServiceAdd
    P-&gt;&gt;P: OnServiceUpdate
    P-&gt;&gt;SCT: Update
    SCT--&gt;&gt;P: Return ServiceMap
    deactivate P
    loop Every minSyncPeriod ~ syncPeriod
        SR-&gt;&gt;+P: syncProxyRules
        P-&gt;&gt;I: UpdateChain
        P-&gt;&gt;P: writeLine x N
        P-&gt;&gt;I: RestoreAll
        deactivate P
    end
</code></pre>
<p>Mermaid</p>
<p>变更追踪对象会根据 <code>Service</code> 或 <code>Endpoint</code> 对象的前后变化改变 <code>ServiceChangeTracker</code> 本身的状态，这些变更会每隔一段时间通过一个 700 行的巨大方法 <code>syncProxyRules</code> 同步，在这里就不介绍这个方法的具体实现了，它的主要功能就是根据 <code>Service</code> 和 <code>Endpoint</code> 对象的变更生成一条一条的 iptables 规则，比较感兴趣的读者，可以点击 <a href="https://sourcegraph.com/github.com/kubernetes/kubernetes@master/-/blob/pkg/proxy/iptables/proxier.go#L640-1379" target="_blank">proxier.go#L640-1379</a> 查看代码。</p>
<p>当我们使用 iptables 的方式启动节点上的代理时，所有的流量都会先经过 <code>PREROUTING</code> 或者 <code>OUTPUT</code> 链，随后进入 Kubernetes 自定义的链入口 KUBE-SERVICES、单个 Service 对应的链 <code>KUBE-SVC-XXXX</code> 以及每个 Pod 对应的链 <code>KUBE-SEP-XXXX</code>，经过这些链的处理，最终才能够访问当一个服务的真实 IP 地址。</p>
<p>虽然相比于用户空间来说，直接运行在内核态的 iptables 能够增加代理的吞吐量，但是当集群中的节点数量非常多时，iptables 并不能达到生产级别的可用性要求，每次对规则进行匹配时都会遍历 iptables 中的所有 Service 链。</p>
<p>规则的更新也不是增量式的，当集群中的 Service 达到 5,000 个，每增加一条规则都需要耗时 11min，当集群中的 Service 达到 20,000 个时，每增加一条规则都需要消耗 5h 的时间，这也就是告诉我们在大规模集群中使用 iptables 作为代理模式是完全不可用的。</p>
<h4 id="ipvs">ipvs</h4>
<p>ipvs 就是用于解决在大量 Service 时，iptables 规则同步变得不可用的性能问题。与 iptables 比较像的是，ipvs 的实现虽然也基于 netfilter 的钩子函数，但是它却使用哈希表作为底层的数据结构并且工作在内核态，这也就是说 ipvs 在重定向流量和同步代理规则有着更好的性能。</p>
<p><img class="img-zoomable" src="https://img.draveness.me/2019-01-01-kubernetes-ipvs-proxy-mode.png" alt="kubernetes-ipvs-proxy-mode" />
</p>
<p>在处理 Service 的变化时，ipvs 包和 iptables 其实就有非常相似了，它们都同样使用 <code>ServiceChangeTracker</code> 对象来追踪变更，只是两者对于同步变更的方法 <code>syncProxyRules</code> 实现上有一些不同。</p>
<pre><code class="language-mermaid">sequenceDiagram
    participant P as Proxier
    participant SR as SyncRunner
    participant IP as ipvs
    participant I as iptable
    loop Every minSyncPeriod ~ syncPeriod
        SR-&gt;&gt;+P: syncProxyRules
        P-&gt;&gt;P: writeLine(iptable)
        P-&gt;&gt;IP: Add/UpdateVirtualServer(syncService)
        IP--&gt;&gt;P: result
        P-&gt;&gt;IP: AddRealServer(syncEndpoint)
        IP--&gt;&gt;P: result
        P-&gt;&gt;I: RestoreAll
        deactivate P
    end
</code></pre>
<p>Mermaid</p>
<p>我们从 ipvs 的源代码和上述的时序图中可以看到，Kubernetes ipvs 的实现其实是依赖于 iptables 的，后者能够辅助它完成一些功能，使用 ipvs 相比 iptables 能够减少节点上的 iptables 规则数量，这也是因为 ipvs 接管了原来存储在 iptables 中的规则。</p>
<p>除了能够提升性能之外，ipvs 也提供了多种类型的负载均衡算法，除了最常见的 Round-Robin 之外，还支持最小连接、目标哈希、最小延迟等算法，能够很好地提升负载均衡的效率。</p>
<h4 id="小结-4">小结</h4>
<p>三种不同的代理模式其实是一个逐渐演化的过程，从最开始运行在用户空间需要『手动』监听端口并对数据包进行转发的用户空间模式，到之后使用运行在内核空间的 iptables 模式，再到 Kubernetes 1.9 版本中出现的 ipvs 模式，几种不同的模式在大量 Service 存在时有数量级别效率差异。</p>
<h3 id="总结-3">总结</h3>
<p>Kubernetes 中的 Service 将一组 Pod 以统一的形式对外暴露成一个服务，它利用运行在内核空间的 iptables 或者 ipvs 高效地转发来自节点内部和外部的流量。除此之外，作为非常重要的 Kubernetes 对象，Service 不仅在逻辑上提供了微服务的概念，还引入 LoadBalancer 类型的 Service 无缝对接云服务商提供的复杂资源。</p>
<p>理解 Kubernetes 的 Service 对象能够帮助我们梳理集群内部的网络拓扑关系，也能让我们更清楚它是如何在集群内部实现服务发现、负载均衡等功能的，在后面的文章中我们会展开介绍 kube-proxy 的作用和实现。</p>
<h2 id="kubernetes-volume">Kubernetes Volume</h2>
<p>在 Kubernetes 集群中，虽然无状态的服务非常常见，但是在实际的生产中仍然会需要在集群中部署一些有状态的节点，比如一些存储中间件、消息队列等等。</p>
<p>然而 Kubernetes 中的每一个容器随时都可能因为某些原因而被删除和重启，容器中的文件也会随着它的删除而丢失，所以我们需要对集群中的某些文件和数据进行『持久化』；除此之外，由于同一个 Pod 中的多个 Container 可能也会有共享文件的需求，比如通过共享文件目录的方式为 nginx 生成需要代理的静态文件，所以我们需要一种方式来解决这两个问题。</p>
<p>作为 Kubernetes 集群中除了 Pod 和 Service 之外最常见的基本对象，Volume 不仅能够解决 Container 中文件的临时性问题，也能够让同一个 Pod 中的多个 Container 共享文件。</p>
<blockquote>
<p>这篇文章并不会介绍 Kubernetes 中 Volume 的使用方法和 API，而是会着重介绍 Volume 的工作原理，包含其创建过程、多种 Volume 实现的异同以及如何与云服务提供商进行适配。</p>
</blockquote>
<h3 id="概述-1">概述</h3>
<p>Kubernetes 中的 Volume 种类非常多，它不仅要支持临时的、易失的磁盘文件，还需要解决持久存储的问题；第一个问题往往都比较容易解决，后者作为持久存储在很多时候都需要与云服务商提供的存储方案打交道，如果是 Kubernetes 中已经支持的存储类型倒是还好，遇到不支持的类型还是比较麻烦的。</p>
<p><img class="img-zoomable" src="https://img.draveness.me/2019-01-14-kubernetes-storage.png" alt="kubernetes-storage" />
</p>
<p>除了卷和持久卷之外，Kubernetes 还有另外一种更加复杂的概念 - 动态存储供应，它能够允许存储卷按需进行创建，不再需要集群的管理员手动调用云服务商提供的接口或者界面创建新的存储卷。</p>
<p>集群中的每一个卷在被 Pod 使用时都会经历四个操作，也就是附着（Attach）、挂载（Mount）、卸载（Unmount）和分离（Detach）。</p>
<p>如果 Pod 中使用的是 EmptyDir、HostPath 这种类型的卷，那么这些卷并不会经历附着和分离的操作，它们只会被挂载和卸载到某一个的 Pod 中，不过如果使用的云服务商提供的存储服务，这些持久卷只有附着到某一个节点之后才可以被挂在到相应的目录下，不过在其他节点使用这些卷时，该存储资源也需要先与当前的节点分离。</p>
<h4 id="卷-1">卷</h4>
<p>在这一节中提到的卷（Volume）其实是一个比较特定的概念，它并不是一个持久化存储，可能会随着 Pod 的删除而删除，常见的卷就包括 EmptyDir、HostPath、ConfigMap 和 Secret，这些卷与所属的 Pod 具有相同的生命周期，它们可以通过如下的方式挂载到 Pod 下面的某一个目录中：</p>
<pre><code class="language-yaml">apiVersion: v1
kind: Pod
metadata:
  name: test-pod
spec:
  containers:
  - name: test-container
    image: k8s.gcr.io/busybox
    volumeMounts:
    - name: cache-volume
      mountPath: /cache
    - name: test-volume
      mountPath: /hostpath
    - name: config-volume
      mountPath: /data/configmap
    - name: special-volume
      mountPath: /data/secret
  volumes:
  - name: cache-volume
    emptyDir: {}
  - name: hostpath-volume
    hostPath:
      path: /data/hostpath
      type: Directory
  - name: config-volume
    configMap:
      name: special-config
  - name: secret-volume
    secret:
      secretName: secret-config
</code></pre>
<p>YAML</p>
<p>需要注意的是，当我们将 ConfigMap 或者 Secret 『包装』成卷并挂载到某个目录时，我们其实创建了一些新的 Volume，这些 Volume 并不是 Kubernetes 中的对象，它们只存在于当前 Pod 中，随着 Pod 的删除而删除，但是需要注意的是这些『临时卷』的删除并不会导致相关 <code>ConfigMap</code> 或者 <code>Secret</code> 对象的删除。</p>
<p>从上面我们其实可以看出 Volume 没有办法脱离 Pod 而生存，它与 Pod 拥有完全相同的生命周期，而且它们也不是 Kubernetes 对象，所以 Volume 的主要作用还是用于跨节点或者容器对数据进行同步和共享。</p>
<h4 id="持久卷">持久卷</h4>
<p>临时的卷没有办法解决数据持久存储的问题，想要让数据能够持久化，首先就需要将 Pod 和卷的声明周期分离，这也就是引入持久卷 <code>PersistentVolume(PV)</code> 的原因。我们可以将 <code>PersistentVolume</code> 理解为集群中资源的一种，它与集群中的节点 Node 有些相似，PV 为 Kubernete 集群提供了一个如何提供并且使用存储的抽象，与它一起被引入的另一个对象就是 <code>PersistentVolumeClaim(PVC)</code>，这两个对象之间的关系与节点和 Pod 之间的关系差不多：</p>
<p><img class="img-zoomable" src="https://img.draveness.me/2019-01-14-kubernetes-pv-and-pvc.png" alt="kubernetes-pv-and-pvc" />
</p>
<p><code>PersistentVolume</code> 是集群中的一种被管理员分配的存储资源，而 <code>PersistentVolumeClaim</code> 表示用户对存储资源的申请，它与 Pod 非常相似，PVC 消耗了持久卷资源，而 Pod 消耗了节点上的 CPU 和内存等物理资源。</p>
<p>因为 PVC 允许用户消耗抽象的存储资源，所以用户需要不同类型、属性和性能的 PV 就是一个比较常见的需求了，在这时我们可以通过 <code>StorageClass</code> 来提供不同种类的 PV 资源，上层用户就可以直接使用系统管理员提供好的存储类型。</p>
<h5 id="访问模式">访问模式</h5>
<p>Kubernetes 中的 PV 提供三种不同的访问模式，分别是 <code>ReadWriteOnce</code>、<code>ReadOnlyMany</code> 和 <code>ReadWriteMany</code>，这三种模式的含义和用法我们可以通过它们的名字推测出来：</p>
<ul>
<li><code>ReadWriteOnce</code> 表示当前卷可以被一个节点使用读写模式挂载；</li>
<li><code>ReadOnlyMany</code> 表示当前卷可以被多个节点使用只读模式挂载；</li>
<li><code>ReadWriteMany</code> 表示当前卷可以被多个节点使用读写模式挂载；</li>
</ul>
<p>不同的卷插件对于访问模式其实有着不同的支持，AWS 上的 <code>AWSElasticBlockStore</code> 和 GCP 上的 <code>GCEPersistentDisk</code> 就只支持 <code>ReadWriteOnce</code> 方式的挂载，不能同时挂载到多个节点上，但是 <code>CephFS</code> 就同时支持这三种访问模式。</p>
<h5 id="回收策略">回收策略</h5>
<p>当某个服务使用完某一个卷之后，它们会从 apiserver 中删除 PVC 对象，这时 Kubernetes 就需要对卷进行回收（Reclaim），持久卷也同样包含三种不同的回收策略，这三种回收策略会指导 Kubernetes 选择不同的方式对使用过的卷进行处理。</p>
<p><img class="img-zoomable" src="https://img.draveness.me/2019-01-14-kubernetes-pv-reclaiming-strategy.png" alt="kubernetes-pv-reclaiming-strategy" />
</p>
<p>第一种回收策略就是保留（Retain）PV 中的数据，如果希望 PV 能够被重新使用，系统管理员需要删除被使用的 <code>PersistentVolume</code> 对象并手动清除存储和相关存储上的数据。</p>
<p>另一种常见的回收策略就是删除（Delete），当 PVC 被使用者删除之后，如果当前卷支持删除的回收策略，那么 PV 和相关的存储会被自动删除，如果当前 PV 上的数据确实不再需要，那么将回收策略设置成 Delete 能够节省手动处理的时间并快速释放无用的资源。</p>
<h5 id="存储供应">存储供应</h5>
<p>Kubernetes 集群中包含了很多的 PV 资源，而 PV 资源有两种供应的方式，一种是静态的，另一种是动态的，静态存储供应要求集群的管理员预先创建一定数量的 PV，然后使用者通过 PVC 的方式对 PV 资源的使用进行声明和申请；但是当系统管理员创建的 PV 对象不能满足使用者的需求时，就会进入动态存储供应的逻辑，供应的方式是基于集群中的 <code>StorageClass</code> 对象，当然这种动态供应的方式也可以通过配置进行关闭。</p>
<h3 id="管理">管理</h3>
<p>Volume 的创建和管理在 Kubernetes 中主要由卷管理器 <code>VolumeManager</code> 和 <code>AttachDetachController</code> 和 <code>PVController</code> 三个组件负责。其中卷管理器会负责卷的创建和管理的大部分工作，而 <code>AttachDetachController</code> 主要负责对集群中的卷进行 Attach 和 Detach，<code>PVController</code> 负责处理持久卷的变更，文章接下来的内容会详细介绍这几部分之间的关系、工作原理以及它们是如何协作的。</p>
<h4 id="kubelet">kubelet</h4>
<p>作者在 <a href="https://draveness.me/kubernetes-pod" target="_blank">详解 Kubernetes Pod 的实现原理</a> 一文中曾简单介绍过 kubelet 和 Pod 的关系，前者会负责后者的创建和管理，kubelet 中与 Pod 相关的信息都是从 apiserver 中获取的：</p>
<pre><code class="language-mermaid">graph LR
    apiserver-.-&gt;u
    u((updates))-.-&gt;kubelet
    kubelet-.-&gt;podWorkers
    podWorkers-.-&gt;worker1
    podWorkers-.-&gt;worker2
    style u fill:#fffede,stroke:#ebebb7
</code></pre>
<p>Mermaid</p>
<p>两者的通信会使用一个 <code>kubetypes.PodUpdate</code> 类型的 Channel，kubelet 从 apiserver 中获取 Pod 时也会通过字段过滤器 <code>fields.OneTermEqualSelector(api.PodHostField, string(nodeName))</code> 仅选择被调度到 kubelet 所在节点上的 Pod：</p>
<pre><code class="language-go">func NewSourceApiserver(c clientset.Interface, nodeName types.NodeName, updates chan&lt;- interface{}) {
	lw := cache.NewListWatchFromClient(c.CoreV1().RESTClient(), &quot;pods&quot;, metav1.NamespaceAll, fields.OneTermEqualSelector(api.PodHostField, string(nodeName)))
	newSourceApiserverFromLW(lw, updates)
}
</code></pre>
<p>Go</p>
<p>所有对 Pod 的变更最终都会通知给具体的 PodWorker，这些 Worker 协程会调用 kubelet <code>syncPod</code> 函数完成对 Pod 的同步：</p>
<pre><code class="language-mermaid">sequenceDiagram
    participant PW as PodWorker
    participant K as Kubelet
    participant VL as VolumeManager
    participant DSOWP as DesiredStateOfWorldPopulator
    participant ASOW as ActualStateOfWorld
    PW-&gt;&gt;+K: syncPod
    K-&gt;&gt;+VL: WaitForAttachAndMount
    VL-xDSOWP: ReprocessPod
    loop verifyVolumesMounted
        VL-&gt;&gt;+ASOW: getUnmountedVolumes
        ASOW--&gt;&gt;-VL: Volumes
    end
    VL--&gt;&gt;-K: Attached/Timeout
    K--&gt;&gt;-PW: return
</code></pre>
<p>Mermaid</p>
<p>在一个 100 多行的 <code>syncPod</code> 方法中，kubelet 会调用 <code>WaitForAttachAndMount</code> 方法，等待某一个 Pod 中的全部卷已经被成功地挂载：</p>
<pre><code class="language-go">func (kl *Kubelet) syncPod(o syncPodOptions) error {
	pod := o.pod
	
	// ...
	if !kl.podIsTerminated(pod) {
		kl.volumeManager.WaitForAttachAndMount(pod)
	}
	// ...

	return nil
}
</code></pre>
<p>Go</p>
<p>这个方法会将当前的 Pod 加入需要重新处理卷挂载的队列并在循环中持续调用 <code>verifyVolumesMounted</code> 方法来比较期望挂载的卷和实际挂载卷的区别，这个循环会等待两者变得完全相同或者超时后才会返回，当前方法的返回一般也意味着 Pod 中的全部卷已经挂载成功了。</p>
<h4 id="卷管理器">卷管理器</h4>
<p>当前节点卷的管理就都是由 <code>VolumeManager</code> 来负责了，在 Kubernetes 集群中的每一个节点（Node）上的 kubelet 启动时都会运行一个 <code>VolumeManager</code> Goroutine，它会负责在当前节点上的 Pod 和 Volume 发生变动时对 Volume 进行挂载和卸载等操作。</p>
<pre><code class="language-mermaid">graph TD
    subgraph Node
        VolumeManager-.-&gt;Kubelet
        DesiredStateOfWorldPopulator-.-&gt;VolumeManager
        Reconciler-.-&gt;VolumeManager
    end
</code></pre>
<p>Mermaid</p>
<p>这个组件会在运行时启动两个 Goroutine 来管理节点中的卷，其中一个是 <code>DesiredStateOfWorldPopulator</code>，另一个是 <code>Reconciler</code>：</p>
<pre><code class="language-mermaid">graph LR
    VM(VolumeManager)-. run .-&gt;R(Reconciler)
    VM-. run .-&gt;DSWP(DesiredStateOfWorldPopulator)
    DSWP-. update .-&gt;DSW[DesiredStateOfWorld]
    ASW[ActualStateOfWorld]-. get .-&gt;DSWP
    DSW-. get .-&gt;R
    R-. update .-&gt;ASW
    DSWP-. getpods .-&gt;PodManager
    style ASW fill:#fffede,stroke:#ebebb7
    style DSW fill:#fffede,stroke:#ebebb7
</code></pre>
<p>Mermaid</p>
<p>如上图所示，这里的 <code>DesiredStateOfWorldPopulator</code> 和 <code>Reconciler</code> 两个 Goroutine 会通过图中两个的 <code>XXXStateOfWorld</code> 状态进行通信，<code>DesiredStateOfWorldPopulator</code> 主要负责从 Kubernetes 节点中获取新的 Pod 对象并更新 <code>DesiredStateOfWorld</code> 结构；而后者会根据实际状态和当前状态的区别对当前节点的状态进行迁移，也就是通过 <code>DesiredStateOfWorld</code> 中状态的变更更新 <code>ActualStateOfWorld</code> 中的内容。</p>
<p>卷管理器中的两个 Goroutine，一个根据工程师的需求更新节点的期望状态 <code>DesiredStateOfWorld</code>，另一个 Goroutine 保证节点向期望状态『迁移』，也就是说 <code>DesiredStateOfWorldPopulator</code> 是卷管理器中的生产者，而 <code>Reconciler</code> 是消费者，接下来我们会分别介绍这两个 Goroutine 的工作和实现。</p>
<h5 id="desiredstateofworldpopulator">DesiredStateOfWorldPopulator</h5>
<p>作为卷管理器中的消费者，<code>DesiredStateOfWorldPopulator</code> 会根据工程师的请求不断修改当前节点的期望状态，我们可以通过以下的时序图来了解它到底做了哪些工作：</p>
<pre><code class="language-mermaid">sequenceDiagram
    participant DSOWP as DesiredStateOfWorldPopulator
    participant ASOW as ActualStateOfWorld
    participant DSOW as DesiredStateOfWorld
    participant PM as PodManager
    participant VPM as VolumePluginManager
    loop populatorLoop
        DSOWP-&gt;&gt;+DSOWP: findAndAddNewPods
        DSOWP-&gt;&gt;+ASOW: GetMountedVolumes
        ASOW--&gt;&gt;-DSOWP: mountedVolume
        DSOWP-&gt;&gt;+PM: GetPods
        PM--&gt;&gt;-DSOWP: pods
        loop Every Pod
            DSOWP-&gt;&gt;+DSOW: AddPodToVolume
            DSOW-&gt;&gt;+VPM: FindPluginBySpec
            VPM--&gt;&gt;-DSOW: volumePlugin
            DSOW--&gt;&gt;-DSOWP: volumeName
        end
        deactivate DSOWP
        
        DSOWP-&gt;&gt;+DSOWP: findAndRemoveDeletedPods
        DSOWP-&gt;&gt;+DSOW: GetVolumesToMount
        DSOW--&gt;&gt;-DSOWP: volumeToMount
        loop Every Volume
            DSOWP-&gt;&gt;+PM: GetPodByUID
            PM--&gt;&gt;-DSOWP: pods
            DSOWP-&gt;&gt;DSOW: DeletePodFromVolume
        end
        deactivate DSOWP
    end
</code></pre>
<p>Mermaid</p>
<p>整个 <code>DesiredStateOfWorldPopulator</code> 运行在一个大的循环 <code>populatorLoop</code> 中，当前循环会通过两个方法 <code>findAndAddNewPods</code> 和 <code>findAndRemoveDeletedPods</code> 分别获取节点中被添加的新 Pod 或者已经被删除的老 Pod，获取到 Pod 之后会根据当前的状态修改期望状态：</p>
<pre><code class="language-go">func (dswp *desiredStateOfWorldPopulator) findAndAddNewPods() {
	mountedVolumesForPod := make(map[volumetypes.UniquePodName]map[string]cache.MountedVolume)

	processedVolumesForFSResize := sets.NewString()
	for _, pod := range dswp.podManager.GetPods() {
		if dswp.isPodTerminated(pod) {
			continue
		}
		dswp.processPodVolumes(pod, mountedVolumesForPod, processedVolumesForFSResize)
	}
}
</code></pre>
<p>Go</p>
<p>就像时序图和代码中所描述的，<code>DesiredStateOfWorldPopulator</code> 会从 <code>PodManager</code> 中获取当前节点中的 Pod，随后调用 <code>processPodVolumes</code> 方法为将所有的 Pod 对象加入 <code>DesiredStateOfWorld</code> 结构中：</p>
<pre><code class="language-go">func (dswp *desiredStateOfWorldPopulator) processPodVolumes(pod *v1.Pod, mountedVolumesForPod map[volumetypes.UniquePodName]map[string]cache.MountedVolume, processedVolumesForFSResize sets.String) {
	uniquePodName := util.GetUniquePodName(pod)
	if dswp.podPreviouslyProcessed(uniquePodName) {
		return
	}

	mountsMap, devicesMap := dswp.makeVolumeMap(pod.Spec.Containers)

	for _, podVolume := range pod.Spec.Volumes {
		pvc, volumeSpec, volumeGidValue, _ := dswp.createVolumeSpec(podVolume, pod.Name, pod.Namespace, mountsMap, devicesMap)
		dswp.desiredStateOfWorld.AddPodToVolume(uniquePodName, pod, volumeSpec, podVolume.Name, volumeGidValue)
	}

	dswp.markPodProcessed(uniquePodName)
	dswp.actualStateOfWorld.MarkRemountRequired(uniquePodName)
}
</code></pre>
<p>Go</p>
<p><code>findAndAddNewPods</code> 方法做的主要就是将节点中加入的新 Pod 添加到 <code>DesiredStateOfWorld</code> 中，而另一个方法 <code>findAndRemoveDeletedPods</code> 其实也做着类似的事情，它会将已经被删除的节点从 <code>DesiredStateOfWorld</code> 中剔除，总而言之 <code>DesiredStateOfWorldPopulator</code> 就是将当前节点的期望状态同步到 <code>DesiredStateOfWorld</code> 中，等待消费者的处理。</p>
<h5 id="reconciler">Reconciler</h5>
<p><code>VolumeManager</code> 持有的另一个 Goroutine <code>Reconciler</code> 会负责对当前节点上的 Volume 进行管理，它在正常运行时会启动 <code>reconcile</code> 循环，在这个方法中会分三次对当前状态和期望状态不匹配的卷进行卸载、挂载等操作：</p>
<pre><code class="language-mermaid">sequenceDiagram
    participant R as Reconciler
    participant ASOW as ActualStateOfWorld
    participant DSOW as DesiredStateOfWorld
    participant OE as OperationExecutor
    loop reconcile
        R-&gt;&gt;+ASOW: GetMountedVolumes
        activate R
        ASOW--&gt;&gt;-R: MountedVolumes
        R-&gt;&gt;DSOW: PodExistsInVolume
        R-&gt;&gt;OE: UnmountVolume
        deactivate R

        R-&gt;&gt;+DSOW: GetVolumesToMount
        activate R
        DSOW--&gt;&gt;-R: volumeToMount
        R-&gt;&gt;ASOW: PodExistsInVolume
        R-&gt;&gt;OE: AttachVolume/MountVolume
        deactivate R

        R-&gt;&gt;+ASOW: GetUnmountedVolumes
        activate R
        R-&gt;&gt;DSOW: VolumeExists
        R-&gt;&gt;OE: UnmountDevice/DetachVolume
        deactivate R
    end
</code></pre>
<p>Mermaid</p>
<p>在当前的循环中首先会保证应该被卸载但是仍然在节点中存在的卷被卸载，然后将应该挂载的卷挂载到合适的位置，最后将设备与节点分离或者卸载，所有挂载和卸载的操作都是通过 <code>OperationExecutor</code> 完成的，这个结构体负责调用相应的插件执行操作，我们会在文章的后面展开进行介绍。</p>
<h4 id="附着分离控制器">附着分离控制器</h4>
<p>除了 <code>VolumeManager</code> 之外，另一个负责管理 Kubernetes 卷的组件就是 <code>AttachDetachController</code> 了，引入这个组件的目的主要是：</p>
<ol>
<li>让卷的挂载和卸载能够与节点的可用性脱离；
<ul>
<li>一旦节点或者 kubelet 宕机，附着（Attach）在当前节点上的卷应该能够被分离（Detach），分离之后的卷就能够再次附着到其他节点上；</li>
</ul>
</li>
<li>保证云服务商秘钥的安全；
<ul>
<li>如果每一个 kubelet 都需要触发卷的附着和分离逻辑，那么每一个节点都应该有操作卷的权限，但是这些权限应该只由主节点掌握，这样能够降低秘钥泄露的风险；</li>
</ul>
</li>
<li>提高卷附着和分离部分代码的稳定性；</li>
</ol>
<blockquote>
<p>这些内容都是在 Kubernetes 官方项目的 GitHub issue <a href="https://github.com/kubernetes/kubernetes/issues/20262" target="_blank">Detailed Design for Volume Attach/Detach Controller #20262</a> 中讨论的，想要了解 <code>AttachDetachController</code> 出现的原因可以阅读相关的内容。</p>
</blockquote>
<p>每一个 <code>AttachDetachController</code> 其实也包含 <code>Reconciler</code> 和 <code>DesiredStateOfWorldPopulator</code> 两个组件，这两个组件虽然与 <code>VolumeManager</code> 中的两个组件重名，实现的功能也非常类似，与 <code>VolumeManager</code> 具有几乎相同的数据流向，但是这两个 Goroutine 是跑在 Kubernetes 主节点中的，所以实现上可能一些差异：</p>
<pre><code class="language-mermaid">graph LR
    ADC(AttachDetachController)-. run .-&gt;R(Reconciler)
    ADC-. run .-&gt;DSWP(DesiredStateOfWorldPopulator)
    DSWP-. update .-&gt;DSW[DesiredStateOfWorld]
    ASW[ActualStateOfWorld]-. get .-&gt;DSWP
    DSW-. get .-&gt;R
    R-. update .-&gt;ASW
    DSWP-. getpods .-&gt;PodManager
    style ASW fill:#fffede,stroke:#ebebb7
    style DSW fill:#fffede,stroke:#ebebb7
</code></pre>
<p>Mermaid</p>
<p>首先，无论是 <code>Reconciler</code> 还是 <code>DesiredStateOfWorldPopulator</code>，它们同步的就不再只是某个节点上 Pod 的信息了，它们需要对整个集群中的 Pod 对象负责，相关数据也不再是通过 apiserver 拉取了，而是使用 <code>podInformer</code> 在 Pod 对象发生变更时调用相应的方法。</p>
<h5 id="desiredstateofworldpopulator-1">DesiredStateOfWorldPopulator</h5>
<p>作为 <code>AttachDetachController</code> 启动的 Goroutine，<code>DesiredStateOfWorldPopulator</code> 的主要作用是从当前集群的状态中获取 Pod 对象并修改 <code>DesiredStateOfWorld</code> 结构，与 <code>VolumeManager</code> 中的同名 Goroutine 起到相同的作用，作为整个链路的生产者，它们只是在实现上由于处理 Pod 范围的不同有一些区别：</p>
<pre><code class="language-mermaid">sequenceDiagram
    participant DSOWP as DesiredStateOfWorldPopulator
    participant ASOW as ActualStateOfWorld
    participant DSOW as DesiredStateOfWorld
    participant PL as PodLister
    participant VPM as VolumePluginManager
    loop populatorLoopFunc
        DSOWP-&gt;&gt;+DSOWP: findAndRemoveDeletedPods
        DSOWP-&gt;&gt;+DSOW: GetPodToAdd
        DSOW--&gt;&gt;-DSOWP: podsToAdd
        loop Every Pod
        
            DSOWP-&gt;&gt;+PL: GetPod
            alt PodNotFound
                PL--&gt;&gt;-DSOWP: return
                DSOWP-&gt;&gt;DSOW: DeletePod
            else
            end
        end
        deactivate DSOWP

        DSOWP-&gt;&gt;+DSOWP: findAndAddActivePods
        DSOWP-&gt;&gt;+PL: List
        PL--&gt;&gt;-DSOWP: pods
        loop Every Pod
            DSOWP-&gt;&gt;+VPM: FindAttachablePluginBySpec
            VPM--&gt;&gt;-DSOW: attachableVolumePlugin
            DSOWP-&gt;&gt;+DSOW: AddPod/DeletePod
            DSOW--&gt;&gt;-DSOWP: volumeName
        end
        deactivate DSOWP
   end
</code></pre>
<p>Mermaid</p>
<p><code>AttachDetachController</code> 中的 <code>DesiredStateOfWorldPopulator</code> 协程就主要会先处理 Pod 的删除逻辑，添加 Pod 的逻辑都是根据 <code>listPodsRetryDuration</code> 的设置周期性被触发的，所以从这里我们就能看到 <code>AttachDetachController</code> 其实主要还是处理被删除 Pod 中 Volume 的分离工作，当节点或者 kubelet 宕机时能够将节点中的卷进行分离，保证 Pod 在其他节点重启时不会出现问题。</p>
<h5 id="reconciler-1">Reconciler</h5>
<p>另一个用于调节当前状态与期望状态的 Goroutine 在执行它内部的循环时，也会优先处理分离卷的逻辑，后处理附着卷的工作，整个时序图与 <code>VolumeManager</code> 中的 <code>Reconciler</code> 非常相似：</p>
<pre><code class="language-mermaid">sequenceDiagram
    participant R as Reconciler
    participant ASOW as ActualStateOfWorld
    participant DSOW as DesiredStateOfWorld
    participant OE as OperationExecutor
    loop reconcile
        R-&gt;&gt;+ASOW: GetAttachedVolumes
        activate R
        ASOW--&gt;&gt;-R: attachedVolumes
        R-&gt;&gt;+DSOW: VolumeExists
        alt VolumeNotExists
            DSOW--&gt;&gt;-R: return
            R-&gt;&gt;OE: DetachVolume
            deactivate R
        else
        end

        R-&gt;&gt;+DSOW: GetVolumesToAttach
        activate R
        DSOW--&gt;&gt;-R: volumeToAttach
        R-&gt;&gt;+ASOW: VolumeNodeExists
        alt VolumeNotExists
            ASOW--&gt;&gt;-R: return
            R-&gt;&gt;OE: AttachVolume
        else
        end
        deactivate R
    end
</code></pre>
<p>Mermaid</p>
<p>这里处理的工作其实相对更少一些，<code>Reconciler</code> 会将期望状态中的卷与实际状态进行比较，然后分离需要分离的卷、附着需要附着的卷，逻辑非常的清晰和简单。</p>
<h4 id="持久卷控制器">持久卷控制器</h4>
<p>作为集群中与 PV 和 PVC 打交道的控制器，持久卷控制器同时运行着三个 Goroutine 用于处理相应的逻辑，其中 <code>Resync</code> 协程负责从 Kubernetes 集群中同步 PV 和 PVC 的信息，而另外两个工作协程主要负消费队列中的任务：</p>
<pre><code class="language-mermaid">graph LR
    PVC(PVController)-.-&gt;R(Resync)
    PVC-.-&gt;VW(VolumeWorker)
    R-. enqueue .-&gt;VQ(VolumeQueue)
    R-. enqueue .-&gt;CQ(ClaimQueue)
    VQ-. dequeue .-&gt;VW
    CQ-. dequeue .-&gt;CW
    PVC-.-&gt;CW(ClaimWorker)
    style VQ fill:#fffede,stroke:#ebebb7
    style CQ fill:#fffede,stroke:#ebebb7
</code></pre>
<p>Mermaid</p>
<p>这两个工作协程主要负责对需要绑定或者解绑的 PV 和 PVC 进行处理，例如，当用户创建了新的 PVC 对象时，从集群中查找该 PVC 选择的 PV 并绑定到当前的 PVC 上。</p>
<h5 id="volumeworker">VolumeWorker</h5>
<p><code>VolumeWorker</code> 协程中执行的最重要的方法其实就是 <code>syncVolume</code>，在这个方法中会根据当前 PV 对象的规格对 PV 和 PVC 进行绑定或者解绑：</p>
<pre><code class="language-go">func (ctrl *PersistentVolumeController) syncVolume(volume *v1.PersistentVolume) error {
	if volume.Spec.ClaimRef == nil {
		return nil
	} else {
		if volume.Spec.ClaimRef.UID == &quot;&quot; {
			return nil
		}
		var claim *v1.PersistentVolumeClaim
		claimName := claimrefToClaimKey(volume.Spec.ClaimRef)
		obj, _, _ := ctrl.claims.GetByKey(claimName)
		claim, _ = obj.(*v1.PersistentVolumeClaim)

		if claim != nil &amp;&amp; claim.UID != volume.Spec.ClaimRef.UID {
			claim = nil
		}

		if claim == nil {
			ctrl.reclaimVolume(volume)
		} else if claim.Spec.VolumeName == &quot;&quot; {
			ctrl.claimQueue.Add(claimToClaimKey(claim))
		} else if claim.Spec.VolumeName == volume.Name {
		} else {
			if metav1.HasAnnotation(volume.ObjectMeta, annDynamicallyProvisioned) &amp;&amp; volume.Spec.PersistentVolumeReclaimPolicy == v1.PersistentVolumeReclaimDelete {
				ctrl.reclaimVolume(volume)
			} else {
				ctrl.unbindVolume(volume)
			}
		}
	}
	return nil
}
</code></pre>
<p>Go</p>
<p>如果当前 PV 没有绑定的 PVC 对象，那么这里的 <code>reclaimVolume</code> 可能会将当前的 PV 对象根据回收策略将其放回资源池等待重用、回收或者保留；而 <code>unbindVolume</code> 会删除 PV 与 PVC 之间的关系并更新 apiserver 中保存的 Kubernetes 对象数据。</p>
<h5 id="claimworker">ClaimWorker</h5>
<p><code>ClaimWorker</code> 就是控制器用来决定如何处理一个 PVC 对象的方法了，它会在一个 PVC 对象被创建、更新或者同步时被触发，<code>syncClaim</code> 会根据当前对象中的注解决定调用 <code>syncUnboundClaim</code> 或者 <code>syncBoundClaim</code> 方法来处理相应的逻辑：</p>
<pre><code class="language-go">func (ctrl *PersistentVolumeController) syncClaim(claim *v1.PersistentVolumeClaim) error {
	if !metav1.HasAnnotation(claim.ObjectMeta, annBindCompleted) {
		return ctrl.syncUnboundClaim(claim)
	} else {
		return ctrl.syncBoundClaim(claim)
	}
}
</code></pre>
<p>Go</p>
<p><code>syncUnboundClaim</code> 会处理绑定没有结束的 PVC 对象，如果当前 PVC 对象没有对应合适的 PV 存在，那么就会调用 <code>provisionClaim</code> 尝试从集群中获取新的 PV 供应，如果能够找到 PV 对象，就会通过 <code>bind</code> 方法将两者绑定：</p>
<pre><code class="language-go">func (ctrl *PersistentVolumeController) syncUnboundClaim(claim *v1.PersistentVolumeClaim) error {
	if claim.Spec.VolumeName == &quot;&quot; {
		delayBinding, err := ctrl.shouldDelayBinding(claim)

		volume, err := ctrl.volumes.findBestMatchForClaim(claim, delayBinding)
		if volume == nil {
			switch {
			case delayBinding:
			case v1helper.GetPersistentVolumeClaimClass(claim) != &quot;&quot;:
				ctrl.provisionClaim(claim)
			}
		} else {
			ctrl.bind(volume, claim)
		}
	} else {
		obj, found, _ := ctrl.volumes.store.GetByKey(claim.Spec.VolumeName)
		if found {
			volume, _ := obj.(*v1.PersistentVolume)
			if volume.Spec.ClaimRef == nil {
				ctrl.bind(volume, claim)
			} else if isVolumeBoundToClaim(volume, claim) {
				ctrl.bind(volume, claim)
			}
		}
	}
	return nil
}
</code></pre>
<p>Go</p>
<p>绑定的过程其实就是将 PV 和 PVC 之间建立起新的关系，更新 Spec 中的数据让两者能够通过引用 Ref 找到另一个对象并将更新后的 Kubernetes 对象存储到 apiserver 中。</p>
<p>另一个用于绑定 PV 和 PVC 对象的方法就是 <code>syncBoundClaim</code> 了，相比于 <code>syncUnboundClaim</code> 方法，该方法的实现更为简单，直接从缓存中尝试获取对应的 PV 对象：</p>
<pre><code class="language-go">func (ctrl *PersistentVolumeController) syncBoundClaim(claim *v1.PersistentVolumeClaim) error {
	if claim.Spec.VolumeName == &quot;&quot; {
		return nil
	}
	obj, found, _ := ctrl.volumes.store.GetByKey(claim.Spec.VolumeName)
	if found {
		volume, _ := obj.(*v1.PersistentVolume)
		if volume.Spec.ClaimRef == nil {
			ctrl.bind(volume, claim)
		} else if volume.Spec.ClaimRef.UID == claim.UID {
			ctrl.bind(volume, claim)
		}
	}
	return nil
}
</code></pre>
<p>Go</p>
<p>如果找到了 PV 对象并且该对象没有绑定的 PVC 或者当前 PV 和 PVC 已经存在了引用就会调用 <code>bind</code> 方法对两者进行绑定。</p>
<h5 id="小结-5">小结</h5>
<p>无论是 <code>VolumeWorker</code> 还是 <code>ClaimWorker</code> 最终都可能会通过 apiserver 更新集群中 etcd 的数据，当然它们也会调用一些底层的插件获取新的存储供应、删除或者重用一些持久卷，我们会在下面介绍插件的工作原理。</p>
<h3 id="插件">插件</h3>
<p>Kubernetes 中的所有对卷的操作最终基本都是通过 <code>OperationExecutor</code> 来完成的，这个组件包含了用于附着、挂载、卸载和分离几个常见的操作以及对设备进行操作的一些方法：</p>
<pre><code class="language-go">type OperationExecutor interface {
	AttachVolume(volumeToAttach VolumeToAttach, actualStateOfWorld ActualStateOfWorldAttacherUpdater) error
	DetachVolume(volumeToDetach AttachedVolume, verifySafeToDetach bool, actualStateOfWorld ActualStateOfWorldAttacherUpdater) error
	MountVolume(waitForAttachTimeout time.Duration, volumeToMount VolumeToMount, actualStateOfWorld ActualStateOfWorldMounterUpdater, isRemount bool) error
	UnmountVolume(volumeToUnmount MountedVolume, actualStateOfWorld ActualStateOfWorldMounterUpdater, podsDir string) error
	// ...
}
</code></pre>
<p>Go</p>
<p>实现 <code>OperationExecutor</code> 接口的私有结构体会通过 <code>OperatorGenerator</code> 来生成一个用于挂载和卸载卷的方法，并将这个方法包装在一个 <code>GeneratedOperations</code> 结构中，在这之后操作执行器会启动一个新的 Goroutine 用于执行生成好的方法：</p>
<pre><code class="language-mermaid">graph LR
    OE(OperationExexutor)-. 1. 获取相关方法 .-&gt;OG(OperationGenerator)
    OG-. 2. 根据 Spec 获取插件 .-&gt;VM(VolumePluginManager)
    VM-. 3. 返回 VolumePlugin .-&gt;OG
    OG-. 4. 构建方法 .-&gt;OG
    OG-. 5. 生成一个 Operation 结构 .-&gt;OE
    OE-. 6. 运行 Operation .-&gt;NPO(NestedPendingOperations)
    NPO-. 7. 启动 Goroutine 运行生成的方法 .-&gt;Goroutine
</code></pre>
<p>Mermaid</p>
<p><code>VolumePluginManager</code> 和 <code>VolumePlugin</code> 这两个组件在整个流程中帮我们屏蔽了底层不同类型卷的实现差异，我们能直接在上层调用完全相同的接口，剩下的逻辑都由底层的插件来负责。</p>
<p>Kubernetes 提供了插件的概念，通过 <code>Volume</code> 和 <code>Mounter</code> 两个接口支持卷类型的扩展，作为存储提供商或者不同类型的文件系统，我们都可以通过实现以上的两个接口成为 Kubernetes 存储系统中一个新的存储类型：</p>
<pre><code class="language-go">type VolumePlugin interface {
	Init(host VolumeHost) error
	GetPluginName() string
	GetVolumeName(spec *Spec) (string, error)
	NewMounter(spec *Spec, podRef *v1.Pod, opts VolumeOptions) (Mounter, error)
	// ...
}

type Mounter interface {
	Volume
	CanMount() error
	SetUp(fsGroup *int64) error
	SetUpAt(dir string, fsGroup *int64) error
	GetAttributes() Attributes
}
</code></pre>
<p>Go</p>
<p>在这一节中我们将介绍几种不同卷插件的实现，包括最常见的 EmptyDir、ConfigMap、Secret 和 Google 云上的 GCEPersistentDisk，这一节会简单介绍不同卷插件的实现方式，想要了解详细实现的读者可以阅读相关的源代码。</p>
<h4 id="emptydir">EmptyDir</h4>
<p>EmptyDir 是 Kubernetes 中最简单的卷了，当我们为一个 Pod 设置一个 EmptyDir 类型的卷时，其实就是在当前 Pod 对应的目录创建了一个空的文件夹，这个文件夹会随着 Pod 的删除而删除。</p>
<pre><code class="language-go">func (ed *emptyDir) SetUpAt(dir string, fsGroup *int64) error {
	ed.setupDir(dir)
	volume.SetVolumeOwnership(ed, fsGroup)
	volumeutil.SetReady(ed.getMetaDir())

	return nil
}

func (ed *emptyDir) setupDir(dir string) error {
	if err := os.MkdirAll(dir, perm); err != nil {
		return err
	}
	
	// ...

	return nil
}
</code></pre>
<p>Go</p>
<p><code>SetUpAt</code> 方法其实就实现了对这种类型卷的创建工作，每当 Pod 被分配到了某个节点上，对应的文件目录就会通过 <code>MkdirAll</code> 方法创建，如果使用者配置了 medium 字段，也会选择使用相应的文件系统挂载到当前目录上，例如：tmpfs、nodev 等。</p>
<pre><code class="language-yaml">apiVersion: v1
kind: Pod
metadata:
  name: test-pd
spec:
  containers:
  - image: k8s.gcr.io/test-webserver
    name: test-container
    volumeMounts:
    - mountPath: /cache
      name: cache-volume
  volumes:
  - name: cache-volume
    emptyDir: {}
</code></pre>
<p>YAML</p>
<p>我们经常会使用 EmptyDir 类型的卷在多个容器之间共享文件、充当缓存或者保留一些临时的日志，总而言之，这是一种经常被使用的卷类型。</p>
<h4 id="configmap-和-secret">ConfigMap 和 Secret</h4>
<p>另一种比较常见的卷就是 ConfigMap 了，首先，ConfigMap 本身就是 Kubernetes 中常见的对象了，其中的 <code>data</code> 就是一个存储了从文件名到文件内容的字段，这里的 ConfigMap 对象被挂载到文件目录时就会创建一个名为 <code>redis-config</code> 的文件，然后将文件内容写入该文件：</p>
<pre><code class="language-yaml">apiVersion: v1
kind: ConfigMap
data:
  redis-config: |
    maxmemory 2mb
    maxmemory-policy allkeys-lru    
</code></pre>
<p>YAML</p>
<p>在对 ConfigMap 类型的卷进行挂载时，总共需要完成三部分工作，首先从 apiserver 中获取当前 ConfigMap 对象，然后根据当前的 ConfigMap 生成一个从文件名到文件内容的键值对，最后构造一个 Writer 并执行 <code>Write</code> 方法写入内容：</p>
<pre><code class="language-go">func (b *configMapVolumeMounter) SetUpAt(dir string, fsGroup *int64) error {
	configMap, _ := b.getConfigMap(b.pod.Namespace, b.source.Name)

	totalBytes := totalBytes(configMap)
	payload, _ := MakePayload(b.source.Items, configMap, b.source.DefaultMode, false)

	writerContext := fmt.Sprintf(&quot;pod %v/%v volume %v&quot;, b.pod.Namespace, b.pod.Name, b.volName)
	writer, _ := volumeutil.NewAtomicWriter(dir, writerContext)
	writer.Write(payload)

	return nil
}
</code></pre>
<p>Go</p>
<p>在涉及挂载的函数几个中，作者想要着重介绍的也就是在底层直接与文件系统打交道的 <code>writePayloadToDir</code> 方法：</p>
<pre><code class="language-go">func (w *AtomicWriter) writePayloadToDir(payload map[string]FileProjection, dir string) error {
	for userVisiblePath, fileProjection := range payload {
		content := fileProjection.Data
		mode := os.FileMode(fileProjection.Mode)
		fullPath := path.Join(dir, userVisiblePath)
		baseDir, _ := filepath.Split(fullPath)

		os.MkdirAll(baseDir, os.ModePerm)
		ioutil.WriteFile(fullPath, content, mode)
		os.Chmod(fullPath, mode)
	}

	return nil
}
</code></pre>
<p>Go</p>
<p>这个方法使用了 <code>os</code> 包提供的接口完成了拼接文件名、创建相应文件目录、写入文件并且修改文件模式的工作，将 ConfigMap <code>data</code> 中的数据映射到了一个文件夹中，达到了让 Pod 中的容器可以直接通过文件系统获取内容的目的。</p>
<p>对于另一个非常常见的卷类型 Secret，Kubernetes 其实也做了几乎完全相同的工作，也是先获取 Secret 对象，然后构建最终写入到文件的键值对，最后初始化一个 Writer 并调用它的 <code>Write</code> 方法，从这里我们也能看出在卷插件这一层对于 ConfigMap 和 Secret 的处理几乎完全相同，并没有出现需要对 Secret 对象中的内容进行解密的工作。</p>
<h4 id="gcepersistentdisk">GCEPersistentDisk</h4>
<p>最后一个要介绍的卷与上面的几种都非常的不同，它在底层使用的是云服务商提供的网络磁盘，想要在一个节点上使用云磁盘其实总共需要两个步骤，首先是要将云磁盘附着到当前的节点上，这部分的工作其实就是由 <code>gcePersistentDiskAttacher</code> 完成的，每当调用 <code>AttachDisk</code> 方法时，最终都会执行云服务商提供的接口，将磁盘附着到相应的节点实例上：</p>
<pre><code class="language-mermaid">sequenceDiagram
    participant GPDA as gcePersistentDiskAttacher
    participant C as Cloud
    participant GCESM as gceServiceManager
    participant I as GCEInstances
    GPDA-&gt;&gt;+C: DiskIsAttached
    alt NotAttached
        C--&gt;&gt;-GPDA: return NotAttached
        GPDA-&gt;&gt;+C: AttachDisk
        C-&gt;&gt;+GCESM: AttachDiskOnCloudProvider
        GCESM-&gt;&gt;+I: AttachDisk
        I--&gt;&gt;-GCESM: return
        GCESM--&gt;&gt;-C: return
        C--&gt;&gt;-GPDA: return
    else
    end
</code></pre>
<p>Mermaid</p>
<p>在方法的最后会将该请求包装成一个 HTTP 的方法调用向 <code>https://www.googleapis.com/compute/v1/projects/{project}/zones/{zone}/instances/{resourceId}/attachDisk</code> 链接发出一个 POST 请求，这个请求会将某个 GCE 上的磁盘附着到目标实例上，详细的内容可以阅读 <a href="https://cloud.google.com/compute/docs/reference/rest/v1/instances/attachDisk" target="_blank">相关文档</a>。</p>
<p>一旦当前的磁盘被附着到了当前节点上，我们就能跟使用其他的插件一样，把磁盘挂载到某个目录上，完成从附着到挂载的全部操作。</p>
<h3 id="总结-4">总结</h3>
<p>Volume 和存储系统是 Kubernetes 非常重要的一部分，它能够帮助我们在多个容器之间共享文件，同时也能够为集群提供持久存储的功能，假如 Kubernetes 没有用于持久存储的对象，我们也很难在集群中运行有状态的服务，例如：消息队列、分布式存储等。</p>
<p>对于刚刚使用 Kubernetes 的开发者来说，Volume、PV 和 PVC 确实是比较难以理解的概念，但是这却是深入使用 Kubernetes 必须要了解和掌握的，希望这篇文章能够帮助各位读者更好地理解存储系统底层的实现原理。</p>
<h2 id="kubernetes-replicaset">Kubernetes ReplicaSet</h2>
<p>Kubernetes 中的 ReplicaSet 主要的作用是维持一组 <a href="https://draveness.me/kubernetes-pod" target="_blank">Pod</a> 副本的运行，它的主要作用就是保证一定数量的 Pod 能够在集群中正常运行，它会持续监听这些 Pod 的运行状态，在 Pod 发生故障重启数量减少时重新运行新的 Pod 副本。</p>
<p>这篇文章会介绍 ReplicaSet 的工作原理，其中包括在 Kubernetes 中是如何被创建的、如何创建并持有 Pod 并在出现问题时重启它们。</p>
<h3 id="概述-2">概述</h3>
<p>在具体介绍 ReplicaSet 的实现原理之前，我们还是会先简单介绍它的使用，与其他的 Kubernetes 对象一样，我们会在 Kubernetes 集群中使用 YAML 文件创建新的 ReplicaSet 对象，一个常见的 ReplicaSet 的定义其实是这样的：</p>
<pre><code class="language-yaml">apiVersion: apps/v1
kind: ReplicaSet
metadata:
  name: frontend
  labels:
    app: guestbook
    tier: frontend
spec:
  replicas: 3
  selector:
    matchLabels:
      tier: frontend
  template:
    metadata:
      labels:
        tier: frontend
    spec:
      containers:
      - name: php-redis
        image: gcr.io/google_samples/gb-frontend:v3
</code></pre>
<p>YAML</p>
<p>这里的 YAML 文件除了常见的 <code>apiVersion</code>、<code>kind</code> 和 <code>metadata</code> 属性之外，规格中总共包含三部分重要内容，也就是 Pod 副本数目 <code>replicas</code>、选择器 <code>selector</code> 和 Pod 模板 <code>template</code>，这三个部分共同定义了 ReplicaSet 的规格：</p>
<p><img class="img-zoomable" src="https://img.draveness.me/2019-02-16-kubernetes-replicaset-spec.png" alt="kubernetes-replicaset-spe" />
</p>
<p>同一个 ReplicaSet 会使用选择器 <code>selector</code> 中的定义查找集群中自己持有的 <code>Pod</code> 对象，它们会根据标签的匹配获取能够获得的 Pod，下面就是持有三个 Pod 对象的 Replica 拓扑图：</p>
<pre><code class="language-mermaid">graph TD
    ReplicaSet-.-&gt;Pod1
    ReplicaSet-.-&gt;Pod2
    ReplicaSet-.-&gt;Pod3
</code></pre>
<p>Mermaid</p>
<p>被 ReplicaSet 持有的 Pod 有一个 <code>metadata.ownerReferences</code> 指针指向当前的 ReplicaSet，表示当前 Pod 的所有者，这个引用主要会被集群中的 <a href="https://draveness.me/kubernetes-garbage-collector" target="_blank">垃圾收集器</a> 使用以清理失去所有者的 Pod 对象。</p>
<h3 id="实现原理-1">实现原理</h3>
<p>所有 ReplicaSet 对象的增删改查都是由 <code>ReplicaSetController</code> 控制器完成的，该控制器会通过 <code>Informer</code> 监听 ReplicaSet 和 Pod 的变更事件并将其加入持有的待处理队列:</p>
<pre><code class="language-mermaid">graph TD
    PI[PodInformer]-. Event .-&gt;RSC[ReplicaSetController]
    RSI[ReplicaSetInformer]-. Event .-&gt;RSC
    RSC--&gt;worker1
    RSC-. Add ReplicaSet .-&gt;queue
    worker1-. Loop .-&gt;worker1
    queue-. Get ReplicaSet .-&gt;worker1
    RSC--&gt;worker2
    worker2-. Loop .-&gt;worker2
    queue-. Get ReplicaSet .-&gt;worker2
    style worker1 fill:#fffede,stroke:#ebebb7
    style worker2 fill:#fffede,stroke:#ebebb7
</code></pre>
<p>Mermaid</p>
<p><code>ReplicaSetController</code> 中的 <code>queue</code> 其实就是一个存储待处理 ReplicaSet 的『对象池』，它运行的几个 Goroutine 会从队列中取出最新的数据进行处理，上图展示了事件从发生到被处理的流向，我们接下来将分别介绍 ReplicaSet 中常见的同步过程。</p>
<h4 id="同步">同步</h4>
<p><code>ReplicaSetController</code> 启动的多个 Goroutine 会从队列中取出待处理的任务，然后调用 <code>syncReplicaSet</code> 进行同步，这个方法会按照传入的 <code>key</code> 从 etcd 中取出 ReplicaSet 对象，然后取出全部 Active 的 Pod：</p>
<pre><code class="language-go">func (rsc *ReplicaSetController) syncReplicaSet(key string) error {
	namespace, name, _ := cache.SplitMetaNamespaceKey(key)
	rs, _ := rsc.rsLister.ReplicaSets(namespace).Get(name)

	rsNeedsSync := rsc.expectations.SatisfiedExpectations(key)
	selector, _ := metav1.LabelSelectorAsSelector(rs.Spec.Selector)

	allPods, _ := rsc.podLister.Pods(rs.Namespace).List(labels.Everything())
	filteredPods := controller.FilterActivePods(allPods)

	filteredPods, _ = rsc.claimPods(rs, selector, filteredPods)

	var manageReplicasErr error
	if rsNeedsSync &amp;&amp; rs.DeletionTimestamp == nil {
		manageReplicasErr = rsc.manageReplicas(filteredPods, rs)
	}
	newStatus := calculateStatus(rs, filteredPods, manageReplicasErr)
	updatedRS, _ := updateReplicaSetStatus(rsc.kubeClient.AppsV1().ReplicaSets(rs.Namespace), rs, newStatus)

	return manageReplicasErr
}
</code></pre>
<p>Go</p>
<p>随后执行的 <code>ClaimPods</code> 方法会获取一系列 Pod 的所有权，如果当前的 Pod 与 ReplicaSet 的选择器匹配就会建立从属关系，否则就会释放持有的对象，或者直接忽视无关的 Pod，建立和释放关系的方法就是 <code>AdoptPod</code> 和 <code>ReleasePod</code>，<code>AdoptPod</code> 会设置目标对象的 <code>metadata.OwnerReferences</code> 字段：</p>
<pre><code class="language-json">// AdoptPod
{
   &quot;metadata&quot;: {
      &quot;ownerReferences&quot;: [
         {
            &quot;apiVersion&quot;: m.controllerKind.GroupVersion(),
            &quot;kind&quot;: m.controllerKind.Kind,
            &quot;name&quot;: m.Controller.GetName(),
            &quot;uid&quot;: m.Controller.GetUID(),
            &quot;controller&quot;: true,
            &quot;blockOwnerDeletion&quot;: true
         }
      ],
      &quot;uid&quot;: pod.UID
   }
}
</code></pre>
<p>JSON</p>
<p>而 <code>ReleasePod</code> 会使用如下的 JSON 数据删除目标 Pod 中的 <code>metadata.OwnerReferences</code> 属性：</p>
<pre><code class="language-json">// ReleasePod
{
   &quot;metadata&quot;: {
      &quot;ownerReferences&quot;: [
         {
            &quot;$patch&quot;:&quot;delete&quot;,
            &quot;uid&quot;: m.Controller.GetUID()
         }
      ],
      &quot;uid&quot;: pod.UID
   }
}
</code></pre>
<p>JSON</p>
<p>无论是建立还是释放从属关系，都是根据 ReplicaSet 的选择器配置进行的，它们根据匹配的标签执行不同的操作。</p>
<p>在对已经存在 Pod 进行更新之后，<code>manageReplicas</code> 方法会检查并更新当前 ReplicaSet 持有的副本，如果已经存在的 Pod 数量小于 ReplicaSet 的期望数量，那么就会根据模板的配置创建一些新的 Pod 并与这些 Pod 建立从属关系，创建使用 <code>slowStartBatch</code> 方法分组批量创建 Pod 以减少失败的次数：</p>
<pre><code class="language-go">func (rsc *ReplicaSetController) manageReplicas(filteredPods []*v1.Pod, rs *apps.ReplicaSet) error {
	diff := len(filteredPods) - int(*(rs.Spec.Replicas))
	rsKey, _ := controller.KeyFunc(rs)
	if diff &lt; 0 {
		diff *= -1
		if diff &gt; rsc.burstReplicas {
			diff = rsc.burstReplicas
		}
		successfulCreations, err := slowStartBatch(diff, controller.SlowStartInitialBatchSize, func() error {
			boolPtr := func(b bool) *bool { return &amp;b }
			controllerRef := &amp;metav1.OwnerReference{
				APIVersion:         rsc.GroupVersion().String(),
				Kind:               rsc.Kind,
				Name:               rs.Name,
				UID:                rs.UID,
				BlockOwnerDeletion: boolPtr(true),
				Controller:         boolPtr(true),
			}
			rsc.podControl.CreatePodsWithControllerRef(rs.Namespace, &amp;rs.Spec.Template, rs, controllerRef)
			return nil
		})

		return err
</code></pre>
<p>Go</p>
<p>删除 Pod 的方式就是并发进行的了，代码使用 <code>WaitGroup</code> 等待全部的删除任务运行结束才会返回：</p>
<pre><code class="language-go">	} else if diff &gt; 0 {
		if diff &gt; rsc.burstReplicas {
			diff = rsc.burstReplicas
		}

		podsToDelete := getPodsToDelete(filteredPods, diff)

		var wg sync.WaitGroup
		wg.Add(diff)
		for _, pod := range podsToDelete {
			go func(targetPod *v1.Pod) {
				defer wg.Done()
				rsc.podControl.DeletePod(rs.Namespace, targetPod.Name, rs)				}
			}(pod)
		}
		wg.Wait()
	}

	return nil
}
</code></pre>
<p>Go</p>
<p>如果需要删除全部的 Pod 就不对传入的 <code>filteredPods</code> 进行排序，否则就会按照三个不同的维度对 Pod 进行排序：</p>
<ol>
<li>NotReady &lt; Ready</li>
<li>Unscheduled &lt; Scheduled</li>
<li>Pending &lt; Running</li>
</ol>
<p>按照上述规则进行排序的 Pod 能够保证删除在早期阶段的 Pod 对象，简单总结一下，<code>manageReplicas</code> 方法会在与已经存在的 Pod 建立关系之后，对持有的数量和期望的数量进行比较之后，会根据 Pod 模板创建或者删除 Pod:</p>
<p><img class="img-zoomable" src="https://img.draveness.me/2019-02-16-kubernetes-manage-replicas.png" alt="kubernetes-manage-replicas" />
</p>
<p>到这里整个处理 ReplicaSet 的主要工作就结束了，<code>syncReplicaSet</code> 中剩下的代码会更新 ReplicaSet 的状态并结束同步 ReplicaSet 的工作。</p>
<h4 id="删除-1">删除</h4>
<p>如果我们在 Kubernetes 集群中删除一个 ReplicaSet 持有的 Pod，那么控制器会重新同步 ReplicaSet 的状态并启动一个新的 Pod，但是如果删除集群中的 ReplicaSet 所有相关的 Pod 也都会被删除：</p>
<pre><code class="language-bash">$ kubectl delete rs example
replicaset.extensions &quot;example&quot; deleted

$ kubectl get pods --watch
example-z4fvc   0/1       Terminating   0         54s
example-zswpk   0/1       Terminating   0         54s
example-v8wwn   0/1       Terminating   0         54s
</code></pre>
<p>Bash</p>
<p>删除相关 Pod 的工作并不是 <code>ReplicaSetController</code> 负责的，而是由集群中的 <a href="https://draveness.me/kubernetes-garbage-collector" target="_blank">垃圾收集器</a>，也就是 <code>GarbageCollector</code> 实现的。</p>
<p>Kubernetes 中的垃圾收集器会负责删除<strong>以前有所有者但是现在没有</strong>的对象，<code>metadata.ownerReference</code> 属性标识了一个对象的所有者，当垃圾收集器发现对象的所有者被删除时，就会自动删除这些无用的对象，这也是 ReplicaSet 持有的 Pod 被自动删除的原因，我们会在 <a href="https://draveness.me/kubernetes-garbage-collector" target="_blank">垃圾收集器</a> 一节中具体介绍垃圾收集器的原理。</p>
<h3 id="总结-5">总结</h3>
<p>Kubernetes 中的 ReplicaSet 并不是一个工程师经常需要直接接触的对象，常用的 Deployment 其实使用 ReplicaSet 实现了很多复杂的特性，例如滚动更新，虽然作为使用者我们并不会经常直接与 ReplicaSet 这一对象打交道，但是如果需要对 Kubernetes 进行一些定制化开发，可能会用 ReplicaSet 和其他对象实现一些更复杂的功能。</p>
<h2 id="kubernetes-垃圾收集器">Kubernetes 垃圾收集器</h2>
<p>垃圾收集器在 Kubernetes 中的作用就是删除之前有所有者但是现在所有者已经不存在的对象，例如删除 ReplicaSet 时会删除它依赖的 Pod，虽然它的名字是垃圾收集器，但是它在 Kubernetes 中还是以控制器的形式进行设计和实现的。</p>
<p>在 Kubernetes 引入垃圾收集器之前，所有的级联删除逻辑都是在客户端完成的，kubectl 会先删除 ReplicaSet 持有的 Pod 再删除 ReplicaSet，但是垃圾收集器的引入就让级联删除的实现移到了服务端，我们在这里就会介绍垃圾收集器的设计和实现原理。</p>
<h3 id="概述-3">概述</h3>
<p>垃圾收集主要提供的功能就是级联删除，它向对象的 API 中加入了 <code>metadata.ownerReferences</code> 字段，这一字段会包含当前对象的所有依赖者，在默认情况下，如果当前对象的所有依赖者都被删除，那么当前对象就会被删除：</p>
<pre><code class="language-go">type ObjectMeta struct {
	...
	OwnerReferences []OwnerReference
}

type OwnerReference struct {
	APIVersion string
	Kind string
	Name string
	UID types.UID
}
</code></pre>
<p>Go</p>
<p><code>OwnerReference</code> 包含了足够的信息来标识当前对象的依赖者，对象的依赖者必须与当前对象位于同一个命名空间 <code>namespace</code>，否则两者就无法建立起依赖关系。</p>
<p>通过引入 <code>metadata.ownerReferences</code> 能够建立起不同对象的关系，但是我们依然需要其他的组件来负责处理对象之间的联系并在所有依赖者不存在时将对象删除，这个处理不同对象联系的组件就是 <code>GarbageCollector</code>，也是 Kubernetes 控制器的一种。</p>
<h3 id="实现原理-2">实现原理</h3>
<p><code>GarbageCollector</code> 中包含一个 <code>GraphBuilder</code> 结构体，这个结构体会以 Goroutine 的形式运行并使用 Informer 监听集群中几乎全部资源的变动，一旦发现任何的变更事件 — 增删改，就会将事件交给主循环处理，主循环会根据事件的不同选择将待处理对象加入不同的队列，与此同时 <code>GarbageCollector</code> 持有的另外两组队列会负责删除或者孤立目标对象。</p>
<pre><code class="language-mermaid">graph TD
    M1[PodMonitor]-. event .-&gt;WQ[WorkQueue]
    M2[ReplicaSetMonitor]-. event .-&gt;WQ[WorkQueue]
    GB[GraphBuilder]-- owns --&gt;M1
    GB-- owns --&gt;M2
    WQ-. event .-&gt;PGC(ProcessGraphChanges)
    GB-- owns --&gt;PGC
    PGC-. item .-&gt;ATD[AttemptsToDelete]
    PGC-. item .-&gt;ATO[AttemptsToOrphan]
    GC[GarbageCollector]-- owns --&gt;DW[DeleteWorker]
    ATD-.-&gt;DW
    ATO-.-&gt;OW
    GC[GarbageCollector]-- owns --&gt;OW[OrphanWorker]
    GC-- owns --&gt;GB
    style M1 fill:#fffede,stroke:#ebebb7
    style M2 fill:#fffede,stroke:#ebebb7
    style GB fill:#fffede,stroke:#ebebb7
    style PGC fill:#fffede,stroke:#ebebb7
    style DW fill:#fffede,stroke:#ebebb7
    style OW fill:#fffede,stroke:#ebebb7
</code></pre>
<p>Mermaid</p>
<p>接下来我们会从几个关键点介绍垃圾收集器是如何删除 Kubernetes 集群中的对象以及它们的依赖的。</p>
<h4 id="删除策略">删除策略</h4>
<p>多个资源的 Informer 共同构成了垃圾收集器中的 <code>Propagator</code>，它监听所有的资源更新事件并将它们投入到工作队列中，这些事件会更新内存中的 DAG，这个 DAG 表示了集群中不同对象之间的从属关系，垃圾收集器的多个 Worker 会从两个队列中获取待处理的对象并调用 <code>attemptToDeleteItem</code> 和 <code>attempteToOrphanItem</code> 方法，这里我们主要介绍 <code>attemptToDeleteItem</code> 的实现：</p>
<pre><code class="language-go">func (gc *GarbageCollector) attemptToDeleteItem(item *node) error {
	latest, _ := gc.getObject(item.identity)
	ownerReferences := latest.GetOwnerReferences()

	solid, dangling, waitingForDependentsDeletion, _ := gc.classifyReferences(item, ownerReferences)
</code></pre>
<p>Go</p>
<p>该方法会先获取待处理的对象以及所有者的引用列表，随后使用 <code>classifyReferences</code> 方法将引用进行分类并按照不同的条件分别进行处理：</p>
<pre><code class="language-go">	switch {
	case len(solid) != 0:
		ownerUIDs := append(ownerRefsToUIDs(dangling), ownerRefsToUIDs(waitingForDependentsDeletion)...)
		patch := deleteOwnerRefStrategicMergePatch(item.identity.UID, ownerUIDs...)
		gc.patch(item, patch, func(n *node) ([]byte, error) {
			return gc.deleteOwnerRefJSONMergePatch(n, ownerUIDs...)
		})
		return err
</code></pre>
<p>Go</p>
<p>如果当前对象的所有者还有存在于集群中的，那么当前的对象就不会被删除，上述代码会将已经被删除或等待删除的引用从对象中删掉。</p>
<p>当正在被删除的所有者不存在任何的依赖并且该对象的 <code>ownerReference.blockOwnerDeletion</code> 属性为 <code>true</code> 时会阻止依赖方的删除，所以当前的对象会等待属性 <code>ownerReference.blockOwnerDeletion=true</code> 的所有对象的删除后才会被删除。</p>
<pre><code class="language-go">	// ...
	case len(waitingForDependentsDeletion) != 0 &amp;&amp; item.dependentsLength() != 0:
		deps := item.getDependents()
		for _, dep := range deps {
			if dep.isDeletingDependents() {
				patch, _ := item.unblockOwnerReferencesStrategicMergePatch()
				gc.patch(item, patch, gc.unblockOwnerReferencesJSONMergePatch)				
				break
			}
		}
		policy := metav1.DeletePropagationForeground
		return gc.deleteObject(item.identity, &amp;policy)
	// ...	
</code></pre>
<p>Go</p>
<p>在默认情况下，也就是当前对象已经不包含任何依赖，那么如果当前对象可能会选择三种不同的策略处理依赖：</p>
<pre><code class="language-go">	// ...
	default:
		var policy metav1.DeletionPropagation
		switch {
		case hasOrphanFinalizer(latest):
			policy = metav1.DeletePropagationOrphan
		case hasDeleteDependentsFinalizer(latest):
			policy = metav1.DeletePropagationForeground
		default:
			policy = metav1.DeletePropagationBackground
		}
		return gc.deleteObject(item.identity, &amp;policy)
	}
}
</code></pre>
<p>Go</p>
<ol>
<li>如果当前对象有 <code>FinalizerOrphanDependents</code> 终结器，<code>DeletePropagationOrphan</code> 策略会让对象所有的依赖变成孤立的；</li>
<li>如果当前对象有 <code>FinalizerDeleteDependents</code> 终结器，<code>DeletePropagationBackground</code> 策略在前台等待所有依赖被删除后才会删除，整个删除过程都是同步的；</li>
<li>默认情况下会使用 <code>DeletePropagationDefault</code> 策略在后台删除当前对象的全部依赖；</li>
</ol>
<h4 id="终结器">终结器</h4>
<p>对象的终结器是在对象删除之前需要执行的逻辑，所有的对象在删除之前，它的终结器字段必须为空，终结器提供了一个通用的 API，它的功能不只是用于阻止级联删除，还能过通过它在对象删除之前加入钩子：</p>
<pre><code class="language-go">type ObjectMeta struct {
	// ...
	Finalizers []string
}
</code></pre>
<p>Go</p>
<p>终结器在对象被删之前运行，每当终结器成功运行之后，就会将它自己从 <code>Finalizers</code> 数组中删除，当最后一个终结器被删除之后，API Server 就会删除该对象。</p>
<p>在默认情况下，删除一个对象会删除它的全部依赖，但是我们在一些特定情况下我们只是想删除当前对象本身并不想造成复杂的级联删除，垃圾回收机制在这时引入了 <code>OrphanFinalizer</code>，它会在对象被删除之前向 <code>Finalizers</code> 数组添加或者删除 <code>OrphanFinalizer</code>。</p>
<p>该终结器会监听对象的更新事件并将它自己从它全部依赖对象的 <code>OwnerReferences</code> 数组中删除，与此同时会删除所有依赖对象中已经失效的 <code>OwnerReferences</code> 并将 <code>OrphanFinalizer</code> 从 <code>Finalizers</code> 数组中删除。</p>
<p>通过 <code>OrphanFinalizer</code> 我们能够在删除一个 Kubernetes 对象时保留它的全部依赖，为使用者提供一种更灵活的办法来保留和删除对象。</p>
<h3 id="总结-6">总结</h3>
<p>Kubernetes 中垃圾收集器的实现还是比较容易理解的，它的主要作用就是监听集群中对象的变更事件并根据两个字段 <code>OwnerReferences</code> 和 <code>Finalizers</code> 确定对象的删除策略，其中包括同步和后台的选择、是否应该触发级联删除移除当前对象的全部依赖；在默认情况下，当我们删除 Kubernetes 集群中的 ReplicaSet、Deployment 对象时都会删除这些对象的全部依赖，不过我们也可以通过 <code>OrphanFinalizer</code> 终结器删除单独的对象。</p>
<h2 id="kubernetes-deployment">Kubernetes Deployment</h2>
<p>如果你在生产环境中使用过 Kubernetes，那么相信你对 Deployment 一定不会陌生，Deployment 提供了一种对 <a href="https://draveness.me/kubernetes-pod" target="_blank">Pod</a> 和 <a href="https://draveness.me/kubernetes-replicaset" target="_blank">ReplicaSet</a> 的管理方式，每一个 Deployment 都对应集群中的一次部署，是非常常见的 Kubernetes 对象。</p>
<p>我们在这篇文章中就会介绍 Deployment 的实现原理，包括它是如何处理 Pod 的滚动更新、回滚以及支持副本的水平扩容。</p>
<h3 id="概述-4">概述</h3>
<p>作为最常用的 Kubernetes 对象，Deployment 经常会用来创建 <a href="https://draveness.me/kubernetes-replicaset" target="_blank">ReplicaSet</a> 和 <a href="https://draveness.me/kubernetes-pod" target="_blank">Pod</a>，我们往往不会直接在集群中使用 ReplicaSet 部署一个新的微服务，一方面是因为 ReplicaSet 的功能其实不够强大，一些常见的更新、扩容和缩容运维操作都不支持，Deployment 的引入就是为了就是为了支持这些复杂的操作。</p>
<pre><code class="language-yaml">apiVersion: apps/v1
kind: Deployment
metadata:
  name: nginx-deployment
  labels:
    app: nginx
spec:
  replicas: 3
  selector:
    matchLabels:
      app: nginx
  template:
    metadata:
      labels:
        app: nginx
    spec:
      containers:
      - name: nginx
        image: nginx:1.7.9
        ports:
        - containerPort: 80
</code></pre>
<p>YAML</p>
<p>当我们在 Kubernetes 集群中创建上述 Deployment 对象时，它不只会创建 Deployment 资源，还会创建另外的 ReplicaSet 以及三个 Pod 对象：</p>
<pre><code class="language-bash">$ kubectl get deployments.apps
NAME               READY     UP-TO-DATE   AVAILABLE   AGE
nginx-deployment   3/3       3            3           6m55s

$ kubectl get replicasets.apps
NAME                          DESIRED   CURRENT   READY     AGE
nginx-deployment-76bf4969df   3         3         3         7m27s

$ kubectl get pods
NAME                                READY     STATUS    RESTARTS   AGE
nginx-deployment-76bf4969df-58gxj   1/1       Running   0          7m42s
nginx-deployment-76bf4969df-9jgk9   1/1       Running   0          7m42s
nginx-deployment-76bf4969df-m4pkg   1/1       Running   0          7m43s
</code></pre>
<p>Bash</p>
<p>每一个 Deployment 都会和它的依赖组成以下的拓扑结构，在这个拓扑结构中的子节点都是『稳定』的，任意节点的删除都会被 Kubernetes 的控制器重启：</p>
<pre><code class="language-mermaid">graph TD
    Deployment-.-&gt;ReplicaSet
    ReplicaSet-.-&gt;Pod1
    ReplicaSet-.-&gt;Pod2
    ReplicaSet-.-&gt;Pod3
</code></pre>
<p>Mermaid</p>
<p>所有的 Deployment 对象都是由 Kubernetes 集群中的 <code>DeploymentController</code> 进行管理，家下来我们将开始介绍该控制器的实现原理。</p>
<h3 id="实现原理-3">实现原理</h3>
<p><code>DeploymentController</code> 作为管理 Deployment 资源的控制器，会在启动时通过 <code>Informer</code> 监听三种不同资源的通知，Pod、ReplicaSet 和 Deployment，这三种资源的变动都会触发 <code>DeploymentController</code> 中的回调。</p>
<pre><code class="language-mermaid">graph TD
    DI[DeploymentInformer]-. Add/Update/Delete .-&gt;DC[DeploymentController]
    ReplicaSetInformer-. Add/Update/Delete .-&gt;DC
    PodInformer-. Delete .-&gt;DC
</code></pre>
<p>Mermaid</p>
<p>不同的事件最终都会在被过滤后进入控制器持有的队列，等待工作进程的消费，下面的这些事件都会触发 Deployment 的同步：</p>
<ol>
<li>Deployment 的变动；</li>
<li>Deployment 相关的 ReplicaSet 变动；</li>
<li>Deployment 相关的 Pod 数量为 0 时，Pod 的删除事件；</li>
</ol>
<p><code>DeploymentController</code> 会在调用 <code>Run</code> 方法时启动多个工作进程，这些工作进程会运行 <code>worker</code> 方法从队列中读取最新的 Deployment 对象进行同步。</p>
<h4 id="同步-1">同步</h4>
<p>Deployment 对象的同步都是通过以下的 <code>syncDeployment</code> 方法进行的，该方法包含了同步、回滚以及更新的逻辑，是同步 Deployment 资源的唯一入口：</p>
<pre><code class="language-go">func (dc *DeploymentController) syncDeployment(key string) error {
	namespace, name, _ := cache.SplitMetaNamespaceKey(key)
	deployment, _ := dc.dLister.Deployments(namespace).Get(name)

	d := deployment.DeepCopy()

	rsList, _ := dc.getReplicaSetsForDeployment(d)
	podMap, _ := dc.getPodMapForDeployment(d, rsList)

	dc.checkPausedConditions(d)

	if d.Spec.Paused {
		return dc.sync(d, rsList)
	}

	scalingEvent, _ := dc.isScalingEvent(d, rsList)
	if scalingEvent {
		return dc.sync(d, rsList)
	}

	switch d.Spec.Strategy.Type {
	case apps.RecreateDeploymentStrategyType:
		return dc.rolloutRecreate(d, rsList, podMap)
	case apps.RollingUpdateDeploymentStrategyType:
		return dc.rolloutRolling(d, rsList)
	}
	return fmt.Errorf(&quot;unexpected deployment strategy type: %s&quot;, d.Spec.Strategy.Type)
}
</code></pre>
<p>Go</p>
<ol>
<li>
<p>根据传入的键获取 Deployment 资源；</p>
</li>
<li>
<p>调用</p>
<pre><code>getReplicaSetsForDeployment
</code></pre>
<p>获取集群中与 Deployment 相关的全部 ReplicaSet；</p>
<ol>
<li>查找集群中的全部 ReplicaSet；</li>
<li>根据 Deployment 的选择器对 ReplicaSet 建立或者释放从属关系；</li>
</ol>
</li>
<li>
<p>调用</p>
<pre><code>getPodMapForDeployment
</code></pre>
<p>获取当前 Deployment 对象相关的从 ReplicaSet 到 Pod 的映射；</p>
<ol>
<li>根据选择器查找全部的 Pod；</li>
<li>根据 Pod 的控制器 ReplicaSet 对上述 Pod 进行分类；</li>
</ol>
</li>
<li>
<p>如果当前的 Deployment 处于暂停状态或者需要进行扩容，就会调用 <code>sync</code> 方法同步 Deployment;</p>
</li>
<li>
<p>在正常情况下会根据规格中的策略对 Deployment 进行更新；</p>
<ol>
<li><code>Recreate</code> 策略会调用 <code>rolloutRecreate</code> 方法，它会先杀掉所有存在的 Pod 后启动新的 Pod 副本；</li>
<li><code>RollingUpdate</code> 策略会调用 <code>rolloutRolling</code> 方法，根据 <code>maxSurge</code> 和 <code>maxUnavailable</code> 配置对 Pod 进行滚动更新；</li>
</ol>
</li>
</ol>
<p>这就是 Deployment 资源同步的主要流程，我们在这里可以关注一下 <code>getReplicaSetsForDeployment</code> 方法：</p>
<pre><code class="language-go">func (dc *DeploymentController) getReplicaSetsForDeployment(d *apps.Deployment) ([]*apps.ReplicaSet, error) {
	rsList, _ := dc.rsLister.ReplicaSets(d.Namespace).List(labels.Everything())
	deploymentSelector, _ := metav1.LabelSelectorAsSelector(d.Spec.Selector)
	canAdoptFunc := controller.RecheckDeletionTimestamp(func() (metav1.Object, error) {
		return dc.client.AppsV1().Deployments(d.Namespace).Get(d.Name, metav1.GetOptions{})
	})
	cm := controller.NewReplicaSetControllerRefManager(dc.rsControl, d, deploymentSelector, controllerKind, canAdoptFunc)
	return cm.ClaimReplicaSets(rsList)
}
</code></pre>
<p>Go</p>
<p>该方法获取 Deployment 持有的 ReplicaSet 时会重新与集群中符合条件的 ReplicaSet 通过 <code>ownerReferences</code> 建立关系，执行的逻辑与 <a href="https://draveness.me/kubernetes-replicaset" target="_blank">ReplicaSet</a> 调用 <code>AdoptPod/ReleasePod</code> 几乎完全相同。</p>
<h5 id="扩容">扩容</h5>
<p>如果当前需要更新的 Deployment 经过 <code>isScalingEvent</code> 的检查发现更新事件实际上是一次扩容或者缩容，也就是 ReplicaSet 持有的 Pod 数量和规格中的 <code>Replicas</code> 字段并不一致，那么就会调用 <code>sync</code> 方法对 Deployment 进行同步：</p>
<pre><code class="language-go">func (dc *DeploymentController) sync(d *apps.Deployment, rsList []*apps.ReplicaSet) error {
	newRS, oldRSs, _ := dc.getAllReplicaSetsAndSyncRevision(d, rsList, false)
	dc.scale(d, newRS, oldRSs)

	allRSs := append(oldRSs, newRS)
	return dc.syncDeploymentStatus(allRSs, newRS, d)
}
</code></pre>
<p>Go</p>
<p>同步的过程其实比较简单，该方法会从 apiserver 中拿到当前 Deployment 对应的最新 ReplicaSet 和历史的 ReplicaSet 并调用 <code>scale</code> 方法开始扩容，<code>scale</code> 就是扩容需要执行的主要方法，我们将下面的方法分成几部分依次进行介绍：</p>
<pre><code class="language-go">func (dc *DeploymentController) scale(deployment *apps.Deployment, newRS *apps.ReplicaSet, oldRSs []*apps.ReplicaSet) error {
	if activeOrLatest := deploymentutil.FindActiveOrLatest(newRS, oldRSs); activeOrLatest != nil {
		if *(activeOrLatest.Spec.Replicas) == *(deployment.Spec.Replicas) {
			return nil
		}
		dc.scaleReplicaSetAndRecordEvent(activeOrLatest, *(deployment.Spec.Replicas), deployment)
		return nil
	}

	if deploymentutil.IsSaturated(deployment, newRS) {
		for _, old := range controller.FilterActiveReplicaSets(oldRSs) {
			dc.scaleReplicaSetAndRecordEvent(old, 0, deployment)
		}
		return nil
	}
</code></pre>
<p>Go</p>
<p>如果集群中只有一个活跃的 ReplicaSet，那么就会对该 ReplicaSet 进行扩缩容，但是如果不存在活跃的 ReplicaSet 对象，就会选择最新的 ReplicaSet 进行操作，这部分选择 ReplicaSet 的工作都是由 <code>FindActiveOrLatest</code> 和 <code>scaleReplicaSetAndRecordEvent</code> 共同完成的。</p>
<p>当调用 <code>IsSaturated</code> 方法发现当前的 Deployment 对应的副本数量已经饱和时就会删除所有历史版本 ReplicaSet 持有的 Pod 副本。</p>
<p><img class="img-zoomable" src="https://img.draveness.me/2019-02-24-kubernetes-deployment-scale-replicas.png" alt="kubernetes-deployment-scale-replicas" />
</p>
<p>但是在 Deployment 使用滚动更新策略时，如果发现当前的 ReplicaSet 并没有饱和并且存在多个活跃的 ReplicaSet 对象就会按照比例分别对各个活跃的 ReplicaSet 进行扩容或者缩容：</p>
<pre><code class="language-go">	if deploymentutil.IsRollingUpdate(deployment) {
		allRSs := controller.FilterActiveReplicaSets(append(oldRSs, newRS))
		allRSsReplicas := deploymentutil.GetReplicaCountForReplicaSets(allRSs)

		allowedSize := int32(0)
		if *(deployment.Spec.Replicas) &gt; 0 {
			allowedSize = *(deployment.Spec.Replicas) + deploymentutil.MaxSurge(*deployment)
		}

		deploymentReplicasToAdd := allowedSize - allRSsReplicas

		var scalingOperation string
		switch {
		case deploymentReplicasToAdd &gt; 0:
			sort.Sort(controller.ReplicaSetsBySizeNewer(allRSs))
			scalingOperation = &quot;up&quot;

		case deploymentReplicasToAdd &lt; 0:
			sort.Sort(controller.ReplicaSetsBySizeOlder(allRSs))
			scalingOperation = &quot;down&quot;
		}
</code></pre>
<p>Go</p>
<ol>
<li>
<p>通过 <code>FilterActiveReplicaSets</code> 获取所有活跃的 ReplicaSet 对象；</p>
</li>
<li>
<p>调用 <code>GetReplicaCountForReplicaSets</code> 计算当前 Deployment 对应 ReplicaSet 持有的全部 Pod 副本个数；</p>
</li>
<li>
<p>根据 Deployment 对象配置的 <code>Replicas</code> 和最大额外可以存在的副本数 <code>maxSurge</code> 以计算 Deployment 允许创建的 Pod 数量；</p>
</li>
<li>
<p>通过 <code>allowedSize</code> 和 <code>allRSsReplicas</code> 计算出需要增加或者删除的副本数；</p>
</li>
<li>
<p>根据</p>
<pre><code>deploymentReplicasToAdd
</code></pre>
<p>变量的符号对 ReplicaSet 数组进行排序并确定当前的操作时扩容还是缩容；</p>
<ol>
<li>如果 <code>deploymentReplicasToAdd &gt; 0</code>，ReplicaSet 将按照从新到旧的顺序依次进行扩容；</li>
<li>如果 <code>deploymentReplicasToAdd &lt; 0</code>，ReplicaSet 将按照从旧到新的顺序依次进行缩容；</li>
</ol>
</li>
</ol>
<blockquote>
<p><code>maxSurge</code>、<code>maxUnavailable</code> 是两个处理滚动更新时需要关注的参数，我们会在滚动更新一节中具体介绍。</p>
</blockquote>
<pre><code class="language-go">		deploymentReplicasAdded := int32(0)
		nameToSize := make(map[string]int32)
		for i := range allRSs {
			rs := allRSs[i]

			if deploymentReplicasToAdd != 0 {
				proportion := deploymentutil.GetProportion(rs, *deployment, deploymentReplicasToAdd, deploymentReplicasAdded)

				nameToSize[rs.Name] = *(rs.Spec.Replicas) + proportion
				deploymentReplicasAdded += proportion
			} else {
				nameToSize[rs.Name] = *(rs.Spec.Replicas)
			}
		}
</code></pre>
<p>Go</p>
<p>因为当前的 Deployment 持有了多个活跃的 ReplicaSet，所以在计算了需要增加或者删除的副本个数 <code>deploymentReplicasToAdd</code> 之后，就会为多个活跃的 ReplicaSet 分配每个 ReplicaSet 需要改变的副本数，<code>GetProportion</code> 会根据以下几个参数决定最后的结果:</p>
<ol>
<li>Deployment 期望的 Pod 副本数量；</li>
<li>需要新增或者减少的副本数量；</li>
<li>Deployment 当前通过 ReplicaSet 持有 Pod 的总数量；</li>
</ol>
<p>Kubernetes 会在 <code>getReplicaSetFraction</code> 使用下面的公式计算每一个 ReplicaSet 在 Deployment 资源中的占比，最后会返回该 ReplicaSet 需要改变的副本数：</p>
<p><img class="img-zoomable" src="https://img.draveness.me/2019-02-24-deployment-replicaset-get-proportion.png" alt="deployment-replicaset-get-proportion" />
</p>
<p>该结果又会与目前期望的剩余变化量进行对比，保证变化的副本数量不会超过期望值。</p>
<pre><code class="language-go">		for i := range allRSs {
			rs := allRSs[i]

			// ...

			dc.scaleReplicaSet(rs, nameToSize[rs.Name], deployment, scalingOperation)
		}
	}
	return nil
}
</code></pre>
<p>Go</p>
<p>在 <code>scale</code> 方法的最后会直接调用 <code>scaleReplicaSet</code> 将每一个 ReplicaSet 都扩容或者缩容到我们期望的副本数：</p>
<pre><code class="language-go">func (dc *DeploymentController) scaleReplicaSet(rs *apps.ReplicaSet, newScale int32, deployment *apps.Deployment, scalingOperation string) (bool, *apps.ReplicaSet, error) {
	sizeNeedsUpdate := *(rs.Spec.Replicas) != newScale

	annotationsNeedUpdate := deploymentutil.ReplicasAnnotationsNeedUpdate(rs, *(deployment.Spec.Replicas), *(deployment.Spec.Replicas)+deploymentutil.MaxSurge(*deployment))

	if sizeNeedsUpdate || annotationsNeedUpdate {
		rsCopy := rs.DeepCopy()
		*(rsCopy.Spec.Replicas) = newScale
		deploymentutil.SetReplicasAnnotations(rsCopy, *(deployment.Spec.Replicas), *(deployment.Spec.Replicas)+deploymentutil.MaxSurge(*deployment))
		rs, _ = dc.client.AppsV1().ReplicaSets(rsCopy.Namespace).Update(rsCopy)
	}
	return true, rs, err
}
</code></pre>
<p>Go</p>
<p>这里会直接修改目标 ReplicaSet 规格中的 <code>Replicas</code> 参数和注解 <code>deployment.kubernetes.io/desired-replicas</code> 的值并通过 API 请求更新当前的 ReplicaSet 对象：</p>
<pre><code class="language-bash">$ kubectl describe rs nginx-deployment-76bf4969df
Name:           nginx-deployment-76bf4969df
Namespace:      default
Selector:       app=nginx,pod-template-hash=76bf4969df
Labels:         app=nginx
                pod-template-hash=76bf4969df
Annotations:    deployment.kubernetes.io/desired-replicas=3
                deployment.kubernetes.io/max-replicas=4
...
</code></pre>
<p>Bash</p>
<p>我们可以通过 <code>describe</code> 命令查看 ReplicaSet 的注解，其实能够发现当前 ReplicaSet 的期待副本数和最大副本数，<code>deployment.kubernetes.io/desired-replicas</code> 注解就是在上述方法中被 Kubernetes 的 <code>DeploymentController</code> 更新的。</p>
<h5 id="重新创建">重新创建</h5>
<p>当 Deployment 使用的更新策略类型是 <code>Recreate</code> 时，<code>DeploymentController</code> 就会使用如下的 <code>rolloutRecreate</code> 方法对 Deployment 进行更新：</p>
<pre><code class="language-go">func (dc *DeploymentController) rolloutRecreate(d *apps.Deployment, rsList []*apps.ReplicaSet, podMap map[types.UID]*v1.PodList) error {
	newRS, oldRSs, _ := dc.getAllReplicaSetsAndSyncRevision(d, rsList, false)
	allRSs := append(oldRSs, newRS)
	activeOldRSs := controller.FilterActiveReplicaSets(oldRSs)

	scaledDown, _ := dc.scaleDownOldReplicaSetsForRecreate(activeOldRSs, d)
	if scaledDown {
		return dc.syncRolloutStatus(allRSs, newRS, d)
	}

	if oldPodsRunning(newRS, oldRSs, podMap) {
		return dc.syncRolloutStatus(allRSs, newRS, d)
	}

	if newRS == nil {
		newRS, oldRSs, _ = dc.getAllReplicaSetsAndSyncRevision(d, rsList, true)
		allRSs = append(oldRSs, newRS)
	}

	dc.scaleUpNewReplicaSetForRecreate(newRS, d)

	if util.DeploymentComplete(d, &amp;d.Status) {
		dc.cleanupDeployment(oldRSs, d)
	}

	return dc.syncRolloutStatus(allRSs, newRS, d)
}
</code></pre>
<p>Go</p>
<ol>
<li>利用 <code>getAllReplicaSetsAndSyncRevision</code> 和 <code>FilterActiveReplicaSets</code> 两个方法获取 Deployment 中所有的 ReplicaSet 以及其中活跃的 ReplicaSet 对象；</li>
<li>调用 <code>scaleDownOldReplicaSetsForRecreate</code> 方法将所有活跃的历史 ReplicaSet 持有的副本 Pod 数目降至 0；</li>
<li>同步 Deployment 的最新状态并等待 Pod 的终止；</li>
<li>在需要时通过 <code>getAllReplicaSetsAndSyncRevision</code> 方法创建新的 ReplicaSet 并调用 <code>scaleUpNewReplicaSetForRecreate</code> 函数对 ReplicaSet 进行扩容；</li>
<li>更新完成之后会调用 <code>cleanupDeployment</code> 方法删除历史全部的 ReplicaSet 对象并更新 Deployment 的状态；</li>
</ol>
<p><img class="img-zoomable" src="https://img.draveness.me/2019-02-24-kubernetes-deployment-rollout-recreate.png" alt="kubernetes-deployment-rollout-recreate" />
</p>
<p>也就是说在更新的过程中，之前创建的 ReplicaSet 和 Pod 资源全部都会被删除，只是 Pod 会先被删除而 ReplicaSet 会后被删除；上述方法也会创建新的 ReplicaSet 和 Pod 对象，需要注意的是在这个过程中旧的 Pod 副本一定会先被删除，所以会有一段时间不存在可用的 Pod。</p>
<h5 id="滚动更新">滚动更新</h5>
<p>Deployment 的另一个更新策略 <code>RollingUpdate</code> 其实更加常见，在具体介绍滚动更新的流程之前，我们首先需要了解滚动更新策略使用的两个参数 <code>maxUnavailable</code> 和 <code>maxSurge</code>：</p>
<ul>
<li><code>maxUnavailable</code> 表示在更新过程中能够进入不可用状态的 Pod 的最大值；</li>
<li><code>maxSurge</code> 表示能够额外创建的 Pod 个数；</li>
</ul>
<p><code>maxUnavailable</code> 和 <code>maxSurge</code> 这两个滚动更新的配置都可以使用绝对值或者百分比表示，使用百分比时需要用 <code>Replicas * Strategy.RollingUpdate.MaxSurge</code> 公式计算相应的数值。</p>
<p><img class="img-zoomable" src="https://img.draveness.me/2019-02-24-kubernetes-deployment-rolling-update-spec.png" alt="kubernetes-deployment-rolling-update-spe" />
</p>
<p><code>rolloutRolling</code> 方法就是 <code>DeploymentController</code> 用于处理滚动更新的方法：</p>
<pre><code class="language-go">func (dc *DeploymentController) rolloutRolling(d *apps.Deployment, rsList []*apps.ReplicaSet) error {
	newRS, oldRSs, _ := dc.getAllReplicaSetsAndSyncRevision(d, rsList, true)
	allRSs := append(oldRSs, newRS)

	scaledUp, _ := dc.reconcileNewReplicaSet(allRSs, newRS, d)
	if scaledUp {
		return dc.syncRolloutStatus(allRSs, newRS, d)
	}

	scaledDown, _ := dc.reconcileOldReplicaSets(allRSs, controller.FilterActiveReplicaSets(oldRSs), newRS, d)
	if scaledDown {
		return dc.syncRolloutStatus(allRSs, newRS, d)
	}

	if deploymentutil.DeploymentComplete(d, &amp;d.Status) {
		dc.cleanupDeployment(oldRSs, d)
	}

	return dc.syncRolloutStatus(allRSs, newRS, d)
}
</code></pre>
<p>Go</p>
<ol>
<li>首先获取 Deployment 对应的全部 ReplicaSet 资源；</li>
<li>通过 <code>reconcileNewReplicaSet</code> 调解新 ReplicaSet 的副本数，创建新的 Pod 并保证额外的副本数量不超过 <code>maxSurge</code>；</li>
<li>通过 <code>reconcileOldReplicaSets</code> 调解历史 ReplicaSet 的副本数，删除旧的 Pod 并保证不可用的部分数不会超过 <code>maxUnavailable</code>；</li>
<li>最后删除无用的 ReplicaSet 并更新 Deployment 的状态；</li>
</ol>
<p>需要注意的是，在滚动更新的过程中，Kubernetes 并不是一次性就切换到期望的状态，即『新 ReplicaSet 运行指定数量的副本』，而是会先启动新的 ReplicaSet 以及一定数量的 Pod 副本，然后删除历史 ReplicaSet 中的副本，再启动一些新 ReplicaSet 的副本，不断对新 ReplicaSet 进行扩容并对旧 ReplicaSet 进行缩容最终达到了集群期望的状态。</p>
<p>当我们使用如下的 <code>reconcileNewReplicaSet</code> 方法对新 ReplicaSet 进行调节时，我们会发现在新 ReplicaSet 中副本数量满足期望时会直接返回，在超过期望时会进行缩容：</p>
<pre><code class="language-go">func (dc *DeploymentController) reconcileNewReplicaSet(allRSs []*apps.ReplicaSet, newRS *apps.ReplicaSet, deployment *apps.Deployment) (bool, error) {
	if *(newRS.Spec.Replicas) == *(deployment.Spec.Replicas) {
		return false, nil
	}
	if *(newRS.Spec.Replicas) &gt; *(deployment.Spec.Replicas) {
		scaled, _, err := dc.scaleReplicaSetAndRecordEvent(newRS, *(deployment.Spec.Replicas), deployment)
		return scaled, err
	}
	newReplicasCount, _ := deploymentutil.NewRSNewReplicas(deployment, allRSs, newRS)
	scaled, _, err := dc.scaleReplicaSetAndRecordEvent(newRS, newReplicasCount, deployment)
	return scaled, err
}
</code></pre>
<p>Go</p>
<p>如果 ReplicaSet 的数量不够就会调用 <code>NewRSNewReplicas</code> 函数计算新的副本个数，计算的过程使用了如下所示的公式：</p>
<pre><code class="language-go">maxTotalPods = deployment.Spec.Replicas + 
currentPodCount = sum(deployement.ReplicaSets.Replicas)
scaleUpCount = maxTotalPods - currentPodCount
scaleUpCount = min(scaleUpCount, deployment.Spec.Replicas - newRS.Spec.Replicas))
newRSNewReplicas = newRS.Spec.Replicas + scaleUpCount
</code></pre>
<p>Go</p>
<p>该过程总共需要考虑 Deployment 期望的副本数量、当前可用的副本数量以及新 ReplicaSet 持有的副本，还有一些最大值和最小值的限制，例如额外 Pod 数量不能超过 <code>maxSurge</code>、新 ReplicaSet 的 Pod 数量不能超过 Deployment 的期望数量，遵循这些规则我们就能计算出 <code>newRSNewReplicas</code>。</p>
<p>另一个滚动更新中使用的方法 <code>reconcileOldReplicaSets</code> 主要作用就是对历史 ReplicaSet 对象持有的副本数量进行缩容：</p>
<pre><code class="language-go">func (dc *DeploymentController) reconcileOldReplicaSets(allRSs []*apps.ReplicaSet, oldRSs []*apps.ReplicaSet, newRS *apps.ReplicaSet, deployment *apps.Deployment) (bool, error) {
	oldPodsCount := deploymentutil.GetReplicaCountForReplicaSets(oldRSs)
	if oldPodsCount == 0 {
		return false, nil
	}

	allPodsCount := deploymentutil.GetReplicaCountForReplicaSets(allRSs)
	maxUnavailable := deploymentutil.MaxUnavailable(*deployment)

	minAvailable := *(deployment.Spec.Replicas) - maxUnavailable
	newRSUnavailablePodCount := *(newRS.Spec.Replicas) - newRS.Status.AvailableReplicas
	maxScaledDown := allPodsCount - minAvailable - newRSUnavailablePodCount
	if maxScaledDown &lt;= 0 {
		return false, nil
	}

	oldRSs, cleanupCount, _ := dc.cleanupUnhealthyReplicas(oldRSs, deployment, maxScaledDown)

	allRSs = append(oldRSs, newRS)
	scaledDownCount, _ := dc.scaleDownOldReplicaSetsForRollingUpdate(allRSs, oldRSs, deployment)

	totalScaledDown := cleanupCount + scaledDownCount
	return totalScaledDown &gt; 0, nil
}
</code></pre>
<p>Go</p>
<ol>
<li>计算历史 ReplicaSet 持有的副本总数量；</li>
<li>计算全部 ReplicaSet 持有的副本总数量；</li>
<li>根据 Deployment 期望的副本数、最大不可用副本数以及新 ReplicaSet 中不可用的 Pod 数量计算最大缩容的副本个数；</li>
<li>通过 <code>cleanupUnhealthyReplicas</code> 方法清理 ReplicaSet 中处于不健康状态的副本；</li>
<li>调用 <code>scaleDownOldReplicaSetsForRollingUpdate</code> 方法对历史 ReplicaSet 中的副本进行缩容；</li>
</ol>
<pre><code class="language-go">minAvailable = deployment.Spec.Replicas - maxUnavailable(deployment)
maxScaledDown = allPodsCount - minAvailable - newReplicaSetPodsUnavailable
</code></pre>
<p>Go</p>
<p>该方法会使用上述简化后的公式计算这次总共能够在历史 ReplicaSet 中删除的最大 Pod 数量，并调用 <code>cleanupUnhealthyReplicas</code> 和 <code>scaleDownOldReplicaSetsForRollingUpdate</code> 两个方法进行缩容，这两个方法的实现都相对简单，它们都对历史 ReplicaSet 按照创建时间进行排序依次对这些资源进行缩容，两者的区别在于前者主要用于删除不健康的副本。</p>
<h4 id="回滚">回滚</h4>
<p>Kubernetes 中的每一个 Deployment 资源都包含有 <code>revision</code> 这个概念，版本的引入可以让我们在更新发生问题时及时通过 Deployment 的版本对其进行回滚，当我们在更新 Deployment 时，之前 Deployment 持有的 ReplicaSet 其实会被 <code>cleanupDeployment</code> 方法清理：</p>
<pre><code class="language-go">func (dc *DeploymentController) cleanupDeployment(oldRSs []*apps.ReplicaSet, deployment *apps.Deployment) error {
	aliveFilter := func(rs *apps.ReplicaSet) bool {
		return rs != nil &amp;&amp; rs.ObjectMeta.DeletionTimestamp == nil
	}
	cleanableRSes := controller.FilterReplicaSets(oldRSs, aliveFilter)

	diff := int32(len(cleanableRSes)) - *deployment.Spec.RevisionHistoryLimit
	if diff &lt;= 0 {
		return nil
	}
	sort.Sort(controller.ReplicaSetsByCreationTimestamp(cleanableRSes))

	for i := int32(0); i &lt; diff; i++ {
		rs := cleanableRSes[i]
		if rs.Status.Replicas != 0 || *(rs.Spec.Replicas) != 0 || rs.Generation &gt; rs.Status.ObservedGeneration || rs.DeletionTimestamp != nil {
			continue
		}
		dc.client.AppsV1().ReplicaSets(rs.Namespace).Delete(rs.Name, nil)
	}

	return nil
}
</code></pre>
<p>Go</p>
<p>Deployment 资源在规格中由一个 <code>spec.revisionHistoryLimit</code> 的配置，这个配置决定了 Kubernetes 会保存多少个 ReplicaSet 的历史版本，这些历史上的 ReplicaSet 并不会被删除，它们只是不再持有任何的 Pod 副本了，假设我们有一个 <code>spec.revisionHistoryLimit=2</code> 的 Deployment 对象，那么当前资源最多持有两个历史的 ReplicaSet 版本：</p>
<p><img class="img-zoomable" src="https://img.draveness.me/2019-02-24-kubernetes-deployment-revision.png" alt="kubernetes-deployment-revision" />
</p>
<p>这些资源的保留能够方便 Deployment 的回滚，而回滚其实是通过 kubectl 在客户端实现的，我们可以使用如下的命令将 Deployment 回滚到上一个版本：</p>
<pre><code class="language-bash">$ kubectl rollout undo deployment.v1.apps/nginx-deployment
deployment.apps/nginx-deployment
</code></pre>
<p>Bash</p>
<p>上述 kubectl 命令没有指定回滚到的版本号，所以在默认情况下会回滚到上一个版本，在回滚时会直接根据传入的版本查找历史的 ReplicaSet 资源，拿到这个 ReplicaSet 对应的 Pod 模板后会触发一个资源更新的请求：</p>
<pre><code class="language-go">func (r *DeploymentRollbacker) Rollback(obj runtime.Object, updatedAnnotations map[string]string, toRevision int64, dryRun bool) (string, error) {
	accessor, _ := meta.Accessor(obj)
	name := accessor.GetName()
	namespace := accessor.GetNamespace()

	deployment, _ := r.c.AppsV1().Deployments(namespace).Get(name, metav1.GetOptions{})
	rsForRevision, _ := deploymentRevision(deployment, r.c, toRevision)

	annotations := ...
	patchType, patch, _ := getDeploymentPatch(&amp;rsForRevision.Spec.Template, annotations)

	r.c.AppsV1().Deployments(namespace).Patch(name, patchType, patch)
	return rollbackSuccess, nil
}
</code></pre>
<p>Go</p>
<p>回滚对于 Kubernetes 服务端来说其实与其他的更新操作没有太多的区别，在每次更新时都会在 <code>FindNewReplicaSet</code> 函数中根据 Deployment 的 Pod 模板在历史 ReplicaSet 中查询是否有相同的 ReplicaSet 存在：</p>
<pre><code class="language-go">func FindNewReplicaSet(deployment *apps.Deployment, rsList []*apps.ReplicaSet) *apps.ReplicaSet {
	sort.Sort(controller.ReplicaSetsByCreationTimestamp(rsList))
	for i := range rsList {
		if EqualIgnoreHash(&amp;rsList[i].Spec.Template, &amp;deployment.Spec.Template) {
			return rsList[i]
		}
	}
	return nil
}
</code></pre>
<p>Go</p>
<p>如果存在规格完全相同的 ReplicaSet，就会保留这个 ReplicaSet 历史上使用的版本号并对该 ReplicaSet 重新进行扩容并对正在工作的 ReplicaSet 进行缩容以实现集群的期望状态。</p>
<pre><code class="language-bash">$ k describe deployments.apps nginx-deployment
Name:                   nginx-deployment
Namespace:              default
CreationTimestamp:      Thu, 21 Feb 2019 10:14:29 +0800
Labels:                 app=nginx
Annotations:            deployment.kubernetes.io/revision: 11
                        kubectl.kubernetes.io/last-applied-configuration:
                          {&quot;apiVersion&quot;:&quot;apps/v1&quot;,&quot;kind&quot;:&quot;Deployment&quot;,&quot;metadata&quot;:{&quot;annotations&quot;:{},&quot;labels&quot;:{&quot;app&quot;:&quot;nginx&quot;},&quot;name&quot;:&quot;nginx-deployment&quot;,&quot;namespace&quot;:&quot;d...
Selector:               app=nginx
Replicas:               3 desired | 3 updated | 3 total | 3 available | 0 unavailable
StrategyType:           RollingUpdate
...
Events:
  Type    Reason              Age   From                   Message
  ----    ------              ----  ----                   -------
  Normal  ScalingReplicaSet   20s   deployment-controller  Scaled up replica set nginx-deployment-5cc74f885d to 1
  Normal  ScalingReplicaSet   19s   deployment-controller  Scaled down replica set nginx-deployment-7c6cf994f6 to 2
  Normal  ScalingReplicaSet   19s   deployment-controller  Scaled up replica set nginx-deployment-5cc74f885d to 2
  Normal  ScalingReplicaSet   17s   deployment-controller  Scaled down replica set nginx-deployment-7c6cf994f6 to 1
  Normal  ScalingReplicaSet   17s   deployment-controller  Scaled up replica set nginx-deployment-5cc74f885d to 3
  Normal  ScalingReplicaSet   14s   deployment-controller  Scaled down replica set nginx-deployment-7c6cf994f6 to 0
</code></pre>
<p>Bash</p>
<p>在之前的 Kubernetes 版本中，客户端还会使用注解来实现 Deployment 的回滚，但是在最新的 kubectl 版本中这种使用注解的方式已经被废弃了。</p>
<h4 id="暂停和恢复">暂停和恢复</h4>
<p>Deployment 中有一个不是特别常用的功能，也就是 Deployment 进行暂停，暂停之后的 Deployment 哪怕发生了改动也不会被 Kubernetes 更新，这时我们可以对 Deployment 资源进行更新或者修复，随后当重新恢复 Deployment 时，<code>DeploymentController</code> 才会重新对其进行滚动更新向期望状态迁移：</p>
<pre><code class="language-go">func defaultObjectPauser(obj runtime.Object) ([]byte, error) {
	switch obj := obj.(type) {
	case *appsv1.Deployment:
		if obj.Spec.Paused {
			return nil, errors.New(&quot;is already paused&quot;)
		}
		obj.Spec.Paused = true
		return runtime.Encode(scheme.Codecs.LegacyCodec(appsv1.SchemeGroupVersion), obj)

	// ...
	default:
		return nil, fmt.Errorf(&quot;pausing is not supported&quot;)
	}
}
</code></pre>
<p>Go</p>
<p>暂停和恢复也都是由 kubectl 在客户端实现的，其实就是通过更改 <code>spec.paused</code> 属性，这里的更改会变成一个更新操作修改 Deployment 资源。</p>
<pre><code class="language-bash">$ kubectl rollout pause deployment.v1.apps/nginx-deployment
deployment.apps/nginx-deployment paused

$ kubectl get deployments.apps nginx-deployment -o yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  # ...
  name: nginx-deployment
  namespace: default
  selfLink: /apis/apps/v1/namespaces/default/deployments/nginx-deployment
  uid: 6b44965f-357e-11e9-af24-0800275e8310
spec:
  paused: true
  # ...
</code></pre>
<p>Bash</p>
<p>如果我们使用 YAML 文件和 <code>kubectl apply</code> 命令来更新整个 Deployment 资源，那么其实用不到暂停这一功能，我们只需要在文件里对资源进行修改并进行一次更新就可以了，但是我们可以在出现问题时，暂停一次正在进行的滚动更新以防止错误的扩散。</p>
<h4 id="删除-2">删除</h4>
<p>如果我们在 Kubernetes 集群中删除了一个 Deployment 资源，那么 Deployment 持有的 ReplicaSet 以及 ReplicaSet 持有的副本都会被 Kubernetes 中的 <a href="https://draveness.me/kubernetes-garbage-collector" target="_blank">垃圾收集器</a> 删除：</p>
<pre><code class="language-go">$ kubectl delete deployments.apps nginx-deployment
deployment.apps &quot;nginx-deployment&quot; deleted

$ kubectl get replicasets --watch
nginx-deployment-7c6cf994f6   0     0     0     2d1h
nginx-deployment-5cc74f885d   0     0     0     2d1h
nginx-deployment-c5d875444   3     3     3     30h

$ kubectl get pods --watch
nginx-deployment-c5d875444-6r4q6   1/1   Terminating   2     30h
nginx-deployment-c5d875444-7ssgj   1/1   Terminating   2     30h
nginx-deployment-c5d875444-4xvvz   1/1   Terminating   2     30h
</code></pre>
<p>Go</p>
<p>由于与当前 Deployment 有关的 ReplicaSet 历史和最新版本都会被删除，所以对应的 Pod 副本也都会随之被删除，这些对象之间的关系都是通过 <code>metadata.ownerReference</code> 这一字段关联的，<a href="https://draveness.me/kubernetes-garbage-collector" target="_blank">垃圾收集器</a> 一节详细介绍了它的实现原理。</p>
<h3 id="总结-7">总结</h3>
<p>Deployment 是 Kubernetes 中常用的对象类型，它解决了 ReplicaSet 更新的诸多问题，通过对 ReplicaSet 和 Pod 进行组装支持了滚动更新、回滚以及扩容等高级功能，通过对 Deployment 的学习既能让我们了解整个常见资源的实现也能帮助我们理解如何将 Kubernetes 内置的对象组合成更复杂的自定义资源。</p>
<h2 id="kubernetes-statefulset">Kubernetes StatefulSet</h2>
<p>在 Kubernetes 的世界中，<a href="https://draveness.me/kubernetes-replicaset" target="_blank">ReplicaSet</a> 和 <a href="https://draveness.me/kubernetes-deployment" target="_blank">Deployment</a> 主要用于处理无状态的服务，无状态服务的需求往往非常简单并且轻量，每一个无状态节点存储的数据在重启之后就会被删除，虽然这种服务虽然常见，但是我们仍然需要有状态的服务来实现一些特殊的需求，StatefulSet 就是 Kubernetes 为了运行有状态服务引入的资源，例如 Zookeeper、Kafka 等。</p>
<p>这篇文章会介绍 Kubernetes 如何在集群中运行有状态服务，同时会分析这些有状态服务 StatefulSet 的同步过程以及实现原理。</p>
<h3 id="概述-5">概述</h3>
<p>StatefulSet 是用于管理有状态应用的工作负载对象，与 <a href="https://draveness.me/kubernetes-replicaset" target="_blank">ReplicaSet</a> 和 <a href="https://draveness.me/kubernetes-deployment" target="_blank">Deployment</a> 这两个对象不同，StatefulSet 不仅能管理 Pod 的对象，还它能够保证这些 Pod 的顺序性和唯一性。</p>
<p>与 Deployment 一样，StatefulSet 也使用规格中声明的 <code>template</code> 模板来创建 Pod 资源，但是这些 Pod 相互之间是不能替换的；除此之外 StatefulSet 会为每个 Pod 设置一个单独的持久标识符，这些用于标识序列的标识符在发生调度时也不会丢失。</p>
<pre><code class="language-yaml">apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: web
spec:
  serviceName: &quot;nginx&quot;
  replicas: 2
  selector:
    matchLabels:
      app: nginx
  template:
    metadata:
      labels:
        app: nginx
    spec:
      containers:
      - name: nginx
        image: k8s.gcr.io/nginx-slim:0.8
        volumeMounts:
        - name: www
          mountPath: /usr/share/nginx/html
  volumeClaimTemplates:
  - metadata:
      name: www
    spec:
      accessModes: [ &quot;ReadWriteOnce&quot; ]
      resources:
        requests:
          storage: 1Gi
</code></pre>
<p>YAML</p>
<p>如果我们在 Kubernetes 集群中创建如上所示的 StatefulSet 对象，会得到以下结果，Kubernetes 不仅会创建 StatefulSet 对象，还会自动创建两个 Pod 副本：</p>
<pre><code class="language-go">$ kubectl get statefulsets.apps
kNAME   READY   AGE
web    2/2     2m27s

$ kubectl get pods
NAME    READY   STATUS    RESTARTS   AGE
web-0   1/1     Running   0          2m31s
web-1   1/1     Running   0          105s

$ kubectl get persistentvolumes
NAME                                       CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS   CLAIM               STORAGECLASS       REASON   AGE
pvc-19ef374f-39d1-11e9-b870-9efb418608da   1Gi        RWO            Delete           Bound    default/www-web-1   do-block-storage            21m
pvc-fe53d5f7-39d0-11e9-b870-9efb418608da   1Gi        RWO            Delete           Bound    default/www-web-0   do-block-storage            21m

$ kubectl get persistentvolumeclaims
NAME        STATUS   VOLUME                                     CAPACITY   ACCESS MODES   STORAGECLASS       AGE
www-web-0   Bound    pvc-fe53d5f7-39d0-11e9-b870-9efb418608da   1Gi        RWO            do-block-storage   21m
www-web-1   Bound    pvc-19ef374f-39d1-11e9-b870-9efb418608da   1Gi        RWO            do-block-storage   21m
</code></pre>
<p>Go</p>
<p>除此之外，上述 YAML 文件中的 <code>volumeClaimTemplates</code> 配置还会创建持久卷<code>PersistentVolume</code> 和用于绑定持久卷和 Pod 的 <code>PersistentVolumeClaim</code> 资源；两个 Pod 对象名中包含了它们的序列号，该序列号会在 StatefulSet 存在的时间内保持不变，哪怕 Pod 被重启或者重新调度，也不会出现任何的改变。</p>
<pre><code class="language-mermaid">graph TD
  SS[StatefulSet]-.-&gt;Pod1
  SS[StatefulSet]-.-&gt;Pod2
  Pod1-.-PersistentVolumeClaim1
  Pod2-.-PersistentVolumeClaim2
  PersistentVolumeClaim1-.-&gt;PersistentVolume1
  PersistentVolumeClaim2-.-&gt;PersistentVolume2
</code></pre>
<p>Mermaid</p>
<p>StatefulSet 的拓扑结构和其他用于部署的资源其实比较类似，比较大的区别在于 StatefulSet 引入了 PV 和 PVC 对象来持久存储服务产生的状态，这样所有的服务虽然可以被杀掉或者重启，但是其中的数据由于 PV 的原因不会丢失。</p>
<blockquote>
<p>这里不会展开介绍 PV 和 PVC，感兴趣的读者可以阅读 <a href="https://draveness.me/kubernetes-volume" target="_blank">详解 Kubernetes Volume 的实现原理</a> 了解 Kubernetes 存储系统的实现原理。</p>
</blockquote>
<h3 id="实现原理-4">实现原理</h3>
<p>与 <a href="https://draveness.me/kubernetes-replicaset" target="_blank">ReplicaSet</a> 和 <a href="https://draveness.me/kubernetes-deployment" target="_blank">Deployment</a> 资源一样，StatefulSet 也使用控制器的方式实现，它主要由 <code>StatefulSetController</code>、<code>StatefulSetControl</code> 和 <code>StatefulPodControl</code> 三个组件协作来完成 StatefulSet 的管理，<code>StatefulSetController</code> 会同时从 <code>PodInformer</code> 和 <code>ReplicaSetInformer</code> 中接受增删改事件并将事件推送到队列中：</p>
<pre><code class="language-mermaid">graph TD
    PI[PodInformer]-. Add/Update/Delete .-&gt;SSC[StatefulSetController]
    RSI[ReplicaSetInformer]-. Add/Update/Delete .-&gt;SSC
    SSC--&gt;worker1
    SSC-. Add StatefulSet .-&gt;queue
    worker1-. Loop .-&gt;worker1
    queue-. Get StatefulSet .-&gt;worker1
    SSC--&gt;worker2
    worker2-. Loop .-&gt;worker2
    queue-. Get StatefulSet .-&gt;worker2
    style worker1 fill:#fffede,stroke:#ebebb7
    style worker2 fill:#fffede,stroke:#ebebb7
</code></pre>
<p>Mermaid</p>
<p>控制器 <code>StatefulSetController</code> 会在 <code>Run</code> 方法中启动多个 Goroutine 协程，这些协程会从队列中获取待处理的 StatefulSet 资源进行同步，接下来我们会先介绍 Kubernetes 同步 StatefulSet 的过程。</p>
<h4 id="同步-2">同步</h4>
<p><code>StatefulSetController</code> 使用 <code>sync</code> 方法同步 StatefulSet 资源，这是同步该资源的唯一入口，下面是这个方法的具体实现：</p>
<pre><code class="language-go">func (ssc *StatefulSetController) sync(key string) error {
	namespace, name, _ := cache.SplitMetaNamespaceKey(key)
	set, _ := ssc.setLister.StatefulSets(namespace).Get(name)

	ssc.adoptOrphanRevisions(set)

	selector, _ := metav1.LabelSelectorAsSelector(set.Spec.Selector)
	pods, _ := ssc.getPodsForStatefulSet(set, selector)

	return ssc.syncStatefulSet(set, pods)
}

func (ssc *StatefulSetController) syncStatefulSet(set *apps.StatefulSet, pods []*v1.Pod) error {
	ssc.control.UpdateStatefulSet(set.DeepCopy(), pods); err != nil
	return nil
}
</code></pre>
<p>Go</p>
<ol>
<li>先重新获取 StatefulSet 对象；</li>
<li>收养集群中与 StatefulSet 有关的孤立控制器版本；</li>
<li>获取当前 StatefulSet 对应的全部 Pod 副本；</li>
<li>调用 <code>syncStatefulSet</code> 方法同步资源；</li>
</ol>
<p><code>syncStatefulSet</code> 方法只是将方法的调用转发到了一个 <code>StatefulSetControlInterface</code> 的实现 <code>defaultStatefulSetControl</code> 上，<code>StatefulSetControlInterface</code> 定义了用与控制 StatefulSet 和 Pod 副本的接口，这里调用的 <code>UpdateStatefulSet</code> 函数执行了一个 StatefulSet 的核心逻辑，它会负责获取 StatefulSet 版本、更新 StatefulSet 以及它的状态和历史：</p>
<pre><code class="language-go">func (ssc *defaultStatefulSetControl) UpdateStatefulSet(set *apps.StatefulSet, pods []*v1.Pod) error {
	revisions, err := ssc.ListRevisions(set)
	history.SortControllerRevisions(revisions)

	currentRevision, updateRevision, collisionCount, err := ssc.getStatefulSetRevisions(set, revisions)

	status, err := ssc.updateStatefulSet(set, currentRevision, updateRevision, collisionCount, pods)

	ssc.updateStatefulSetStatus(set, status)

	return ssc.truncateHistory(set, pods, revisions, currentRevision, updateRevision)
}
</code></pre>
<p>Go</p>
<p>它会使用默认的单调递增策略，按照升序依次创建副本并按照降序删除副本，当出现 Pod 处于不健康的状态时，那么新的 Pod 就不会被创建，<code>StatefulSetController</code> 会等待 Pod 恢复后继续执行下面的逻辑。</p>
<p>上述代码会在获取 StatefulSet 的历史版本之后调用 <code>updateStatefulSet</code> 方法开始更新 StatefulSet，这个将近 300 行的代码会按照执行以下的执行：</p>
<ol>
<li>将当前 StatefulSet 持有的 Pod 副本按照序列号进行分组，超出数量的副本将被分入 <code>condemned</code> 中等待后续的删除操作，这次同步中需要保留的副本将进入 <code>replicas</code> 分组；</li>
<li>对当前的 StatefulSet 进行扩容，让集群达到目标的副本数；</li>
<li>获取副本数组中第一个不健康的 Pod；</li>
<li>根据副本的序列号检查各个副本的状态；
<ul>
<li>如果发现了失败的副本就会进行重启；</li>
<li>如果当前副本没有正常运行就会退出循环，直到当前副本达到正常运行的状态；</li>
</ul>
</li>
<li>按照降序依次删除 <code>condemned</code> 数组中的副本；</li>
<li>按照降序依次更新 <code>replicas</code> 数组中的副本；</li>
</ol>
<pre><code class="language-go">func (ssc *defaultStatefulSetControl) updateStatefulSet(set *apps.StatefulSet, currentRevision *apps.ControllerRevision, updateRevision *apps.ControllerRevision, collisionCount int32, pods []*v1.Pod) (*apps.StatefulSetStatus, error) {
	currentSet, _ := ApplyRevision(set, currentRevision)
	updateSet, _ := ApplyRevision(set, updateRevision)

	status := apps.StatefulSetStatus{}
	// ...

	replicaCount := int(*set.Spec.Replicas)
	replicas := make([]*v1.Pod, replicaCount)
	condemned := make([]*v1.Pod, 0, len(pods))
	unhealthy := 0
	var firstUnhealthyPod *v1.Pod

	for i := range pods {
		if ord := getOrdinal(pods[i]); 0 &lt;= ord &amp;&amp; ord &lt; replicaCount {
			replicas[ord] = pods[i]
		} else if ord &gt;= replicaCount {
			condemned = append(condemned, pods[i])
		}
	}

	sort.Sort(ascendingOrdinal(condemned))

	for ord := 0; ord &lt; replicaCount; ord++ {
		if replicas[ord] == nil {
			replicas[ord] = newVersionedStatefulSetPod(currentSet, updateSet, currentRevision.Name, updateRevision.Name, ord)
		}
	}
</code></pre>
<p>Go</p>
<p>这里通过 StatefulSet 应该持有的副本数对当前的副本进行分组，一部分是需要保证存活的 <code>replicas</code>，另一部分是需要被终止的副本 <code>condemned</code>，如果分组后的 <code>replicas</code> 数量不足，就会通过 <code>newVersionedStatefulSetPod</code> 函数创建新的 Pod，不过这里的 Pod 也只是待创建的模板。</p>
<p><img class="img-zoomable" src="https://img.draveness.me/2019-02-28-kubernetes-statefulset-parititon.png" alt="kubernetes-statefulset-parititon" />
</p>
<p>拿到线上应该存在的 <code>replicas</code> 数组时，我们就可以进行通过 <code>CreateStatefulPod</code> 进行扩容了，每个 Pod 的更新和创建都会等待前面所有 Pod 正常运行，它会调用 <code>isFailed</code>、<code>isCreated</code>、<code>isTerminating</code> 等方法保证每一个 Pod 都正常运行时才会继续处理下一个 Pod，如果使用滚动更新策略，那么会在完成扩容之后才会对当前的 Pod 进行更新：</p>
<pre><code class="language-go">	for i := range replicas {
		if isFailed(replicas[i]) {
			ssc.podControl.DeleteStatefulPod(set, replicas[i])
			replicas[i] = newVersionedStatefulSetPod(i)
		}
		if !isCreated(replicas[i]) {
			ssc.podControl.CreateStatefulPod(set, replicas[i])
			return &amp;status, nil
		}
		if isTerminating(replicas[i]) || !isRunningAndReady(replicas[i]) {
			return &amp;status, nil
		}
		if identityMatches(set, replicas[i]) &amp;&amp; storageMatches(set, replicas[i]) {
			continue
		}
		replica := replicas[i].DeepCopy()
		ssc.podControl.UpdateStatefulPod(updateSet, replica)
	}
</code></pre>
<p>Go</p>
<p>当 <code>StatefulSetController</code> 处理完副本的创建和更新任务之后，就开始删除需要抛弃的节点了，节点的删除也需要确定按照降序依次进行：</p>
<pre><code class="language-go">	for target := len(condemned) - 1; target &gt;= 0; target-- {
		if isTerminating(condemned[target]) {
			return &amp;status, nil
		}
		if !isRunningAndReady(condemned[target]) &amp;&amp; condemned[target] != firstUnhealthyPod {
			return &amp;status, nil
		}

		ssc.podControl.DeleteStatefulPod(set, condemned[target])
		return &amp;status, nil
	}

	updateMin := 0
	if set.Spec.UpdateStrategy.RollingUpdate != nil {
		updateMin = int(*set.Spec.UpdateStrategy.RollingUpdate.Partition)
	}
	for target := len(replicas) - 1; target &gt;= updateMin; target-- {
		if getPodRevision(replicas[target]) != updateRevision.Name &amp;&amp; !isTerminating(replicas[target]) {
			ssc.podControl.DeleteStatefulPod(set, replicas[target])
			return &amp;status, err
		}

		if !isHealthy(replicas[target]) {
			return &amp;status, nil
		}
	}
	return &amp;status, nil
}
</code></pre>
<p>Go</p>
<p>我们首先会删除待抛弃列表中的副本，其次根据滚动更新 <code>RollingUpdate</code> 的配置从高到低依次删除所有 Pod 版本已经过时的节点，所有删除节点的方式都会通过 <code>DeleteStatefulPod</code> 方法进行，该方法会通过客户端的接口直接根据 Pod 名称删除对应的资源。</p>
<h4 id="序列号">序列号</h4>
<p>Pod 的序列号（Ordinal）是其唯一性和顺序性的保证，在创建和删除 StatefulSet 的副本时，我们都需要按照 Pod 的序列号对它们按照顺序操作，副本的创建会按照序列号升序处理，副本的更新和删除会按照序列号降序处理。</p>
<p><img class="img-zoomable" src="https://img.draveness.me/2019-02-28-kubernetes-statefulset-ordinal.png" alt="kubernetes-statefulset-ordina" />
</p>
<p>创建 StatefulSet 中的副本时，就会在 <code>newStatefulSetPod</code> 函数中传入当前 Pod 的 <code>ordinal</code> 信息，该方法会调用 <code>GetPodFromTemplate</code> 获取 <code>StatefulSet</code> 中的 Pod 模板并且初始化 Pod 的 <code>metadata</code> 和引用等配置：</p>
<pre><code class="language-go">func newStatefulSetPod(set *apps.StatefulSet, ordinal int) *v1.Pod {
	pod, _ := controller.GetPodFromTemplate(&amp;set.Spec.Template, set, metav1.NewControllerRef(set, controllerKind))
	pod.Name = getPodName(set, ordinal)
	initIdentity(set, pod)
	updateStorage(set, pod)
	return pod
}

func getPodName(set *apps.StatefulSet, ordinal int) string {
	return fmt.Sprintf(&quot;%s-%d&quot;, set.Name, ordinal)
}
</code></pre>
<p>Go</p>
<p><code>getPodName</code> 函数的实现非常简单，它将 StatefulSet 的名字和传入的序列号通过破折号连接起来组成我们经常见到的 <code>web-0</code>、<code>web-1</code> 等形式的副本名；<code>initIdentity</code> 会更新 Pod 的主机名、资源名、命名空间标签，而 <code>updateStorage</code> 会为待创建的副本设置卷：</p>
<pre><code class="language-go">func updateStorage(set *apps.StatefulSet, pod *v1.Pod) {
	currentVolumes := pod.Spec.Volumes
	claims := getPersistentVolumeClaims(set, pod)
	newVolumes := make([]v1.Volume, 0, len(claims))
	for name, claim := range claims {
		newVolumes = append(newVolumes, v1.Volume{
			Name: name,
			VolumeSource: v1.VolumeSource{
				PersistentVolumeClaim: &amp;v1.PersistentVolumeClaimVolumeSource{
					ClaimName: claim.Name,
					ReadOnly: false,
				},
			},
		})
	}
	for i := range currentVolumes {
		if _, ok := claims[currentVolumes[i].Name]; !ok {
			newVolumes = append(newVolumes, currentVolumes[i])
		}
	}
	pod.Spec.Volumes = newVolumes
}
</code></pre>
<p>Go</p>
<p>设置卷的配置主要来自于 StatefulSet 规格中的 <code>volumeClaimTemplates</code> 模板，所有卷相关的配置信息都会通过该方法传递过来。</p>
<p>Pod 通过当前名字存储自己对应的序列号，在 <code>StatefulSetController</code> 同步时就会从 Pod 的名字中取出序列号并进行排序，随后的各种循环就可以选择使用正序或者倒序的方式依次处理各个节点了。</p>
<h4 id="删除-3">删除</h4>
<p>当我们删除一个 Kubernetes 中的 StatefulSet 资源时，它对应的全部 Pod 副本都会被 <a href="https://draveness.me/kubernetes-garbage-collector" target="_blank">垃圾收集器</a> 自动删除，该收集器在检查到当前 Pod 的 <code>metadata.ownerReferences</code> 已经不再存在时就会删除 Pod 资源，读者可以阅读 <a href="https://draveness.me/kubernetes-garbage-collector" target="_blank">垃圾收集器</a> 了解具体的执行过程和实现原理。</p>
<pre><code class="language-go">$ kubectl delete statefulsets.apps web
statefulset.apps &quot;web&quot; deleted

$ kubectl get pods --watch
NAME    READY   STATUS    RESTARTS   AGE
web-2   1/1   Terminating   0     14h
web-1   1/1   Terminating   0     14h
web-0   1/1   Terminating   0     14h
</code></pre>
<p>Go</p>
<p>我们会发现除了 StatefulSet 和 Pod 之外的任何其他资源都没有被删除，之前创建的 <code>PersistentVolume</code> 和 <code>PersistentVolumeClaim</code> 对象都没有发生任何的变化，这也是 StatefulSet 的行为，它会在服务被删除之后仍然保留其中的状态，也就是数据，这些数据就都存储在 <code>PersistentVolume</code> 中。</p>
<p>如果我们重新创建相同的 StatefulSet，它还会使用之前的 PV 和 PVC 对象，不过也可以选择手动删除所有的 PV 和 PVC 来生成新的存储，这两个对象都属于 Kubernetes 的存储系统，感兴趣的读者可以通过 <a href="https://draveness.me/kubernetes-volume" target="_blank">存储系统</a> 了解 Kubernetes 中 Volume 的设计和实现。</p>
<h3 id="总结-8">总结</h3>
<p>StatefulSet 是 Kubernetes 为了处理有状态服务引入的概念，在有状态服务中，它为无序和短暂的 Pod 引入了顺序性和唯一性，使得 Pod 的创建和删除更容易被掌控和预测，同时加入 PV 和 PVC 对象来存储这些 Pod 的状态，我们可以使用 StatefulSet 实现一些偏存储的有状态系统，例如 Zookeeper、Kafka、MongoDB 等，这些系统大多数都需要持久化的存储数据，防止在服务宕机时发生数据丢失。</p>
<h2 id="kubernetes-daemonset">Kubernetes DaemonSet</h2>
<p>不同的维度解决了集群中的问题 — 如何同时在集群中的所有节点上提供基础服务和守护进程。</p>
<p>我们在这里将介绍 DaemonSet 如何进行状态的同步、Pod 与节点（Node）之间的调度方式和滚动更新的过程以及实现原理。</p>
<h3 id="概述-6">概述</h3>
<p>DaemonSet 可以保证集群中所有的或者部分的节点都能够运行同一份 Pod 副本，每当有新的节点被加入到集群时，Pod 就会在目标的节点上启动，如果节点被从集群中剔除，节点上的 Pod 也会被垃圾收集器清除；DaemonSet 的作用就像是计算机中的守护进程，它能够运行集群存储、日志收集和监控等『守护进程』，这些服务一般是集群中必备的基础服务。</p>
<p>Google Cloud 的 Kubernetes 集群就会在所有的节点上启动 fluentd 和 Prometheus 来收集节点上的日志和监控数据，想要创建用于日志收集的守护进程其实非常简单，我们可以使用如下所示的代码：</p>
<pre><code class="language-yaml">apiVersion: apps/v1
kind: DaemonSet
metadata:
  name: fluentd-elasticsearch
  namespace: kube-system
spec:
  selector:
    matchLabels:
      name: fluentd-elasticsearch
  template:
    metadata:
      labels:
        name: fluentd-elasticsearch
    spec:
      containers:
      - name: fluentd-elasticsearch
        image: k8s.gcr.io/fluentd-elasticsearch:1.20
        volumeMounts:
        - name: varlog
          mountPath: /var/log
        - name: varlibdockercontainers
          mountPath: /var/lib/docker/containers
          readOnly: true
      volumes:
      - name: varlog
        hostPath:
          path: /var/log
      - name: varlibdockercontainers
        hostPath:
          path: /var/lib/docker/containers
</code></pre>
<p>YAML</p>
<p>当我们使用 <code>kubectl apply -f</code> 创建上述的 DaemonSet 时，它会在 Kubernetes 集群的 <code>kube-system</code> 命名空间中创建 DaemonSet 资源并在所有的节点上创建新的 Pod：</p>
<pre><code class="language-go">$ kubectl get daemonsets.apps fluentd-elasticsearch --namespace kube-system
NAME                    DESIRED   CURRENT   READY   UP-TO-DATE   AVAILABLE   NODE SELECTOR   AGE
fluentd-elasticsearch   1         1         1       1            1           &lt;none&gt;          19h

$ kubectl get pods --namespace kube-system --label name=fluentd-elasticsearch
NAME                          READY   STATUS    RESTARTS   AGE
fluentd-elasticsearch-kvtwj   1/1     Running   0          19h
</code></pre>
<p>Go</p>
<p>由于集群中只存在一个 Pod，所以 Kubernetes 只会在该节点上创建一个 Pod，如果我们向当前的集群中增加新的节点时，Kubernetes 就会创建在新节点上创建新的副本，总的来说，我们能够得到以下的拓扑结构：</p>
<p><img class="img-zoomable" src="https://img.draveness.me/2019-03-01-DaemonSet-Topology.png" alt="DaemonSet-Topology" />
</p>
<p>集群中的 Pod 和 Node 一一对应，而 DaemonSet 会管理全部机器上的 Pod 副本，负责对它们进行更新和删除。</p>
<h3 id="实现原理-5">实现原理</h3>
<p>所有的 DaemonSet 都是由控制器负责管理的，与其他的资源一样，用于管理 DaemonSet 的控制器是 <code>DaemonSetsController</code>，该控制器会监听 DaemonSet、ControllerRevision、Pod 和 Node 资源的变动。</p>
<p><img class="img-zoomable" src="https://img.draveness.me/2019-03-01-DaemonSet-FlowChart.png" alt="DaemonSet-FlowChart" />
</p>
<p>大多数的触发事件最终都会将一个待处理的 DaemonSet 资源入栈，下游 <code>DaemonSetsController</code> 持有的多个工作协程就会从队列里面取出资源进行消费和同步。</p>
<h4 id="同步-3">同步</h4>
<p><code>DaemonSetsController</code> 同步 DaemonSet 资源使用的方法就是 <code>syncDaemonSet</code>，这个方法从队列中拿到 DaemonSet 的名字时，会先从集群中获取最新的 DaemonSet 对象并通过 <code>constructHistory</code> 方法查找当前 DaemonSet 全部的历史版本：</p>
<pre><code class="language-go">func (dsc *DaemonSetsController) syncDaemonSet(key string) error {
	namespace, name, _ := cache.SplitMetaNamespaceKey(key)
	ds, _ := dsc.dsLister.DaemonSets(namespace).Get(name)
	dsKey, _ := controller.KeyFunc(ds)

	cur, old, _ := dsc.constructHistory(ds)
	hash := cur.Labels[apps.DefaultDaemonSetUniqueLabelKey]

	dsc.manage(ds, hash)

	switch ds.Spec.UpdateStrategy.Type {
	case apps.OnDeleteDaemonSetStrategyType:
	case apps.RollingUpdateDaemonSetStrategyType:
		dsc.rollingUpdate(ds, hash)
	}

	dsc.cleanupHistory(ds, old)

	return dsc.updateDaemonSetStatus(ds, hash, true)
}
</code></pre>
<p>Go</p>
<p>然后调用的 <code>manage</code> 方法会负责管理 DaemonSet 在节点上 Pod 的调度和运行，<code>rollingUpdate</code> 会负责 DaemonSet 的滚动更新；前者会先找出找出需要运行 Pod 和不需要运行 Pod 的节点，并调用 <code>syncNodes</code> 对这些需要创建和删除的 Pod 进行同步：</p>
<pre><code class="language-go">func (dsc *DaemonSetsController) syncNodes(ds *apps.DaemonSet, podsToDelete, nodesNeedingDaemonPods []string, hash string) error {
	dsKey, _ := controller.KeyFunc(ds)
	generation, err := util.GetTemplateGeneration(ds)
	template := util.CreatePodTemplate(ds.Spec.Template, generation, hash)

	createDiff := len(nodesNeedingDaemonPods)
	createWait := sync.WaitGroup{}
	createWait.Add(createDiff)
	for i := 0; i &lt; createDiff; i++ {
		go func(ix int) {
			defer createWait.Done()

			podTemplate := template.DeepCopy()
			if utilfeature.DefaultFeatureGate.Enabled(features.ScheduleDaemonSetPods) {
				podTemplate.Spec.Affinity = util.ReplaceDaemonSetPodNodeNameNodeAffinity(podTemplate.Spec.Affinity, nodesNeedingDaemonPods[ix])
				dsc.podControl.CreatePodsWithControllerRef(ds.Namespace, podTemplate, ds, metav1.NewControllerRef(ds, controllerKind))
			} else {
				podTemplate.Spec.SchedulerName = &quot;kubernetes.io/daemonset-controller&quot;
				dsc.podControl.CreatePodsOnNode(nodesNeedingDaemonPods[ix], ds.Namespace, podTemplate, ds, metav1.NewControllerRef(ds, controllerKind))
			}

		}(i)
	}
	createWait.Wait()
</code></pre>
<p>Go</p>
<p>获取了 DaemonSet 中的模板之之后，就会开始并行地为节点创建 Pod 副本，并发创建的过程使用了 for 循环、Goroutine 和 <code>WaitGroup</code> 保证程序运行的正确，然而这里使用了特性开关来对调度新 Pod 的方式进行了控制，我们会在接下来的调度一节介绍 DaemonSet 调度方式的变迁和具体的执行过程。</p>
<p>当 Kubernetes 创建了需要创建的 Pod 之后，就需要删除所有节点上不必要的 Pod 了，这里使用同样地方式并发地对 Pod 进行删除：</p>
<pre><code class="language-go">	deleteDiff := len(podsToDelete)
	deleteWait := sync.WaitGroup{}
	deleteWait.Add(deleteDiff)
	for i := 0; i &lt; deleteDiff; i++ {
		go func(ix int) {
			defer deleteWait.Done()
			dsc.podControl.DeletePod(ds.Namespace, podsToDelete[ix], ds)
		}(i)
	}
	deleteWait.Wait()

	return nil
}
</code></pre>
<p>Go</p>
<p>到了这里我们就完成了节点上 Pod 的调度和运行，为一些节点创建 Pod 副本的同时删除另一部分节点上的副本，<code>manage</code> 方法执行完成之后就会调用 <code>rollingUpdate</code> 方法对 DaemonSet 的节点进行滚动更新并对控制器版本进行清理并更新 DaemonSet 的状态，文章后面的部分会介绍滚动更新的过程和实现。</p>
<h4 id="调度">调度</h4>
<p>在早期的 Kubernetes 版本中，所有 DaemonSet Pod 的创建都是由 <code>DaemonSetsController</code> 负责的，而其他的资源都是由 kube-scheduler 进行调度，这就导致了如下的一些问题：</p>
<ol>
<li><code>DaemonSetsController</code> 没有办法在节点资源变更时收到通知 (<a href="https://github.com/kubernetes/kubernetes/issues/46935" target="_blank">#46935</a>, <a href="https://github.com/kubernetes/kubernetes/issues/58868" target="_blank">#58868</a>)；</li>
<li><code>DaemonSetsController</code> 没有办法遵循 Pod 的亲和性和反亲和性设置 (<a href="https://github.com/kubernetes/kubernetes/issues/29276" target="_blank">#29276</a>)；</li>
<li><code>DaemonSetsController</code> 可能需要二次实现 Pod 调度的重要逻辑，造成了重复的代码逻辑 (<a href="https://github.com/kubernetes/kubernetes/issues/42028" target="_blank">#42028</a>)；</li>
<li>多个组件负责调度会导致 Debug 和抢占等功能的实现非常困难；</li>
</ol>
<p>设计文档 <a href="https://github.com/kubernetes/community/blob/master/contributors/design-proposals/scheduling/schedule-DS-pod-by-scheduler.md" target="_blank">Schedule DaemonSet Pods by default scheduler, not DaemonSet controller</a> 中包含了使用 <code>DaemonSetsController</code> 调度时遇到的问题以及新设计给出的解决方案。</p>
<p>如果我们选择使用过去的调度方式，<code>DeamonSetsController</code> 就会负责在节点上创建 Pod，通过这种方式创建的 Pod 的 <code>schedulerName</code> 都会被设置成 <code>kubernetes.io/daemonset-controller</code>，但是在默认情况下这个字段一般为 <code>default-scheduler</code>，也就是使用 Kubernetes 默认的调度器 kube-scheduler 进行调度：</p>
<pre><code class="language-go">func (dsc *DaemonSetsController) syncNodes(ds *apps.DaemonSet, podsToDelete, nodesNeedingDaemonPods []string, hash string) error {
    // ...
	for i := 0; i &lt; createDiff; i++ {
		go func(ix int) {
			podTemplate := template.DeepCopy()
			if utilfeature.DefaultFeatureGate.Enabled(features.ScheduleDaemonSetPods) {
                // ...
			} else {
				podTemplate.Spec.SchedulerName = &quot;kubernetes.io/daemonset-controller&quot;
				dsc.podControl.CreatePodsOnNode(nodesNeedingDaemonPods[ix], ds.Namespace, podTemplate, ds, metav1.NewControllerRef(ds, controllerKind))
			}

		}(i)
	}
    
    // ...
}
</code></pre>
<p>Go</p>
<p><code>DaemonSetsController</code> 在调度 Pod 时都会使用 <code>CreatePodsOnNode</code> 方法，这个方法的实现非常简单，它会先对 Pod 模板进行验证，随后调用 <code>createPods</code> 方法通过 Kubernetes 提供的 API 创建新的副本：</p>
<pre><code class="language-go">func (r RealPodControl) CreatePodsWithControllerRef(namespace string, template *v1.PodTemplateSpec, controllerObject runtime.Object, controllerRef *metav1.OwnerReference) error {
	if err := validateControllerRef(controllerRef); err != nil {
		return err
	}
	return r.createPods(&quot;&quot;, namespace, template, controllerObject, controllerRef)
}
</code></pre>
<p>Go</p>
<p><code>DaemonSetsController</code> 通过节点选择器和调度器的谓词对节点进行过滤，<code>createPods</code> 会直接为当前的 Pod 设置 <code>spec.NodeName</code> 属性，最后得到的 Pod 就会被目标节点上的 kubelet 创建。</p>
<p>除了这种使用 <code>DaemonSetsController</code> 管理和调度 DaemonSet 的方法之外，我们还可以使用 Kubernetes 默认的方式 kube-scheduler 创建新的 Pod 副本：</p>
<pre><code class="language-go">func (dsc *DaemonSetsController) syncNodes(ds *apps.DaemonSet, podsToDelete, nodesNeedingDaemonPods []string, hash string) error {
    // ...
	for i := 0; i &lt; createDiff; i++ {
		go func(ix int) {
			podTemplate := template.DeepCopy()
			if utilfeature.DefaultFeatureGate.Enabled(features.ScheduleDaemonSetPods) {
				podTemplate.Spec.Affinity = util.ReplaceDaemonSetPodNodeNameNodeAffinity(podTemplate.Spec.Affinity, nodesNeedingDaemonPods[ix])
				dsc.podControl.CreatePodsWithControllerRef(ds.Namespace, podTemplate, ds, metav1.NewControllerRef(ds, controllerKind))
			} else {
                // ...
			}

		}(i)
	}
    
    // ...
}
</code></pre>
<p>Go</p>
<p>这种情况会使用 NodeAffinity 特性来避免发生在 <code>DaemonSetsController</code> 中的调度：</p>
<ol>
<li>
<p><code>DaemonSetsController</code> 会在 <code>podsShouldBeOnNode</code> 方法中根据节点选择器过滤所有的节点；</p>
</li>
<li>
<p>对于每一个节点，控制器都会创建一个遵循以下节点亲和的 Pod；</p>
<pre><code class="language-yaml">nodeAffinity:
  requiredDuringSchedulingIgnoredDuringExecution:
  - nodeSelectorTerms:
      matchExpressions:
      - key: kubernetes.io/hostname
        operator: in
        values:
        - dest_hostname
</code></pre>
<p>YAML</p>
</li>
<li>
<p>当节点进行同步时，DaemonSetsController 会根据节点亲和的设置来验证节点和 Pod 的关系；</p>
</li>
<li>
<p>如果调度的谓词失败了，DaemonSet 持有的 Pod 就会保持在 Pending 的状态，所以可以通过修改 Pod 的优先级和抢占保证集群在高负载下也能正常运行 DaemonSet 的副本；</p>
</li>
</ol>
<p>Pod 的优先级和抢占功能在 Kubernetes 1.8 版本引入，1.11 时转变成 beta 版本，在目前最新的 1.13 中依然是 beta 版本，感兴趣的读者可以阅读 <a href="https://kubernetes.io/docs/concepts/configuration/pod-priority-preemption/" target="_blank">Pod Priority and Preemption</a> 文档了解相关的内容。</p>
<h4 id="滚动更新-1">滚动更新</h4>
<p><code>DaemonSetsController</code> 对滚动更新的实现其实比较简单，它其实就是根据 DaemonSet 规格中的配置，删除集群中的 Pod 并保证同时不可用的副本数不会超过 <code>spec.updateStrategy.rollingUpdate.maxUnavailable</code>，这个参数也是 DaemonSet 滚动更新可以配置的唯一参数：</p>
<pre><code class="language-go">func (dsc *DaemonSetsController) rollingUpdate(ds *apps.DaemonSet, hash string) error {
	nodeToDaemonPods, err := dsc.getNodesToDaemonPods(ds)

	_, oldPods := dsc.getAllDaemonSetPods(ds, nodeToDaemonPods, hash)
	maxUnavailable, numUnavailable, err := dsc.getUnavailableNumbers(ds, nodeToDaemonPods)
	oldAvailablePods, oldUnavailablePods := util.SplitByAvailablePods(ds.Spec.MinReadySeconds, oldPods)

	var oldPodsToDelete []string
	for _, pod := range oldUnavailablePods {
		if pod.DeletionTimestamp != nil {
			continue
		}
		oldPodsToDelete = append(oldPodsToDelete, pod.Name)
	}

	for _, pod := range oldAvailablePods {
		if numUnavailable &gt;= maxUnavailable {
			break
		}
		oldPodsToDelete = append(oldPodsToDelete, pod.Name)
		numUnavailable++
	}
	return dsc.syncNodes(ds, oldPodsToDelete, []string{}, hash)
}
</code></pre>
<p>Go</p>
<p>删除 Pod 的顺序其实也非常简单并且符合直觉，上述代码会将不可用的 Pod 先加入到待删除的数组中，随后将历史版本的可用 Pod 加入待删除数组 <code>oldPodsToDelete</code>，最后调用 <code>syncNodes</code> 完成对副本的删除。</p>
<h4 id="删除-4">删除</h4>
<p>与 <a href="https://draveness.me/kubernetes-deployment" target="_blank">Deployment</a>、<a href="https://draveness.me/kubernetes-replicaset" target="_blank">ReplicaSet</a> 和 <a href="https://draveness.me/kubernetes-statefulset" target="_blank">StatefulSet</a> 一样，DaemonSet 的删除也会导致它持有的 Pod 的删除，如果我们使用如下的命令删除该对象，我们能观察到如下的现象：</p>
<pre><code class="language-bash">$ kubectl delete daemonsets.apps fluentd-elasticsearch --namespace kube-system
daemonset.apps &quot;fluentd-elasticsearch&quot; deleted

$ kubectl get pods --watch --namespace kube-system
fluentd-elasticsearch-wvffx   1/1   Terminating   0     14s
</code></pre>
<p>Bash</p>
<p>这部分的工作就都是由 Kubernetes 中的垃圾收集器完成的，读者可以阅读 <a href="https://draveness.me/kubernetes-garbage-collector" target="_blank">垃圾收集器</a> 一文了解集群中的不同对象是如何进行关联的以及在删除单一对象时如何触发级联删除的原理。</p>
<h3 id="总结-9">总结</h3>
<p>DaemonSet 其实就是 Kubernetes 中的守护进程，它会在每一个节点上创建能够提供服务的副本，很多云服务商都会使用 DaemonSet 在所有的节点上内置一些用于提供日志收集、统计分析和安全策略的服务。</p>
<p>在研究 DaemonSet 的调度策略的过程中，我们其实能够通过一些历史的 issue 和 PR 了解到 DaemonSet 调度策略改动的原因，也能让我们对于 Kubernetes 的演进过程和设计决策有一个比较清楚的认识。</p>
<h2 id="kubernetes-job--cronjob">Kubernetes Job &amp; CronJob</h2>
<p>之前介绍了 Kubernetes 中用于长期提供服务的 <a href="https://draveness.me/kubernetes-replicaset" target="_blank">ReplicaSet</a>、<a href="https://draveness.me/kubernetes-deployment" target="_blank">Deployment</a>、<a href="https://draveness.me/kubernetes-statefulset" target="_blank">StatefulSet</a> 和 <a href="https://draveness.me/kubernetes-daemonset" target="_blank">DaemonSet</a> 等资源，但是作为一个容器编排引擎，任务和定时任务的支持是一个必须要支持的功能。</p>
<p>Kubernetes 中使用 Job 和 CronJob 两个资源分别提供了一次性任务和定时任务的特性，这两种对象也使用控制器模型来实现资源的管理，我们在这篇文章种就会介绍它们的实现原理。</p>
<h3 id="概述-7">概述</h3>
<p>Kubernetes 中的 Job 可以创建并且保证一定数量 Pod 的成功停止，当 Job 持有的一个 Pod 对象成功完成任务之后，Job 就会记录这一次 Pod 的成功运行；当一定数量的Pod 的任务执行结束之后，当前的 Job 就会将它自己的状态标记成结束。</p>
<p><img class="img-zoomable" src="https://img.draveness.me/2019-03-05-Job-Topology.png" alt="Job-Topology" />
</p>
<p>上述图片中展示了一个 <code>spec.completions=3</code> 的任务的状态随着 Pod 的成功执行而更新和迁移状态的过程，从图中我们能比较清楚的看到 Job 和 Pod 之间的关系，假设我们有一个用于计算圆周率的如下任务：</p>
<pre><code class="language-yaml">apiVersion: batch/v1
kind: Job
metadata:
  name: pi
spec:
  completions: 10
  parallelism: 5
  template:
    spec:
      containers:
      - name: pi
        image: perl
        command: [&quot;perl&quot;,  &quot;-Mbignum=bpi&quot;, &quot;-wle&quot;, &quot;print bpi(2000)&quot;]
      restartPolicy: Never
  backoffLimit: 4
</code></pre>
<p>YAML</p>
<p>当我们在 Kubernetes 中创建上述任务时，使用 <code>kubectl</code> 能够观测到以下的信息：</p>
<pre><code class="language-bash">$ k apply -f job.yaml
job.batch/pi created

$ kubectl get job --watch
NAME  COMPLETIONS   DURATION   AGE
pi    0/10          1s         1s
pi    1/10          36s        36s
pi    2/10          46s        46s
pi    3/10          54s        54s
pi    4/10          60s        60s
pi    5/10          65s        65s
pi    6/10          90s        90s
pi    7/10          99s        99s
pi    8/10          104s       104s
pi    9/10          107s       107s
pi    10/10         109s       109s
</code></pre>
<p>Bash</p>
<p>由于任务 <code>pi</code> 在配置时指定了 <code>spec.completions=10</code>，所以当前的任务需要等待 10 个 Pod 的成功执行，另一个比较重要的 <code>spec.parallelism=5</code> 表示最多有多少个并发执行的任务，如果 <code>spec.parallelism=1</code> 那么所有的任务都会依次顺序执行，只有前一个任务执行成功时，后一个任务才会开始工作。</p>
<p><img class="img-zoomable" src="https://img.draveness.me/2019-03-05-CronJob-Topology.png" alt="CronJob-Topology" />
</p>
<p>每一个 Job 对象都会持有一个或者多个 Pod，而每一个 CronJob 就会持有多个 Job 对象，CronJob 能够按照时间对任务进行调度，它与 <a href="https://en.wikipedia.org/wiki/Cron" target="_blank">crontab</a> 非常相似，我们可以使用 Cron 格式快速指定任务的调度时间：</p>
<pre><code class="language-yaml">apiVersion: batch/v1beta1
kind: CronJob
metadata:
  name: pi
spec:
  schedule: &quot;*/1 * * * *&quot;
  jobTemplate:
    spec:
      completions: 2
      parallelism: 1
      template:
        spec:
          containers:
          - name: pi
            image: perl
            command: [&quot;perl&quot;,  &quot;-Mbignum=bpi&quot;, &quot;-wle&quot;, &quot;print bpi(2000)&quot;]
          restartPolicy: OnFailure
</code></pre>
<p>YAML</p>
<p>上述的 CronJob 对象被创建之后，每分钟都会创建一个新的 Job 对象，所有的 CronJob 创建的任务都会带有调度时的时间戳，例如：pi-1551660600 和 pi-1551660660 两个任务：</p>
<pre><code class="language-bash">$ k get cronjob --watch
NAME   SCHEDULE      SUSPEND   ACTIVE   LAST SCHEDULE   AGE
pi    */1 * * * *    False     0        &lt;none&gt;          3s
pi    */1 * * * *    False     1        1s              7s

$ k get job --watch
NAME            COMPLETIONS   DURATION   AGE
pi-1551660600   0/3   0s    0s
pi-1551660600   1/3   16s   16s
pi-1551660600   2/3   31s   31s
pi-1551660600   3/3   44s   44s
pi-1551660660   0/3         1s
pi-1551660660   0/3   1s    1s
pi-1551660660   1/3   14s   14s
pi-1551660660   2/3   28s   28s
pi-1551660660   3/3   42s   43s
</code></pre>
<p>Bash</p>
<p>CronJob 中保存的任务其实是有上限的，<code>spec.successfulJobsHistoryLimit</code> 和 <code>spec.failedJobsHistoryLimit</code> 分别记录了能够保存的成功或者失败的任务上限，超过这个上限的任务都会被删除，默认情况下这两个属性分别为 <code>spec.successfulJobsHistoryLimit=3</code> 和 <code>spec.failedJobsHistoryLimit=1</code>。</p>
<h3 id="任务">任务</h3>
<p>Job 遵循 Kubernetes 的控制器模式进行设计，在发生需要监听的事件时，Informer 就会调用控制器中的回调将需要处理的资源 Job 加入队列，而控制器持有的工作协程就会处理这些任务。</p>
<p><img class="img-zoomable" src="https://img.draveness.me/2019-03-05-Job-FlowChart.png" alt="Job-FlowChart" />
</p>
<p>用于处理 Job 资源的 <code>JobController</code> 控制器会监听 Pod 和 Job 这两个资源的变更事件，而资源的同步还是需要运行 <code>syncJob</code> 方法。</p>
<h4 id="同步-4">同步</h4>
<p><code>syncJob</code> 是 <code>JobController</code> 中主要用于同步 Job 资源的方法，这个方法的主要作用就是对资源进行同步，它的大体架构就是先获取一些当前同步的基本信息，然后调用 <code>manageJob</code> 方法管理 Job 对应的 Pod 对象，最后计算出处于 <code>active</code>、<code>failed</code> 和 <code>succeed</code> 三种状态的 Pod 数量并更新 Job 的状态：</p>
<pre><code class="language-go">func (jm *JobController) syncJob(key string) (bool, error) {
	ns, name, _ := cache.SplitMetaNamespaceKey(key)
	sharedJob, _ := jm.jobLister.Jobs(ns).Get(name)
	job := *sharedJob

	jobNeedsSync := jm.expectations.SatisfiedExpectations(key)

	pods, _ := jm.getPodsForJob(&amp;job)

	activePods := controller.FilterActivePods(pods)
	active := int32(len(activePods))
	succeeded, failed := getStatus(pods)
	conditions := len(job.Status.Conditions)
</code></pre>
<p>Go</p>
<p>这一部分代码会从 apiserver 中该名字对应的 Job 对象，然后获取 Job 对应的 Pod 数组并根据计算不同状态 Pod 的数量，为之后状态的更新和比对做准备。</p>
<pre><code class="language-go">	var manageJobErr error
	if jobNeedsSync &amp;&amp; job.DeletionTimestamp == nil {
		active, manageJobErr = jm.manageJob(activePods, succeeded, &amp;job)
	}
</code></pre>
<p>Go</p>
<p>准备工作完成之后，会调用 <code>manageJob</code> 方法，该方法主要负责管理 Job 持有的一系列 Pod 的运行，它最终会返回目前集群中当前 Job 对应的活跃任务的数量。</p>
<pre><code class="language-go">	completions := succeeded
	complete := false
	if job.Spec.Completions == nil {
		if succeeded &gt; 0 &amp;&amp; active == 0 {
			complete = true
		}
	} else {
		if completions &gt;= *job.Spec.Completions {
			complete = true
		}
	}
	if complete {
		job.Status.Conditions = append(job.Status.Conditions, newCondition(batch.JobComplete, &quot;&quot;, &quot;&quot;))
		now := metav1.Now()
		job.Status.CompletionTime = &amp;now
	}

	if job.Status.Active != active || job.Status.Succeeded != succeeded || job.Status.Failed != failed || len(job.Status.Conditions) != conditions {
		job.Status.Active = active
		job.Status.Succeeded = succeeded
		job.Status.Failed = failed

		jm.updateHandler(&amp;job)
	}

	return forget, manageJobErr
}
</code></pre>
<p>Go</p>
<p>最后的这段代码会将 Job 规格中设置的 <code>spec.completions</code> 和已经完成的任务数量进行比对，确认当前的 Job 是否已经结束运行，如果任务已经结束运行就会更新当前 Job 的完成时间，同时当 <code>JobController</code> 发现有一些状态没有正确同步时，也会调用 <code>updateHandler</code> 更新资源的状态。</p>
<h4 id="并行执行">并行执行</h4>
<p>Pod 的创建和删除都是由 <code>manageJob</code> 这个方法负责的，这个方法根据 Job 的 <code>spec.parallelism</code> 配置对目前集群中的节点进行创建和删除。</p>
<p><img class="img-zoomable" src="https://img.draveness.me/2019-03-05-Job-Kill-More-Pods.png" alt="Job-Kill-More-Pods" />
</p>
<p>如果当前正在执行的活跃节点数量超过了 <code>spec.parallelism</code>，那么就会按照创建的升删除多余的任务，删除任务时会使用 <code>DeletePod</code> 方法：</p>
<pre><code class="language-go">func (jm *JobController) manageJob(activePods []*v1.Pod, succeeded int32, job *batch.Job) (int32, error) {
	active := int32(len(activePods))
	parallelism := *job.Spec.Parallelism

	if active &gt; parallelism {
		diff := active - parallelism
		sort.Sort(controller.ActivePods(activePods))

		active -= diff
		wait := sync.WaitGroup{}
		wait.Add(int(diff))
		for i := int32(0); i &lt; diff; i++ {
			go func(ix int32) {
				defer wait.Done()
				jm.podControl.DeletePod(job.Namespace, activePods[ix].Name, job)
			}(i)
		}
		wait.Wait()
</code></pre>
<p>Go</p>
<p>当正在活跃的节点数量小于 <code>spec.parallelism</code> 时，我们就会根据当前未完成的任务数和并行度计算出最大可以处于活跃的 Pod 个数 <code>wantActive</code> 以及与当前的活跃 Pod 数相差的 <code>diff</code>：</p>
<pre><code class="language-go">	} else if active &lt; parallelism {
		wantActive := int32(0)
		if job.Spec.Completions == nil {
			if succeeded &gt; 0 {
				wantActive = active
			} else {
				wantActive = parallelism
			}
		} else {
			wantActive = *job.Spec.Completions - succeeded
			if wantActive &gt; parallelism {
				wantActive = parallelism
			}
		}
		diff := wantActive - active
		if diff &lt; 0 {
			diff = 0
		}

		active += diff
</code></pre>
<p>Go</p>
<p>在方法的最后就会以批量的方式并行创建 Pod，所有 Pod 的创建都是通过 <code>CreatePodsWithControllerRef</code> 方法在 Goroutine 中执行的。</p>
<p><img class="img-zoomable" src="https://img.draveness.me/2019-03-05-Job-Create-More-Pods.png" alt="Job-Create-More-Pods" />
</p>
<p>这里会使用 <code>WaitGroup</code> 等待每个 Batch 中创建的结果返回才会执行下一个 Batch，Batch 的大小是从 1 开始指数增加的，以冷启动的方式避免首次创建的任务过多造成失败：</p>
<pre><code class="language-go">		wait := sync.WaitGroup{}
		for batchSize := int32(integer.IntMin(int(diff), controller.SlowStartInitialBatchSize)); diff &gt; 0; batchSize = integer.Int32Min(2*batchSize, diff) {
			wait.Add(int(batchSize))
			for i := int32(0); i &lt; batchSize; i++ {
				go func() {
					defer wait.Done()
					jm.podControl.CreatePodsWithControllerRef(job.Namespace, &amp;job.Spec.Template, job, metav1.NewControllerRef(job, controllerKind))
				}()
			}
			wait.Wait()
			diff -= batchSize
		}
	}

	return active, nil
}
</code></pre>
<p>Go</p>
<p>通过对 <code>manageJob</code> 的分析，我们其实能够看出这个方法就是根据规格中的配置对 Pod 进行管理，它在较多并行时删除 Pod，较少并行时创建 Pod，也算是一个简单的资源利用和调度机制，代码非常直白并且容易理解，不需要花太大的篇幅。</p>
<h3 id="定时任务">定时任务</h3>
<p>用于管理 CronJob 资源的 <code>CronJobController</code> 虽然也使用了控制器模式，但是它的实现与其他的控制器不太一样，他没有从 Informer 中接受其他消息变动的通知，而是直接访问 apiserver 中的数据：</p>
<p><img class="img-zoomable" src="https://img.draveness.me/2019-03-05-CronJobController-FlowChart.png" alt="CronJobController-FlowChart" />
</p>
<p>从 apiserver 中获取了 Job 和 CronJob 的信息之后就会调用 <code>JobControl</code> 向 apiserver 发起请求创建新的 Job 资源，这一个过程都是由 <code>CronJobController</code> 的 <code>syncAll</code> 方法驱动的，我们接下来就来介绍这一方法的实现。</p>
<h4 id="同步-5">同步</h4>
<p><code>syncAll</code> 方法会从 apiserver 中取出所有的 Job 和 CronJob 对象，然后通过 <code>groupJobsByParent</code> 将任务按照 <code>spec.ownerReferences</code> 进行分类并遍历去同步所有的 CronJob：</p>
<pre><code class="language-go">func (jm *CronJobController) syncAll() {
	jl, _ := jm.kubeClient.BatchV1().Jobs(metav1.NamespaceAll).List(metav1.ListOptions{})
	js := jl.Items

	sjl, _ := jm.kubeClient.BatchV1beta1().CronJobs(metav1.NamespaceAll).List(metav1.ListOptions{})
	sjs := sjl.Items

	jobsBySj := groupJobsByParent(js)

	for _, sj := range sjs {
		syncOne(&amp;sj, jobsBySj[sj.UID], time.Now(), jm.jobControl, jm.sjControl, jm.recorder)
		cleanupFinishedJobs(&amp;sj, jobsBySj[sj.UID], jm.jobControl, jm.sjControl, jm.recorder)
	}
}
</code></pre>
<p>Go</p>
<p><code>syncOne</code> 就是用于同步单个 CronJob 对象的方法，这个方法会首先遍历全部的 Job 对象，只保留正在运行的活跃对象并更新 CronJob 的状态：</p>
<pre><code class="language-go">func syncOne(sj *batchv1beta1.CronJob, js []batchv1.Job, now time.Time, jc jobControlInterface, sjc sjControlInterface, recorder record.EventRecorder) {
	childrenJobs := make(map[types.UID]bool)
	for _, j := range js {
		childrenJobs[j.ObjectMeta.UID] = true
		found := inActiveList(*sj, j.ObjectMeta.UID)
		if found &amp;&amp; IsJobFinished(&amp;j) {
			deleteFromActiveList(sj, j.ObjectMeta.UID)
		}
	}

	for _, j := range sj.Status.Active {
		if found := childrenJobs[j.UID]; !found {
			deleteFromActiveList(sj, j.UID)
		}
	}

	updatedSJ, _ := sjc.UpdateStatus(sj)
	*sj = *updatedSJ
</code></pre>
<p>Go</p>
<p>随后的 <code>getRecentUnmetScheduleTimes</code> 方法会根据 CronJob 的调度配置 <code>spec.schedule</code> 和上一次执行任务的时间计算出我们缺失的任务次数。</p>
<pre><code class="language-go">	times, _ := getRecentUnmetScheduleTimes(*sj, now)
	if len(times) == 0 {
		return
	}

	scheduledTime := times[len(times)-1]
	if sj.Spec.ConcurrencyPolicy == batchv1beta1.ForbidConcurrent &amp;&amp; len(sj.Status.Active) &gt; 0 {
		return
	}
	if sj.Spec.ConcurrencyPolicy == batchv1beta1.ReplaceConcurrent {
		for _, j := range sj.Status.Active {
			job, _ := jc.GetJob(j.Namespace, j.Name)
			if !deleteJob(sj, job, jc, recorder) {
				return
			}
		}
	}
</code></pre>
<p>Go</p>
<p>如果现在需要调度新的任务，但是当前已经存在活跃的任务，就会根据并发策略的配置执行不同的操作：</p>
<ol>
<li>使用 <code>ForbidConcurrent</code> 策略跳过这一次任务的调度直接返回；</li>
<li>使用 <code>ReplaceConcurrent</code> 策略获取并删除全部活跃的任务，通过创建新的 Pod 替换这些正在执行的活跃 Pod；</li>
</ol>
<pre><code class="language-go">	jobReq, _ := getJobFromTemplate(sj, scheduledTime)
	jobResp, _ := jc.CreateJob(sj.Namespace, jobReq)

	ref,_ := getRef(jobResp)
	sj.Status.Active = append(sj.Status.Active, *ref)
	sj.Status.LastScheduleTime = &amp;metav1.Time{Time: scheduledTime}
	sjc.UpdateStatus(sj)

	return
}
</code></pre>
<p>Go</p>
<p>在方法的最后，它会从 CronJob 的 <code>spec.jobTemplate</code> 中拿到创建 Job 使用的模板并调用 <code>JobControl</code> 的 <code>CreateJob</code> 向 apiserver 发起创建任务的 HTTP 请求，接下来的操作就都是由 <code>JobController</code> 负责了。</p>
<h3 id="总结-10">总结</h3>
<p>Job 作为 Kubernetes 中用于处理任务的资源，与其他的资源没有太多的区别，它也使用 Kubernetes 中常见的控制器模式，监听 Informer 中的事件并运行 <code>syncHandler</code> 同步任务</p>
<p>而 CronJob 由于其功能的特殊性，每隔 10s 会从 apiserver 中取出资源并进行检查是否应该触发调度创建新的资源，需要注意的是 CronJob 并不能保证在准确的目标时间执行，执行会有一定程度的滞后。</p>
<p>两个控制器的实现都比较清晰，只是边界条件比较多，分析其实现原理时一定要多注意。</p>
<h2 id="如何为-kubernetes-定制特性">如何为 Kubernetes 定制特性</h2>
<p>Kubernetes 是非常复杂的集群编排系统，然而哪怕包含丰富的功能和特性，因为容器的调度和管理本身就有较高的复杂性，所以它无法满足所有场景下的需求。虽然 Kubernetes 能够解决大多数场景中的常见问题，但是为了实现更加灵活的策略，我们需要使用 Kubernetes 提供的扩展能力实现特定目的。</p>
<p>每个项目在不同的周期会着眼于不同的特性，我们可以将项目的演进过程简单分成三个不同的阶段：</p>
<ul>
<li><strong>最小可用</strong>：项目在早期更倾向于解决通用的、常见的问题，给出开箱即用的解决方案以吸引用户，这时代码库的规模还相对比较小，提供的功能较为有限，能够覆盖领域内 90% 的场景；</li>
<li><strong>功能完善</strong>：随着项目得到更多的使用者和支持者，社区会不断实现相对重要的功能，社区治理和自动化工具也逐渐变得完善，能够解决覆盖内 95% 的场景；</li>
<li><strong>扩展能力</strong>：因为项目的社区变得完善，代码库变得逐渐庞大，项目的每个变动都会影响下游的开发者，任何新功能的加入都需要社区成员的讨论和审批，这时社区会选择增强项目的扩展性，让使用者能够为自己的场景定制需求，能够解决覆盖内 99% 的场景；</li>
</ul>
<p><img class="img-zoomable" src="https://img.draveness.me/evolving-of-open-source-project-2021-03-24-16165170057421.png" alt="evolving-of-open-source-project" />
</p>
<p><strong>图 1 - 开源项目的演进</strong></p>
<p>从 90%、95% 到 99%，每个步骤都需要社区成员花费很多精力，但是哪怕提供了较好的扩展性也无法解决领域内的全部问题，在一些极端场景下仍然需要维护自己的分支或者另起炉灶满足业务上的需求。</p>
<p>然而无论是维护自己的分支，还是另起炉灶都会带来较高的开发和维护成本，这需要结合实际需求进行抉择。但是能够利用项目提供的配置能力和扩展能力就可以明显地降低定制化的开发成本，而我们今天要梳理的就是 Kubernetes 的可扩展性。</p>
<h3 id="扩展接口">扩展接口</h3>
<p>API 服务器是 Kubernetes 中的核心组件，它承担着集群中资源读写的重任，虽然社区提供的资源和接口可以满足大多数的日常需求，但是我们仍然会有一些场景需要扩展 API 服务器的能力，这一节简单介绍几个扩展该服务的方法。</p>
<h4 id="自定义资源">自定义资源</h4>
<p>自定义资源（Custom Resource Definition、CRD）应该是 Kubernetes 最常见的扩展方式<a href="https://draveness.me/cloud-native-kubernetes-extension/#fn:1" target="_blank">1</a>，它是扩展 Kubernetes API 的方式之一。Kubernetes 的 API 就是我们向集群提交的 YAML，系统中的各个组件会根据提交的 YAML 启动应用、创建网络路由规则以及运行工作负载。</p>
<pre><code class="language-yaml">apiVersion: v1
kind: Pod
metadata:
  name: static-web
  labels:
    role: myrole
spec:
  containers:
    - name: web
      image: nginx
      ports:
        - name: web
          containerPort: 80
          protocol: TCP
</code></pre>
<p>YAML</p>
<p><code>Pod</code>、<code>Service</code> 以及 <code>Ingress</code> 都是 Kubernetes 对外暴露的接口，当我们在集群中提交上述 YAML 时，Kubernetes 中的控制器会根据配置创建满足条件的容器。</p>
<pre><code class="language-yaml">apiVersion: apiextensions.k8s.io/v1
kind: CustomResourceDefinition
metadata:
  name: crontabs.stable.example.com
spec:
  group: stable.example.com
  versions:
    - name: v1
      served: true
      storage: true
      schema:
        openAPIV3Schema:
          type: object
          properties:
            spec:
              type: object
              properties:
                cronSpec:
                  type: string
                image:
                  type: string
                replicas:
                  type: integer
  scope: Namespaced
  names:
    plural: crontabs
    singular: crontab
    kind: CronTab
    shortNames:
    - ct
</code></pre>
<p>YAML</p>
<p>除了这些系统内置的 API 之外，想要实现定制的接口就需要使用 CRD，然而 CRD 仅仅是实现自定义资源的冰山一角，因为它只定义了资源中的字段，我们还需要遵循 Kubernetes 的控制器模式，实现消费 CRD 的 Operator，通过组合 Kubernetes 提供的资源实现更复杂、更高级的功能。</p>
<p><img class="img-zoomable" src="https://img.draveness.me/modular-kubernetes-api-2021-03-24-16165170057450.png" alt="modular-kubernetes-api" />
</p>
<p><strong>图 2 - Kubernetes API 模块化设计</strong></p>
<p>如上图所示，Kubernetes 中的控制器等组件会消费 <code>Deployment</code>、<code>StatefulSet</code> 等资源，而用户自定义的 CRD 会由自己实现的控制器消费，这种设计极大地降低了系统之间各个模块的耦合，让不同模块可以无缝协作。</p>
<p>当我们想要让 Kubernetes 集群提供更加复杂的功能时，选择 CRD 和控制器是首先需要考虑的方法，这种方式与现有的功能耦合性非常低，同时也具有较强的灵活性，但是在定义接口时应该遵循社区 API 的最佳实践设计出优雅的接口<a href="https://draveness.me/cloud-native-kubernetes-extension/#fn:2" target="_blank">2</a>。</p>
<h4 id="聚合层">聚合层</h4>
<p>Kubernetes API 聚合层是 v1.7 版本实现的功能，它的目的是将单体的 API 服务器拆分成多个聚合服务，每个开发者都能够实现聚合 API 服务暴露它们需要的接口，这个过程不需要重新编译 Kubernetes 的任何代码<a href="https://draveness.me/cloud-native-kubernetes-extension/#fn:3" target="_blank">3</a>。</p>
<p><img class="img-zoomable" src="https://img.draveness.me/kubernetes-api-aggregation-2021-03-24-16165170057457.png" alt="kubernetes-api-aggregation" />
</p>
<p><strong>图 3 - Kubernetes API 聚合</strong></p>
<p>当我们需要在集群中加入新的 API 聚合服务时，需要提交一个 <code>APIService</code> 资源，这个资源描述了接口所属的组、版本号以及处理该接口的服务，下面是 Kubernetes 社区中 metrics-server 服务对应的 <code>APIService</code>：</p>
<pre><code class="language-yaml">apiVersion: apiregistration.k8s.io/v1
kind: APIService
metadata:
  name: v1beta1.metrics.k8s.io
spec:
  service:
    name: metrics-server
    namespace: kube-system
  group: metrics.k8s.io
  version: v1beta1
  insecureSkipTLSVerify: true
  groupPriorityMinimum: 100
  versionPriority: 100
</code></pre>
<p>YAML</p>
<p>如果我们将上述资源提交到 Kubernetes 集群中后，用户在访问 API 服务器的 <code>/apis/metrics.k8s.io/v1beta1</code> 路径时，会被转发到集群中的 <code>metrics-server.kube-system.svc</code> 服务上。</p>
<p>与应用范围很广的 CRD 相比，API 聚合机制在项目中比较少见，它的主要目的还是扩展 API 服务器，而大多数的集群都不会有类似的需求，在这里也就不过多介绍了。</p>
<h4 id="准入控制">准入控制</h4>
<p>Kubernetes 的准入控制机制可以修改和验证即将被 API 服务器持久化的资源，API 服务器收到的全部写请求都会经过如下所示的阶段持久化到 etcd 中<a href="https://draveness.me/cloud-native-kubernetes-extension/#fn:4" target="_blank">4</a>：</p>
<p><img class="img-zoomable" src="https://img.draveness.me/kubernetes-admission-control-2021-03-24-16165170057463.png" alt="kubernetes-admission-control" />
</p>
<p><strong>图 4 - Kubernetes 准入控制</strong></p>
<p>Kubernetes 的代码仓库中包含 20 多个准入控制插件<a href="https://draveness.me/cloud-native-kubernetes-extension/#fn:5" target="_blank">5</a>，我们以 <code>TaintNodesByCondition</code> 插件<a href="https://draveness.me/cloud-native-kubernetes-extension/#fn:6" target="_blank">6</a>为例简单介绍一下它们的实现原理：</p>
<pre><code class="language-go">func (p *Plugin) Admit(ctx context.Context, a admission.Attributes, o admission.ObjectInterfaces) error {
	if a.GetResource().GroupResource() != nodeResource || a.GetSubresource() != &quot;&quot; {
		return nil
	}

	node, ok := a.GetObject().(*api.Node)
	if !ok {
		return admission.NewForbidden(a, fmt.Errorf(&quot;unexpected type %T&quot;, a.GetObject()))
	}

	addNotReadyTaint(node)
	return nil
}
</code></pre>
<p>Go</p>
<p>所有的准入控制插件都可以实现上述的 <code>Admit</code> 方法修改即将提交到存储中的资源，也就是上面提到的 Mutating 修改阶段，这段代码会为所有传入节点加上 <code>NotReady</code> 污点保证节点在更新期间不会有任务调度到该节点上；除了 <code>Admit</code> 方法之外，插件还可以实现 <code>Validate</code> 方法验证传入资源的合法性。</p>
<p>在 Kubernetes 实现自定义的准入控制器相对比较复杂，我们需要构建一个实现准入控制接口的 API 服务并将该 API 服务通过 <code>MutatingWebhookConfiguration</code> 和 <code>ValidatingWebhookConfiguration</code> 两种资源将服务的地址和接口注册到集群中，而 Kubernetes 的 API 服务器会在修改资源时调用 <code>WebhookConfiguration</code> 中定义的服务修改和验证资源。Kubernetes 社区中的比较热门的服务网格 Istio 就利用该特性实现了一些功能<a href="https://draveness.me/cloud-native-kubernetes-extension/#fn:7" target="_blank">7</a>。</p>
<h3 id="容器接口">容器接口</h3>
<p>Kubernetes 作为容器编排系统，它的主要逻辑还是调度和管理集群中运行的容器，虽然它不需要从零开始实现新的容器运行时，但是因为网络和存储等模块是容器运行的必需品，所以它要与这些模块打交道。Kubernetes 选择的方式是设计网络、存储和运行时接口隔离实现细节，自己把精力放在容器编排上，让第三方社区实现这些复杂而且极具专业性的模块。</p>
<h4 id="网络插件">网络插件</h4>
<p>容器网络接口（Container Network Interface、CNI）包含一组用于开发插件去配置 Linux 容器中网卡的接口和框架。CNI 仅会关注容器的网络连通性并在容器删除时回收所有分配的网络资源<a href="https://draveness.me/cloud-native-kubernetes-extension/#fn:8" target="_blank">8</a>。</p>
<p><img class="img-zoomable" src="https://img.draveness.me/cni-banner-2021-03-24-16165170057469.png" alt="cni-banner" />
</p>
<p><strong>图 5 - 容器网络接口</strong></p>
<p>CNI 插件虽然与 Kubernetes 有密切的关系，但是不同的容器管理系统都可以使用 CNI 插件来创建和管理网络，例如：mesos、Cloud Foundry 等。</p>
<p>所有的 CNI 插件都应该实现包含 <code>ADD</code>、<code>DEL</code> 和 <code>CHECK</code> 操作的二进制可执行文件，容器管理系统会执行二进制文件来创建网络<a href="https://draveness.me/cloud-native-kubernetes-extension/#fn:9" target="_blank">9</a>。</p>
<p>在 Kubernetes 中，无论使用哪种网络插件都需要遵循它的网络模型，除了每个 Pod 都需要有独立的 IP 地址之外，Kubernetes 还对网络模型做出了以下的需求：</p>
<ul>
<li>任意节点上的 Pod 在不使用 NAT 的情况下都访问到所有节点上的所有 Pod；</li>
<li>节点上的 Kubelet 和守护进程等服务可以访问节点上的其他 Pod；</li>
</ul>
<pre><code class="language-go">type CNI interface {
	AddNetworkList(ctx context.Context, net *NetworkConfigList, rt *RuntimeConf) (types.Result, error)
	CheckNetworkList(ctx context.Context, net *NetworkConfigList, rt *RuntimeConf) error
	DelNetworkList(ctx context.Context, net *NetworkConfigList, rt *RuntimeConf) error
	GetNetworkListCachedResult(net *NetworkConfigList, rt *RuntimeConf) (types.Result, error)
	GetNetworkListCachedConfig(net *NetworkConfigList, rt *RuntimeConf) ([]byte, *RuntimeConf, error)

	AddNetwork(ctx context.Context, net *NetworkConfig, rt *RuntimeConf) (types.Result, error)
	CheckNetwork(ctx context.Context, net *NetworkConfig, rt *RuntimeConf) error
	DelNetwork(ctx context.Context, net *NetworkConfig, rt *RuntimeConf) error
	GetNetworkCachedResult(net *NetworkConfig, rt *RuntimeConf) (types.Result, error)
	GetNetworkCachedConfig(net *NetworkConfig, rt *RuntimeConf) ([]byte, *RuntimeConf, error)

	ValidateNetworkList(ctx context.Context, net *NetworkConfigList) ([]string, error)
	ValidateNetwork(ctx context.Context, net *NetworkConfig) ([]string, error)
}
</code></pre>
<p>Go</p>
<p>开发 CNI 插件对于多数工程师来说都非常遥远，在正常情况下，我们只需要在一些常见的开源框架中根据需求做出选择，例如：Flannel、Calico 和 Cilium 等，当集群的规模变得非常庞大时，也自然会有网络工程师与 Kubernetes 开发者配合开发相应的插件。</p>
<h4 id="存储插件">存储插件</h4>
<p>容器存储接口（Container Storage Interface、CSI）是 Kubernetes 在 v1.9 引入的新特性，该特性在 v1.13 中达到稳定，目前常见的容器编排系统 Kubernetes、Cloud Foundry、Mesos 和 Nomad 都选择使用该接口扩展集群中容器的存储能力。</p>
<p><img class="img-zoomable" src="https://img.draveness.me/csi-banner-2021-03-24-16165170057477.png" alt="csi-banner" />
</p>
<p><strong>图 6 - 容器存储接口</strong></p>
<p>CSI 是在容器编排系统向容器化的工作负载暴露块存储和文件存储的标准，第三方的存储提供商可以通过实现 CSI 插件在 Kubernetes 集群中提供新的存储<a href="https://draveness.me/cloud-native-kubernetes-extension/#fn:10" target="_blank">10</a>。</p>
<p>Kubernetes 的开发团队在 CSI 的文档中给出了开发和部署 CSI 插件的最佳实践<a href="https://draveness.me/cloud-native-kubernetes-extension/#fn:11" target="_blank">11</a>，其中最主要的工作是创建实现 <code>Identity</code>、<code>Node</code> 和可选的 <code>Controller</code> 接口的容器化应用，并通过官方的 <a href="https://github.com/kubernetes-csi/csi-test/tree/master/pkg/sanity" target="_blank"><code>sanity</code></a> 包测试 CSI 插件的合法性，需要实现的接口都定义在 CSI 的规格文档中<a href="https://draveness.me/cloud-native-kubernetes-extension/#fn:12" target="_blank">12</a>。</p>
<pre><code class="language-protobuf">service Identity {
  rpc GetPluginInfo(GetPluginInfoRequest)
    returns (GetPluginInfoResponse) {}

  rpc GetPluginCapabilities(GetPluginCapabilitiesRequest)
    returns (GetPluginCapabilitiesResponse) {}

  rpc Probe (ProbeRequest)
    returns (ProbeResponse) {}
}

service Controller {
  ...
}

service Node {
  ...
}
</code></pre>
<p>Protocol Buffers</p>
<p>CSI 的规格文档非常复杂，除了详细地定义了不同接口的请求和响应参数。它还定义不同接口在出现相应错误时应该返回的 gRPC 错误码，开发者想要实现一个完全遵循 CSI 接口的插件还是很麻烦的。</p>
<p>Kubernetes 在较早的版本中分别接入了不同的云厂商的接口，其中包括 Google PD、AWS、Azure 以及 OpenStack，但是随着 CSI 接口的成熟，社区未来会在上游移除云厂商特定的实现，减少上游的维护成本，也能加快各个厂商自身存储的迭代和支持<a href="https://draveness.me/cloud-native-kubernetes-extension/#fn:13" target="_blank">13</a>。</p>
<h4 id="运行时接口">运行时接口</h4>
<p>容器运行时接口（Container Runtime Interface、CRI）是一系列用于管理容器运行时和镜像的 gRPC 接口，它是 Kubernetes 在 v1.5 中引入的新接口，Kubelet 可以通过它使用不同的容器运行时。</p>
<p><img class="img-zoomable" src="https://img.draveness.me/cri-and-container-runtimes-2021-03-24-16165170057484.png" alt="cri-and-container-runtimes" />
</p>
<p><strong>图 7 - CRI 和容器运行时</strong></p>
<p>CRI 主要定义的是一组 gRPC 方法，我们能在规格文档中找到 <code>RuntimeService</code> 和 <code>ImageService</code> 两个服务<a href="https://draveness.me/cloud-native-kubernetes-extension/#fn:14" target="_blank">14</a>，它们的名字很好地解释了各自的作用：</p>
<pre><code class="language-protobuf">service RuntimeService {
    rpc Version(VersionRequest) returns (VersionResponse) {}

    rpc RunPodSandbox(RunPodSandboxRequest) returns (RunPodSandboxResponse) {}
    rpc StopPodSandbox(StopPodSandboxRequest) returns (StopPodSandboxResponse) {}
    rpc RemovePodSandbox(RemovePodSandboxRequest) returns (RemovePodSandboxResponse) {}
    rpc PodSandboxStatus(PodSandboxStatusRequest) returns (PodSandboxStatusResponse) {}
    rpc ListPodSandbox(ListPodSandboxRequest) returns (ListPodSandboxResponse) {}

    rpc CreateContainer(CreateContainerRequest) returns (CreateContainerResponse) {}
    rpc StartContainer(StartContainerRequest) returns (StartContainerResponse) {}
    rpc StopContainer(StopContainerRequest) returns (StopContainerResponse) {}
    rpc RemoveContainer(RemoveContainerRequest) returns (RemoveContainerResponse) {}
    rpc ListContainers(ListContainersRequest) returns (ListContainersResponse) {}
    rpc ContainerStatus(ContainerStatusRequest) returns (ContainerStatusResponse) {}
    rpc UpdateContainerResources(UpdateContainerResourcesRequest) returns (UpdateContainerResourcesResponse) {}
    rpc ReopenContainerLog(ReopenContainerLogRequest) returns (ReopenContainerLogResponse) {}

    rpc ExecSync(ExecSyncRequest) returns (ExecSyncResponse) {}
    rpc Exec(ExecRequest) returns (ExecResponse) {}
    rpc Attach(AttachRequest) returns (AttachResponse) {}
    rpc PortForward(PortForwardRequest) returns (PortForwardResponse) {}

    ...
}

service ImageService {
    rpc ListImages(ListImagesRequest) returns (ListImagesResponse) {}
    rpc ImageStatus(ImageStatusRequest) returns (ImageStatusResponse) {}
    rpc PullImage(PullImageRequest) returns (PullImageResponse) {}
    rpc RemoveImage(RemoveImageRequest) returns (RemoveImageResponse) {}
    rpc ImageFsInfo(ImageFsInfoRequest) returns (ImageFsInfoResponse) {}
}
</code></pre>
<p>Protocol Buffers</p>
<p>容器运行时的接口相对比较简单，上面的这些接口不仅暴露了 Pod 沙箱管理、容器管理以及命令执行和端口转发等功能，还包含用于管理镜像的多个接口，容器运行时只要实现上面的二三十个方法可以为 Kubelet 提供服务。</p>
<h3 id="设备插件">设备插件</h3>
<p>CPU、内存、磁盘是主机上常见的资源，然而随着大数据、机器学习和硬件的发展，部分场景可能需要异构的计算资源，例如：GPU、FPGA 等设备。异构资源的出现不仅需要节点代理 Kubelet 的支持，还需要调度器的配合，为了良好的兼容后出现的不同计算设备，Kubernetes 社区在上游引入了设备插件（Device Plugin）用于支持多种类型资源的调度和分配<a href="https://draveness.me/cloud-native-kubernetes-extension/#fn:15" target="_blank">15</a>。</p>
<p><img class="img-zoomable" src="https://img.draveness.me/device-plugin-overview-2021-03-24-16165170057491.png" alt="device-plugin-overview" />
</p>
<p><strong>图 8 - 设备插件概述</strong></p>
<p>设备插件是独立在 Kubelet 之外单独运行的服务，它通过 Kubelet 暴露的 <code>Registration</code> 服务注册自己的相关信息并实现 <code>DevicePlugin</code> 服务用于订阅和分配自定义的设备<a href="https://draveness.me/cloud-native-kubernetes-extension/#fn:16" target="_blank">16</a>。</p>
<pre><code class="language-protobuf">service Registration {
	rpc Register(RegisterRequest) returns (Empty) {}
}

service DevicePlugin {
      rpc GetDevicePluginOptions(Empty) returns (DevicePluginOptions) {}
      rpc ListAndWatch(Empty) returns (stream ListAndWatchResponse) {}
      rpc Allocate(AllocateRequest) returns (AllocateResponse) {}
      rpc GetPreferredAllocation(PreferredAllocationRequest) returns (PreferredAllocationResponse) {}
      rpc PreStartContainer(PreStartContainerRequest) returns (PreStartContainerResponse) {}
}
</code></pre>
<p>Protocol Buffers</p>
<p>当设备插件刚刚启动时，它会调用 Kubelet 的注册接口传入自己的版本号、Unix 套接字和资源名，例如：<code>nvidia.com/gpu</code>；Kubelet 会通过 Unix 套接字与设备插件通信，它会通过 <code>ListAndWatch</code> 接口持续获得设备中资源的最新状态，并在 Pod 申请资源时通过 <code>Allocate</code> 接口分配资源。设备插件的实现逻辑相对比较简单，感兴趣的读者可以研究 Nvidia GPU 插件的实现原理<a href="https://draveness.me/cloud-native-kubernetes-extension/#fn:17" target="_blank">17</a>。</p>
<h3 id="调度框架">调度框架</h3>
<p>调度器是 Kubernetes 中的核心组件之一，它的主要作用是在 Kubernetes 集群中的一组节点中为工作负载做出最优的调度决策，不同场景下的调度需求往往都是很复杂的，然而调度器在 Kubernetes 项目早期并不支持易用的扩展能力，仅支持调度器扩展（Extender）这种比较难用的方法。</p>
<p>Kubernetes 从 v1.15 引入的调度框架才是今天比较主流的调度器扩展技术，通过在 Kubernetes 调度器的内部抽象出关键的扩展点（Extension Point）并通过插件的方式在扩展点上改变调度器做出的调度决策<a href="https://draveness.me/cloud-native-kubernetes-extension/#fn:18" target="_blank">18</a>。</p>
<p><img class="img-zoomable" src="https://img.draveness.me/scheduling-framework-extensions-2021-03-24-16165170057498.png" alt="scheduling-framework-extensions" />
</p>
<p><strong>图 9 - 调度框架扩展点</strong></p>
<p>目前的调度框架总共支持 11 个不同的扩展点，每个扩展点都对应 Kubernetes 调度器中定义的接口，这里仅展示 <code>FilterPlugin</code> 和 <code>ScorePlugin</code> 两个常见接口中的方法<a href="https://draveness.me/cloud-native-kubernetes-extension/#fn:19" target="_blank">19</a>：</p>
<pre><code class="language-go">type FilterPlugin interface {
	Plugin
	Filter(ctx context.Context, state *CycleState, pod *v1.Pod, nodeInfo *NodeInfo) *Status
}

type ScoreExtensions interface {
	NormalizeScore(ctx context.Context, state *CycleState, p *v1.Pod, scores NodeScoreList) *Status
}

type ScorePlugin interface {
	Plugin
	Score(ctx context.Context, state *CycleState, p *v1.Pod, nodeName string) (int64, *Status)
	ScoreExtensions() ScoreExtensions
}
</code></pre>
<p>Go</p>
<p>调度框架的出现让实现复杂的调度策略和调度算法变得更加容易，社区通过调度框架替代更早的谓词和优先级并实现了协作式调度、基于容量调度等功能更强大的插件<a href="https://draveness.me/cloud-native-kubernetes-extension/#fn:20" target="_blank">20</a>。虽然今天的调度框架已经变得非常灵活，但是串行的调度器可能无法满足大集群的调度需求，而 Kubernetes 目前也很难实现多调度器，不知道未来是否会提供更灵活的接口。</p>
<h3 id="总结-11">总结</h3>
<p>Kubernetes 从 2014 年发布至今已经过去将近 7 年了，从一个最小可用的编排系统到今天的庞然大物，社区的每个代码贡献者和成员都有<del>责任</del>。从这篇文章中，我们可以看到随着 Kubernetes 项目的演进方向，社区越来越关注系统的可扩展性，通过设计接口、移除第三方代码降低社区成员的负担，让 Kubernetes 能够更专注于容器的编排和调度。</p>
<h2 id="其它">其它</h2>
<h3 id="为什么-kubernetes-要替换-docker">为什么 Kubernetes 要替换 Docker</h3>
<p>Kubernetes 是今天容器编排领域的事实标准，而 Docker 从诞生之日到今天都在容器中扮演着举足轻重的地位，也都是 Kubernetes 中的默认容器引擎。然而在 2020 年 12 月，Kubernetes 社区决定着手移除仓库中 Dockershim 相关代码<a href="https://draveness.me/whys-the-design-kubernetes-deprecate-docker/#fn:1" target="_blank">1</a>，这对于 Kubernetes 和 Docker 两个社区来说都意义重大。</p>
<p><img class="img-zoomable" src="https://img.draveness.me/kubelet-and-containers-2021-03-10-16153845432597.png" alt="kubelet-and-containers" />
</p>
<p><strong>图 1 - Dockershim</strong></p>
<p>相信大多数的开发者都听说过 Kubernetes 和 Docker，也知道我们可以使用 Kubernetes 管理 Docker 容器，但是可能没有听说过 Dockershim，即 Docker 垫片。如上图所示，Kubernetes 中的节点代理 Kubelet 为了访问 Docker 提供的服务需要先经过社区维护的 Dockershim，Dockershim 会将请求转发给管理容器的 Docker 服务。</p>
<p>其实从上面的架构图中，我们就能猜测出 Kubernetes 社区从代码仓库移除 Dockershim 的原因：</p>
<ul>
<li>Kubernetes 引入容器运行时接口（Container Runtime Interface、CRI）隔离不同容器运行时的实现机制，容器编排系统不应该依赖于某个具体的运行时实现；</li>
<li>Docker 没有支持也不打算支持 Kubernetes 的 CRI 接口，需要 Kubernetes 社区在仓库中维护 Dockershim；</li>
</ul>
<h4 id="可扩展性">可扩展性</h4>
<p>Kubernetes 通过引入新的容器运行时接口将容器管理与具体的运行时解耦，不再依赖于某个具体的运行时实现。很多开源项目在早期为了降低用户的使用成本，都会提供开箱即用的体验，而随着用户群体的扩大，为了满足更多定制化的需求、提供更强的可扩展性，会引入更多的接口。Kubernetes 通过下面的一系列接口为不同模块提供了扩展性：</p>
<p><img class="img-zoomable" src="https://img.draveness.me/kubernetes-extensions-2021-03-10-16153845432607.png" alt="kubernetes-extensions" />
</p>
<p><strong>图 2 - Kubernetes 接口和可扩展性</strong></p>
<p>Kubernetes 在较早期的版本中就引入了 CRD、CNI、CRI 和 CSI 等接口，只有用于扩展调度器的调度框架是 Kubernetes 中比较新的特性。我们在这里就不展开分析其他的接口和扩展了，简单介绍一下容器运行时接口。</p>
<p>Kubernetes 早在 1.3 就在代码仓库中同时支持了 rkt 和 Docker 两种运行时，但是这些代码为 Kubelet 组件的维护带来了很大的困难，不仅需要维护不同的运行时，接入新的运行时也很困难；容器运行时接口（Container Runtime Interface、CRI）是 Kubernetes 在 1.5 中引入的新接口，Kubelet 可以通过这个新接口使用各种各样的容器运行时。其实 CRI 的发布就意味着 Kubernetes 一定会将 Dockershim 的代码从仓库中移除。</p>
<p>CRI 是一系列用于管理容器运行时和镜像的 gRPC 接口，我们能在它的定义中找到 <code>RuntimeService</code> 和 <code>ImageService</code> 两个服务<a href="https://draveness.me/whys-the-design-kubernetes-deprecate-docker/#fn:2" target="_blank">2</a>，它们的名字很好地解释了各自的作用：</p>
<pre><code class="language-protobuf">service RuntimeService {
    rpc Version(VersionRequest) returns (VersionResponse) {}

    rpc RunPodSandbox(RunPodSandboxRequest) returns (RunPodSandboxResponse) {}
    rpc StopPodSandbox(StopPodSandboxRequest) returns (StopPodSandboxResponse) {}
    rpc RemovePodSandbox(RemovePodSandboxRequest) returns (RemovePodSandboxResponse) {}
    rpc PodSandboxStatus(PodSandboxStatusRequest) returns (PodSandboxStatusResponse) {}
    rpc ListPodSandbox(ListPodSandboxRequest) returns (ListPodSandboxResponse) {}

    rpc CreateContainer(CreateContainerRequest) returns (CreateContainerResponse) {}
    rpc StartContainer(StartContainerRequest) returns (StartContainerResponse) {}
    rpc StopContainer(StopContainerRequest) returns (StopContainerResponse) {}
    rpc RemoveContainer(RemoveContainerRequest) returns (RemoveContainerResponse) {}
    rpc ListContainers(ListContainersRequest) returns (ListContainersResponse) {}
    rpc ContainerStatus(ContainerStatusRequest) returns (ContainerStatusResponse) {}
    rpc UpdateContainerResources(UpdateContainerResourcesRequest) returns (UpdateContainerResourcesResponse) {}
    rpc ReopenContainerLog(ReopenContainerLogRequest) returns (ReopenContainerLogResponse) {}

    ...
}

service ImageService {
    rpc ListImages(ListImagesRequest) returns (ListImagesResponse) {}
    rpc ImageStatus(ImageStatusRequest) returns (ImageStatusResponse) {}
    rpc PullImage(PullImageRequest) returns (PullImageResponse) {}
    rpc RemoveImage(RemoveImageRequest) returns (RemoveImageResponse) {}
    rpc ImageFsInfo(ImageFsInfoRequest) returns (ImageFsInfoResponse) {}
}
</code></pre>
<p>Protocol Buffers</p>
<p>对 Kubernetes 稍有了解的人都能从上面的定义中找到一些熟悉的方法，它们都是容器运行时需要暴露给 Kubelet 的接口。Kubernetes 将 CRI 垫片实现成 gRPC 服务器与 Kubelet 中的客户端通信，所有的请求都会被转发给容器运行时处理。</p>
<p><img class="img-zoomable" src="https://img.draveness.me/cri-and-container-runtimes-2021-03-10-16153845432613.png" alt="cri-and-container-runtimes" />
</p>
<p><strong>图 3 - Kubernetes 和 CRI</strong></p>
<p>Kubernetes 中的声明式接口非常常见，作为声明式接口的拥趸，CRI 没有使用声明式的接口是一件听起来『非常怪异』的事情<a href="https://draveness.me/whys-the-design-kubernetes-deprecate-docker/#fn:3" target="_blank">3</a>。不过 Kubernetes 社区考虑过让容器运行时重用 Pod 资源，这样容器运行时可以实现不同的控制逻辑来管理容器，能够极大地简化 Kubelet 和容器运行时之间的接口，但是社区出于以下两点考虑，最终没有选择声明式的接口：</p>
<ol>
<li>所有的运行时都需要重新实现相同的逻辑支持很多 Pod 级别的功能和机制；</li>
<li>Pod 的定义在 CRI 设计时演进地非常快，初始化容器等功能都需要运行时的配合；</li>
</ol>
<p>虽然社区最终为 CRI 选择了命令式的接口，但是 Kubelet 仍然会保证 Pod 的状态会不断地向期望状态迁移。</p>
<h4 id="不兼容接口">不兼容接口</h4>
<p>与容器运行时相比，Docker 更像是一个复杂的开发者工具，它提供了从构建到运行的全套功能。开发者可以很快地上手 Docker 并在本地运行并管理一些 Docker 容器，然而在集群中运行的容器运行时往往不需要这么复杂的功能，Kubernetes 需要的只是 CRI 中定义的那些接口。</p>
<p><img class="img-zoomable" src="https://img.draveness.me/docker-and-cri-2021-03-10-16153845432620.png" alt="docker-and-cri" />
</p>
<p><strong>图 4 - Docker &amp; CRI</strong></p>
<p>Docker 的官方文档加起来可能有一本书的厚度，相信没有任何开发者可以熟练运用 Docker 提供的全部功能。而作为开发者工具，虽然 Docker 中包含 CRI 需要的所有功能，但是都需要实现一层包装以兼容 CRI。除此之外，社区提出的很多新功能都没有办法在 Dockershim 中实现，例如 cgroups v2 以及用户命名空间。</p>
<p>Kubernetes 作为比较松散的开源社区，每个成员尤其是各个 SIG 的成员都只会在开源社区上花费有限的时间，而维护 Kubelet 的 sig-node 又尤其繁忙，很多新的功能都因为维护者没有足够的精力而被搁置，所以既然 Docker 社区看起来没有打算支持 Kubernetes 的 CRI 接口，维护 Dockershim 又需要花费很多精力，那么我们就能理解为什么 Kubernetes 会移除 Dockershim 了。</p>
<h4 id="总结-12">总结</h4>
<p>今天的 Kubernetes 已经是非常成熟的项目，它的关注点也逐渐从提供更完善的功能转变到提供更好的扩展性，这样才能满足不同场景和不同公司定制化的业务需求。Kubernetes 在过去因为 Docker 的热门而选择 Docker，而在今天又因为高昂的维护成本而放弃 Docker，我们能够从这个过程中体会到容器领域的发展和进步。</p>
<p>移除 Docker 的种子其实从 CRI 发布时就种下了，Dockershim 一直都是 Kubernetes 为了兼容 Docker 获得市场采取的临时决定，对于今天已经统治市场的 Kubernetes 来说，Docker 的支持显得非常鸡肋，移除代码也就顺理成章了。我们在这里重新回顾一下 Kubernetes 在仓库中移除 Docker 支持的两个原因：</p>
<ul>
<li>Kubernetes 在早期版本中引入 CRI 摆脱依赖某个具体的容器运行时依赖，屏蔽底层的诸多实现细节，让 Kubernetes 能够更关注容器的编排；</li>
<li>Docker 本身不兼容 CRI 接口，而且官方并没有实现 CRI 的打算，同时也不支持容器的一些新需求，所以 Dockershim 的维护成为了社区的想要摆脱负担；</li>
</ul>
<p>到最后，我们还是来看一些比较开放的相关问题，有兴趣的读者可以仔细思考一下下面的问题：</p>
<ul>
<li>Kubernetes 中还有哪些模块提供良好的扩展性？</li>
<li>除了文中提到的 CRI-O、Containerd，还有哪些支持 CRI 的容器运行时？</li>
</ul>
<blockquote>
<p>如果对文章中的内容有疑问或者想要了解更多软件工程上一些设计决策背后的原因，可以在博客下面留言，作者会及时回复本文相关的疑问并选择其中合适的主题作为后续的内容。</p>
</blockquote>

    </div>
</article>


<div class="license markdown-body">
    <blockquote>
        <p>Unless otherwise noted, the content of this site is licensed under <a rel="license" href="http://creativecommons.org/licenses/by-nc-sa/4.0/"
               target="_blank">CC BY-NC-SA 4.0</a>.</p>
    </blockquote>
</div>



            </div>
            <aside class="col-12 col-md-3 float-left sidebar">
    
    <div class="sidebar-item sidebar-pages">
        <h3>Pages</h3>
        <ul>
            
            <li>
                <a href="/">Home</a>
            </li>
            
            <li>
                <a href="/archives/">Archives</a>
            </li>
            
            <li>
                <a href="/about/">About</a>
            </li>
            
            <li>
                <a href="/search/">Search</a>
            </li>
            
            <li>
                <a href="/index.xml">RSS</a>
            </li>
            
        </ul>
    </div>
    
    <div class="sidebar-item sidebar-links">
        <h3>Links</h3>
        <ul>
            
            <li>
                <a href="https://github.com/amzrk2" target="_blank"><span>GitHub</span></a>
            </li>
            
            <li>
                <a href="https://twitter.com/amzrk2" target="_blank"><span>Twitter</span></a>
            </li>
            
            <li>
                <a href="https://space.bilibili.com/19767474" target="_blank"><span>bilibili</span></a>
            </li>
            
        </ul>
    </div>
    
    <div class="sidebar-item sidebar-tags">
        <h3>Tags</h3>
        <div>
            
            <span>
                <a href="/tags/algorithm/">Algorithm</a>
            </span>
            
            <span>
                <a href="/tags/base/">base</a>
            </span>
            
            <span>
                <a href="/tags/blog/">blog</a>
            </span>
            
            <span>
                <a href="/tags/cloudnative/">cloudnative</a>
            </span>
            
            <span>
                <a href="/tags/coderebuild/">CodeRebuild</a>
            </span>
            
            <span>
                <a href="/tags/db/">db</a>
            </span>
            
            <span>
                <a href="/tags/go.project/">go.project</a>
            </span>
            
            <span>
                <a href="/tags/it/">it</a>
            </span>
            
            <span>
                <a href="/tags/mq/">mq</a>
            </span>
            
            <span>
                <a href="/tags/sys/">sys</a>
            </span>
            
            <span>
                <a href="/tags/tool/">tool</a>
            </span>
            
            <span>
                <a href="/tags/versioncontrol/">versioncontrol</a>
            </span>
            
        </div>
    </div>
    <div class="sidebar-item sidebar-toc">
        <h3>TOC</h3><nav id="TableOfContents">
  <ul>
    <li><a href="#架构设计与实现原理">架构设计与实现原理</a>
      <ul>
        <li><a href="#介绍">介绍</a></li>
        <li><a href="#设计">设计</a></li>
        <li><a href="#架构">架构</a></li>
        <li><a href="#实现原理">实现原理</a></li>
        <li><a href="#总结">总结</a></li>
      </ul>
    </li>
    <li><a href="#kubernetes-对象">Kubernetes 对象</a>
      <ul>
        <li><a href="#简介">简介</a></li>
        <li><a href="#spec">Spec</a></li>
        <li><a href="#status">Status</a></li>
        <li><a href="#总结-1">总结</a></li>
      </ul>
    </li>
    <li><a href="#kubernetes-pod">Kubernetes Pod</a>
      <ul>
        <li><a href="#概述">概述</a></li>
        <li><a href="#生命周期">生命周期</a></li>
        <li><a href="#总结-2">总结</a></li>
      </ul>
    </li>
    <li><a href="#kubernetes-service">Kubernetes Service</a>
      <ul>
        <li><a href="#创建服务">创建服务</a></li>
        <li><a href="#代理模式">代理模式</a></li>
        <li><a href="#总结-3">总结</a></li>
      </ul>
    </li>
    <li><a href="#kubernetes-volume">Kubernetes Volume</a>
      <ul>
        <li><a href="#概述-1">概述</a></li>
        <li><a href="#管理">管理</a></li>
        <li><a href="#插件">插件</a></li>
        <li><a href="#总结-4">总结</a></li>
      </ul>
    </li>
    <li><a href="#kubernetes-replicaset">Kubernetes ReplicaSet</a>
      <ul>
        <li><a href="#概述-2">概述</a></li>
        <li><a href="#实现原理-1">实现原理</a></li>
        <li><a href="#总结-5">总结</a></li>
      </ul>
    </li>
    <li><a href="#kubernetes-垃圾收集器">Kubernetes 垃圾收集器</a>
      <ul>
        <li><a href="#概述-3">概述</a></li>
        <li><a href="#实现原理-2">实现原理</a></li>
        <li><a href="#总结-6">总结</a></li>
      </ul>
    </li>
    <li><a href="#kubernetes-deployment">Kubernetes Deployment</a>
      <ul>
        <li><a href="#概述-4">概述</a></li>
        <li><a href="#实现原理-3">实现原理</a></li>
        <li><a href="#总结-7">总结</a></li>
      </ul>
    </li>
    <li><a href="#kubernetes-statefulset">Kubernetes StatefulSet</a>
      <ul>
        <li><a href="#概述-5">概述</a></li>
        <li><a href="#实现原理-4">实现原理</a></li>
        <li><a href="#总结-8">总结</a></li>
      </ul>
    </li>
    <li><a href="#kubernetes-daemonset">Kubernetes DaemonSet</a>
      <ul>
        <li><a href="#概述-6">概述</a></li>
        <li><a href="#实现原理-5">实现原理</a></li>
        <li><a href="#总结-9">总结</a></li>
      </ul>
    </li>
    <li><a href="#kubernetes-job--cronjob">Kubernetes Job &amp; CronJob</a>
      <ul>
        <li><a href="#概述-7">概述</a></li>
        <li><a href="#任务">任务</a></li>
        <li><a href="#定时任务">定时任务</a></li>
        <li><a href="#总结-10">总结</a></li>
      </ul>
    </li>
    <li><a href="#如何为-kubernetes-定制特性">如何为 Kubernetes 定制特性</a>
      <ul>
        <li><a href="#扩展接口">扩展接口</a></li>
        <li><a href="#容器接口">容器接口</a></li>
        <li><a href="#设备插件">设备插件</a></li>
        <li><a href="#调度框架">调度框架</a></li>
        <li><a href="#总结-11">总结</a></li>
      </ul>
    </li>
    <li><a href="#其它">其它</a>
      <ul>
        <li><a href="#为什么-kubernetes-要替换-docker">为什么 Kubernetes 要替换 Docker</a></li>
      </ul>
    </li>
  </ul>
</nav></div>
</aside>
        </div>
        <div class="btn">
    <div class="btn-menu" id="btn-menu">
        <i class="iconfont icon-grid-sharp"></i>
    </div>
    <div class="btn-toggle-mode">
        <i class="iconfont icon-contrast-sharp"></i>
    </div>
    <div class="btn-scroll-top">
        <i class="iconfont icon-chevron-up-circle-sharp"></i>
    </div>
</div>
<aside class="sidebar-mobile" style="display: none;">
  <div class="sidebar-wrapper">
    
    <div class="sidebar-item sidebar-pages">
        <h3>Pages</h3>
        <ul>
            
            <li>
                <a href="/">Home</a>
            </li>
            
            <li>
                <a href="/archives/">Archives</a>
            </li>
            
            <li>
                <a href="/about/">About</a>
            </li>
            
            <li>
                <a href="/search/">Search</a>
            </li>
            
            <li>
                <a href="/index.xml">RSS</a>
            </li>
            
        </ul>
    </div>
    
    <div class="sidebar-item sidebar-links">
        <h3>Links</h3>
        <ul>
            
            <li>
                <a href="https://github.com/amzrk2" target="_blank"><span>GitHub</span></a>
            </li>
            
            <li>
                <a href="https://twitter.com/amzrk2" target="_blank"><span>Twitter</span></a>
            </li>
            
            <li>
                <a href="https://space.bilibili.com/19767474" target="_blank"><span>bilibili</span></a>
            </li>
            
        </ul>
    </div>
    
    <div class="sidebar-item sidebar-tags">
        <h3>Tags</h3>
        <div>
            
            <span>
                <a href="/tags/algorithm/">Algorithm</a>
            </span>
            
            <span>
                <a href="/tags/base/">base</a>
            </span>
            
            <span>
                <a href="/tags/blog/">blog</a>
            </span>
            
            <span>
                <a href="/tags/cloudnative/">cloudnative</a>
            </span>
            
            <span>
                <a href="/tags/coderebuild/">CodeRebuild</a>
            </span>
            
            <span>
                <a href="/tags/db/">db</a>
            </span>
            
            <span>
                <a href="/tags/go.project/">go.project</a>
            </span>
            
            <span>
                <a href="/tags/it/">it</a>
            </span>
            
            <span>
                <a href="/tags/mq/">mq</a>
            </span>
            
            <span>
                <a href="/tags/sys/">sys</a>
            </span>
            
            <span>
                <a href="/tags/tool/">tool</a>
            </span>
            
            <span>
                <a href="/tags/versioncontrol/">versioncontrol</a>
            </span>
            
        </div>
    </div>
    
    
    
    <div class="sidebar-item sidebar-toc">
        <h3>TOC</h3>
        <nav id="TableOfContents">
  <ul>
    <li><a href="#架构设计与实现原理">架构设计与实现原理</a>
      <ul>
        <li><a href="#介绍">介绍</a></li>
        <li><a href="#设计">设计</a></li>
        <li><a href="#架构">架构</a></li>
        <li><a href="#实现原理">实现原理</a></li>
        <li><a href="#总结">总结</a></li>
      </ul>
    </li>
    <li><a href="#kubernetes-对象">Kubernetes 对象</a>
      <ul>
        <li><a href="#简介">简介</a></li>
        <li><a href="#spec">Spec</a></li>
        <li><a href="#status">Status</a></li>
        <li><a href="#总结-1">总结</a></li>
      </ul>
    </li>
    <li><a href="#kubernetes-pod">Kubernetes Pod</a>
      <ul>
        <li><a href="#概述">概述</a></li>
        <li><a href="#生命周期">生命周期</a></li>
        <li><a href="#总结-2">总结</a></li>
      </ul>
    </li>
    <li><a href="#kubernetes-service">Kubernetes Service</a>
      <ul>
        <li><a href="#创建服务">创建服务</a></li>
        <li><a href="#代理模式">代理模式</a></li>
        <li><a href="#总结-3">总结</a></li>
      </ul>
    </li>
    <li><a href="#kubernetes-volume">Kubernetes Volume</a>
      <ul>
        <li><a href="#概述-1">概述</a></li>
        <li><a href="#管理">管理</a></li>
        <li><a href="#插件">插件</a></li>
        <li><a href="#总结-4">总结</a></li>
      </ul>
    </li>
    <li><a href="#kubernetes-replicaset">Kubernetes ReplicaSet</a>
      <ul>
        <li><a href="#概述-2">概述</a></li>
        <li><a href="#实现原理-1">实现原理</a></li>
        <li><a href="#总结-5">总结</a></li>
      </ul>
    </li>
    <li><a href="#kubernetes-垃圾收集器">Kubernetes 垃圾收集器</a>
      <ul>
        <li><a href="#概述-3">概述</a></li>
        <li><a href="#实现原理-2">实现原理</a></li>
        <li><a href="#总结-6">总结</a></li>
      </ul>
    </li>
    <li><a href="#kubernetes-deployment">Kubernetes Deployment</a>
      <ul>
        <li><a href="#概述-4">概述</a></li>
        <li><a href="#实现原理-3">实现原理</a></li>
        <li><a href="#总结-7">总结</a></li>
      </ul>
    </li>
    <li><a href="#kubernetes-statefulset">Kubernetes StatefulSet</a>
      <ul>
        <li><a href="#概述-5">概述</a></li>
        <li><a href="#实现原理-4">实现原理</a></li>
        <li><a href="#总结-8">总结</a></li>
      </ul>
    </li>
    <li><a href="#kubernetes-daemonset">Kubernetes DaemonSet</a>
      <ul>
        <li><a href="#概述-6">概述</a></li>
        <li><a href="#实现原理-5">实现原理</a></li>
        <li><a href="#总结-9">总结</a></li>
      </ul>
    </li>
    <li><a href="#kubernetes-job--cronjob">Kubernetes Job &amp; CronJob</a>
      <ul>
        <li><a href="#概述-7">概述</a></li>
        <li><a href="#任务">任务</a></li>
        <li><a href="#定时任务">定时任务</a></li>
        <li><a href="#总结-10">总结</a></li>
      </ul>
    </li>
    <li><a href="#如何为-kubernetes-定制特性">如何为 Kubernetes 定制特性</a>
      <ul>
        <li><a href="#扩展接口">扩展接口</a></li>
        <li><a href="#容器接口">容器接口</a></li>
        <li><a href="#设备插件">设备插件</a></li>
        <li><a href="#调度框架">调度框架</a></li>
        <li><a href="#总结-11">总结</a></li>
      </ul>
    </li>
    <li><a href="#其它">其它</a>
      <ul>
        <li><a href="#为什么-kubernetes-要替换-docker">为什么 Kubernetes 要替换 Docker</a></li>
      </ul>
    </li>
  </ul>
</nav>
    </div>
    
    
  </div>
</aside>
    </main>

    <footer>
    <div class="container-lg clearfix">
        <div class="col-12 footer">
            
            <span>&copy; 2020-2021
                <a href="https://yuanyatianchi.github.io">DSRKafuU</a>
                 | <a href="https://github.com/itsme/my_blog">Source code</a> 
                | Powered by <a href="https://github.com/amzrk2/hugo-theme-fuji/"
                   target="_blank">Fuji-v2</a> &amp; <a href="https://gohugo.io/"
                                                    target="_blank">Hugo</a> 
            </span>
        </div>
    </div>
</footer>

    
<script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.6/dist/medium-zoom.min.js"></script>
<script defer src="https://cdn.jsdelivr.net/npm/lazysizes@5.3.0/lazysizes.min.js"></script>
<script defer src="https://cdn.jsdelivr.net/npm/prismjs@1.23.0/components/prism-core.min.js"></script>
<script defer src="https://cdn.jsdelivr.net/npm/prismjs@1.23.0/plugins/autoloader/prism-autoloader.min.js"></script>



<script defer src="/assets/js/fuji.min.js"></script>


</body>

</html>